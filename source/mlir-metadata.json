[
  {
    "name": "affine.apply",
    "summary": "affine apply operation",
    "description": "The `affine.apply` operation applies an [affine mapping](#affine-maps)\n    to a list of SSA values, yielding a single SSA value. The number of\n    dimension and symbol operands to `affine.apply` must be equal to the\n    respective number of dimensional and symbolic inputs to the affine mapping;\n    the affine mapping has to be one-dimensional, and so the `affine.apply`\n    operation always returns one value. The input operands and result must all\n    have ‘index’ type.\n\n    An operand that is a valid dimension as per the [rules on valid affine\n    dimensions and symbols](#restrictions-on-dimensions-and-symbols)\n    cannot be used as a symbolic operand.\n\n    Example:\n\n    ```mlir\n    #map = affine_map<(d0, d1) -> (d0 floordiv 8 + d1 floordiv 128)>\n    ...\n    %1 = affine.apply #map (%s, %t)\n\n    // Inline example.\n    %2 = affine.apply affine_map<(i)[s0] -> (i + s0)> (%42)[%n]\n    ```",
    "inputs": [
      { "name": "mapOperands", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "map", "type": "AffineMapAttr" }
    ]
  },
  {
    "name": "affine.delinearize_index",
    "summary": "delinearize an index",
    "description": "The `affine.delinearize_index` operation takes a single index value and\n    calculates the multi-index according to the given basis.\n\n    Example:\n\n    ```\n    %indices:3 = affine.delinearize_index %linear_index into (%c16, %c224, %c224) : index, index, index\n    ```\n\n    In the above example, `%indices:3` conceptually holds the following:\n\n    ```\n    #map0 = affine_map<()[s0] -> (s0 floordiv 50176)>\n    #map1 = affine_map<()[s0] -> ((s0 mod 50176) floordiv 224)>\n    #map2 = affine_map<()[s0] -> (s0 mod 224)>\n    %indices_0 = affine.apply #map0()[%linear_index]\n    %indices_1 = affine.apply #map1()[%linear_index]\n    %indices_2 = affine.apply #map2()[%linear_index]\n    ```\n\n    In other words, `%0:3 = affine.delinearize_index %x into (B, C)` produces\n    `%0 = {%x / (B * C), (%x mod (B * C)) / C, %x mod C}`.\n\n    The basis may either contain `N` or `N-1` elements, where `N` is the number of results.\n    If there are N basis elements, the first one will not be used during computations,\n    but may be used during analysis and canonicalization to eliminate terms from\n    the `affine.delinearize_index` or to enable conclusions about the total size of\n    `%linear_index`.\n\n    If the basis is fully provided, the delinearize_index operation is said to \"have\n    an outer bound\". The builders assume that an `affine.delinearize_index` has\n    an outer bound by default, as this is how the operation was initially defined.\n\n    That is, the example above could also have been written\n    ```mlir\n    %0:3 = affine.delinearize_index %linear_index into (244, 244) : index, index\n    ```\n\n    Note that, for symmetry with `getPaddedBasis()`, if `hasOuterBound` is `true`\n    when one of the `OpFoldResult` builders is called but the first element of the\n    basis is `nullptr`, that first element is ignored and the builder proceeds as if\n    there was no outer bound.\n\n    Due to the constraints of affine maps, all the basis elements must\n    be strictly positive. A dynamic basis element being 0 or negative causes\n    undefined behavior.\n\n    As with other affine operations, lowerings of delinearize_index may assume\n    that the underlying computations do not overflow the index type in a signed sense\n    - that is, the product of all basis elements is positive as an `index` as well.",
    "inputs": [
      { "name": "linear_index", "type": "Index" },
      { "name": "dynamic_basis", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "multi_index", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "static_basis", "type": "DenseI64ArrayAttr" }
    ],
    "assemblyFormat": "$linear_index `into`\n    custom<DynamicIndexList>($dynamic_basis, $static_basis, \"{}\", \"::mlir::AsmParser::Delimiter::Paren\")\n    attr-dict `:` type($multi_index)"
  },
  {
    "name": "affine.for",
    "summary": "for operation",
    "description": "Syntax:\n\n    ```\n    operation   ::= `affine.for` ssa-id `=` lower-bound `to` upper-bound\n                    (`step` integer-literal)? `{` op* `}`\n\n    lower-bound ::= `max`? affine-map-attribute dim-and-symbol-use-list | shorthand-bound\n    upper-bound ::= `min`? affine-map-attribute dim-and-symbol-use-list | shorthand-bound\n    shorthand-bound ::= ssa-id | `-`? integer-literal\n    ```\n\n    The `affine.for` operation represents an affine loop nest. It has one region\n    containing its body. This region must contain one block that terminates with\n    [`affine.yield`](#affineyield-mliraffineyieldop). *Note:* when\n    `affine.for` is printed in custom format, the terminator is omitted. The\n    block has one argument of [`index`](Builtin.md/#indextype) type that\n    represents the induction variable of the loop.\n\n    The `affine.for` operation executes its body a number of times iterating\n    from a lower bound to an upper bound by a stride. The stride, represented by\n    `step`, is a positive constant integer which defaults to \"1\" if not present.\n    The lower and upper bounds specify a half-open range: the range includes the\n    lower bound but does not include the upper bound.\n\n    The lower and upper bounds of a `affine.for` operation are represented as an\n    application of an affine mapping to a list of SSA values passed to the map.\n    The [same restrictions](#restrictions-on-dimensions-and-symbols) hold for\n    these SSA values as for all bindings of SSA values to dimensions and\n    symbols.\n\n    The affine mappings for the bounds may return multiple results, in which\n    case the `max`/`min` keywords are required (for the lower/upper bound\n    respectively), and the bound is the maximum/minimum of the returned values.\n    There is no semantic ambiguity, but MLIR syntax requires the use of these\n    keywords to make things more obvious to human readers.\n\n    Many upper and lower bounds are simple, so MLIR accepts two custom form\n    syntaxes: the form that accepts a single 'ssa-id' (e.g. `%N`) is shorthand\n    for applying that SSA value to a function that maps a single symbol to\n    itself, e.g., `()[s]->(s)()[%N]`. The integer literal form (e.g. `-42`) is\n    shorthand for a nullary mapping function that returns the constant value\n    (e.g. `()->(-42)()`).\n\n    Example showing reverse iteration of the inner loop:\n\n    ```mlir\n    #map57 = affine_map<(d0)[s0] -> (s0 - d0 - 1)>\n\n    func.func @simple_example(%A: memref<?x?xf32>, %B: memref<?x?xf32>) {\n      %N = dim %A, 0 : memref<?x?xf32>\n      affine.for %i = 0 to %N step 1 {\n        affine.for %j = 0 to %N {   // implicitly steps by 1\n          %0 = affine.apply #map57(%j)[%N]\n          %tmp = call @F1(%A, %i, %0) : (memref<?x?xf32>, index, index)->(f32)\n          call @F2(%tmp, %B, %i, %0) : (f32, memref<?x?xf32>, index, index)->()\n        }\n      }\n      return\n    }\n    ```\n    `affine.for` can also operate on loop-carried variables (`iter_args`) and\n    return the final values after loop termination. The initial values of the\n    variables are passed as additional SSA operands to the `affine.for`\n    following the operands for the loop's lower and upper bounds. The\n    operation's region has equivalent arguments for each variable representing\n    the value of the variable at the current iteration.\n\n    The region must terminate with an `affine.yield` that passes all the current\n    iteration variables to the next iteration, or to the `affine.for`'s results\n    if at the last iteration. For `affine.for`'s that execute zero iterations, the\n    initial values of the loop-carried variables (corresponding to the SSA\n    operands) will be the op's results.\n\n    For example, to sum-reduce a memref:\n\n     ```mlir\n    func.func @reduce(%buffer: memref<1024xf32>) -> (f32) {\n      // Initial sum set to 0.\n      %sum_0 = arith.constant 0.0 : f32\n      // iter_args binds initial values to the loop's region arguments.\n      %sum = affine.for %i = 0 to 10 step 2\n          iter_args(%sum_iter = %sum_0) -> (f32) {\n        %t = affine.load %buffer[%i] : memref<1024xf32>\n        %sum_next = arith.addf %sum_iter, %t : f32\n        // Yield current iteration sum to next iteration %sum_iter or to %sum\n        // if final iteration.\n        affine.yield %sum_next : f32\n      }\n      return %sum : f32\n    }\n    ```\n\n    ```mlir\n    %res:2 = affine.for %i = 0 to 128 iter_args(%arg0 = %init0, %arg1 = %init1)\n               -> (index, index) {\n      %y0 = arith.addi %arg0, %c1 : index\n      %y1 = arith.addi %arg1, %c2 : index\n      affine.yield %y0, %y1 : index, index\n    }\n    ```\n    If the `affine.for` defines any values, a yield terminator must be\n    explicitly present. The number and types of the \"affine.for\" results must\n    match the initial values in the `iter_args` binding and the yield operands.",
    "inputs": [
      { "name": "lowerBoundOperands", "type": "Variadic" },
      { "name": "upperBoundOperands", "type": "Variadic" },
      { "name": "inits", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "results", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "lowerBoundMap", "type": "AffineMapAttr" },
      { "name": "upperBoundMap", "type": "AffineMapAttr" },
      { "name": "step", "type": "IndexAttr" }
    ]
  },
  {
    "name": "affine.if",
    "summary": "if-then-else operation",
    "description": "Syntax:\n\n    ```\n    operation  ::= `affine.if` if-op-cond `{` op* `}` (`else` `{` op* `}`)?\n    if-op-cond ::= integer-set-attr dim-and-symbol-use-list\n    ```\n\n    The `affine.if` operation restricts execution to a subset of the loop\n    iteration space defined by an integer set (a conjunction of affine\n    constraints). A single `affine.if` may end with an optional `else` clause.\n\n    The condition of the `affine.if` is represented by an\n    [integer set](#integer-sets) (a conjunction of affine constraints),\n    and the SSA values bound to the dimensions and symbols in the integer set.\n    The [same restrictions](#restrictions-on-dimensions-and-symbols) hold for\n    these SSA values as for all bindings of SSA values to dimensions and\n    symbols.\n\n    The `affine.if` operation contains two regions for the \"then\" and \"else\"\n    clauses.  `affine.if` may return results that are defined in its regions.\n    The values defined are determined by which execution path is taken.  Each\n    region of the `affine.if` must contain a single block with no arguments,\n    and be terminated by `affine.yield`.  If `affine.if` defines no values,\n    the `affine.yield` can be left out, and will be inserted implicitly.\n    Otherwise, it must be explicit.  If no values are defined, the else block\n    may be empty (i.e. contain no blocks).\n\n    Example:\n\n    ```mlir\n    #set = affine_set<(d0, d1)[s0]: (d0 - 10 >= 0, s0 - d0 - 9 >= 0,\n                                     d1 - 10 >= 0, s0 - d1 - 9 >= 0)>\n    func.func @reduced_domain_example(%A, %X, %N) : (memref<10xi32>, i32, i32) {\n      affine.for %i = 0 to %N {\n         affine.for %j = 0 to %N {\n           %0 = affine.apply #map42(%j)\n           %tmp = call @S1(%X, %i, %0)\n           affine.if #set(%i, %j)[%N] {\n              %1 = affine.apply #map43(%i, %j)\n              call @S2(%tmp, %A, %i, %1)\n           }\n        }\n      }\n      return\n    }\n    ```\n\n    Example with an explicit yield (initialization with edge padding):\n\n    ```mlir\n    #interior = affine_set<(i, j) : (i - 1 >= 0, j - 1 >= 0,  10 - i >= 0, 10 - j >= 0)> (%i, %j)\n    func.func @pad_edges(%I : memref<10x10xf32>) -> (memref<12x12xf32) {\n      %O = alloc memref<12x12xf32>\n      affine.parallel (%i, %j) = (0, 0) to (12, 12) {\n        %1 = affine.if #interior (%i, %j) {\n          %2 = load %I[%i - 1, %j - 1] : memref<10x10xf32>\n          affine.yield %2\n        } else {\n          %2 = arith.constant 0.0 : f32\n          affine.yield %2 : f32\n        }\n        affine.store %1, %O[%i, %j] : memref<12x12xf32>\n      }\n      return %O\n    }\n    ```",
    "outputs": [
      { "name": "results", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "condition", "type": "IntegerSetAttr" }
    ]
  },
  {
    "name": "affine.linearize_index",
    "summary": "linearize an index",
    "description": "The `affine.linearize_index` operation takes a sequence of index values and a\n    basis of the same length and linearizes the indices using that basis.\n\n    That is, for indices `%idx_0` to `%idx_{N-1}` and basis elements `b_0`\n    (or `b_1`) up to `b_{N-1}` it computes\n\n    ```\n    sum(i = 0 to N-1) %idx_i * product(j = i + 1 to N-1) B_j\n    ```\n\n    In other words, `%0 = affine.linearize_index [%z, %y, %x] by (Z, Y, X)`\n    gives `%0 = %x + %y * X + %z * X * Y`, or `%0 = %x + X * (%y + Y * (%z))`.\n\n    The basis may either have `N` or `N-1` elements, where `N` is the number of\n    inputs to linearize_index. If `N` inputs are provided, the first one is not used\n    in computation, but may be used during analysis or canonicalization as a bound\n    on `%idx_0`.\n\n    If all `N` basis elements are provided, the linearize_index operation is said to\n    \"have an outer bound\".\n\n    As a convenience, and for symmetry with `getPaddedBasis()`, ifg the first\n    element of a set of `OpFoldResult`s passed to the builders of this operation is\n    `nullptr`, that element is ignored.\n\n    If the `disjoint` property is present, this is an optimization hint that,\n    for all `i`, `0 <= %idx_i < B_i` - that is, no index affects any other index,\n    except that `%idx_0` may be negative to make the index as a whole negative.\n    In addition, `disjoint` is an assertion that all bases elements are non-negative.\n\n    Note that the outputs of `affine.delinearize_index` are, by definition, `disjoint`.\n\n    As with other affine ops, undefined behavior occurs if the linearization\n    computation overflows in the signed sense.\n\n    Example:\n\n    ```mlir\n    %linear_index = affine.linearize_index [%index_0, %index_1, %index_2] by (2, 3, 5) : index\n    // Same effect\n    %linear_index = affine.linearize_index [%index_0, %index_1, %index_2] by (3, 5) : index\n    ```\n\n    In the above example, `%linear_index` conceptually holds the following:\n\n    ```mlir\n    #map = affine_map<()[s0, s1, s2] -> (s0 * 15 + s1 * 5 + s2)>\n    %linear_index = affine.apply #map()[%index_0, %index_1, %index_2]\n    ```",
    "inputs": [
      { "name": "multi_index", "type": "Variadic" },
      { "name": "dynamic_basis", "type": "Variadic" },
      { "name": "disjoint", "type": "UnitProp" }
    ],
    "outputs": [
      { "name": "linear_index", "type": "Index" }
    ],
    "attributes": [
      { "name": "static_basis", "type": "DenseI64ArrayAttr" }
    ],
    "assemblyFormat": "(`disjoint` $disjoint^)? ` `\n    `[` $multi_index `]` `by`\n    custom<DynamicIndexList>($dynamic_basis, $static_basis, \"{}\", \"::mlir::AsmParser::Delimiter::Paren\")\n    attr-dict `:` type($linear_index)"
  },
  {
    "name": "affine.load",
    "summary": "affine load operation",
    "description": "Syntax:\n\n    ```\n    operation ::= ssa-id `=` `affine.load` ssa-use `[` multi-dim-affine-map-of-ssa-ids `]` `:` memref-type\n    ```\n\n    The `affine.load` op reads an element from a memref, where the index\n    for each memref dimension is an affine expression of loop induction\n    variables and symbols. The output of `affine.load` is a new value with the\n    same type as the elements of the memref. An affine expression of loop IVs\n    and symbols must be specified for each dimension of the memref. The keyword\n    `symbol` can be used to indicate SSA identifiers which are symbolic.\n\n    Example 1:\n\n    ```mlir\n    %1 = affine.load %0[%i0 + 3, %i1 + 7] : memref<100x100xf32>\n    ```\n\n    Example 2: Uses `symbol` keyword for symbols `%n` and `%m`.\n\n    ```mlir\n    %1 = affine.load %0[%i0 + symbol(%n), %i1 + symbol(%m)] : memref<100x100xf32>\n    ```",
    "inputs": [
      { "name": "memref", "type": "Arg" },
      { "name": "indices", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyType" }
    ],
    "attributes": [
      { "name": "map", "type": "AffineMapAttr" }
    ]
  },
  {
    "name": "affine.max",
    "summary": "max operation",
    "description": "The `affine.max` operation computes the maximum value result from a multi-result\n    affine map.\n\n    Example:\n\n    ```mlir\n    %0 = affine.max (d0) -> (1000, d0 + 512) (%i0) : index\n    ```",
    "inputs": [
      { "name": "operands", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "map", "type": "AffineMapAttr" }
    ]
  },
  {
    "name": "affine.min",
    "summary": "min operation",
    "description": "Syntax:\n\n    ```\n    operation ::= ssa-id `=` `affine.min` affine-map-attribute dim-and-symbol-use-list\n    ```\n\n    The `affine.min` operation applies an [affine mapping](#affine-expressions)\n    to a list of SSA values, and returns the minimum value of all result\n    expressions. The number of dimension and symbol arguments to `affine.min`\n    must be equal to the respective number of dimensional and symbolic inputs to\n    the affine mapping; the `affine.min` operation always returns one value. The\n    input operands and result must all have 'index' type.\n\n    Example:\n\n    ```mlir\n    %0 = affine.min affine_map<(d0)[s0] -> (1000, d0 + 512, s0)> (%arg0)[%arg1]\n    ```",
    "inputs": [
      { "name": "operands", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "map", "type": "AffineMapAttr" }
    ]
  },
  {
    "name": "affine.parallel",
    "summary": "multi-index parallel band operation",
    "description": "The `affine.parallel` operation represents a hyper-rectangular affine\n    parallel band, defining zero or more SSA values for its induction variables.\n    It has one region capturing the parallel band body. The induction variables\n    are represented as arguments of this region. These SSA values always have\n    type index, which is the size of the machine word. The strides, represented\n    by steps, are positive constant integers which defaults to \"1\" if not\n    present. The lower and upper bounds specify a half-open range: the range\n    includes the lower bound but does not include the upper bound. The body\n    region must contain exactly one block that terminates with `affine.yield`.\n\n    The lower and upper bounds of a parallel operation are represented as an\n    application of an affine mapping to a list of SSA values passed to the map.\n    The same restrictions hold for these SSA values as for all bindings of SSA\n    values to dimensions and symbols. The list of expressions in each map is\n    interpreted according to the respective bounds group attribute. If a single\n    expression belongs to the group, then the result of this expression is taken\n    as a lower(upper) bound of the corresponding loop induction variable. If\n    multiple expressions belong to the group, then the lower(upper) bound is the\n    max(min) of these values obtained from these expressions. The loop band has\n    as many loops as elements in the group bounds attributes.\n\n    Each value yielded by `affine.yield` will be accumulated/reduced via one of\n    the reduction methods defined in the AtomicRMWKind enum.  The order of\n    reduction is unspecified, and lowering may produce any valid ordering.\n    Loops with a 0 trip count will produce as a result the identity value\n    associated with each reduction (i.e. 0.0 for addf, 1.0 for mulf).  Assign\n    reductions for loops with a trip count != 1 produces undefined results.\n\n    Note: Calling `AffineParallelOp::build` will create the required region and\n    block, and insert the required terminator if it is trivial (i.e. no values\n    are yielded).  Parsing will also create the required region, block, and\n    terminator, even when they are missing from the textual representation.\n\n    Example (3x3 valid convolution):\n\n    ```mlir\n    func.func @conv_2d(%D : memref<100x100xf32>, %K : memref<3x3xf32>) -> (memref<98x98xf32>) {\n      %O = memref.alloc() : memref<98x98xf32>\n      affine.parallel (%x, %y) = (0, 0) to (98, 98) {\n        %0 = affine.parallel (%kx, %ky) = (0, 0) to (2, 2) reduce (\"addf\") -> f32 {\n          %1 = affine.load %D[%x + %kx, %y + %ky] : memref<100x100xf32>\n          %2 = affine.load %K[%kx, %ky] : memref<3x3xf32>\n          %3 = arith.mulf %1, %2 : f32\n          affine.yield %3 : f32\n        }\n        affine.store %0, %O[%x, %y] : memref<98x98xf32>\n      }\n      return %O : memref<98x98xf32>\n    }\n    ```\n\n    Example (tiling by potentially imperfectly dividing sizes):\n\n    ```mlir\n    affine.parallel (%ii, %jj) = (0, 0) to (%N, %M) step (32, 32) {\n      affine.parallel (%i, %j) = (%ii, %jj)\n                              to (min(%ii + 32, %N), min(%jj + 32, %M)) {\n        call @f(%i, %j) : (index, index) -> ()\n      }\n    }\n    ```",
    "inputs": [
      { "name": "mapOperands", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "results", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "reductions", "type": "TypedArrayAttrBase" },
      { "name": "lowerBoundsMap", "type": "AffineMapAttr" },
      { "name": "lowerBoundsGroups", "type": "I32ElementsAttr" },
      { "name": "upperBoundsMap", "type": "AffineMapAttr" },
      { "name": "upperBoundsGroups", "type": "I32ElementsAttr" },
      { "name": "steps", "type": "I64SmallVectorArrayAttr" }
    ]
  },
  {
    "name": "affine.prefetch",
    "summary": "affine prefetch operation",
    "description": "The `affine.prefetch` op prefetches data from a memref location described\n    with an affine subscript similar to affine.load, and has three attributes:\n    a read/write specifier, a locality hint, and a cache type specifier as shown\n    below:\n\n    ```mlir\n    affine.prefetch %0[%i, %j + 5], read, locality<3>, data : memref<400x400xi32>\n    ```\n\n    The read/write specifier is either 'read' or 'write', the locality hint\n    specifier ranges from locality<0> (no locality) to locality<3> (extremely\n    local keep in cache). The cache type specifier is either 'data' or 'instr'\n    and specifies whether the prefetch is performed on data cache or on\n    instruction cache.",
    "inputs": [
      { "name": "memref", "type": "AnyMemRef" },
      { "name": "indices", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "isWrite", "type": "BoolAttr" },
      { "name": "localityHint", "type": "ConfinedAttr" },
      { "name": "isDataCache", "type": "BoolAttr" },
      { "name": "map", "type": "AffineMapAttr" }
    ]
  },
  {
    "name": "affine.store",
    "summary": "affine store operation",
    "description": "Syntax:\n\n    ```\n    operation ::= `affine.store` ssa-use, ssa-use `[` multi-dim-affine-map-of-ssa-ids `]` `:` memref-type\n    ```\n\n    The `affine.store` op writes an element to a memref, where the index\n    for each memref dimension is an affine expression of loop induction\n    variables and symbols. The `affine.store` op stores a new value which is the\n    same type as the elements of the memref. An affine expression of loop IVs\n    and symbols must be specified for each dimension of the memref. The keyword\n    `symbol` can be used to indicate SSA identifiers which are symbolic.\n\n    Example 1:\n\n    ```mlir\n    affine.store %v0, %0[%i0 + 3, %i1 + 7] : memref<100x100xf32>\n    ```\n\n    Example 2: Uses `symbol` keyword for symbols `%n` and `%m`.\n\n    ```mlir\n    affine.store %v0, %0[%i0 + symbol(%n), %i1 + symbol(%m)] : memref<100x100xf32>\n    ```",
    "inputs": [
      { "name": "value", "type": "AnyType" },
      { "name": "memref", "type": "Arg" },
      { "name": "indices", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "map", "type": "AffineMapAttr" }
    ]
  },
  {
    "name": "affine.vector_load",
    "summary": "affine vector load operation",
    "description": "The `affine.vector_load` is the vector counterpart of\n    [affine.load](#affineload-mliraffineloadop). It reads a slice from a\n    [MemRef](Builtin.md/#memreftype), supplied as its first operand,\n    into a [vector](Builtin.md/#vectortype) of the same base elemental type.\n    The index for each memref dimension is an affine expression of loop induction\n    variables and symbols. These indices determine the start position of the read\n    within the memref. The shape of the return vector type determines the shape of\n    the slice read from the memref. This slice is contiguous along the respective\n    dimensions of the shape. Strided vector loads will be supported in the future.\n    An affine expression of loop IVs and symbols must be specified for each\n    dimension of the memref. The keyword `symbol` can be used to indicate SSA\n    identifiers which are symbolic.\n\n    Example 1: 8-wide f32 vector load.\n\n    ```mlir\n    %1 = affine.vector_load %0[%i0 + 3, %i1 + 7] : memref<100x100xf32>, vector<8xf32>\n    ```\n\n    Example 2: 4-wide f32 vector load. Uses `symbol` keyword for symbols `%n` and `%m`.\n\n    ```mlir\n    %1 = affine.vector_load %0[%i0 + symbol(%n), %i1 + symbol(%m)] : memref<100x100xf32>, vector<4xf32>\n    ```\n\n    Example 3: 2-dim f32 vector load.\n\n    ```mlir\n    %1 = affine.vector_load %0[%i0, %i1] : memref<100x100xf32>, vector<2x8xf32>\n    ```\n\n    TODOs:\n    * Add support for strided vector loads.\n    * Consider adding a permutation map to permute the slice that is read from memory\n    (see [vector.transfer_read](../Vector/#vectortransfer_read-mlirvectortransferreadop)).",
    "inputs": [
      { "name": "memref", "type": "Arg" },
      { "name": "indices", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyVectorOfNonZeroRank" }
    ],
    "attributes": [
      { "name": "map", "type": "AffineMapAttr" }
    ]
  },
  {
    "name": "affine.vector_store",
    "summary": "affine vector store operation",
    "description": "The `affine.vector_store` is the vector counterpart of\n    [affine.store](#affinestore-mliraffinestoreop). It writes a\n    [vector](Builtin.md/#vectortype), supplied as its first operand,\n    into a slice within a [MemRef](Builtin.md/#memreftype) of the same base\n    elemental type, supplied as its second operand.\n    The index for each memref dimension is an affine expression of loop\n    induction variables and symbols. These indices determine the start position\n    of the write within the memref. The shape of th input vector determines the\n    shape of the slice written to the memref. This slice is contiguous along the\n    respective dimensions of the shape. Strided vector stores will be supported\n    in the future.\n    An affine expression of loop IVs and symbols must be specified for each\n    dimension of the memref. The keyword `symbol` can be used to indicate SSA\n    identifiers which are symbolic.\n\n    Example 1: 8-wide f32 vector store.\n\n    ```mlir\n    affine.vector_store %v0, %0[%i0 + 3, %i1 + 7] : memref<100x100xf32>, vector<8xf32>\n    ```\n\n    Example 2: 4-wide f32 vector store. Uses `symbol` keyword for symbols `%n` and `%m`.\n\n    ```mlir\n    affine.vector_store %v0, %0[%i0 + symbol(%n), %i1 + symbol(%m)] : memref<100x100xf32>, vector<4xf32>\n    ```\n\n    Example 3: 2-dim f32 vector store.\n\n    ```mlir\n    affine.vector_store %v0, %0[%i0, %i1] : memref<100x100xf32>, vector<2x8xf32>\n    ```\n\n    TODOs:\n    * Add support for strided vector stores.\n    * Consider adding a permutation map to permute the slice that is written to memory\n    (see [vector.transfer_write](../Vector/#vectortransfer_write-mlirvectortransferwriteop)).",
    "inputs": [
      { "name": "value", "type": "AnyVectorOfNonZeroRank" },
      { "name": "memref", "type": "Arg" },
      { "name": "indices", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "map", "type": "AffineMapAttr" }
    ]
  },
  {
    "name": "affine.yield",
    "summary": "Yield values to parent operation",
    "description": "The `affine.yield` yields zero or more SSA values from an affine op region and\n    terminates the region. The semantics of how the values yielded are used\n    is defined by the parent operation.\n    If `affine.yield` has any operands, the operands must match the parent\n    operation's results.\n    If the parent operation defines no values, then the `affine.yield` may be\n    left out in the custom syntax and the builders will insert one implicitly.\n    Otherwise, it has to be present in the syntax to indicate which values are\n    yielded.",
    "inputs": [
      { "name": "operands", "type": "Variadic" }
    ],
    "assemblyFormat": "attr-dict ($operands^ `:` type($operands))?"
  },
  {
    "name": "builtin.module",
    "summary": "A top level container operation",
    "description": "A `module` represents a top-level container operation. It contains a single\n    [graph region](../LangRef.md#control-flow-and-ssacfg-regions) containing a single block\n    which can contain any operations and does not have a terminator. Operations\n    within this region cannot implicitly capture values defined outside the module,\n    i.e. Modules are [IsolatedFromAbove](../Traits#isolatedfromabove). Modules have\n    an optional [symbol name](../SymbolsAndSymbolTables.md) which can be used to refer\n    to them in operations.\n\n    Example:\n\n    ```mlir\n    module {\n      func.func @foo()\n    }\n    ```",
    "attributes": [
      { "name": "sym_name", "type": "OptionalAttr" },
      { "name": "sym_visibility", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "($sym_name^)? attr-dict-with-keyword $bodyRegion"
  },
  {
    "name": "builtin.unrealized_conversion_cast",
    "summary": "An unrealized conversion from one set of types to another",
    "description": "An `unrealized_conversion_cast` operation represents an unrealized\n    conversion from one set of types to another, that is used to enable the\n    inter-mixing of different type systems. This operation should not be\n    attributed any special representational or execution semantics, and is\n    generally only intended to be used to satisfy the temporary intermixing of\n    type systems during the conversion of one type system to another.\n\n    This operation may produce results of arity 1-N, and accept as input\n    operands of arity 0-N.\n\n    Example:\n\n    ```mlir\n    // An unrealized 0-1 conversion. These types of conversions are useful in\n    // cases where a type is removed from the type system, but not all uses have\n    // been converted. For example, imagine we have a tuple type that is\n    // expanded to its element types. If only some uses of an empty tuple type\n    // instance are converted we still need an instance of the tuple type, but\n    // have no inputs to the unrealized conversion.\n    %result = unrealized_conversion_cast to !bar.tuple_type<>\n\n    // An unrealized 1-1 conversion.\n    %result1 = unrealized_conversion_cast %operand : !foo.type to !bar.lowered_type\n\n    // An unrealized 1-N conversion.\n    %results2:2 = unrealized_conversion_cast %tuple_operand : !foo.tuple_type<!foo.type, !foo.type> to !foo.type, !foo.type\n\n    // An unrealized N-1 conversion.\n    %result3 = unrealized_conversion_cast %operand, %operand : !foo.type, !foo.type to !bar.tuple_type<!foo.type, !foo.type>\n    ```",
    "inputs": [
      { "name": "inputs", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "outputs", "type": "Variadic" }
    ],
    "assemblyFormat": "($inputs^ `:` type($inputs))? `to` type($outputs) attr-dict"
  },
  {
    "name": "chlo._asin_acos_kernel",
    "summary": "AsinAcosKernel operator",
    "description": "Returns `AsinAcosKernel(operand)` element-wise.\n\n    ```\n    If\n      w = _asin_acos_kernel(z)\n      w' = _asin_acos_kernel(I * z)\n    Then\n      asin(z) = complex(atan2(z.real, w.real), sign(z.imag) * w.imag)\n      acos(z) = complex(atan2(w.real, z.real), -sign(z.imag) * w.imag)\n      asinh(z) = complex(sign(z.real) * w'.imag, atan2(z.imag, w'.real))\n      acosh(z) = complex(w.imag, sign(z.imag) * atan2(w.real, z.real))\n    ```\n\n    This op is used as an intermediate value in decompositions and\n    should never be constructed directly by frameworks or consumed by\n    backends.",
    "inputs": [
      { "name": "operand", "type": "ArgTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "ResultTensorType" }
    ],
    "assemblyFormat": "$operand attr-dict `:` type($operand) `->` type($result)"
  },
  {
    "name": "chlo.acos",
    "summary": "Acos operator",
    "description": "Returns `Acos(operand)` element-wise.\n\n    $$\n    \\acos(x) = 2 * \\atan(\\sqrt(1 - x^2) / (1 + x)) if x != -1\n             = pi                                  if x == -1\n    $$",
    "inputs": [
      { "name": "operand", "type": "ArgTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "ResultTensorType" }
    ],
    "assemblyFormat": "$operand attr-dict `:` type($operand) `->` type($result)"
  },
  {
    "name": "chlo.acosh",
    "summary": "Acosh operation",
    "description": "Returns `Acosh(operand)` element-wise.\n\n    $$\n    \\acosh(x) = log(x + sqrt(x^2 - 1))      if x >= -1\n    \\acosh(x) = nan                         if x < -1\n    $$",
    "inputs": [
      { "name": "operand", "type": "ArgTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "ResultTensorType" }
    ],
    "assemblyFormat": "$operand attr-dict `:` type($operand) `->` type($result)"
  },
  {
    "name": "chlo.asin",
    "summary": "Asin operator",
    "description": "Returns `Asin(operand)` element-wise.\n\n    $$\n    \\asin(x) = 2 * atan(x / (1 + sqrt(1 - x^2)))\n    $$",
    "inputs": [
      { "name": "operand", "type": "ArgTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "ResultTensorType" }
    ],
    "assemblyFormat": "$operand attr-dict `:` type($operand) `->` type($result)"
  },
  {
    "name": "chlo.asinh",
    "summary": "Asinh operation",
    "description": "Returns `Asinh(operand)` element-wise.\n\n    $$\n    \\asinh(x) = log(x + sqrt(x^2 + 1))\n    $$",
    "inputs": [
      { "name": "operand", "type": "ArgTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "ResultTensorType" }
    ],
    "assemblyFormat": "$operand attr-dict `:` type($operand) `->` type($result)"
  },
  {
    "name": "chlo.atan",
    "summary": "Atan operator",
    "description": "Returns `Atan(operand)` element-wise.\n\n    $$\n    \\atan(x) = \\atan2(x, 1)\n    $$",
    "inputs": [
      { "name": "operand", "type": "ArgTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "ResultTensorType" }
    ],
    "assemblyFormat": "$operand attr-dict `:` type($operand) `->` type($result)"
  },
  {
    "name": "chlo.atanh",
    "summary": "Atanh operator",
    "description": "Returns `Atanh(operand)` element-wise.\n\n    $$\n    \\atanh(x) = 0.5 * log((1 + x) / (1 - x)) if abs(x) <= 1\n              = nan                          otherwise\n    $$",
    "inputs": [
      { "name": "operand", "type": "ArgTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "ResultTensorType" }
    ],
    "assemblyFormat": "$operand attr-dict `:` type($operand) `->` type($result)"
  },
  {
    "name": "chlo.bessel_i1e",
    "summary": "Bessel function of order 1",
    "description": "Returns `bessel_i1e(operand)` element-wise.",
    "inputs": [
      { "name": "operand", "type": "ArgTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "ResultTensorType" }
    ],
    "assemblyFormat": "$operand attr-dict `:` type($operand) `->` type($result)"
  },
  {
    "name": "chlo.broadcast_add",
    "summary": "Addition operator (with optional broadcasting)",
    "description": "Returns `lhs + rhs` element-wise.\n\n    See\n    https://www.tensorflow.org/xla/operation_semantics#element-wise_binary_arithmetic_operations.",
    "inputs": [
      { "name": "lhs", "type": "HLO_AnyTensor" },
      { "name": "rhs", "type": "HLO_AnyTensor" }
    ],
    "attributes": [
      { "name": "broadcast_dimensions", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "$lhs `,` $rhs attr-dict `:`\n    `(` type($lhs) `,` type($rhs) `)` `->` type(results)"
  },
  {
    "name": "chlo.broadcast_and",
    "summary": "Logical and operator (with optional broadcasting)",
    "description": "Returns `logical_and(lhs, rhs)` element-wise.\n\n    See\n    https://www.tensorflow.org/xla/operation_semantics#element-wise_binary_arithmetic_operations.",
    "inputs": [
      { "name": "lhs", "type": "HLO_AnyPredOrIntTensor" },
      { "name": "rhs", "type": "HLO_AnyPredOrIntTensor" }
    ],
    "attributes": [
      { "name": "broadcast_dimensions", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "$lhs `,` $rhs attr-dict `:`\n    `(` type($lhs) `,` type($rhs) `)` `->` type(results)"
  },
  {
    "name": "chlo.broadcast_atan2",
    "summary": "Atan2 operator (with optional broadcasting)",
    "description": "Returns `atan2(lhs/rhs)` element-wise.\n\n    See\n    https://www.tensorflow.org/xla/operation_semantics#element-wise_binary_arithmetic_operations.",
    "inputs": [
      { "name": "lhs", "type": "HLO_AnyTensor" },
      { "name": "rhs", "type": "HLO_AnyTensor" }
    ],
    "attributes": [
      { "name": "broadcast_dimensions", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "$lhs `,` $rhs attr-dict `:`\n    `(` type($lhs) `,` type($rhs) `)` `->` type(results)"
  },
  {
    "name": "chlo.broadcast_compare",
    "summary": "Compare operator (with optional broadcasting)",
    "description": "Compares `lhs` and `rhs` elementwise according to `comparison_direction`\n    and `compare_type`. If unspecified, `compare_type` is FLOAT for float element\n    types, SIGNED for signed element types and UNSIGNED for unsigned element\n    types.\n\n    See\n    https://www.tensorflow.org/xla/operation_semantics#element-wise_comparison_operations.",
    "inputs": [
      { "name": "lhs", "type": "HLO_AnyTensor" },
      { "name": "rhs", "type": "HLO_AnyTensor" }
    ],
    "attributes": [
      { "name": "broadcast_dimensions", "type": "OptionalAttr" },
      { "name": "comparison_direction", "type": "CHLO_ComparisonDirectionAttr" },
      { "name": "compare_type", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "$lhs `,` $rhs attr-dict `:`\n    `(` type($lhs) `,` type($rhs) `)` `->` type(results)"
  },
  {
    "name": "chlo.broadcast_complex",
    "summary": "Complex operator (with optional broadcasting)",
    "description": "Performs element-wise conversion of a pair of real and imaginary values to\n    a complex value.",
    "inputs": [
      { "name": "lhs", "type": "HLO_AnyFpTensor" },
      { "name": "rhs", "type": "HLO_AnyFpTensor" }
    ],
    "attributes": [
      { "name": "broadcast_dimensions", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "$lhs `,` $rhs attr-dict `:`\n    `(` type($lhs) `,` type($rhs) `)` `->` type(results)"
  },
  {
    "name": "chlo.broadcast_divide",
    "summary": "Division operator (with optional broadcasting)",
    "description": "Returns `lhs / rhs` element-wise.\n\n    See\n    https://www.tensorflow.org/xla/operation_semantics#element-wise_binary_arithmetic_operations.",
    "inputs": [
      { "name": "lhs", "type": "HLO_AnyTensor" },
      { "name": "rhs", "type": "HLO_AnyTensor" }
    ],
    "attributes": [
      { "name": "broadcast_dimensions", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "$lhs `,` $rhs attr-dict `:`\n    `(` type($lhs) `,` type($rhs) `)` `->` type(results)"
  },
  {
    "name": "chlo.broadcast_maximum",
    "summary": "Maximum operator (with optional broadcasting)",
    "description": "Returns `max(lhs, rhs)` element-wise.\n\n    See\n    https://www.tensorflow.org/xla/operation_semantics#element-wise_binary_arithmetic_operations.",
    "inputs": [
      { "name": "lhs", "type": "HLO_AnyTensor" },
      { "name": "rhs", "type": "HLO_AnyTensor" }
    ],
    "attributes": [
      { "name": "broadcast_dimensions", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "$lhs `,` $rhs attr-dict `:`\n    `(` type($lhs) `,` type($rhs) `)` `->` type(results)"
  },
  {
    "name": "chlo.broadcast_minimum",
    "summary": "Minimum operator (with optional broadcasting)",
    "description": "Returns `min(lhs, rhs)` element-wise.\n\n    See\n    https://www.tensorflow.org/xla/operation_semantics#element-wise_binary_arithmetic_operations.",
    "inputs": [
      { "name": "lhs", "type": "HLO_AnyTensor" },
      { "name": "rhs", "type": "HLO_AnyTensor" }
    ],
    "attributes": [
      { "name": "broadcast_dimensions", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "$lhs `,` $rhs attr-dict `:`\n    `(` type($lhs) `,` type($rhs) `)` `->` type(results)"
  },
  {
    "name": "chlo.broadcast_multiply",
    "summary": "Multiplication operator (with optional broadcasting)",
    "description": "Returns `lhs * rhs` element-wise.\n\n    See\n    https://www.tensorflow.org/xla/operation_semantics#element-wise_binary_arithmetic_operations.",
    "inputs": [
      { "name": "lhs", "type": "HLO_AnyTensor" },
      { "name": "rhs", "type": "HLO_AnyTensor" }
    ],
    "attributes": [
      { "name": "broadcast_dimensions", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "$lhs `,` $rhs attr-dict `:`\n    `(` type($lhs) `,` type($rhs) `)` `->` type(results)"
  },
  {
    "name": "chlo.broadcast_next_after",
    "summary": "std::nextafter operator (with optional broadcasting)",
    "description": "Returns the next representable value of `lhs` in the direction of `rhs`,\n    element-wise. It can also return a subnormal number.\n\n    Equivalent to the C++ std::nextafter function.",
    "inputs": [
      { "name": "lhs", "type": "HLO_AnyTensor" },
      { "name": "rhs", "type": "HLO_AnyTensor" }
    ],
    "attributes": [
      { "name": "broadcast_dimensions", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "$lhs `,` $rhs attr-dict `:`\n    `(` type($lhs) `,` type($rhs) `)` `->` type(results)"
  },
  {
    "name": "chlo.broadcast_or",
    "summary": "Logical or operator (with optional broadcasting)",
    "description": "Returns `logical_or(lhs, rhs)` element-wise.\n\n    See\n    https://www.tensorflow.org/xla/operation_semantics#element-wise_binary_arithmetic_operations.",
    "inputs": [
      { "name": "lhs", "type": "HLO_AnyPredOrIntTensor" },
      { "name": "rhs", "type": "HLO_AnyPredOrIntTensor" }
    ],
    "attributes": [
      { "name": "broadcast_dimensions", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "$lhs `,` $rhs attr-dict `:`\n    `(` type($lhs) `,` type($rhs) `)` `->` type(results)"
  },
  {
    "name": "chlo.broadcast_polygamma",
    "summary": "Polygamma function (with optional broadcasting)",
    "description": "Returns `Polygamma(operand, operand)` element-wise.",
    "inputs": [
      { "name": "lhs", "type": "HLO_AnyTensor" },
      { "name": "rhs", "type": "HLO_AnyTensor" }
    ],
    "attributes": [
      { "name": "broadcast_dimensions", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "$lhs `,` $rhs attr-dict `:`\n    `(` type($lhs) `,` type($rhs) `)` `->` type(results)"
  },
  {
    "name": "chlo.broadcast_power",
    "summary": "Power operator (with optional broadcasting)",
    "description": "Returns `lhs ^ rhs` element-wise.\n\n    See\n    https://www.tensorflow.org/xla/operation_semantics#element-wise_binary_arithmetic_operations.",
    "inputs": [
      { "name": "lhs", "type": "HLO_AnyTensor" },
      { "name": "rhs", "type": "HLO_AnyTensor" }
    ],
    "attributes": [
      { "name": "broadcast_dimensions", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "$lhs `,` $rhs attr-dict `:`\n    `(` type($lhs) `,` type($rhs) `)` `->` type(results)"
  },
  {
    "name": "chlo.broadcast_remainder",
    "summary": "Remainder operator (with optional broadcasting)",
    "description": "Returns `lhs % rhs` element-wise.\n\n    See\n    https://www.tensorflow.org/xla/operation_semantics#element-wise_binary_arithmetic_operations.",
    "inputs": [
      { "name": "lhs", "type": "HLO_AnyTensor" },
      { "name": "rhs", "type": "HLO_AnyTensor" }
    ],
    "attributes": [
      { "name": "broadcast_dimensions", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "$lhs `,` $rhs attr-dict `:`\n    `(` type($lhs) `,` type($rhs) `)` `->` type(results)"
  },
  {
    "name": "chlo.broadcast_select",
    "summary": "Select operator (with optional numpy-style broadcasting)",
    "description": "Constructs an output array from elements of two input arrays, based on the\n    values of a predicate array.\n\n    See https://www.tensorflow.org/xla/operation_semantics#select",
    "inputs": [
      { "name": "pred", "type": "HLO_PredTensor" },
      { "name": "on_true", "type": "HLO_AnyTensor" },
      { "name": "on_false", "type": "HLO_AnyTensor" }
    ],
    "assemblyFormat": "$pred `,` $on_true `,` $on_false attr-dict `:`\n    `(` type($pred) `,` type($on_true) `,` type($on_false) `)` `->` type(results)"
  },
  {
    "name": "chlo.broadcast_shift_left",
    "summary": "Shift left operator (with optional broadcasting)",
    "description": "Returns `lhs << rhs` element-wise.\n\n    See\n    https://www.tensorflow.org/xla/operation_semantics#element-wise_binary_arithmetic_operations.",
    "inputs": [
      { "name": "lhs", "type": "HLO_AnyTensor" },
      { "name": "rhs", "type": "HLO_AnyTensor" }
    ],
    "attributes": [
      { "name": "broadcast_dimensions", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "$lhs `,` $rhs attr-dict `:`\n    `(` type($lhs) `,` type($rhs) `)` `->` type(results)"
  },
  {
    "name": "chlo.broadcast_shift_right_arithmetic",
    "summary": "Shift right arithmetic operator (with optional broadcasting)",
    "description": "Returns `lhs >> rhs` element-wise.\n\n    See\n    https://www.tensorflow.org/xla/operation_semantics#element-wise_binary_arithmetic_operations.",
    "inputs": [
      { "name": "lhs", "type": "HLO_AnyTensor" },
      { "name": "rhs", "type": "HLO_AnyTensor" }
    ],
    "attributes": [
      { "name": "broadcast_dimensions", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "$lhs `,` $rhs attr-dict `:`\n    `(` type($lhs) `,` type($rhs) `)` `->` type(results)"
  },
  {
    "name": "chlo.broadcast_shift_right_logical",
    "summary": "Shift right logical operator (with optional broadcasting)",
    "description": "Returns `lhs >> rhs` element-wise.\n\n    See\n    https://www.tensorflow.org/xla/operation_semantics#element-wise_binary_arithmetic_operations.",
    "inputs": [
      { "name": "lhs", "type": "HLO_AnyTensor" },
      { "name": "rhs", "type": "HLO_AnyTensor" }
    ],
    "attributes": [
      { "name": "broadcast_dimensions", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "$lhs `,` $rhs attr-dict `:`\n    `(` type($lhs) `,` type($rhs) `)` `->` type(results)"
  },
  {
    "name": "chlo.broadcast_subtract",
    "summary": "Subtraction operator (with optional broadcasting)",
    "description": "Returns `lhs - rhs` element-wise.\n\n    See\n    https://www.tensorflow.org/xla/operation_semantics#element-wise_binary_arithmetic_operations.",
    "inputs": [
      { "name": "lhs", "type": "HLO_AnyTensor" },
      { "name": "rhs", "type": "HLO_AnyTensor" }
    ],
    "attributes": [
      { "name": "broadcast_dimensions", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "$lhs `,` $rhs attr-dict `:`\n    `(` type($lhs) `,` type($rhs) `)` `->` type(results)"
  },
  {
    "name": "chlo.broadcast_xor",
    "summary": "Logical xor operator (with optional broadcasting)",
    "description": "Returns `logical_xor(lhs, rhs)` element-wise.\n\n    See\n    https://www.tensorflow.org/xla/operation_semantics#element-wise_binary_arithmetic_operations.",
    "inputs": [
      { "name": "lhs", "type": "HLO_AnyPredOrIntTensor" },
      { "name": "rhs", "type": "HLO_AnyPredOrIntTensor" }
    ],
    "attributes": [
      { "name": "broadcast_dimensions", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "$lhs `,` $rhs attr-dict `:`\n    `(` type($lhs) `,` type($rhs) `)` `->` type(results)"
  },
  {
    "name": "chlo.broadcast_zeta",
    "summary": "Hurwitz zeta function",
    "description": "Returns `Zeta(operand, operand)` element-wise.\n\n    $$\n    \\(\\zeta(x, q) = \\sum_{n=0}^{\\infty} (q + n)^{-x}\\)\n    $$",
    "inputs": [
      { "name": "lhs", "type": "HLO_AnyFpTensor" },
      { "name": "rhs", "type": "HLO_AnyFpTensor" }
    ],
    "attributes": [
      { "name": "broadcast_dimensions", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "$lhs `,` $rhs attr-dict `:`\n    `(` type($lhs) `,` type($rhs) `)` `->` type(results)"
  },
  {
    "name": "chlo.conj",
    "summary": "Conj operator",
    "description": "Returns `Conj(operand)` element-wise.\n\n    $$\n    \\conj(x) = (\\real(x), \\neg(\\imag(x)))\n    $$",
    "inputs": [
      { "name": "operand", "type": "ArgTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "ResultTensorType" }
    ],
    "assemblyFormat": "$operand attr-dict `:` type($operand) `->` type($result)"
  },
  {
    "name": "chlo.constant",
    "summary": "Constant operator",
    "description": "Represents a constant value.",
    "outputs": [
      { "name": "output", "type": "HLO_StaticShapeTensor" }
    ],
    "attributes": [
      { "name": "value", "type": "ElementsAttr" }
    ],
    "assemblyFormat": "attr-dict $value"
  },
  {
    "name": "chlo.constant_like",
    "summary": "Constant like operator",
    "description": "Returns a splat constant of the same shape as the operand.",
    "inputs": [
      { "name": "operand", "type": "HLO_AnyTensor" }
    ],
    "attributes": [
      { "name": "value", "type": "TypedAttrInterface" }
    ]
  },
  {
    "name": "chlo.cosh",
    "summary": "Cosh operator",
    "description": "Returns `Cosh(operand)` element-wise.\n\n    $$\n    \\cosh(x) = (e^x + e^-x) / 2\n    $$",
    "inputs": [
      { "name": "operand", "type": "ArgTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "ResultTensorType" }
    ],
    "assemblyFormat": "$operand attr-dict `:` type($operand) `->` type($result)"
  },
  {
    "name": "chlo.digamma",
    "summary": "Digamma function",
    "description": "Returns `Digamma(operand)` element-wise.",
    "inputs": [
      { "name": "operand", "type": "ArgTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "ResultTensorType" }
    ],
    "assemblyFormat": "$operand attr-dict `:` type($operand) `->` type($result)"
  },
  {
    "name": "chlo.erf",
    "summary": "Erfc operator",
    "description": "Computes the Gauss error function of `x` element-wise.\n\n    erf(x) = erf_impl(x)            if |x| < 1\n           = 1 - erfc_impl(x)       otherwise",
    "inputs": [
      { "name": "operand", "type": "ArgTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "ResultTensorType" }
    ],
    "assemblyFormat": "$operand attr-dict `:` type($operand) `->` type($result)"
  },
  {
    "name": "chlo.erf_inv",
    "summary": "Inverse Erf",
    "description": "Returns `ErfInv(operand)` element-wise.",
    "inputs": [
      { "name": "operand", "type": "ArgTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "ResultTensorType" }
    ],
    "assemblyFormat": "$operand attr-dict `:` type($operand) `->` type($result)"
  },
  {
    "name": "chlo.erfc",
    "summary": "Erfc operator",
    "description": "Computes an approximation of the error function complement (1 - erf(x)).\n\n    erfc(x) = erfc_impl(x)           if |x| > 1\n            = 1 - erf_impl(x)        otherwise",
    "inputs": [
      { "name": "operand", "type": "ArgTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "ResultTensorType" }
    ],
    "assemblyFormat": "$operand attr-dict `:` type($operand) `->` type($result)"
  },
  {
    "name": "chlo.is_inf",
    "summary": "IsInf predicate",
    "description": "Returns if a value is +/-inf element-wise.",
    "inputs": [
      { "name": "operand", "type": "ArgTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "ResultTensorType" }
    ],
    "assemblyFormat": "$operand attr-dict `:` type($operand) `->` type($result)"
  },
  {
    "name": "chlo.is_neg_inf",
    "summary": "IsNegInf predicate",
    "description": "Returns if a value is -inf element-wise.",
    "inputs": [
      { "name": "operand", "type": "ArgTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "ResultTensorType" }
    ],
    "assemblyFormat": "$operand attr-dict `:` type($operand) `->` type($result)"
  },
  {
    "name": "chlo.is_pos_inf",
    "summary": "IsPosInf predicate",
    "description": "Returns if a value is +inf element-wise.",
    "inputs": [
      { "name": "operand", "type": "ArgTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "ResultTensorType" }
    ],
    "assemblyFormat": "$operand attr-dict `:` type($operand) `->` type($result)"
  },
  {
    "name": "chlo.lgamma",
    "summary": "Lgamma function",
    "description": "Returns `Lgamma(operand)` element-wise.",
    "inputs": [
      { "name": "operand", "type": "ArgTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "ResultTensorType" }
    ],
    "assemblyFormat": "$operand attr-dict `:` type($operand) `->` type($result)"
  },
  {
    "name": "chlo.next_after",
    "summary": "std::nextafter operator",
    "description": "Returns the next representable value of `x` in the direction of `y`,\n    element-wise. It can also return a subnormal number.\n\n    Equivalent to the C++ std::nextafter function.",
    "inputs": [
      { "name": "x", "type": "HLO_AnyFpTensor" },
      { "name": "y", "type": "HLO_AnyFpTensor" }
    ],
    "outputs": [
      { "name": "result", "type": "HLO_AnyFpTensor" }
    ],
    "assemblyFormat": "$x `,` $y attr-dict `:` type($x) `,` type($y) `->` type(results)"
  },
  {
    "name": "chlo.polygamma",
    "summary": "Polygamma function",
    "description": "Returns `Polygamma(operand, operand)` element-wise.",
    "inputs": [
      { "name": "n", "type": "HLO_AnyFpTensor" },
      { "name": "x", "type": "HLO_AnyFpTensor" }
    ],
    "outputs": [
      { "name": "result", "type": "HLO_AnyFpTensor" }
    ],
    "assemblyFormat": "$n `,` $x attr-dict `:` type($n) `,` type($x) `->` type(results)"
  },
  {
    "name": "chlo.ragged_dot",
    "summary": "Computes a matmul over a single ragged dimension",
    "description": "This operation takes three tensor args---lhs, rhs, and group_sizes---and\n    a \"ragged_dot_dimension_numbers\" attribute. Like dot_general, the lhs and\n    rhs are allowed arbitrary batch and contracting dimensions. Additionally,\n    the lhs is required to have one ragged dimension, and the rhs may have at\n    most one group dimension. The op has three modes, depending on the kind of\n    the lhs ragged dimension.\n\n    In mode 1, the shape-signature is `[b,m,k], [g,b,k,n], [b,g] -> [b,m,n]`.\n    Here the ragged dimension is an lhs non-contracting dimension (`m`). The\n    dimensions `b` and `k` represent batch and contracting dimensions\n    respectively. The rhs is required to have a group dimension (`g`).\n\n    In mode 2, the shape-signature is `[b,m,k], [b,k,n], [b,g] -> [g,b,m,n]`.\n    Here the ragged dimension is an lhs/rhs contracting dimension (`k`).\n\n    In mode 3, the shape-signature is `[b,m,k], [b,k,n], [g] -> [b,m,n]`. Here\n    the ragged dimension is an lhs/rhs batch dimension (`b`).",
    "inputs": [
      { "name": "lhs", "type": "HLO_AnyTensor" },
      { "name": "rhs", "type": "HLO_AnyTensor" },
      { "name": "group_sizes", "type": "Arg" },
      { "name": "ragged_dot_dimension_numbers", "type": "CHLO_RaggedDotDimensionNumbers" }
    ],
    "outputs": [
      { "name": "result", "type": "HLO_AnyTensor" }
    ],
    "attributes": [
      { "name": "precision_config", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "chlo.sinh",
    "summary": "Sinh operation",
    "description": "Returns `Sinh(operand)` element-wise.\n\n    $$\n    \\sinh(x) = (e^x - e^-x) / 2                     if |x| < 1\n             = e^(x + log(1/2)) - e^(-x + log(1/2)) otherwise.\n    $$",
    "inputs": [
      { "name": "operand", "type": "ArgTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "ResultTensorType" }
    ],
    "assemblyFormat": "$operand attr-dict `:` type($operand) `->` type($result)"
  },
  {
    "name": "chlo.square",
    "summary": "Square operation",
    "description": "Returns `Square(operand)` element-wise.\n\n    $$\n    \\square(x) = complex((x.real - x.imag) * (x.real + x.imag), x.real * x.imag * 2) if x is a complex number\n               = x * x                                                               otherwise\n    $$",
    "inputs": [
      { "name": "operand", "type": "ArgTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "ResultTensorType" }
    ],
    "assemblyFormat": "$operand attr-dict `:` type($operand) `->` type($result)"
  },
  {
    "name": "chlo.tan",
    "summary": "Tan operation",
    "description": "Returns `Tan(operand)` element-wise.\n\n    $$\n    \\tan(x) = \\sin(x) / \\cos(x)\n    $$",
    "inputs": [
      { "name": "operand", "type": "ArgTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "ResultTensorType" }
    ],
    "assemblyFormat": "$operand attr-dict `:` type($operand) `->` type($result)"
  },
  {
    "name": "chlo.top_k",
    "summary": "Finds values and indices of the `k` largest elements for the last dimension",
    "description": "If the input is a vector (rank-1), finds the `k` largest entries in the\n    vector and outputs their values and indices as vectors.  Thus `values[j]` is\n    the `j`-th largest entry in `input`, and its index is `indices[j]`.\n\n    For matrices (resp. higher rank input), computes the top `k` entries in each\n    row (resp. vector along the last dimension).  Thus,\n\n    ```\n    values.shape = indices.shape = input.shape[:-1] + [k]\n    ```\n\n    If two elements are equal, the lower-index element appears first.",
    "inputs": [
      { "name": "operand", "type": "Arg" },
      { "name": "k", "type": "Arg" }
    ],
    "outputs": [
      { "name": "values", "type": "HLO_AnyTensor" },
      { "name": "indices", "type": "HLO_AnyTensor" }
    ],
    "assemblyFormat": "`(`$operand `,` `k` `=` $k`)` attr-dict `:`\n    type($operand) `->` `(`type($values)`,` type($indices)`)`"
  },
  {
    "name": "chlo.zeta",
    "summary": "Hurwitz zeta function",
    "description": "Returns `Zeta(operand, operand)` element-wise.\n\n    $$\n    \\(\\zeta(x, q) = \\sum_{n=0}^{\\infty} (q + n)^{-x}\\)\n    $$",
    "inputs": [
      { "name": "x", "type": "HLO_AnyFpTensor" },
      { "name": "q", "type": "HLO_AnyFpTensor" }
    ],
    "outputs": [
      { "name": "result", "type": "HLO_AnyFpTensor" }
    ],
    "assemblyFormat": "$x `,` $q attr-dict `:` type($x) `,` type($q) `->` type(results)"
  },
  {
    "name": "flow.call",
    "summary": "Calls a streamable external host function.",
    "description": "Calls a function taking/returning tensor values with stream semantics.\n    Tensors have their shapes captured and may be tied to denote in-place\n    operations. Asynchronous calls must have no side-effects.\n\n    Note that returned tensors must have their shapes declared prior to the call\n    as this is what allows the call to be made on the stream. If external host\n    logic is required to compute the shape (avoid at all costs!) a separate\n    func.call can be used outside of the stream to do so. If shapes are\n    unknowable until the operation is performed it should be made as a normal\n    asynchronous host call with 'coarse-fences' instead.",
    "inputs": [
      { "name": "arguments", "type": "Variadic" },
      { "name": "argument_dims", "type": "FLOW_ShapeDynamicDims" },
      { "name": "result_dims", "type": "FLOW_ShapeDynamicDims" }
    ],
    "outputs": [
      { "name": "results", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "callee", "type": "FlatSymbolRefAttr" },
      { "name": "tied_operands", "type": "OptionalAttr" },
      { "name": "arg_attrs", "type": "OptionalAttr" },
      { "name": "res_attrs", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "$callee\n    `(` $arguments `)` attr-dict `:`\n    custom<ShapedFunctionType>(ref($arguments),\n                               type($arguments), $argument_dims,\n                               type($results), $result_dims,\n                               $tied_operands)"
  },
  {
    "name": "flow.channel.count",
    "summary": "Returns the total number of participants in the group.",
    "description": "Returns the total participant count in the collective communicator group.",
    "inputs": [
      { "name": "channel", "type": "FLOW_Channel" }
    ],
    "outputs": [
      { "name": "result", "type": "Index" }
    ],
    "assemblyFormat": "$channel `:` type($result)\n    attr-dict-with-keyword"
  },
  {
    "name": "flow.channel.default",
    "summary": "Returns a default collective communication channel.",
    "description": "Returns a channel initialized using the runtime environment.",
    "outputs": [
      { "name": "result", "type": "FLOW_Channel" }
    ],
    "attributes": [
      { "name": "group", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "($group^)?\n    `:` type($result)\n    attr-dict-with-keyword"
  },
  {
    "name": "flow.channel.rank",
    "summary": "Returns the rank of the local participant in the group.",
    "description": "Returns the rank the channel represents as a participant in a collective\n    group in `[0, count)`.",
    "inputs": [
      { "name": "channel", "type": "FLOW_Channel" }
    ],
    "outputs": [
      { "name": "result", "type": "Index" }
    ],
    "assemblyFormat": "$channel `:` type($result)\n    attr-dict-with-keyword"
  },
  {
    "name": "flow.channel.split",
    "summary": "Splits a collective communication channel.",
    "description": "Partitions the group associated with the given channel into disjoint\n    subgroups for each unique value of color. Each new subgroup contains all\n    participants of the same color and within each subgroup the key argument\n    is used to define the rank order. When multiple participants in a group\n    use the same key the tie will be broken using their rank in the parent\n    group.",
    "inputs": [
      { "name": "channel", "type": "FLOW_Channel" },
      { "name": "color", "type": "Index" },
      { "name": "key", "type": "Index" }
    ],
    "outputs": [
      { "name": "result", "type": "FLOW_Channel" }
    ],
    "assemblyFormat": "$channel `,` $color `,` $key\n    `:` type($channel) `->` type($result)\n    attr-dict-with-keyword"
  },
  {
    "name": "flow.collective.all_gather",
    "summary": "Performs all-gather operation.",
    "description": "Gathers data from all ranks and concatenates them on the 0-th dimension.",
    "inputs": [
      { "name": "target", "type": "FLOW_Tensor" },
      { "name": "target_dims", "type": "FLOW_ShapeDynamicDims" },
      { "name": "source", "type": "FLOW_Tensor" },
      { "name": "channel", "type": "FLOW_Channel" }
    ],
    "outputs": [
      { "name": "result", "type": "FLOW_Tensor" }
    ],
    "attributes": [
      { "name": "element_type", "type": "FLOW_CollectiveElementTypeAttr" },
      { "name": "tied_operands", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "$element_type `,` $target `,` $source `,` $channel `:`\n    `(` type($target) `,` type($source) `,` type($channel) `)` `->`\n    custom<ShapedTiedResult>(type($result), $target_dims, $tied_operands)\n    attr-dict-with-keyword"
  },
  {
    "name": "flow.collective.all_reduce",
    "summary": "Performs all-reduce operation.",
    "description": "Reduces data across all the ranks in the channel.",
    "inputs": [
      { "name": "target", "type": "FLOW_Tensor" },
      { "name": "target_dims", "type": "FLOW_ShapeDynamicDims" },
      { "name": "source", "type": "FLOW_Tensor" },
      { "name": "channel", "type": "FLOW_Channel" }
    ],
    "outputs": [
      { "name": "result", "type": "FLOW_Tensor" }
    ],
    "attributes": [
      { "name": "reduction_op", "type": "FLOW_CollectiveReductionOpAttr" },
      { "name": "element_type", "type": "FLOW_CollectiveElementTypeAttr" },
      { "name": "tied_operands", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "$reduction_op `,` $element_type `,` $target `,` $source `,` $channel `:`\n    `(` type($target) `,` type($source) `,` type($channel) `)` `->`\n    custom<ShapedTiedResult>(type($result), $target_dims, $tied_operands)\n    attr-dict-with-keyword"
  },
  {
    "name": "flow.collective.all_to_all",
    "summary": "Performs all-to-all operation.",
    "description": "This operation mutually exchanges data acrosss all of the ranks in the channel.",
    "inputs": [
      { "name": "target", "type": "FLOW_Tensor" },
      { "name": "target_dims", "type": "FLOW_ShapeDynamicDims" },
      { "name": "source", "type": "FLOW_Tensor" },
      { "name": "channel", "type": "FLOW_Channel" }
    ],
    "outputs": [
      { "name": "result", "type": "FLOW_Tensor" }
    ],
    "attributes": [
      { "name": "element_type", "type": "FLOW_CollectiveElementTypeAttr" },
      { "name": "tied_operands", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "$element_type `,` $target `,` $source `,` $channel `:`\n    `(` type($target) `,` type($source) `,` type($channel) `)` `->`\n    custom<ShapedTiedResult>(type($result), $target_dims, $tied_operands)\n    attr-dict-with-keyword"
  },
  {
    "name": "flow.collective.reduce_scatter",
    "summary": "Performs reduce and scatter operations.",
    "description": "The operation reduces data across all the ranks in the channel and\n    scatters the result to each rank.",
    "inputs": [
      { "name": "target", "type": "FLOW_Tensor" },
      { "name": "target_dims", "type": "FLOW_ShapeDynamicDims" },
      { "name": "source", "type": "FLOW_Tensor" },
      { "name": "channel", "type": "FLOW_Channel" }
    ],
    "outputs": [
      { "name": "result", "type": "FLOW_Tensor" }
    ],
    "attributes": [
      { "name": "reduction_op", "type": "FLOW_CollectiveReductionOpAttr" },
      { "name": "element_type", "type": "FLOW_CollectiveElementTypeAttr" },
      { "name": "tied_operands", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "$reduction_op `,` $element_type `,` $target `,` $source `,` $channel `:`\n    `(` type($target) `,` type($source) `,` type($channel) `)` `->`\n    custom<ShapedTiedResult>(type($result), $target_dims, $tied_operands)\n    attr-dict-with-keyword"
  },
  {
    "name": "flow.collective.send_recv",
    "summary": "Performs a grouped send and receive operation.",
    "description": "The operation sends data to the rank specificied by send\n    and receives data from the rank specified by recv. If send is -1, this rank\n    will not send any data. If recv is -1, this rank will not receive any data\n    and the output will be all zeros.",
    "inputs": [
      { "name": "target", "type": "FLOW_Tensor" },
      { "name": "target_dims", "type": "FLOW_ShapeDynamicDims" },
      { "name": "source", "type": "FLOW_Tensor" },
      { "name": "channel", "type": "FLOW_Channel" },
      { "name": "send", "type": "Index" },
      { "name": "recv", "type": "Index" }
    ],
    "outputs": [
      { "name": "result", "type": "FLOW_Tensor" }
    ],
    "attributes": [
      { "name": "element_type", "type": "FLOW_CollectiveElementTypeAttr" },
      { "name": "tied_operands", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "$element_type `,` $target `,` $source `,` $channel `,` $send `,` $recv `:`\n    `(` type($target) `,` type($source) `,` type($channel) `,` type($send) `,` type($recv) `)` `->`\n    custom<ShapedTiedResult>(type($result), $target_dims, $tied_operands)\n    attr-dict-with-keyword"
  },
  {
    "name": "flow.dispatch",
    "summary": "A dispatch of workgroups across a grid.",
    "description": "Dispatches workgroups across an grid defined by the captured workload\n    parameters carrying the information required to compute the workgroup count\n    at runtime. The function for converting the workload into a 3D workgroup\n    count is attached to the dispatch entry point and may contain\n    arbitrary host logic.",
    "inputs": [
      { "name": "workload", "type": "Variadic" },
      { "name": "arguments", "type": "Variadic" },
      { "name": "argument_dims", "type": "FLOW_ShapeDynamicDims" },
      { "name": "result_dims", "type": "FLOW_ShapeDynamicDims" }
    ],
    "outputs": [
      { "name": "results", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "entry_points", "type": "SymbolRefArrayAttr" },
      { "name": "tied_operands", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "custom<DispatchEntryPoints>($entry_points)\n    (`[` $workload^ `]`)? ``\n    `(` $arguments `)` attr-dict `:`\n    custom<ShapedFunctionType>(ref($arguments),\n                               type($arguments), $argument_dims,\n                               type($results), $result_dims,\n                               $tied_operands)"
  },
  {
    "name": "flow.dispatch.region",
    "summary": "A grouping of ops with implicit capture.",
    "description": "This op is a container/grouping of ops. It represents a fusion group before\n    being lowered to a dispatch region. Ops are collected inside of the region\n    body of the op. Values from parent regions can be captured. Results are\n    yielded with a `return` terminator and returned from this op.\n\n    `dispatch.region` ops are lowered to `dispatch.workgroups` ops. Workgroups\n    isolated from above. `dispatch.region` ops are a more lightweight\n    abstraction for implementing fusion heuristics, i.e., the process of\n    deciding which ops should form a dispatch region.\n\n    This op also has a second region: `workload_count`. The arguments to the\n    region represent the workload for the dispatch, and returns the number of\n    workgroups for the dispatch. The region is lowered directly to\n    `workload_count` region of `dispatch.workgroups`.",
    "inputs": [
      { "name": "result_dims", "type": "FLOW_ShapeDynamicDims" },
      { "name": "workload", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "result", "type": "Variadic" }
    ]
  },
  {
    "name": "flow.dispatch.tie_shape",
    "summary": "Ties a runtime shape to a dispatch I/O argument.",
    "description": "Metadata op used to tie a runtime-computed shape with dynamic dimensions to\n    a dispatch input/output argument. All uses of the argument should use the\n    pass-through result of this op to allow for SSA-based shape resolution.",
    "inputs": [
      { "name": "operand", "type": "IREETensorExt_DispatchTensor" },
      { "name": "dynamic_dims", "type": "FLOW_ShapeDynamicDims" }
    ],
    "outputs": [
      { "name": "result", "type": "IREETensorExt_DispatchTensor" }
    ],
    "assemblyFormat": "$operand attr-dict\n    `:` type($result) (`{` $dynamic_dims^ `}`)?"
  },
  {
    "name": "flow.dispatch.workgroup.count",
    "summary": "Returns the total workgroup count of the grid.",
    "description": "The total number of workgroups along each dimension in the dispatch grid.\n\n    Represented as a 3D grid classically written as XYZ.\n    Corresponds to the `NumWorkgroups` SPIR-V built-in and the `gridDim` CUDA\n    built-in variable.\n\n    ```mlir\n    %x = flow.dispatch.workgroup.count[0] : index\n    %y = flow.dispatch.workgroup.count[1] : index\n    %z = flow.dispatch.workgroup.count[2] : index\n    ```",
    "outputs": [
      { "name": "result", "type": "FLOW_Dim" }
    ],
    "attributes": [
      { "name": "dimension", "type": "IndexAttr" }
    ],
    "assemblyFormat": "`[` $dimension `]` attr-dict `:` type($result)"
  },
  {
    "name": "flow.dispatch.workgroup.id",
    "summary": "Returns the index of the current workgroup in the grid.",
    "description": "The global workgroup ID of the current workgroup in the range of\n    `[0, flow.dispatch.workgroup.count)` along each dimension.\n\n    Represented as a 3D grid classically written as XYZ.\n    Corresponds to the `WorkgroupId` SPIR-V built-in and the `blockIdx` CUDA\n    built-in variable.\n\n    ```mlir\n    %x = flow.dispatch.workgroup.id[0] : index\n    %y = flow.dispatch.workgroup.id[1] : index\n    %z = flow.dispatch.workgroup.id[2] : index\n    ```",
    "outputs": [
      { "name": "result", "type": "FLOW_Dim" }
    ],
    "attributes": [
      { "name": "dimension", "type": "IndexAttr" }
    ],
    "assemblyFormat": "`[` $dimension `]` attr-dict `:` type($result)"
  },
  {
    "name": "flow.dispatch.workgroup.size",
    "summary": "Returns the size of each workgroup in invocations.",
    "description": "The number of local invocations within the current workgroup along each\n    dimension. Depending on backend this may map to the SIMT thread count or\n    inner loop nest parameters.\n\n    Workgroup sizes are not determined at the flow dialect level as they are\n    dependent on the target backend determined when lowering into the HAL. It's\n    still possible to use the symbolic workgroup size inside of dispatch\n    executables as a placeholder for the resolved value once in the HAL.\n\n    Represented as a 3D grid classically written as XYZ.\n    Corresponds to the `WorkgroupSize` SPIR-V built-in and the `blockDim` CUDA\n    built-in variable.\n\n    ```mlir\n    %x = flow.dispatch.workgroup.size[0] : index\n    %y = flow.dispatch.workgroup.size[1] : index\n    %z = flow.dispatch.workgroup.size[2] : index\n    ```",
    "outputs": [
      { "name": "result", "type": "FLOW_Dim" }
    ],
    "attributes": [
      { "name": "dimension", "type": "IndexAttr" }
    ],
    "assemblyFormat": "`[` $dimension `]` attr-dict `:` type($result)"
  },
  {
    "name": "flow.dispatch.workgroups",
    "summary": "A dispatch of workgroups across a 3-dimensional grid.",
    "description": "Dispatches some number of workgroups across a 3-dimensional grid. The\n    body region will be invoked for each workgroup with a unique\n    `flow.dispatch.workgroup.id` in the range of\n    `[0, flow.dispatch.workgroup.count)` (along each dimension XYZ).\n\n    From the outside the dispatch operation has value semantics: some tensors\n    (and optionally other primitive types) are consumed and one or more new\n    result tensors are produced. Inside each workgroup, however, the input and\n    output tensors are available for arbitrary loads and stores. In many cases\n    each workgroup will load some particular tile(s) from the input tensors and\n    store some particular tile(s) to the output tensors unique to that\n    workgroup. Though it's possible for multiple workgroups to load the same\n    regions of the input tensors behavior is undefined if multiple workgroups\n    store to the same regions of the output tensors.\n\n    Though the representation is similar to the GPU-style grid dispatch model\n    here we still have not yet allocated buffers, determined the target device\n    for execution, or even completed fully resolving shapes/types/etc. Because\n    of this it's important that the workgroup body use the\n    `flow.dispatch.workgroup.*` ops to query the workgroup ID/count/size instead\n    of hardcoding them to a particular set of values. Assume that any workgroup\n    dispatch may end up being specialized for several different target devices\n    and even several different variants for a particular target device\n    (differing workgroup sizes, etc).\n\n    Because at this point in the layering devices have not yet been selected the\n    workgroup count cannot be fully evaluated. Instead workload parameters are\n    captured that are then passed to a function that when later evaluated\n    computes the actual workgroup count based on target information. The\n    workload is not limited to the 3D XYZ grid dispatch of the workgroup count\n    and can contain any number of parameters used to compute it.\n\n    ```mlir\n    %r = flow.dispatch.workgroups[%c5, %c5](%0, %1)\n        : (tensor<5x5xf32>, tensor<5xf32>) -> tensor<5x5xf32> =\n              (%arg0: !iree_tensor_ext.dispatch.tensor<readonly:tensor<5x5xf32>>,\n               %arg1: !iree_tensor_ext.dispatch.tensor<readonly:tensor<5xf32>>,\n               %arg2: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<5x5xf32>>) {\n      ...\n    }\n    ```\n\n    The number of results of the operation is equal to the number of results\n    in the type signature (`(tensor<5x5xf32>, tensor<5xf32>) -> tensor<5x5xf32>`).\n    Each tensor argument and result in the type signature has a corresponding\n    block argument of type `!iree_tensor_ext.dispatch.tensor`. Furthermore, each argument\n    has a corresponding `arguments` operand.\n\n    There are no `arguments` operands for results, but a result can be tied an\n    argument by writing the argument operand's SSA value instead of its type:\n    E.g., in the above example, `-> %0` would tie the first argument to the\n    result. In that case, there would be no separate block argument for the\n    result.",
    "inputs": [
      { "name": "workload", "type": "Variadic" },
      { "name": "arguments", "type": "Variadic" },
      { "name": "argument_dims", "type": "FLOW_ShapeDynamicDims" },
      { "name": "result_dims", "type": "FLOW_ShapeDynamicDims" }
    ],
    "outputs": [
      { "name": "results", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "tied_operands", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "(`[` $workload^ `]`)? ``\n    `(` $arguments `)` `:`\n    custom<ShapedFunctionType>(ref($arguments),\n                               type($arguments), $argument_dims,\n                               type($results), $result_dims,\n                               $tied_operands)\n    attr-dict-with-keyword\n    `=` `\\n` ` ` ` ` ` `\n    custom<DispatchWorkgroupBody>(ref(type($arguments)),\n                                  ref(type($results)),\n                                  $workgroup_body)\n    `` custom<DispatchWorkgroupsCountRegion>($workgroup_count)"
  },
  {
    "name": "flow.executable",
    "summary": "Generic executable module.",
    "description": "An executable module containing one or more public functions. The contents\n    of the functions are safe to dispatch and can be lowered further to\n    target-specific backend IR representations.",
    "attributes": [
      { "name": "sym_visibility", "type": "OptionalAttr" },
      { "name": "sym_name", "type": "SymbolNameAttr" }
    ],
    "assemblyFormat": "custom<SymbolVisibility>($sym_visibility)\n    $sym_name\n    attr-dict-with-keyword\n    regions"
  },
  {
    "name": "flow.executable_end",
    "summary": "Terminator pseudo-op for the executable op.",
    "assemblyFormat": "attr-dict"
  },
  {
    "name": "flow.executable.export",
    "summary": "Defines an executable entry point for dispatch operations.",
    "description": "Specifies an exported function with an externally-visible alias. Multiple\n    exports can reference the same internal function.\n\n    Each entry point can have a unique workgroup count calculation region.\n    This region takes the workload parameters passed to each flow.dispatch and\n    produces an XYZ workgroup count for the 3D grid dispatch.",
    "attributes": [
      { "name": "sym_visibility", "type": "OptionalAttr" },
      { "name": "sym_name", "type": "SymbolNameAttr" },
      { "name": "function_ref", "type": "FlatSymbolRefAttr" }
    ],
    "assemblyFormat": "custom<SymbolVisibility>($sym_visibility)\n    custom<SymbolAlias>($sym_name, $function_ref)\n    custom<WorkgroupCountRegion>($workgroup_count)\n    attr-dict-with-keyword"
  },
  {
    "name": "flow.func",
    "summary": "Streamable function declaration.",
    "description": "Declares a function that can be called as an asynchronous streaming\n    operation via `flow.call`. Today only external functions are allowed.",
    "attributes": [
      { "name": "sym_name", "type": "SymbolNameAttr" },
      { "name": "function_type", "type": "TypeAttrOf" },
      { "name": "tied_operands", "type": "OptionalAttr" },
      { "name": "sym_visibility", "type": "OptionalAttr" },
      { "name": "arg_attrs", "type": "OptionalAttr" },
      { "name": "res_attrs", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "custom<SymbolVisibility>($sym_visibility)\n    $sym_name\n    ``\n    custom<ShapedFunctionSignature>($function_type,\n                                    $tied_operands,\n                                    $arg_attrs,\n                                    $res_attrs)\n    attr-dict-with-keyword\n    ($body^)?"
  },
  {
    "name": "flow.return",
    "summary": "Return from a flow.dispatch_region.",
    "description": "Returns the given values from the region and back to the host code.",
    "inputs": [
      { "name": "operands", "type": "Variadic" }
    ],
    "assemblyFormat": "attr-dict ($operands^ `:` type($operands))?"
  },
  {
    "name": "flow.tensor.alloca",
    "summary": "An empty tensor allocation with undefined contents.",
    "description": "Returns a new transient tensor allocation with undefined contents.\n    Subsequent writes must populate any ranges of the tensor that are later\n    read. The resulting tensor may be long-lived and allocated as part of a\n    dedicated allocation. Prefer using `flow.tensor.empty` whenever possible as\n    this op disables nearly all allocation-related optimizations performed by\n    the compiler. The presence of this op is often an indication of an improper\n    lowering.",
    "inputs": [
      { "name": "result_dims", "type": "FLOW_ShapeDynamicDims" }
    ],
    "outputs": [
      { "name": "result", "type": "FLOW_Tensor" }
    ],
    "assemblyFormat": "`:` type($result) (`{` $result_dims^ `}`)?\n    attr-dict-with-keyword"
  },
  {
    "name": "flow.tensor.barrier",
    "summary": "Indicates a value that must have a specific affinity.",
    "description": "Prevents fusion and scheduling of a value across an affinity boundary.\n    May introduce copy-on-write behavior if the operand value is used as well as\n    the result and users should try to keep the operand to a single use by this\n    op.",
    "inputs": [
      { "name": "operand", "type": "FLOW_Tensor" },
      { "name": "operand_dims", "type": "FLOW_ShapeDynamicDims" }
    ],
    "outputs": [
      { "name": "result", "type": "FLOW_Tensor" }
    ],
    "attributes": [
      { "name": "target", "type": "AnyAttr" }
    ],
    "assemblyFormat": "$operand `:` type($result) (`{` $operand_dims^ `}`)?\n    `on` $target\n    attr-dict-with-keyword"
  },
  {
    "name": "flow.tensor.bitcast",
    "summary": "Bitcasts a tensor without modifying the contents.",
    "description": "Bitcasts a tensor |source| to the shape implied by this operations result\n    type interleaved with |result_dims|, potentially with a different element\n    type. For example,\n\n    ```\n    result_dims = {%0, %1}\n    result_type = tensor<1x?x2x?x3 x!eltype>\n    ```\n\n    produces a tensor of shape [1, %0, 2, %1, 3] and element type `!eltype`.\n    Note that the source and result tensors must serialized to the same size.",
    "inputs": [
      { "name": "source", "type": "FLOW_Tensor" },
      { "name": "source_dims", "type": "FLOW_ShapeDynamicDims" },
      { "name": "result_dims", "type": "FLOW_ShapeDynamicDims" }
    ],
    "outputs": [
      { "name": "result", "type": "FLOW_Tensor" }
    ],
    "assemblyFormat": "$source `:`\n    type($source) (`{` $source_dims^ `}`)? `->`\n    type($result) (`{` $result_dims^ `}`)?\n    attr-dict-with-keyword"
  },
  {
    "name": "flow.tensor.clone",
    "summary": "Performs a full tensor clone operation.",
    "description": "Clones the input tensor into an identical output tensor.",
    "inputs": [
      { "name": "operand", "type": "FLOW_Tensor" },
      { "name": "operand_dims", "type": "FLOW_ShapeDynamicDims" }
    ],
    "outputs": [
      { "name": "result", "type": "FLOW_Tensor" }
    ],
    "assemblyFormat": "$operand `:` type($result) (`{` $operand_dims^ `}`)?\n    attr-dict-with-keyword"
  },
  {
    "name": "flow.tensor.constant",
    "summary": "Tensor constant that can have dynamic dimensions.",
    "description": "Allows specifying a tensor constant of IREE-specific types/attributes.\n\n    ```mlir\n    %cst = flow.tensor.constant #something_tensor_like : tensor<2x2xf32>\n    %res = math.absf %cst : tensor<2x2xf32>\n    ```",
    "outputs": [
      { "name": "result", "type": "AnyTensor" }
    ],
    "attributes": [
      { "name": "value", "type": "TypedAttrInterface" }
    ],
    "assemblyFormat": "attr-dict $value"
  },
  {
    "name": "flow.tensor.dynamic_constant",
    "summary": "Tensor constant that can have dynamic dimensions.",
    "description": "Allows specifying a tensor constant of IREE-specific types/attributes with\n    a dynamic shape that approximates a value as passed from the user. This\n    disables many optimizations and should only be used when testing or\n    benchmarking and wanting to ensure that dynamic dimension behavior is\n    preserved.\n\n    ```mlir\n    %cst = flow.tensor.dynamic_constant #something_tensor_like : tensor<2x2xf32> -> tensor<?x2xf32>\n    %res = math.absf %cst : tensor<?x2xf32>\n    ```",
    "outputs": [
      { "name": "result", "type": "AnyTensor" }
    ],
    "attributes": [
      { "name": "value", "type": "TypedAttrInterface" }
    ],
    "assemblyFormat": "attr-dict $value `->` type($result)"
  },
  {
    "name": "flow.tensor.empty",
    "summary": "An empty tensor carrying metadata but no contents.",
    "description": "Returns a tensor with undefined contents. Subsequent writes must populate\n    any ranges of the tensor that are later read.",
    "inputs": [
      { "name": "result_dims", "type": "FLOW_ShapeDynamicDims" }
    ],
    "outputs": [
      { "name": "result", "type": "FLOW_Tensor" }
    ],
    "assemblyFormat": "`:` type($result) (`{` $result_dims^ `}`)?\n    attr-dict-with-keyword"
  },
  {
    "name": "flow.tensor.encode",
    "summary": "Performs a full tensor encode operation.",
    "description": "Encode the input tensor into an encoded output tensor.",
    "inputs": [
      { "name": "operand", "type": "FLOW_Tensor" },
      { "name": "operand_dims", "type": "FLOW_ShapeDynamicDims" },
      { "name": "result_dims", "type": "FLOW_ShapeDynamicDims" }
    ],
    "outputs": [
      { "name": "result", "type": "FLOW_Tensor" }
    ],
    "assemblyFormat": "$operand `:`\n    type($operand) (`{` $operand_dims^ `}`)? `->`\n    type($result) (`{` $result_dims^ `}`)?\n    attr-dict-with-keyword"
  },
  {
    "name": "flow.tensor.load",
    "summary": "Loads a value from a tensor element.",
    "description": "Returns the element at the given location from within the tensor.",
    "inputs": [
      { "name": "source", "type": "FLOW_Tensor" },
      { "name": "source_dims", "type": "FLOW_ShapeDynamicDims" },
      { "name": "indices", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTypeOf" }
    ],
    "assemblyFormat": "$source (`[` $indices^ `]`)? `:`\n    type($source) (`{` $source_dims^ `}`)?\n    attr-dict-with-keyword"
  },
  {
    "name": "flow.tensor.reshape",
    "summary": "Reshapes a tensor without modifying the contents.",
    "description": "Reshapes a tensor |source| to the shape implied by this operations result\n    type interleaved with |result_dims|. For example,\n\n    ```\n    result_dims = {%0, %1}\n    result_type = tensor<1x?x2x?x3>\n    ```\n\n    produces a tensor of shape [1, %0, 2, %1, 3] and the same element type.",
    "inputs": [
      { "name": "source", "type": "FLOW_Tensor" },
      { "name": "source_dims", "type": "FLOW_ShapeDynamicDims" },
      { "name": "result_dims", "type": "FLOW_ShapeDynamicDims" }
    ],
    "outputs": [
      { "name": "result", "type": "FLOW_Tensor" }
    ],
    "assemblyFormat": "$source `:`\n    type($source) (`{` $source_dims^ `}`)? `->`\n    type($result) (`{` $result_dims^ `}`)?\n    attr-dict-with-keyword"
  },
  {
    "name": "flow.tensor.slice",
    "summary": "Clones a subregion of a tensor.",
    "inputs": [
      { "name": "source", "type": "FLOW_Tensor" },
      { "name": "source_dims", "type": "FLOW_ShapeDynamicDims" },
      { "name": "start_indices", "type": "Variadic" },
      { "name": "lengths", "type": "Variadic" },
      { "name": "result_dims", "type": "FLOW_ShapeDynamicDims" }
    ],
    "outputs": [
      { "name": "result", "type": "FLOW_Tensor" }
    ],
    "assemblyFormat": "$source `[` $start_indices `for` $lengths `]` `:`\n    type($source) (`{` $source_dims^ `}`)? `->`\n    type($result) (`{` $result_dims^ `}`)?\n    attr-dict-with-keyword"
  },
  {
    "name": "flow.tensor.splat",
    "summary": "Splats a value into a shaped tensor.",
    "description": "Returns a tensor initialized to the given primitive value.",
    "inputs": [
      { "name": "value", "type": "FLOW_PrimitiveType" },
      { "name": "result_dims", "type": "FLOW_ShapeDynamicDims" }
    ],
    "outputs": [
      { "name": "result", "type": "FLOW_Tensor" }
    ],
    "assemblyFormat": "$value `:` type($result) (`{` $result_dims^ `}`)?\n    attr-dict-with-keyword"
  },
  {
    "name": "flow.tensor.store",
    "summary": "Stores a value into a tensor element.",
    "description": "Returns a tensor with the element at the given index set to the given value.",
    "inputs": [
      { "name": "value", "type": "AnyTypeOf" },
      { "name": "target", "type": "FLOW_Tensor" },
      { "name": "target_dims", "type": "FLOW_ShapeDynamicDims" },
      { "name": "indices", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "result", "type": "FLOW_Tensor" }
    ],
    "assemblyFormat": "$value `,` $target (`[` $indices^ `]`)? `:`\n    type($target) (`{` $target_dims^ `}`)?\n    attr-dict-with-keyword"
  },
  {
    "name": "flow.tensor.tie_shape",
    "summary": "Ties a runtime shape to a tensor value.",
    "description": "Metadata op used to tie tensors with their runtime-computed dynamic\n    dimensions. This only exists transiently in the IR as a witness to shape\n    calculations and is removed during lowering.",
    "inputs": [
      { "name": "operand", "type": "FLOW_Tensor" },
      { "name": "dynamic_dims", "type": "FLOW_ShapeDynamicDims" }
    ],
    "outputs": [
      { "name": "result", "type": "FLOW_Tensor" }
    ],
    "assemblyFormat": "$operand attr-dict\n    `:` type($result) (`{` $dynamic_dims^ `}`)?"
  },
  {
    "name": "flow.tensor.trace",
    "summary": "Traces one or more tensor values at runtime.",
    "description": "Traces out to a runtime trace sink (console, log file, etc) the given\n    tensors. The key is arbitrary and can be used for identifying the set of\n    values being traced.",
    "inputs": [
      { "name": "values", "type": "Variadic" },
      { "name": "value_dims", "type": "FLOW_ShapeDynamicDims" }
    ],
    "attributes": [
      { "name": "key", "type": "StrAttr" }
    ],
    "assemblyFormat": "$key `=` `[`\n    custom<ShapedOperandList>($values, type($values), $value_dims)\n    `]` attr-dict-with-keyword"
  },
  {
    "name": "flow.tensor.transfer",
    "summary": "Transfers a tensor to a target by copying if needed.",
    "description": "Transfers the tensor from whichever context it may be in to the specified\n    target context. If the contexts are compatible and can access each others\n    memory the operation may be elided and otherwise will become one or more\n    copies to transfer the tensor in cases where staging through an intermediate\n    context is required.",
    "inputs": [
      { "name": "operand", "type": "FLOW_Tensor" },
      { "name": "operand_dims", "type": "FLOW_ShapeDynamicDims" }
    ],
    "outputs": [
      { "name": "result", "type": "FLOW_Tensor" }
    ],
    "attributes": [
      { "name": "target", "type": "AnyAttr" }
    ],
    "assemblyFormat": "$operand `:` type($result) (`{` $operand_dims^ `}`)?\n    `to` $target\n    attr-dict-with-keyword"
  },
  {
    "name": "flow.tensor.update",
    "summary": "Updates a tensor with the contents of another tensor.",
    "description": "Updates the target tensor with the contents of the update tensor at the\n    given offset indices.",
    "inputs": [
      { "name": "target", "type": "FLOW_Tensor" },
      { "name": "target_dims", "type": "FLOW_ShapeDynamicDims" },
      { "name": "start_indices", "type": "Variadic" },
      { "name": "update", "type": "FLOW_Tensor" },
      { "name": "update_dims", "type": "FLOW_ShapeDynamicDims" }
    ],
    "outputs": [
      { "name": "result", "type": "FLOW_Tensor" }
    ],
    "assemblyFormat": "$update `,` $target `[` $start_indices `]` `:`\n    type($update) (`{` $update_dims^ `}`)? `->`\n    custom<ShapedTiedResult>(type($result), $target_dims)\n    attr-dict-with-keyword"
  },
  {
    "name": "func.call",
    "summary": "call operation",
    "description": "The `func.call` operation represents a direct call to a function that is\n    within the same symbol scope as the call. The operands and result types of\n    the call must match the specified function type. The callee is encoded as a\n    symbol reference attribute named \"callee\".\n\n    Example:\n\n    ```mlir\n    %2 = func.call @my_add(%0, %1) : (f32, f32) -> f32\n    ```",
    "inputs": [
      { "name": "operands", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "callee", "type": "FlatSymbolRefAttr" },
      { "name": "arg_attrs", "type": "OptionalAttr" },
      { "name": "res_attrs", "type": "OptionalAttr" },
      { "name": "no_inline", "type": "UnitAttr" }
    ],
    "assemblyFormat": "$callee `(` $operands `)` attr-dict `:` functional-type($operands, results)"
  },
  {
    "name": "func.call_indirect",
    "summary": "indirect call operation",
    "description": "The `func.call_indirect` operation represents an indirect call to a value\n    of function type. The operands and result types of the call must match the\n    specified function type.\n\n    Function values can be created with the\n    [`func.constant` operation](#funcconstant-constantop).\n\n    Example:\n\n    ```mlir\n    %func = func.constant @my_func : (tensor<16xf32>, tensor<16xf32>) -> tensor<16xf32>\n    %result = func.call_indirect %func(%0, %1) : (tensor<16xf32>, tensor<16xf32>) -> tensor<16xf32>\n    ```",
    "inputs": [
      { "name": "callee", "type": "FunctionType" },
      { "name": "callee_operands", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "results", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "arg_attrs", "type": "OptionalAttr" },
      { "name": "res_attrs", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "$callee `(` $callee_operands `)` attr-dict `:` type($callee)"
  },
  {
    "name": "func.constant",
    "summary": "constant",
    "description": "The `func.constant` operation produces an SSA value from a symbol reference\n    to a `func.func` operation\n\n    Example:\n\n    ```mlir\n    // Reference to function @myfn.\n    %2 = func.constant @myfn : (tensor<16xf32>, f32) -> tensor<16xf32>\n\n    // Equivalent generic forms\n    %2 = \"func.constant\"() { value = @myfn } : () -> ((tensor<16xf32>, f32) -> tensor<16xf32>)\n    ```\n\n    MLIR does not allow direct references to functions in SSA operands because\n    the compiler is multithreaded, and disallowing SSA values to directly\n    reference a function simplifies this\n    ([rationale](../Rationale/Rationale.md#multithreading-the-compiler)).",
    "attributes": [
      { "name": "value", "type": "FlatSymbolRefAttr" }
    ],
    "assemblyFormat": "attr-dict $value `:` type(results)"
  },
  {
    "name": "func.func",
    "summary": "An operation with a name containing a single `SSACFG` region",
    "description": "Operations within the function cannot implicitly capture values defined\n    outside of the function, i.e. Functions are `IsolatedFromAbove`. All\n    external references must use function arguments or attributes that establish\n    a symbolic connection (e.g. symbols referenced by name via a string\n    attribute like SymbolRefAttr). An external function declaration (used when\n    referring to a function declared in some other module) has no body. While\n    the MLIR textual form provides a nice inline syntax for function arguments,\n    they are internally represented as “block arguments” to the first block in\n    the region.\n\n    Only dialect attribute names may be specified in the attribute dictionaries\n    for function arguments, results, or the function itself.\n\n    Example:\n\n    ```mlir\n    // External function definitions.\n    func.func private @abort()\n    func.func private @scribble(i32, i64, memref<? x 128 x f32, #layout_map0>) -> f64\n\n    // A function that returns its argument twice:\n    func.func @count(%x: i64) -> (i64, i64)\n      attributes {fruit = \"banana\"} {\n      return %x, %x: i64, i64\n    }\n\n    // A function with an argument attribute\n    func.func private @example_fn_arg(%x: i32 {swift.self = unit})\n\n    // A function with a result attribute\n    func.func private @example_fn_result() -> (f64 {dialectName.attrName = 0 : i64})\n\n    // A function with an attribute\n    func.func private @example_fn_attr() attributes {dialectName.attrName = false}\n    ```",
    "attributes": [
      { "name": "sym_name", "type": "SymbolNameAttr" },
      { "name": "function_type", "type": "TypeAttrOf" },
      { "name": "sym_visibility", "type": "OptionalAttr" },
      { "name": "arg_attrs", "type": "OptionalAttr" },
      { "name": "res_attrs", "type": "OptionalAttr" },
      { "name": "no_inline", "type": "UnitAttr" }
    ]
  },
  {
    "name": "func.return",
    "summary": "Function return operation",
    "description": "The `func.return` operation represents a return operation within a function.\n    The operation takes variable number of operands and produces no results.\n    The operand number and types must match the signature of the function\n    that contains the operation.\n\n    Example:\n\n    ```mlir\n    func.func @foo() -> (i32, f8) {\n      ...\n      return %0, %1 : i32, f8\n    }\n    ```",
    "inputs": [
      { "name": "operands", "type": "Variadic" }
    ],
    "assemblyFormat": "attr-dict ($operands^ `:` type($operands))?"
  },
  {
    "name": "hal.allocator.allocate",
    "summary": "Empty buffer allocation operation.",
    "description": "Allocates a buffer of the given size from the allocator.\n    The size of the buffer returned may be larger than the requested size if the\n    allocator has specific alignment requirements or minimum allocation sizes.",
    "inputs": [
      { "name": "allocator", "type": "HAL_Allocator" },
      { "name": "queue_affinity", "type": "HAL_DeviceQueueAffinity" },
      { "name": "memory_types", "type": "HAL_MemoryType" },
      { "name": "buffer_usage", "type": "HAL_BufferUsage" },
      { "name": "result_size", "type": "HAL_DeviceSize" }
    ],
    "outputs": [
      { "name": "result", "type": "HAL_Buffer" }
    ],
    "assemblyFormat": "`<` $allocator `:` type($allocator) `>`\n    `affinity` `(` $queue_affinity `)`\n    `type` `(` $memory_types `)`\n    `usage` `(` $buffer_usage `)`\n    `:` custom<SizeAwareType>(type($result), $result_size)\n    attr-dict-with-keyword"
  },
  {
    "name": "hal.allocator.import",
    "summary": "Allocator-supported host buffer import operation.",
    "description": "Tries importing host memory backed by the given byte buffer into a\n    device accessible `!hal.buffer`. The returned buffer may be host-only and\n    not directly usable on devices. If the mapping cannot be completed (such as\n    trying to map the host memory as device-local on devices with discrete\n    memory) then `did_import` will indicate that the returned buffer is null.",
    "inputs": [
      { "name": "allocator", "type": "HAL_Allocator" },
      { "name": "queue_affinity", "type": "HAL_DeviceQueueAffinity" },
      { "name": "memory_types", "type": "HAL_MemoryType" },
      { "name": "buffer_usage", "type": "HAL_BufferUsage" },
      { "name": "source", "type": "Util_BufferType" },
      { "name": "offset", "type": "HAL_DeviceSize" },
      { "name": "length", "type": "HAL_DeviceSize" }
    ],
    "outputs": [
      { "name": "did_import", "type": "I1" },
      { "name": "result", "type": "HAL_Buffer" }
    ],
    "assemblyFormat": "`<` $allocator `:` type($allocator) `>`\n    `source` `(` $source `:` type($source) `)` `` `[` $offset `,` $length `]`\n    `affinity` `(` $queue_affinity `)`\n    `type` `(` $memory_types `)`\n    `usage` `(` $buffer_usage `)`\n    `:` type($did_import) `,` type($result)\n    attr-dict-with-keyword"
  },
  {
    "name": "hal.allocator.resolve_memory_properties",
    "summary": "Resolves memory properties from resource lifetime and affinity.",
    "description": "Resolves the required memory types and buffer usage for a resource with\n    the given lifetime and affinity. This operation encapsulates the logic\n    for deriving buffer properties based on stream resource semantics.\n\n    This operation can be resolved later in compilation either by canonicalization\n    for single device affinties or using the topology attribute for multiple devices.\n\n    Example:\n    ```mlir\n    %memory_types, %buffer_usage = hal.allocator.resolve_memory_properties\n        for(#hal.device.affinity<@device_a>)\n        lifetime(transient) : i32, i32\n    ```",
    "outputs": [
      { "name": "memory_types", "type": "HAL_MemoryType" },
      { "name": "buffer_usage", "type": "HAL_BufferUsage" }
    ],
    "attributes": [
      { "name": "affinity", "type": "OptionalAttr" },
      { "name": "lifetime", "type": "HAL_LifetimeAttr" }
    ],
    "assemblyFormat": "(`for` `(` $affinity^ `)`)?\n    `lifetime` `(` $lifetime `)`\n    `:` type($memory_types) `,` type($buffer_usage)\n    attr-dict-with-keyword"
  },
  {
    "name": "hal.allocator.select",
    "summary": "Selects a device/queue from the given set for allocations.",
    "description": "Chooses one device and queue affinity pair from the given optimal set that\n    can service the requested memory type and usage for all devices in the set.\n    Returns a null device if no pair is able to satisfy the conditions.",
    "inputs": [
      { "name": "devices", "type": "Variadic" },
      { "name": "queue_affinities", "type": "Variadic" },
      { "name": "memory_types", "type": "HAL_MemoryType" },
      { "name": "buffer_usage", "type": "HAL_BufferUsage" }
    ],
    "outputs": [
      { "name": "selected_device", "type": "HAL_Device" },
      { "name": "selected_queue_affinity", "type": "HAL_DeviceQueueAffinity" }
    ],
    "assemblyFormat": "`from` `(` custom<DeviceQueueAffinityList>($devices, type($devices), $queue_affinities) `)`\n    `type` `(` $memory_types `)`\n    `usage` `(` $buffer_usage `)`\n    `:`\n    type($selected_device) `,` type($selected_queue_affinity)\n    attr-dict-with-keyword"
  },
  {
    "name": "hal.buffer_usage",
    "summary": "An iree_hal_buffer_usage_t for the given usage bits.",
    "description": "Maps buffer usage bits to a runtime `iree_hal_buffer_usage_t` value.",
    "outputs": [
      { "name": "result", "type": "HAL_BufferUsage" }
    ],
    "attributes": [
      { "name": "usage", "type": "HAL_BufferUsageBitfieldAttr" }
    ],
    "assemblyFormat": "`<` $usage `>`\n    attr-dict\n    `:` type($result)"
  },
  {
    "name": "hal.buffer_view.assert",
    "summary": "Buffer view contents assertion.",
    "description": "Asserts that the buffer view contains a data compatible tensor with the\n    given encoding. Program execution will abort as if `std.assert` had been\n    used.",
    "inputs": [
      { "name": "buffer_view", "type": "HAL_BufferView" },
      { "name": "element_type", "type": "HAL_ElementType" },
      { "name": "encoding_type", "type": "HAL_EncodingType" },
      { "name": "shape", "type": "HAL_Shape" }
    ],
    "attributes": [
      { "name": "message", "type": "StrAttr" }
    ],
    "assemblyFormat": "`<` $buffer_view `:` type($buffer_view) `>`\n    `message` `(` $message `)`\n    `shape` `(` `[` $shape `]` `)`\n    `type` `(` $element_type `)`\n    `encoding` `(` $encoding_type `)`\n    attr-dict-with-keyword"
  },
  {
    "name": "hal.buffer_view.buffer",
    "summary": "Buffer view buffer accessor.",
    "description": "Returns the buffer backing this view's contents.",
    "inputs": [
      { "name": "buffer_view", "type": "HAL_BufferView" }
    ],
    "outputs": [
      { "name": "result", "type": "HAL_BufferType" }
    ],
    "assemblyFormat": "`<` $buffer_view `:` type($buffer_view) `>`\n    `:` type($result)\n    attr-dict-with-keyword"
  },
  {
    "name": "hal.buffer_view.create",
    "summary": "Buffer view reference initializer.",
    "description": "Creates a reference to a buffer with a particular shape and element type.\n    The buffer is not copied and both the original and view references must be\n    synchronized. This makes it easier to associate commonly-carried metadata\n    along with the contents.",
    "inputs": [
      { "name": "source_buffer", "type": "HAL_BufferType" },
      { "name": "source_offset", "type": "HAL_DeviceSize" },
      { "name": "source_length", "type": "HAL_DeviceSize" },
      { "name": "element_type", "type": "HAL_ElementType" },
      { "name": "encoding_type", "type": "HAL_EncodingType" },
      { "name": "shape", "type": "HAL_Shape" }
    ],
    "outputs": [
      { "name": "result", "type": "HAL_BufferView" }
    ],
    "assemblyFormat": "`buffer` `(` $source_buffer `:` type($source_buffer) `)`\n    `` `[` $source_offset `,` $source_length `]`\n    `shape` `(` `[` $shape `]` `)`\n    `type` `(` $element_type `)`\n    `encoding` `(` $encoding_type `)`\n    `:` type($result)\n    attr-dict-with-keyword"
  },
  {
    "name": "hal.buffer_view.dim",
    "summary": "Buffer view dimension value query.",
    "description": "Returns the value of the given dimension.",
    "inputs": [
      { "name": "buffer_view", "type": "HAL_BufferView" }
    ],
    "outputs": [
      { "name": "result", "type": "HAL_Dim" }
    ],
    "attributes": [
      { "name": "index", "type": "IndexAttr" }
    ],
    "assemblyFormat": "`<` $buffer_view `:` type($buffer_view) `>`\n    `` `[` $index `]`\n    `:` type($result)\n    attr-dict-with-keyword"
  },
  {
    "name": "hal.buffer_view.element_type",
    "summary": "Buffer view element type query.",
    "description": "Returns the element type of the buffer view.",
    "inputs": [
      { "name": "buffer_view", "type": "HAL_BufferView" }
    ],
    "outputs": [
      { "name": "result", "type": "HAL_ElementType" }
    ],
    "assemblyFormat": "`<` $buffer_view `:` type($buffer_view) `>`\n    `:` type($result)\n    attr-dict-with-keyword"
  },
  {
    "name": "hal.buffer_view.encoding_type",
    "summary": "Buffer view encoding type query.",
    "description": "Returns the encoding type of the buffer view.",
    "inputs": [
      { "name": "buffer_view", "type": "HAL_BufferView" }
    ],
    "outputs": [
      { "name": "result", "type": "HAL_EncodingType" }
    ],
    "assemblyFormat": "`<` $buffer_view `:` type($buffer_view) `>`\n    `:` type($result)\n    attr-dict-with-keyword"
  },
  {
    "name": "hal.buffer_view.rank",
    "summary": "Buffer view rank query.",
    "description": "Returns the rank of the buffer view.",
    "inputs": [
      { "name": "buffer_view", "type": "HAL_BufferView" }
    ],
    "outputs": [
      { "name": "result", "type": "HAL_Dim" }
    ],
    "assemblyFormat": "`<` $buffer_view `:` type($buffer_view) `>`\n    `:` type($result)\n    attr-dict-with-keyword"
  },
  {
    "name": "hal.buffer_view.trace",
    "summary": "Trace value(s) operation.",
    "description": "Traces out to a runtime trace sink (console, log file, etc) the given buffer\n    views and titles them with the given key. The key is informational only and\n    useful for titling/marking specific sets of buffers for easier searching.",
    "inputs": [
      { "name": "operands", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "key", "type": "StrAttr" }
    ],
    "assemblyFormat": "$key `=`\n    $operands `:` type($operands)\n    attr-dict-with-keyword"
  },
  {
    "name": "hal.buffer.allocation.discard",
    "summary": "Discards ownership of the underlying buffer allocation by the caller.",
    "description": "Decrementing the preserve count indicates that the owner is releasing its\n    ownership. When the last owner discards their ownership it is safe to\n    deallocate the buffer allocation even if there are still references\n    remaining to the buffer object.\n\n    Any code that _may_ receive asynchronously allocated buffers must properly\n    balance their preserves and discards. Code that will never receive\n    asynchronously allocated buffers - such as those using the inline HAL - can\n    ignore tracking as there's no asynchronous deallocation and allocation\n    lifetime is tied to buffer object lifetime. Note that unbalanced discards\n    will result in either correctness issues (buffer is deallocated too early)\n    or extended lifetime (buffer cannot be deallocated until all buffer object\n    references have been released).\n\n    Returns true if the caller was the last owner of the allocation and it can\n    now be deallocated.",
    "inputs": [
      { "name": "buffer", "type": "Arg" }
    ],
    "outputs": [
      { "name": "result", "type": "I1" }
    ],
    "assemblyFormat": "`<` $buffer `:` type($buffer) `>`\n    `:` type($result)\n    attr-dict-with-keyword"
  },
  {
    "name": "hal.buffer.allocation.is_terminal",
    "summary": "Returns true if the underlying buffer allocation has a single owner.",
    "description": "This can be used to reuse a buffer that has no other owners.\n\n    Note that an allocated buffer may have multiple suballocations referencing\n    it and this query is only for the entire allocation. When reusing a buffer\n    one should ensure the allocation size matches (or is within threshold) so\n    that a reuse of 16MB doesn't keep an underlying allocation of 16GB wired.\n\n    Since device allocators are expected to reuse memory if in doubt prefer to\n    dealloca and alloca. This method should only be used in situations where the\n    buffer types are known to the application (such as fixed input and output\n    buffers).",
    "inputs": [
      { "name": "buffer", "type": "Arg" }
    ],
    "outputs": [
      { "name": "result", "type": "I1" }
    ],
    "assemblyFormat": "`<` $buffer `:` type($buffer) `>`\n    `:` type($result)\n    attr-dict-with-keyword"
  },
  {
    "name": "hal.buffer.allocation.preserve",
    "summary": "Preserves the underlying buffer allocation for the caller.",
    "description": "Preservation is a way to track lifetime of an asynchronously-allocated\n    buffer on multiple device timelines. Incrementing the preserve count\n    indicates that there is a new co-owner of the buffer lifetime and that owner\n    must make a corresponding `hal.buffer.allocation.discard` call to release\n    their ownership and possibly deallocate the buffer.\n\n    Though intended for asynchronously-allocated buffers it is fine to preserve\n    synchronously-allocated ones. Any code that _may_ receive asynchronously\n    allocated buffers must properly balance their preserves and discards. Code\n    that will never receive asynchronously allocated buffers - such as those\n    using the inline HAL - can ignore tracking.\n\n    This preservation roughly translates to retaining logical ownership of the\n    allocation and may differ from the buffer object reference count. As an\n    example if the Python GC hasn't run there may still be several references to\n    the buffer object even after the application has stopped using the buffer.\n    Tracking the preserve count independently allows the application to eagerly\n    deallocate the buffer without relying on the lifetime of the object to do\n    so.\n\n    A preserved buffer will still be deallocated if there are no longer any\n    references to the buffer object. Preserving the buffer only prevents any\n    other owner from deallocating it while there are references outstanding.\n    See `hal.buffer.allocation.discard` for more information about releasing\n    ownership.",
    "inputs": [
      { "name": "buffer", "type": "Arg" }
    ],
    "assemblyFormat": "`<` $buffer `:` type($buffer) `>`\n    attr-dict-with-keyword"
  },
  {
    "name": "hal.buffer.assert",
    "summary": "Buffer compatibility assertion.",
    "description": "Asserts that the buffer is compatible with the given allocator and usage.\n    Program execution will abort as if `std.assert` had been used.\n\n    This only checks that the buffer can be used and not that it matches the\n    given parameters exactly. Buffers may be from other allocators so long as\n    the allocators are compatible (devices can address each other's memory),\n    the type and usage contain all the requested bits (having more bits is ok),\n    and the length is at least the requested minimum (as padding may be\n    ignored).",
    "inputs": [
      { "name": "buffer", "type": "HAL_Buffer" },
      { "name": "allocator", "type": "HAL_Allocator" },
      { "name": "minimum_length", "type": "HAL_DeviceSize" }
    ],
    "attributes": [
      { "name": "message", "type": "StrAttr" },
      { "name": "memory_types", "type": "HAL_MemoryTypeBitfieldAttr" },
      { "name": "buffer_usage", "type": "HAL_BufferUsageBitfieldAttr" }
    ],
    "assemblyFormat": "`<` $buffer `:` type($buffer) `>`\n    `message` `(` $message `)`\n    `allocator` `(` $allocator `:` type($allocator) `)`\n    `minimum_length` `(` $minimum_length `)`\n    `type` `(` $memory_types `)`\n    `usage` `(` $buffer_usage `)`\n    attr-dict-with-keyword"
  },
  {
    "name": "hal.buffer.length",
    "summary": "Buffer byte length accessor.",
    "description": "Returns the allocated size of a buffer in bytes.\n    May be less than the underlying buffer allocation if this is a subspan or\n    view into another buffer.",
    "inputs": [
      { "name": "buffer", "type": "HAL_BufferType" }
    ],
    "outputs": [
      { "name": "result", "type": "HAL_DeviceSize" }
    ],
    "assemblyFormat": "`<` $buffer `:` type($buffer) `>`\n    `:` type($result)\n    attr-dict-with-keyword"
  },
  {
    "name": "hal.buffer.load",
    "summary": "Buffer element load operation.",
    "description": "Loads a value from a buffer by mapping it.",
    "inputs": [
      { "name": "source_buffer", "type": "Arg" },
      { "name": "source_offset", "type": "HAL_DeviceSize" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTypeOf" }
    ],
    "assemblyFormat": "`<` $source_buffer `:` type($source_buffer) `>`\n    `` `[` $source_offset `]`\n    `:` type($result)\n    attr-dict-with-keyword"
  },
  {
    "name": "hal.buffer.store",
    "summary": "Buffer element store operation.",
    "description": "Stores a value into a buffer by mapping it.",
    "inputs": [
      { "name": "value", "type": "AnyTypeOf" },
      { "name": "target_buffer", "type": "Arg" },
      { "name": "target_offset", "type": "HAL_DeviceSize" }
    ],
    "assemblyFormat": "`<` $target_buffer `:` type($target_buffer) `>`\n    `` `[` $target_offset `]`\n    `value` `(` $value `:` type($value) `)`\n    attr-dict-with-keyword"
  },
  {
    "name": "hal.buffer.subspan",
    "summary": "Buffer subspan operation.",
    "description": "Returns a reference to a subspan of the buffer.",
    "inputs": [
      { "name": "source_buffer", "type": "HAL_BufferType" },
      { "name": "source_offset", "type": "HAL_DeviceSize" },
      { "name": "length", "type": "HAL_DeviceSize" }
    ],
    "outputs": [
      { "name": "result", "type": "HAL_BufferType" }
    ],
    "assemblyFormat": "`<` $source_buffer `:` type($source_buffer) `>`\n    `` `[` $source_offset `,` $length `]`\n    `:` type($result)\n    attr-dict-with-keyword"
  },
  {
    "name": "hal.channel.create",
    "summary": "Creates a new channel for collective communication.",
    "description": "Returns a new channel with the given rank associated with the given device\n    queue. Collective operations using this channel must only be submitted on\n    compatible queues.\n\n    The group and ID are optional and may be null. A rank or count of -1 can be\n    used to indicate a default inherited from the environment or device\n    configuration.",
    "inputs": [
      { "name": "device", "type": "HAL_Device" },
      { "name": "queue_affinity", "type": "HAL_DeviceQueueAffinity" },
      { "name": "id", "type": "Util_BufferType" },
      { "name": "group", "type": "Util_BufferType" },
      { "name": "rank", "type": "I32" },
      { "name": "count", "type": "I32" }
    ],
    "outputs": [
      { "name": "result", "type": "HAL_Channel" }
    ],
    "attributes": [
      { "name": "flags", "type": "HAL_ChannelFlagBitfieldAttr" }
    ],
    "assemblyFormat": "`device` `(` $device `:` type($device) `)`\n    `affinity` `(` $queue_affinity `)`\n    `flags` `(` $flags `)`\n    `id` `(` $id `)`\n    `group` `(` $group `)`\n    `rank` `(` $rank `)`\n    `count` `(` $count `)`\n    `:` type($result)\n    attr-dict-with-keyword"
  },
  {
    "name": "hal.channel.rank_and_count",
    "summary": "Returns the rank of the local participant in the group.",
    "description": "Returns the rank the channel represents as a participant in a collective\n    group in `[0, count)` and the total participant count.",
    "inputs": [
      { "name": "channel", "type": "HAL_Channel" }
    ],
    "outputs": [
      { "name": "rank", "type": "I32" },
      { "name": "count", "type": "I32" }
    ],
    "assemblyFormat": "`<` $channel `:` type($channel) `>`\n    `:` type($rank) `,` type($count)\n    attr-dict-with-keyword"
  },
  {
    "name": "hal.channel.split",
    "summary": "Splits a collective communication channel.",
    "description": "Partitions the group associated with the given channel into disjoint\n    subgroups for each unique value of color. Each new subgroup contains all\n    participants of the same color and within each subgroup the key argument\n    is used to define the rank order. When multiple participants in a group\n    use the same key the tie will be broken using their rank in the parent\n    group. A color of -1 indicates that the rank does not participate in any\n    subgroup and will return a null channel.",
    "inputs": [
      { "name": "channel", "type": "HAL_Channel" },
      { "name": "color", "type": "I32" },
      { "name": "key", "type": "I32" }
    ],
    "outputs": [
      { "name": "result", "type": "HAL_Channel" }
    ],
    "attributes": [
      { "name": "flags", "type": "HAL_ChannelFlagBitfieldAttr" }
    ],
    "assemblyFormat": "`<` $channel `:` type($channel) `>`\n    `color` `(` $color `)`\n    `key` `(` $key `)`\n    `flags` `(` $flags `)`\n    `:` type($result)\n    attr-dict-with-keyword"
  },
  {
    "name": "hal.command_buffer.begin_debug_group",
    "summary": "Pushes a command buffer debug group label.",
    "description": "Pushes a new debug group with the given label.\n    All commands between this and a mandatory matching call to\n    `hal.command_buffer.end_debug_group` will be grouped together with the\n    given label.",
    "inputs": [
      { "name": "command_buffer", "type": "HAL_CommandBuffer" }
    ],
    "attributes": [
      { "name": "label", "type": "StrAttr" }
    ],
    "assemblyFormat": "`<` $command_buffer `:` type($command_buffer) `>`\n    `label` `(` $label `)`\n    attr-dict-with-keyword"
  },
  {
    "name": "hal.command_buffer.collective",
    "summary": "Command buffer collective dispatch recording operation.",
    "description": "Dispatches a collective operation defined by op using the given buffers.",
    "inputs": [
      { "name": "command_buffer", "type": "HAL_CommandBuffer" },
      { "name": "channel", "type": "HAL_Channel" },
      { "name": "element_count", "type": "HAL_DeviceSize" },
      { "name": "param", "type": "Optional" },
      { "name": "send_buffer", "type": "Optional" },
      { "name": "send_offset", "type": "Optional" },
      { "name": "send_length", "type": "Optional" },
      { "name": "recv_buffer", "type": "Optional" },
      { "name": "recv_offset", "type": "Optional" },
      { "name": "recv_length", "type": "Optional" }
    ],
    "attributes": [
      { "name": "op", "type": "HAL_CollectiveAttr" }
    ],
    "assemblyFormat": "`<` $command_buffer `:` type($command_buffer) `>`\n    `channel` `(` $channel `:` type($channel) `)`\n    `op` `(` $op `)`\n    (`param` `(` $param^ `:` type($param) `)`)?\n    (`send` `(` $send_buffer^ `:` type($send_buffer) `)`\n     `` `[` $send_offset `,` $send_length `]`)?\n    (`recv` `(` $recv_buffer^ `:` type($recv_buffer) `)`\n     `` `[` $recv_offset `,` $recv_length `]`)?\n    `count` `(` $element_count `)`\n    attr-dict-with-keyword"
  },
  {
    "name": "hal.command_buffer.copy_buffer",
    "summary": "Command buffer buffer copy recording operation.",
    "description": "Copies a range of one buffer to another.",
    "inputs": [
      { "name": "command_buffer", "type": "HAL_CommandBuffer" },
      { "name": "source_buffer", "type": "AnyTypeOf" },
      { "name": "source_offset", "type": "HAL_DeviceSize" },
      { "name": "target_buffer", "type": "AnyTypeOf" },
      { "name": "target_offset", "type": "HAL_DeviceSize" },
      { "name": "length", "type": "HAL_DeviceSize" }
    ],
    "attributes": [
      { "name": "flags", "type": "HAL_CopyFlagBitfieldAttr" }
    ],
    "assemblyFormat": "`<` $command_buffer `:` type($command_buffer) `>`\n    `source` `(` $source_buffer `:` type($source_buffer) `)`\n    `` `[` $source_offset `]`\n    `target` `(` $target_buffer `:` type($target_buffer) `)`\n    `` `[` $target_offset `]`\n    `length` `(` $length `)`\n    `flags` `(` $flags `)`\n    attr-dict-with-keyword"
  },
  {
    "name": "hal.command_buffer.create",
    "summary": "Command buffer allocation operation.",
    "description": "Returns a command buffer from the device pool ready to begin recording.",
    "inputs": [
      { "name": "device", "type": "HAL_Device" },
      { "name": "queue_affinity", "type": "HAL_DeviceQueueAffinity" },
      { "name": "binding_capacity", "type": "Optional" }
    ],
    "outputs": [
      { "name": "result", "type": "HAL_CommandBuffer" }
    ],
    "attributes": [
      { "name": "modes", "type": "HAL_CommandBufferModeBitfieldAttr" },
      { "name": "command_categories", "type": "HAL_CommandCategoryBitfieldAttr" }
    ],
    "assemblyFormat": "`device` `(` $device `:` type($device) `)`\n    `mode` `(` $modes `)`\n    `categories` `(` $command_categories `)`\n    `affinity` `(` $queue_affinity `)`\n    (`bindings` `(` $binding_capacity^ `)`)?\n    `:` type($result)\n    attr-dict-with-keyword"
  },
  {
    "name": "hal.command_buffer.device",
    "summary": "Command buffer device query operation.",
    "description": "Used during conversion to access the device used to create a command buffer.",
    "inputs": [
      { "name": "command_buffer", "type": "HAL_CommandBuffer" }
    ],
    "outputs": [
      { "name": "device", "type": "HAL_Device" }
    ],
    "assemblyFormat": "`<` $command_buffer `:` type($command_buffer) `>`\n    `:` type($device)\n    attr-dict-with-keyword"
  },
  {
    "name": "hal.command_buffer.dispatch",
    "summary": "Command buffer dispatch recording operation.",
    "description": "Dispatches an execution request.\n    The request may execute overlapped with any other transfer operation or\n    dispatch made within the same barrier-defined sequence.\n\n    The provided constant data and binding list will be recorded into the\n    command buffer and need not remain live beyond the call. Push constants are\n    always 4-byte values and treated as opaque, meaning that they may be\n    bit-casted floats, bit-packed booleans, etc. The provided buffers may either\n    be HAL buffers or indirect references into the command buffer binding table.",
    "inputs": [
      { "name": "command_buffer", "type": "HAL_CommandBuffer" },
      { "name": "executable", "type": "HAL_Executable" },
      { "name": "entry_point", "type": "HAL_Ordinal" },
      { "name": "workgroup_x", "type": "HAL_Dim" },
      { "name": "workgroup_y", "type": "HAL_Dim" },
      { "name": "workgroup_z", "type": "HAL_Dim" },
      { "name": "constants", "type": "Variadic" },
      { "name": "binding_buffers", "type": "Variadic" },
      { "name": "binding_offsets", "type": "Variadic" },
      { "name": "binding_lengths", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "flags", "type": "HAL_DispatchFlagsAttr" }
    ],
    "assemblyFormat": "`<` $command_buffer `:` type($command_buffer) `>`\n    `target` `(` $executable `:` type($executable) `)`\n    `` `[` $entry_point `]`\n    `workgroups` `(` `[`\n        $workgroup_x `,`\n        $workgroup_y `,`\n        $workgroup_z\n    `]` `)`\n    (`constants` `(` `[` $constants^ `]` `)`)?\n    `bindings` `(` `[`\n    custom<Bindings>($binding_buffers,\n                     type($binding_buffers),\n                     $binding_offsets,\n                     $binding_lengths)\n    `]` `)`\n    `flags` `(` $flags `)`\n    attr-dict-with-keyword"
  },
  {
    "name": "hal.command_buffer.dispatch.indirect",
    "summary": "Command buffer indirect dispatch recording operation.",
    "description": "Dispatches an execution request with a deferred workgroup count.\n    This is the same as iree_hal_command_buffer_dispatch but the workgroup count\n    is read from the given |workgroups_ref| buffer at the specified offset as\n    3 uint32_t XYZ values immediately before performing the dispatch. This\n    allows prior dispatches within the command sequence to populate the\n    workgroup count or the workgroup count to change across submissions of the\n    same reusable command buffer.\n\n    The provided constant data and binding list will be recorded into the\n    command buffer and need not remain live beyond the call. Push constants are\n    always 4-byte values and treated as opaque, meaning that they may be\n    bit-casted floats, bit-packed booleans, etc. The provided buffers may either\n    be HAL buffers or indirect references into the command buffer binding table.",
    "inputs": [
      { "name": "command_buffer", "type": "HAL_CommandBuffer" },
      { "name": "executable", "type": "HAL_Executable" },
      { "name": "entry_point", "type": "HAL_Ordinal" },
      { "name": "workgroups_buffer", "type": "AnyTypeOf" },
      { "name": "workgroups_offset", "type": "HAL_DeviceSize" },
      { "name": "constants", "type": "Variadic" },
      { "name": "binding_buffers", "type": "Variadic" },
      { "name": "binding_offsets", "type": "Variadic" },
      { "name": "binding_lengths", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "flags", "type": "HAL_DispatchFlagsAttr" }
    ],
    "assemblyFormat": "`<` $command_buffer `:` type($command_buffer) `>`\n    `target` `(` $executable `:` type($executable) `)`\n    `` `[` $entry_point `]`\n    `workgroups` `(` $workgroups_buffer `:` type($workgroups_buffer) `)`\n    `` `[` $workgroups_offset `]`\n    (`constants` `(` `[` $constants^ `]` `)`)?\n    `bindings` `(` `[`\n    custom<Bindings>($binding_buffers,\n                     type($binding_buffers),\n                     $binding_offsets,\n                     $binding_lengths)\n    `]` `)`\n    `flags` `(` $flags `)`\n    attr-dict-with-keyword"
  },
  {
    "name": "hal.command_buffer.end_debug_group",
    "summary": "Pops a command buffer debug group label.",
    "description": "Pops a debug group from the stack.",
    "inputs": [
      { "name": "command_buffer", "type": "HAL_CommandBuffer" }
    ],
    "assemblyFormat": "`<` $command_buffer `:` type($command_buffer) `>`\n    attr-dict-with-keyword"
  },
  {
    "name": "hal.command_buffer.execution_barrier",
    "summary": "Command buffer execution barrier recording operation.",
    "description": "Defines an execution dependency between all commands recorded before the\n    barrier and all commands recorded after the barrier. Only the stages\n    provided will be affected.",
    "inputs": [
      { "name": "command_buffer", "type": "HAL_CommandBuffer" }
    ],
    "attributes": [
      { "name": "source_stage_mask", "type": "HAL_ExecutionStageBitfieldAttr" },
      { "name": "target_stage_mask", "type": "HAL_ExecutionStageBitfieldAttr" },
      { "name": "flags", "type": "HAL_ExecutionBarrierFlagBitfieldAttr" }
    ],
    "assemblyFormat": "`<` $command_buffer `:` type($command_buffer) `>`\n    `source` `(` $source_stage_mask `)`\n    `target` `(` $target_stage_mask `)`\n    `flags` `(` $flags `)`\n    attr-dict-with-keyword"
  },
  {
    "name": "hal.command_buffer.fill_buffer",
    "summary": "Command buffer buffer fill recording operation.",
    "description": "Fills the target buffer with the given repeating value.",
    "inputs": [
      { "name": "command_buffer", "type": "HAL_CommandBuffer" },
      { "name": "target_buffer", "type": "AnyTypeOf" },
      { "name": "target_offset", "type": "HAL_DeviceSize" },
      { "name": "length", "type": "HAL_DeviceSize" },
      { "name": "pattern", "type": "HAL_FillPatternType" }
    ],
    "attributes": [
      { "name": "flags", "type": "HAL_FillFlagBitfieldAttr" }
    ],
    "assemblyFormat": "`<` $command_buffer `:` type($command_buffer) `>`\n    `target` `(` $target_buffer `:` type($target_buffer) `)`\n    `` `[` $target_offset `,` $length `]`\n    `pattern` `(` $pattern `:` type($pattern) `)`\n    `flags` `(` $flags `)`\n    attr-dict-with-keyword"
  },
  {
    "name": "hal.command_buffer.finalize",
    "summary": "Finalizes command buffer recording.",
    "description": "Ends recording into the command buffer and prepares it for submission.\n    No more commands may be recorded into the command buffer.",
    "inputs": [
      { "name": "command_buffer", "type": "HAL_CommandBuffer" }
    ],
    "assemblyFormat": "`<` $command_buffer `:` type($command_buffer) `>`\n    attr-dict-with-keyword"
  },
  {
    "name": "hal.command_buffer.update_buffer",
    "summary": "Command buffer buffer update recording operation.",
    "description": "Copies a range of a host buffer into a device buffer. The host buffer\n    contents will be captured at the time of the call and embedded in the\n    command buffer.",
    "inputs": [
      { "name": "command_buffer", "type": "HAL_CommandBuffer" },
      { "name": "source_buffer", "type": "Util_BufferType" },
      { "name": "source_size", "type": "Util_Size" },
      { "name": "source_offset", "type": "Util_Size" },
      { "name": "target_buffer", "type": "AnyTypeOf" },
      { "name": "target_offset", "type": "HAL_DeviceSize" },
      { "name": "length", "type": "HAL_DeviceSize" }
    ],
    "attributes": [
      { "name": "flags", "type": "HAL_UpdateFlagBitfieldAttr" }
    ],
    "assemblyFormat": "`<` $command_buffer `:` type($command_buffer) `>`\n    `source` `(` $source_buffer `:` type($source_buffer) `{` $source_size `}` `)`\n    `` `[` $source_offset `]`\n    `target` `(` $target_buffer `:` type($target_buffer) `)`\n    `` `[` $target_offset `]`\n    `length` `(` $length `)`\n    `flags` `(` $flags `)`\n    attr-dict-with-keyword"
  },
  {
    "name": "hal.device.allocator",
    "summary": "Device allocator accessor operation.",
    "description": "Returns the allocator that can be used to allocate buffers compatible with\n    the device.",
    "inputs": [
      { "name": "device", "type": "HAL_Device" }
    ],
    "outputs": [
      { "name": "result", "type": "HAL_Allocator" }
    ],
    "assemblyFormat": "`<` $device `:` type($device) `>` `:` type($result) attr-dict-with-keyword"
  },
  {
    "name": "hal.device.memoize",
    "summary": "Memoizes resources for a particular device and queue affinity.",
    "description": "Executes the nested region once per device and affinity mask and memoizes\n    the results such that future references return the previously memoized\n    values. The initial execution may happen on demand or be hoisted to module\n    initialization time.\n\n    Any uses of the device or affinity specified within the nested region will\n    be substituted with the appropriate device and affinity during memoization.\n    All other implicitly captured values must be either constant or global\n    values available at the time the memoization occurs.\n\n    It is valid for the nested region contents to be inlined in place and never\n    memoized. This can be useful when diagnosing memoization issues and can be\n    forced with the `--iree-hal-memoization=false` flag.",
    "inputs": [
      { "name": "device", "type": "HAL_Device" },
      { "name": "queue_affinity", "type": "HAL_DeviceQueueAffinity" }
    ],
    "outputs": [
      { "name": "results", "type": "Variadic" }
    ],
    "assemblyFormat": "`<` $device `:` type($device) `>`\n    `affinity` `(` $queue_affinity `)`\n    `->` type($results)\n    attr-dict-with-keyword\n    $body"
  },
  {
    "name": "hal.device.query",
    "summary": "Returns a runtime configuration parameter from the device.",
    "description": "Queries a device configuration parameter with the given key.\n    Returns a status indicating whether the pair was recognized/available and if\n    it was the value converted to the specified type. Queries must return the\n    same value for the lifetime of the module though may vary from run to run.\n\n    This is roughly equivalent to the `sysconf` linux syscall\n    (https://man7.org/linux/man-pages/man3/sysconf.3.html) in that the exact\n    set of keys available and their interpretation is target-dependent.\n\n    Users of the op must check the `ok` result before using the value as what\n    set of keys is available may change over time. If in doubt: don't use this.\n    Each key used adds additional versioning and testing complexity as runtime\n    code path changes will explode combinatorially and should be treated with as\n    much care as a binary file format change. Keys should be prefixed with `ex.`\n    when experimental indicating that they are not expected to be present\n    forever; all non-experimental keys should be vetted.\n\n    Well-known keys:\n\n    * hal.device.id :: {some id pattern}\n      Returns 1 if the device identifier matches the given pattern string.\n\n    * hal.executable.format :: {some format pattern}\n      Returns 1 if the given format is supported by the device loader.\n\n    * hal.device :: concurrency\n      The maximum concurrently executable submissions, mapping roughly to the\n      queue count. The actual concurrency available may be less than this based\n      on dynamic runtime parameters such as power/thermal modes, quota limits,\n      or user choice.\n\n    * hal.dispatch :: concurrency\n      The maximum concurrently executable workgroups for a particular dispatch.\n      The actual concurrency available may be less depending on device state.",
    "inputs": [
      { "name": "device", "type": "HAL_Device" }
    ],
    "outputs": [
      { "name": "ok", "type": "I1" },
      { "name": "value", "type": "AnyType" }
    ],
    "attributes": [
      { "name": "category", "type": "StrAttr" },
      { "name": "key", "type": "StrAttr" },
      { "name": "default_value", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "`<` $device `:` type($device) `>`\n    `key` `(` $category `:` `` `:` $key `)`\n    `:` type($ok) `,` type($value)\n    (`=` $default_value^)?\n    attr-dict-with-keyword"
  },
  {
    "name": "hal.device.queue.alloca",
    "summary": "Allocates a queue-ordered transient buffer.",
    "description": "Returns a queue-ordered transient buffer that will be available for use when\n    the signal fence is reached. The allocation will not be made until the\n    wait fence has been reached.\n\n    The size of the buffer returned may be larger than the requested size if the\n    allocator has specific alignment requirements or minimum allocation sizes.\n\n    The buffer handle will remain live so long as there are retainers but the\n    contents are undefined before the allocation signal fence has been signaled\n    and after the deallocation wait fence has been reached.",
    "inputs": [
      { "name": "device", "type": "HAL_Device" },
      { "name": "queue_affinity", "type": "HAL_DeviceQueueAffinity" },
      { "name": "wait_fence", "type": "HAL_Fence" },
      { "name": "signal_fence", "type": "HAL_Fence" },
      { "name": "pool", "type": "HAL_DeviceQueuePool" },
      { "name": "memory_types", "type": "HAL_MemoryType" },
      { "name": "buffer_usage", "type": "HAL_BufferUsage" },
      { "name": "result_size", "type": "HAL_DeviceSize" }
    ],
    "outputs": [
      { "name": "result", "type": "HAL_Buffer" }
    ],
    "attributes": [
      { "name": "flags", "type": "HAL_AllocaFlagBitfieldAttr" }
    ],
    "assemblyFormat": "`<` $device `:` type($device) `>`\n    `affinity` `(` $queue_affinity `)`\n    `wait` `(` $wait_fence `)`\n    `signal` `(` $signal_fence `)`\n    `pool` `(` $pool `)`\n    `type` `(` $memory_types `)`\n    `usage` `(` $buffer_usage `)`\n    `flags` `(` $flags `)`\n    `:` custom<SizeAwareType>(type($result), $result_size)\n    attr-dict-with-keyword"
  },
  {
    "name": "hal.device.queue.barrier",
    "summary": "Enqueues an execution barrier.",
    "description": "Signals the provided fence once the wait fence is reached.",
    "inputs": [
      { "name": "device", "type": "HAL_Device" },
      { "name": "queue_affinity", "type": "HAL_DeviceQueueAffinity" },
      { "name": "wait_fence", "type": "HAL_Fence" },
      { "name": "signal_fence", "type": "HAL_Fence" }
    ],
    "attributes": [
      { "name": "flags", "type": "HAL_ExecuteFlagBitfieldAttr" }
    ],
    "assemblyFormat": "`<` $device `:` type($device) `>`\n    `affinity` `(` $queue_affinity `)`\n    `wait` `(` $wait_fence `)`\n    `signal` `(` $signal_fence `)`\n    `flags` `(` $flags `)`\n    attr-dict-with-keyword"
  },
  {
    "name": "hal.device.queue.copy",
    "summary": "Copies one device-visible buffer to another.",
    "description": "The source buffer and target buffer must both be visible to the device\n    queue performing the copy. In most cases the queue affinity should be set to\n    where the target buffer will be consumed so that it has a chance of being\n    cached. The source buffer must have transfer-source usage and the target\n    buffer must have transfer-target usage.\n\n    Note that individual queue transfer operations have a high overhead and they\n    should be batched with other operations in command buffers.",
    "inputs": [
      { "name": "device", "type": "HAL_Device" },
      { "name": "queue_affinity", "type": "HAL_DeviceQueueAffinity" },
      { "name": "wait_fence", "type": "HAL_Fence" },
      { "name": "signal_fence", "type": "HAL_Fence" },
      { "name": "source_buffer", "type": "HAL_Buffer" },
      { "name": "source_offset", "type": "HAL_DeviceSize" },
      { "name": "target_buffer", "type": "HAL_Buffer" },
      { "name": "target_offset", "type": "HAL_DeviceSize" },
      { "name": "length", "type": "HAL_DeviceSize" }
    ],
    "attributes": [
      { "name": "flags", "type": "HAL_CopyFlagBitfieldAttr" }
    ],
    "assemblyFormat": "`<` $device `:` type($device) `>`\n    `affinity` `(` $queue_affinity `)`\n    `wait` `(` $wait_fence `)`\n    `signal` `(` $signal_fence `)`\n    `source` `(` $source_buffer `:` type($source_buffer) `)`\n    `` `[` $source_offset `]`\n    `target` `(` $target_buffer `:` type($target_buffer) `)`\n    `` `[` $target_offset `]`\n    `length` `(` $length `)`\n    `flags` `(` $flags `)`\n    attr-dict-with-keyword"
  },
  {
    "name": "hal.device.queue.dealloca",
    "summary": "Deallocates a queue-ordered transient buffer.",
    "description": "Deallocates a queue-ordered transient buffer.\n    The deallocation will not be made until the wait fence has been reached and\n    once the storage is available for reuse the signal fence will be signaled.\n\n    After deallocation the contents of the buffer may still be accessible but\n    will have undefined contents as other operations reuse the memory.",
    "inputs": [
      { "name": "device", "type": "HAL_Device" },
      { "name": "queue_affinity", "type": "HAL_DeviceQueueAffinity" },
      { "name": "wait_fence", "type": "HAL_Fence" },
      { "name": "signal_fence", "type": "HAL_Fence" },
      { "name": "buffer", "type": "HAL_Buffer" }
    ],
    "attributes": [
      { "name": "flags", "type": "HAL_DeallocaFlagBitfieldAttr" }
    ],
    "assemblyFormat": "`<` $device `:` type($device) `>`\n    `affinity` `(` $queue_affinity `)`\n    `wait` `(` $wait_fence `)`\n    `signal` `(` $signal_fence `)`\n    `buffer` `(` $buffer `:` type($buffer) `)`\n    `flags` `(` $flags `)`\n    attr-dict-with-keyword"
  },
  {
    "name": "hal.device.queue.execute",
    "summary": "Enqueues command buffer execution.",
    "description": "Executes a command buffer on a device queue.\n    No commands will execute until the wait fence has been reached and the\n    signal fence will be signaled when all commands have completed.",
    "inputs": [
      { "name": "device", "type": "HAL_Device" },
      { "name": "queue_affinity", "type": "HAL_DeviceQueueAffinity" },
      { "name": "wait_fence", "type": "HAL_Fence" },
      { "name": "signal_fence", "type": "HAL_Fence" },
      { "name": "command_buffer", "type": "HAL_CommandBuffer" }
    ],
    "attributes": [
      { "name": "flags", "type": "HAL_ExecuteFlagBitfieldAttr" }
    ],
    "assemblyFormat": "`<` $device `:` type($device) `>`\n    `affinity` `(` $queue_affinity `)`\n    `wait` `(` $wait_fence `)`\n    `signal` `(` $signal_fence `)`\n    `commands` `(` $command_buffer `)`\n    `flags` `(` $flags `)`\n    attr-dict-with-keyword"
  },
  {
    "name": "hal.device.queue.execute.indirect",
    "summary": "Enqueues command buffer execution.",
    "description": "Executes a command buffer on a device queue with the given binding table.\n    No commands will execute until the wait fence has been reached and the\n    signal fence will be signaled when all commands have completed.",
    "inputs": [
      { "name": "device", "type": "HAL_Device" },
      { "name": "queue_affinity", "type": "HAL_DeviceQueueAffinity" },
      { "name": "wait_fence", "type": "HAL_Fence" },
      { "name": "signal_fence", "type": "HAL_Fence" },
      { "name": "command_buffer", "type": "HAL_CommandBuffer" },
      { "name": "binding_buffers", "type": "Variadic" },
      { "name": "binding_offsets", "type": "Variadic" },
      { "name": "binding_lengths", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "flags", "type": "HAL_ExecuteFlagBitfieldAttr" }
    ],
    "assemblyFormat": "`<` $device `:` type($device) `>`\n    `affinity` `(` $queue_affinity `)`\n    `wait` `(` $wait_fence `)`\n    `signal` `(` $signal_fence `)`\n    `commands` `(` $command_buffer `)`\n    `bindings` `(` `[`\n    custom<BindingTable>($binding_buffers,\n                         type($binding_buffers),\n                         $binding_offsets,\n                         $binding_lengths)\n    `]` `)`\n    `flags` `(` $flags `)`\n    attr-dict-with-keyword"
  },
  {
    "name": "hal.device.queue.fill",
    "summary": "Fills a buffer with a repeating pattern.",
    "description": "The target buffer must be visible to the device queue performing the update.\n    In most cases the queue affinity should be set to where the target buffer\n    will be consumed so that it has a chance of being cached.\n\n    Note that individual queue transfer operations have a high overhead and they\n    should be batched with other operations in command buffers.",
    "inputs": [
      { "name": "device", "type": "HAL_Device" },
      { "name": "queue_affinity", "type": "HAL_DeviceQueueAffinity" },
      { "name": "wait_fence", "type": "HAL_Fence" },
      { "name": "signal_fence", "type": "HAL_Fence" },
      { "name": "target_buffer", "type": "HAL_Buffer" },
      { "name": "target_offset", "type": "HAL_DeviceSize" },
      { "name": "length", "type": "HAL_DeviceSize" },
      { "name": "pattern", "type": "HAL_FillPatternType" }
    ],
    "attributes": [
      { "name": "flags", "type": "HAL_FillFlagBitfieldAttr" }
    ],
    "assemblyFormat": "`<` $device `:` type($device) `>`\n    `affinity` `(` $queue_affinity `)`\n    `wait` `(` $wait_fence `)`\n    `signal` `(` $signal_fence `)`\n    `target` `(` $target_buffer `:` type($target_buffer) `)`\n    `` `[` $target_offset `]`\n    `length` `(` $length `)`\n    `pattern` `(` $pattern `:` type($pattern) `)`\n    `flags` `(` $flags `)`\n    attr-dict-with-keyword"
  },
  {
    "name": "hal.device.queue.flush",
    "summary": "Flushes locally-pending submissions to the queue.",
    "description": "Flushes any locally-pending submissions in the queue.\n    When submitting many queue operations this can be used to eagerly flush\n    earlier submissions while later ones are still being constructed.\n    This may be a no-op.",
    "inputs": [
      { "name": "device", "type": "HAL_Device" },
      { "name": "queue_affinity", "type": "HAL_DeviceQueueAffinity" }
    ],
    "assemblyFormat": "`<` $device `:` type($device) `>`\n    `affinity` `(` $queue_affinity `)`\n    attr-dict-with-keyword"
  },
  {
    "name": "hal.device.queue.read",
    "summary": "Reads a segment from a file into a device buffer.",
    "description": "Enqueues a file read operation that streams a segment of the source file\n    defined by the source offset and length into the target HAL buffer at the\n    specified target offset. The queue affinity should be set to where the\n    target buffer will be consumed. The source file must have read permission\n    and the target buffer must have transfer-target usage. Read failure will\n    result in propagated semaphore failure or device loss.",
    "inputs": [
      { "name": "device", "type": "HAL_Device" },
      { "name": "queue_affinity", "type": "HAL_DeviceQueueAffinity" },
      { "name": "wait_fence", "type": "HAL_Fence" },
      { "name": "signal_fence", "type": "HAL_Fence" },
      { "name": "source_file", "type": "HAL_File" },
      { "name": "source_offset", "type": "I64" },
      { "name": "target_buffer", "type": "HAL_Buffer" },
      { "name": "target_offset", "type": "HAL_DeviceSize" },
      { "name": "length", "type": "HAL_DeviceSize" }
    ],
    "attributes": [
      { "name": "flags", "type": "HAL_ReadFlagBitfieldAttr" }
    ],
    "assemblyFormat": "`<` $device `:` type($device) `>`\n    `affinity` `(` $queue_affinity `)`\n    `wait` `(` $wait_fence `)`\n    `signal` `(` $signal_fence `)`\n    `source` `(` $source_file `:` type($source_file) `)`\n    `` `[` $source_offset `]`\n    `target` `(` $target_buffer `:` type($target_buffer) `)`\n    `` `[` $target_offset `]`\n    `length` `(` $length `)`\n    `flags` `(` $flags `)`\n    attr-dict-with-keyword"
  },
  {
    "name": "hal.device.queue.update",
    "summary": "Updates a buffer with the contents of a host buffer.",
    "description": "The provided host source buffer will be captured and need not remain live or\n    unchanged while the operation is queued. The target buffer must be visible\n    to the device queue performing the update. In most cases the queue affinity\n    should be set to where the target buffer will be consumed so that it has a\n    chance of being cached.\n\n    Some implementations may have limits on the size of the update or may\n    perform poorly if the size is larger than an implementation-defined limit.\n    Updates should be kept as small and infrequent as possible.\n\n    Note that individual queue transfer operations have a high overhead and they\n    should be batched with other operations in command buffers.",
    "inputs": [
      { "name": "device", "type": "HAL_Device" },
      { "name": "queue_affinity", "type": "HAL_DeviceQueueAffinity" },
      { "name": "wait_fence", "type": "HAL_Fence" },
      { "name": "signal_fence", "type": "HAL_Fence" },
      { "name": "source_buffer", "type": "Util_BufferType" },
      { "name": "source_offset", "type": "HAL_DeviceSize" },
      { "name": "target_buffer", "type": "HAL_Buffer" },
      { "name": "target_offset", "type": "HAL_DeviceSize" },
      { "name": "length", "type": "HAL_DeviceSize" }
    ],
    "attributes": [
      { "name": "flags", "type": "HAL_UpdateFlagBitfieldAttr" }
    ],
    "assemblyFormat": "`<` $device `:` type($device) `>`\n    `affinity` `(` $queue_affinity `)`\n    `wait` `(` $wait_fence `)`\n    `signal` `(` $signal_fence `)`\n    `source` `(` $source_buffer `:` type($source_buffer) `)`\n    `` `[` $source_offset `]`\n    `target` `(` $target_buffer `:` type($target_buffer) `)`\n    `` `[` $target_offset `]`\n    `length` `(` $length `)`\n    `flags` `(` $flags `)`\n    attr-dict-with-keyword"
  },
  {
    "name": "hal.device.queue.write",
    "summary": "Writes a segment from a device buffer into a file.",
    "description": "Enqueues a file write operation that streams a segment of the source HAL\n    buffer defined by the source offset and length into the target file at the\n    specified target offset. The queue affinity should be set to where the\n    source buffer was produced. The source buffer must have transfer-source\n    usage and the target file must have write permission. Write failure will\n    result in propagated semaphore failure or device loss.",
    "inputs": [
      { "name": "device", "type": "HAL_Device" },
      { "name": "queue_affinity", "type": "HAL_DeviceQueueAffinity" },
      { "name": "wait_fence", "type": "HAL_Fence" },
      { "name": "signal_fence", "type": "HAL_Fence" },
      { "name": "source_buffer", "type": "HAL_Buffer" },
      { "name": "source_offset", "type": "HAL_DeviceSize" },
      { "name": "target_file", "type": "HAL_File" },
      { "name": "target_offset", "type": "I64" },
      { "name": "length", "type": "HAL_DeviceSize" }
    ],
    "attributes": [
      { "name": "flags", "type": "HAL_WriteFlagBitfieldAttr" }
    ],
    "assemblyFormat": "`<` $device `:` type($device) `>`\n    `affinity` `(` $queue_affinity `)`\n    `wait` `(` $wait_fence `)`\n    `signal` `(` $signal_fence `)`\n    `source` `(` $source_buffer `:` type($source_buffer) `)`\n    `` `[` $source_offset `]`\n    `target` `(` $target_file `:` type($target_file) `)`\n    `` `[` $target_offset `]`\n    `length` `(` $length `)`\n    `flags` `(` $flags `)`\n    attr-dict-with-keyword"
  },
  {
    "name": "hal.device.resolve",
    "summary": "Resolves device handles based on affinity.",
    "description": "Examples:\n    ```\n    // Returns a HAL device.\n    = hal.device.resolve on(#something) : !hal.device\n    // Returns a HAL device, allocator, and (optional) queue affinity.\n    = hal.device.resolve on(#something) : !hal.device, !hal.allocator, i64\n    // Returns a HAL allocator and (optional) queue affinity.\n    = hal.device.resolve on(#something) : !hal.allocator, i64\n    // Returns \"any\" device. Should only be used as a fallback.\n    = hal.device.resolve : !hal.device\n    ```",
    "outputs": [
      { "name": "results", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "affinity", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "(`on` `(` qualified($affinity)^ `)`)?\n    attr-dict `:` type($results)"
  },
  {
    "name": "hal.devices.count",
    "summary": "Returns the number of available devices.",
    "description": "Returns the total number of available devices registered at runtime.",
    "outputs": [
      { "name": "result", "type": "Index" }
    ],
    "assemblyFormat": "attr-dict `:` type($result)"
  },
  {
    "name": "hal.devices.get",
    "summary": "Returns the device with the given index.",
    "description": "Returns the device with the given index in the [0, hal.devices.count) range.\n    Devices may be lazily initialized upon first use.",
    "inputs": [
      { "name": "index", "type": "Index" }
    ],
    "outputs": [
      { "name": "result", "type": "HAL_Device" }
    ],
    "assemblyFormat": "$index attr-dict `:` type($result)"
  },
  {
    "name": "hal.dispatch.extern",
    "summary": "A dispatch of workgroups across a 3-dimensional grid.",
    "description": "Dispatches some number of workgroups across a 3-dimensional grid using a\n    function defined externally in one or more referenced objects. Objects are\n    declared per executable target and selected automatically during linking\n    based on where the dispatch is used. Semantically this is equivalent to\n    a `flow.dispatch.workgroups` but with the workgroup region invisible to the\n    compiler. See `hal.executable` for more information about object linkage.\n\n    Note that since this happens at tensor level the dispatch operation has\n    value semantics: some tensors (and optionally other primitive types) are\n    consumed and one or more new result tensors are produced. Inside each\n    workgroup, however, the input and output tensors are available for arbitrary\n    loads and stores. In many cases each workgroup will load some particular\n    tile(s) from the input tensors and store some particular tile(s) to the\n    output tensors unique to that workgroup. Though it's possible for multiple\n    workgroups to load the same regions of the input tensors behavior is\n    undefined if multiple workgroups store to the same regions of the output\n    tensors. Codegen guarantees this behavior but when sourcing externally\n    authored dispatch functions it's critical that this behavior is observed.\n\n    Though the representation is similar to the GPU-style grid dispatch model\n    here we still have not yet allocated buffers, determined the target device\n    for execution, or even completed fully resolving shapes/types/etc. Because\n    of this it's important that the workgroup body use the platform-dependent\n    primitives for accessing workgroup ID, size, and count intrinsics instead\n    of hardcoding them to a particular set of values. Assume that any workgroup\n    dispatch may end up being specialized for several different target devices\n    and even several different variants for a particular target device\n    (differing workgroup sizes, etc). To aid deduplication code producing these\n    external dispatches should try not to specialize early for particular shapes\n    and instead emit the most generic code possible as having 500 slightly\n    different `hal.dispatch.extern` ops pointing at the same object file is\n    likely to require 500 copies of the object instead of 500 calls to the same\n    object.\n\n    Because at this point in the layering devices have not yet been selected the\n    workgroup count cannot be fully evaluated. Instead workload parameters are\n    captured that are then passed to a function that when later evaluated\n    computes the actual workgroup count based on target information. The\n    workload is not limited to the 3D XYZ grid dispatch of the workgroup count\n    and can contain any number of parameters used to compute it. If workgroup\n    size or distribution varies based on the target device a `!hal.device`\n    argument can be used by the workgroup count calculation region to factor in\n    device parameters. See `hal.device.query` for more information on how to\n    query information.\n\n    ```mlir\n    %r = hal.dispatch.extern \"some_function\"[%c5, %c5](%0, %1)\n        : (tensor<5x5xf32>, tensor<5xf32>) -> tensor<5x5xf32>\n      ...\n    ```\n\n    The number of results of the operation is equal to the number of results\n    in the type signature (`(tensor<5x5xf32>, tensor<5xf32>) -> tensor<5x5xf32>`).\n    Each tensor argument and result in the type signature has a corresponding\n    pipeline layout slot and must be declared. If multiple arguments or results\n    share the same layout slot they can be aliased using the `bindings`\n    attribute and otherwise each is assumed unique.\n\n    There are no `arguments` operands for results, but a result can be tied an\n    argument by writing the argument operand's SSA value instead of its type:\n    E.g., in the above example, `-> %0` would tie the first argument to the\n    result. In that case, there would be no separate block argument for the\n    result.\n\n    Objects for multiple targets can be specified and the ones used are selected\n    based on their target and an optional condition region that returns true if\n    the variant is valid for use on the provided runtime `!hal.device`. If no\n    variants within an executable are valid then loading will fail at runtime.\n    If multiple variants are valid the first valid one found will be loaded and\n    used for execution.",
    "inputs": [
      { "name": "workload", "type": "Variadic" },
      { "name": "arguments", "type": "Variadic" },
      { "name": "argument_dims", "type": "HAL_ShapeDynamicDims" },
      { "name": "result_dims", "type": "HAL_ShapeDynamicDims" }
    ],
    "outputs": [
      { "name": "results", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "export_name", "type": "StrAttr" },
      { "name": "layout", "type": "HAL_PipelineLayoutAttr" },
      { "name": "targets", "type": "ArrayAttr" },
      { "name": "target_ordinals", "type": "HAL_OrdinalArrayAttr" },
      { "name": "target_objects", "type": "ArrayAttr" },
      { "name": "workgroup_size", "type": "OptionalAttr" },
      { "name": "subgroup_size", "type": "OptionalAttr" },
      { "name": "workgroup_local_memory", "type": "OptionalAttr" },
      { "name": "tied_operands", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "$export_name\n    (`[` $workload^ `]`)? ``\n    `(` $arguments `)` `:`\n    custom<ShapedFunctionType>(ref($arguments),\n                               type($arguments), $argument_dims,\n                               type($results), $result_dims,\n                               $tied_operands)\n    `count` `` custom<WorkgroupCountRegion>($workgroup_count)\n    `layout` `(` $layout `)`\n    `objects` `(` `{` custom<TargetConditionObjects>($targets,\n                                                     $target_ordinals,\n                                                     $target_objects,\n                                                     $target_regions) `}` `)`\n    attr-dict-with-keyword"
  },
  {
    "name": "hal.element_type",
    "summary": "An iree_hal_element_type_t for the given MLIR type.",
    "description": "Maps an MLIR type to a runtime `iree_hal_element_type_t` value for all types\n    that are convertable.",
    "outputs": [
      { "name": "result", "type": "HAL_ElementType" }
    ],
    "attributes": [
      { "name": "type", "type": "TypeAttr" }
    ],
    "assemblyFormat": "`<` $type `>`\n    attr-dict\n    `:` type($result)"
  },
  {
    "name": "hal.encoding_type",
    "summary": "An iree_hal_encoding_type_t for the given MLIR encoding.",
    "description": "Maps an MLIR encoding to a runtime `iree_hal_encoding_type_t` value for all\n    encodings that are convertable.",
    "outputs": [
      { "name": "result", "type": "HAL_EncodingType" }
    ],
    "attributes": [
      { "name": "encoding", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "`<` ($encoding^):( `` `dense_row_major`)? `>`\n    attr-dict\n    `:` type($result)"
  },
  {
    "name": "hal.ex.file.from_memory",
    "summary": "Creates a file mapped into a byte range of a host buffer.",
    "description": "Returns a file handle that is backed by the given `buffer` contents.\n    Behavior is undefined if the buffer contents change while the accesses are\n    in-flight.\n\n    Experimental as the exact interface for getting files from module contents\n    still needs iteration. Most hardware APIs require a file descriptor or\n    native platform handle but here we only have host pointers. When\n    memory-mapped some systems allow for retrieval of the platform handle from\n    a virtual address (GetMappedFileNameA/posix_mem_offset) but the APIs are\n    sketchy and likely slow. Instead we should probably have a way to query for\n    a file handle derived from the calling module by stack-walking and asking\n    the VM module for its handle. Until we can figure this out this method will\n    be marked epxerimental.",
    "inputs": [
      { "name": "device", "type": "HAL_Device" },
      { "name": "queue_affinity", "type": "HAL_DeviceQueueAffinity" },
      { "name": "buffer", "type": "Util_BufferType" },
      { "name": "offset", "type": "HAL_DeviceSize" },
      { "name": "length", "type": "HAL_DeviceSize" },
      { "name": "flags", "type": "I32" }
    ],
    "outputs": [
      { "name": "result", "type": "HAL_File" }
    ],
    "attributes": [
      { "name": "access", "type": "HAL_MemoryAccessBitfieldAttr" }
    ],
    "assemblyFormat": "`device` `(` $device `:` type($device) `)`\n    `affinity` `(` $queue_affinity `)`\n    `access` `(` $access `)`\n    `buffer` `(` $buffer `:` type($buffer) `)`\n    `` `[` $offset `for` $length `]`\n    `flags` `(` $flags `)`\n    `:` type($result)\n    attr-dict-with-keyword"
  },
  {
    "name": "hal.executable",
    "summary": "Target-specific executable module.",
    "description": "An executable module representing a target-specific compiled\n    kernel/shader/etc. Executables are treated as independent compilation units\n    and may contain multiple exported entry points that are able to share code\n    internally. To support multi-targeting each executable may have one or more\n    target-specific variants that are lowered independently during compilation\n    while still appearing as one executable at runtime (ala fat binaries).\n\n    At runtime executables are loaded during module initialization and cached\n    for the lifetime of the module. If the `lazy` attribute is set the\n    executable _may_ have its loading deferred until first use.",
    "attributes": [
      { "name": "sym_visibility", "type": "OptionalAttr" },
      { "name": "sym_name", "type": "SymbolNameAttr" },
      { "name": "lazy", "type": "UnitAttr" }
    ],
    "assemblyFormat": "custom<SymbolVisibility>($sym_visibility)\n    (`lazy` $lazy^)?\n    $sym_name\n    attr-dict-with-keyword\n    regions"
  },
  {
    "name": "hal.executable_end",
    "summary": "Terminator pseudo-op for the executable op.",
    "assemblyFormat": "attr-dict"
  },
  {
    "name": "hal.executable.binary",
    "summary": "Compiled executable binary data.",
    "description": "A compiled executable binary with an optional nested module containing the\n    IR prior to serialization (for debugging).",
    "attributes": [
      { "name": "sym_visibility", "type": "OptionalAttr" },
      { "name": "sym_name", "type": "SymbolNameAttr" },
      { "name": "format", "type": "StrAttr" },
      { "name": "data", "type": "Util_AnySerializableAttr" },
      { "name": "mime_type", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "custom<SymbolVisibility>($sym_visibility)\n    $sym_name\n    attr-dict-with-keyword"
  },
  {
    "name": "hal.executable.calculate_workgroups",
    "summary": "Calculates workgroup count from workload for an exported function.",
    "description": "Calculates the workgroup count (grid XYZ) based on the given workload using\n    the workgroup count calculation region of the target\n    `hal.executable.export` op.",
    "inputs": [
      { "name": "device", "type": "HAL_Device" },
      { "name": "workload", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "workgroup_x", "type": "HAL_Dim" },
      { "name": "workgroup_y", "type": "HAL_Dim" },
      { "name": "workgroup_z", "type": "HAL_Dim" }
    ],
    "attributes": [
      { "name": "entry_point", "type": "SymbolRefAttr" }
    ],
    "assemblyFormat": "`device` `(` $device `:` type($device) `)`\n    `target` `(` $entry_point `)`\n    (`workload` `(` `[` $workload^ `]` `)`)?\n    `:` type($workgroup_x) `,` type($workgroup_y) `,` type($workgroup_z)\n    attr-dict-with-keyword"
  },
  {
    "name": "hal.executable.condition",
    "summary": "Host code to determine if the executable is enabled.",
    "description": "Variants are selected based on their target and this optional condition\n    op that returns true if the variant is valid for use on the provided\n    runtime `!hal.device`. If no variants within an executable are valid then\n    loading will fail at runtime. If multiple variants are valid the first valid\n    one found will be loaded and used for execution.",
    "attributes": [
      { "name": "function_type", "type": "TypeAttrOf" },
      { "name": "arg_attrs", "type": "OptionalAttr" },
      { "name": "res_attrs", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "hal.executable.constant.block",
    "summary": "Executable constant block initializer.",
    "description": "Initializes one or more constants in the executable constant block by\n    returning one value per identified constant. Each constant block is\n    evaluated on the host prior to instantiating the executable for a given\n    device and allows for the executable to be specialized based on device\n    capabilities and limits.\n\n    The keys specified are unique per variant and will be deduplicated across\n    multiple constant blocks when present. They are only used during lowering\n    and will not survive to runtime so they need only have descriptive enough\n    names to avoid collisions and represent the semantics of the value.\n\n    Constant values can be loaded in the device code with the\n    `hal.executable.constant.load` op:\n\n    ```mlir\n    hal.executable.variant public @target {\n      hal.executable.constant.block(%device: !hal.device) -> (i32, i32) as (\"foo\", \"bar\") {\n        %0 = hal.device.query<%device> key(\"some.device.prop\")...\n        %1 = hal.device.query<%device> key(\"another.device.prop\")...\n        hal.return %0, %1 : i32, i32\n      }\n      builtin.module {\n        func @dispatch0() {\n          %0 = hal.executable.constant.load \"foo\" : i32\n          %1 = hal.executable.constant.load \"bar\" : i32\n          return\n        }\n      }\n    }\n    ```\n\n    Each target backend will implement the constant initialization and access in\n    a way compatible with its execution model. Examples:\n    - CPU: read-only buffer initialized on load and passed to each dispatch\n    - CUDA: read-only buffer initialized on load and passed to each dispatch\n    - SPIR-V: specialization constants\n    - Metal: function constants\n    - WebGPU: pipeline-overridable constants",
    "attributes": [
      { "name": "function_type", "type": "TypeAttrOf" },
      { "name": "keys", "type": "ArrayAttr" },
      { "name": "arg_attrs", "type": "OptionalAttr" },
      { "name": "res_attrs", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "hal.executable.constant.load",
    "summary": "Loads a constant value from the executable constant block.",
    "description": "Loads a scalar constant value from the static executable constant block.\n    The value provided by a constant block with the given key will be loaded and\n    bitcast (possibly with truncation or zero-extension) to the result type.\n\n    Note that backends are allowed to implement their own mechanisms for\n    referencing constant block values and this is provided only as a default for\n    those not needing special behavior.",
    "outputs": [
      { "name": "result", "type": "HAL_PrimitiveType" }
    ],
    "attributes": [
      { "name": "key", "type": "StrAttr" }
    ],
    "assemblyFormat": "$key attr-dict `:` type($result)"
  },
  {
    "name": "hal.executable.create",
    "summary": "Creates an executable.",
    "description": "Creates a target-dependent executable cached on the provided device. Entry\n    points contained within the executable can be dispatched using the resulting\n    executable handle.\n\n    Depending on the driver creation may take a non-trivial amount of time\n    (such as when JITing/etc). As the cache is internally synchronized callers\n    can issue preparation requests from multiple threads - even for the same\n    executables - and calls will block until preparation completes.\n\n    Optional constants provide for specialization of the executable based on\n    runtime-derived parameters.",
    "inputs": [
      { "name": "device", "type": "HAL_Device" },
      { "name": "queue_affinity", "type": "HAL_DeviceQueueAffinity" },
      { "name": "constants", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "result", "type": "HAL_Executable" }
    ],
    "attributes": [
      { "name": "executable_target", "type": "SymbolRefAttr" }
    ],
    "assemblyFormat": "`device` `(` $device `:` type($device) `)`\n    `affinity` `(` $queue_affinity `)`\n    `target` `(` $executable_target `)`\n    (`constants` `(` `[` $constants^ `]` `)`)?\n    `:` type($result)\n    attr-dict-with-keyword"
  },
  {
    "name": "hal.executable.export",
    "summary": "Executable entry point declaration.",
    "description": "An entry point exported by the executable with statically-available\n    information describing the IO interface it uses and other dispatch metadata.\n\n    The `workgroup_count` region represents the computation that\n    returns the number of workgroups to use in the 3D grid dispatch.\n    The arguments to the region represents the workload as captured by each\n    dispatch. It returns the number of workgroups along x, y, and z.\n\n    The optional `condition` region provides boolean logic determining whether\n    the export should be dispatched given the device and workload or if a\n    specified fallback export in the same executable should be dispatched\n    instead. Multiple exports can be chained together as fallbacks to allow for\n    arbitrarily complex decisions trees. Fallbacks for an export must match the\n    layout and workload exactly but may vary any other attribute (such as\n    workgroup size or translation configuration).\n\n    Workgroup count and condition regions that have dependencies on dynamic\n    workload information will be executed using indirect dispatch. If the\n    information is available on the host at the time a command buffer containing\n    the dispatch is available the indirect dispatch _may_ have lower overhead\n    by using `IREE_HAL_DISPATCH_FLAG_STATIC_INDIRECT_PARAMETERS`. If the\n    information required is data-dependent on work within the same command\n    buffer some backends will suffer a performance penalty. Condition regions\n    consuming dynamic workloads in particular may result in long chains of\n    indirect dispatches sent to the device or even host round-trips.",
    "attributes": [
      { "name": "sym_visibility", "type": "OptionalAttr" },
      { "name": "sym_name", "type": "SymbolNameAttr" },
      { "name": "ordinal", "type": "OptionalAttr" },
      { "name": "layout", "type": "HAL_PipelineLayoutAttr" },
      { "name": "condition_fallback", "type": "OptionalAttr" },
      { "name": "workgroup_size", "type": "OptionalAttr" },
      { "name": "subgroup_size", "type": "OptionalAttr" },
      { "name": "workgroup_local_memory", "type": "OptionalAttr" },
      { "name": "source_locs", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "custom<SymbolVisibility>($sym_visibility)\n    $sym_name\n    (`ordinal` `(` $ordinal^ `)`)?\n    `layout` `(` qualified($layout) `)`\n    (`condition` `` custom<ExportConditionRegion>($condition)^)?\n    (`fallback` `(` $condition_fallback^ `)`)?\n    (`count` `` custom<WorkgroupCountRegion>($workgroup_count)^)?\n    attr-dict-with-keyword"
  },
  {
    "name": "hal.executable.export.ordinal",
    "summary": "Executable export ordinal lookup pseudo-op.",
    "description": "Resolves an executable export ordinal to a value once ordinals have been\n    assigned.",
    "outputs": [
      { "name": "result", "type": "Index" }
    ],
    "attributes": [
      { "name": "entry_point", "type": "SymbolRefAttr" }
    ],
    "assemblyFormat": "`target` `(` $entry_point `)`\n    `:` type($result)\n    attr-dict-with-keyword"
  },
  {
    "name": "hal.executable.lookup",
    "summary": "Executable cache lookup pseudo-op.",
    "description": "Used during conversion to provide a placeholder for a globally cached and\n    possibly lazy-initialized executable.",
    "inputs": [
      { "name": "device", "type": "HAL_Device" }
    ],
    "outputs": [
      { "name": "result", "type": "HAL_Executable" }
    ],
    "attributes": [
      { "name": "executable", "type": "FlatSymbolRefAttr" }
    ],
    "assemblyFormat": "`device` `(` $device `:` type($device) `)`\n    `executable` `(` $executable `)`\n    `:` type($result)\n    attr-dict-with-keyword"
  },
  {
    "name": "hal.executable.source",
    "summary": "Generic source contents of an executable op.",
    "description": "This is an unspecialized source representation of an executable\n    module without an assigned target. This is useful for hand-authoring\n    executables prior to device specification.",
    "attributes": [
      { "name": "sym_visibility", "type": "OptionalAttr" },
      { "name": "sym_name", "type": "SymbolNameAttr" },
      { "name": "objects", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "custom<SymbolVisibility>($sym_visibility)\n    $sym_name\n    attr-dict-with-keyword\n    $body"
  },
  {
    "name": "hal.executable.source_end",
    "summary": "Terminator pseudo-op for the executable source op.",
    "assemblyFormat": "attr-dict"
  },
  {
    "name": "hal.executable.variant",
    "summary": "Target-specific variant of an executable op.",
    "description": "The target IR for the executable. This can be preserved for debugging but\n    is usually removed during transformation.\n\n    Variants are selected based on their target and an optional condition\n    op that returns true if the variant is valid for use on the provided\n    runtime `!hal.device`. If no variants within an executable are valid then\n    loading will fail at runtime. If multiple variants are valid the first valid\n    one found will be loaded and used for execution.",
    "attributes": [
      { "name": "sym_visibility", "type": "OptionalAttr" },
      { "name": "sym_name", "type": "SymbolNameAttr" },
      { "name": "target", "type": "HAL_ExecutableTargetAttr" },
      { "name": "objects", "type": "OptionalAttr" },
      { "name": "sources", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "custom<SymbolVisibility>($sym_visibility)\n    $sym_name\n    `target` `(` $target `)`\n    (`objects` `(` $objects^ `)` )?\n    (`sources` `(` $sources^ `)` )?\n    attr-dict-with-keyword\n    $body"
  },
  {
    "name": "hal.executable.variant_end",
    "summary": "Terminator pseudo-op for the executable variant op.",
    "assemblyFormat": "attr-dict"
  },
  {
    "name": "hal.fence.await",
    "summary": "Asynchronous fence wait operation.",
    "description": "Yields the caller until all fences is reached. Returns the `status` of the\n    fence after the wait, with a non-zero value indicating failure.",
    "inputs": [
      { "name": "timeout_millis", "type": "I32" },
      { "name": "fences", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "status", "type": "Util_Status" }
    ],
    "attributes": [
      { "name": "flags", "type": "HAL_WaitFlagBitfieldAttr" }
    ],
    "assemblyFormat": "`until` `(` `[` $fences `]` `)`\n    `timeout_millis` `(` $timeout_millis `)`\n    `flags` `(` $flags `)`\n    `:` type($status)\n    attr-dict-with-keyword"
  },
  {
    "name": "hal.fence.create",
    "summary": "Creates an unsignaled fence.",
    "description": "Returns a fence that defines a point in time. By default fences will remain\n    unsignaled unless they are explicitly signaled with `hal.fence.signal` or\n    asynchronously signaled by the device by passing them as an operand to\n    queue submission ops.",
    "inputs": [
      { "name": "device", "type": "HAL_Device" }
    ],
    "outputs": [
      { "name": "result", "type": "HAL_Fence" }
    ],
    "attributes": [
      { "name": "flags", "type": "HAL_FenceFlagBitfieldAttr" }
    ],
    "assemblyFormat": "`device` `(` $device `:` type($device) `)`\n    `flags` `(` $flags `)`\n    `:` type($result)\n    attr-dict-with-keyword"
  },
  {
    "name": "hal.fence.fail",
    "summary": "Fence failure operation.",
    "description": "Signals the fence with a failure. The `status` will be returned from\n    each timepoint semaphores `hal.semaphore.query` and `hal.semaphore.signal`\n    for the lifetime of each semaphore.",
    "inputs": [
      { "name": "fence", "type": "HAL_Fence" },
      { "name": "status", "type": "Util_Status" }
    ],
    "assemblyFormat": "`<` $fence `:` type($fence) `>`\n    `status` `(` $status `)`\n    attr-dict-with-keyword"
  },
  {
    "name": "hal.fence.join",
    "summary": "Creates a fence from the given timepoints.",
    "description": "Returns a fence that joins the input fences as a wait-all operation.",
    "inputs": [
      { "name": "fences", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "result", "type": "HAL_Fence" }
    ],
    "attributes": [
      { "name": "flags", "type": "HAL_FenceFlagBitfieldAttr" }
    ],
    "assemblyFormat": "`at` `(` `[` $fences `]` `)`\n    `flags` `(` $flags `)`\n    `->` type($result)\n    attr-dict-with-keyword"
  },
  {
    "name": "hal.fence.query",
    "summary": "Fence query operation.",
    "description": "Queries whether the fence has been reached and its status.\n    Returns OK if the fence has been signaled successfully, DEFERRED if it is\n    unsignaled, and otherwise an error indicating the failure.",
    "inputs": [
      { "name": "fence", "type": "HAL_Fence" }
    ],
    "outputs": [
      { "name": "status", "type": "Util_Status" }
    ],
    "assemblyFormat": "`<` $fence `:` type($fence) `>`\n    `:` type($status)\n    attr-dict-with-keyword"
  },
  {
    "name": "hal.fence.signal",
    "summary": "Fence signal operation.",
    "description": "Signals the fence to indicate that the timepoints contained have been\n    reached. Waiting work may begin immediately.",
    "inputs": [
      { "name": "fence", "type": "HAL_Fence" }
    ],
    "assemblyFormat": "`<` $fence `:` type($fence) `>`\n    attr-dict-with-keyword"
  },
  {
    "name": "hal.instrument.memory.load",
    "summary": "Emits a memory load instrumentation event.",
    "description": "Emits a workgroup-specific memory load event indicating that a number of\n    bytes from the given resolved pointer have been loaded by the workgroup.",
    "inputs": [
      { "name": "buffer", "type": "AnyMemRef" },
      { "name": "workgroupKey", "type": "Index" },
      { "name": "loadValue", "type": "AnyType" },
      { "name": "base", "type": "AnyMemRef" },
      { "name": "indices", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyType" }
    ],
    "assemblyFormat": "`` `[` $buffer `:` type($buffer) `for` $workgroupKey `]`\n    $base `[` $indices `]` `,` $loadValue\n    attr-dict `:` type($base) `,` type($result)"
  },
  {
    "name": "hal.instrument.memory.store",
    "summary": "Emits a memory store instrumentation event.",
    "description": "Emits a workgroup-specific memory store event indicating that a number of\n    bytes have been stored to the given resolved pointer by the workgroup.",
    "inputs": [
      { "name": "buffer", "type": "AnyMemRef" },
      { "name": "workgroupKey", "type": "Index" },
      { "name": "storeValue", "type": "AnyType" },
      { "name": "base", "type": "AnyMemRef" },
      { "name": "indices", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyType" }
    ],
    "assemblyFormat": "`` `[` $buffer `:` type($buffer) `for` $workgroupKey `]`\n    $base `[` $indices `]` `,` $storeValue\n    attr-dict `:` type($base) `,` type($result)"
  },
  {
    "name": "hal.instrument.print",
    "summary": "Emits a human-readable printf-style string event.",
    "description": "Formats a string using a limited subset of printf format specifiers and the\n    provided values and then emits an `iree_instrument_dispatch_print_t` event. Final\n    formatted string lengths may be limited to as much as 1024 characters and\n    should be kept as small as possible to avoid easily exceeding the\n    instrumentation storage buffers with redundant strings.",
    "inputs": [
      { "name": "buffer", "type": "AnyMemRef" },
      { "name": "workgroupKey", "type": "Index" },
      { "name": "values", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "format", "type": "StrAttr" }
    ],
    "assemblyFormat": "`` `[` $buffer `:` type($buffer) `for` $workgroupKey `]`\n    $format (`*` `(` $values^ `:` type($values) `)`)?\n    attr-dict"
  },
  {
    "name": "hal.instrument.value",
    "summary": "Emits a scalar value instrumentation event.",
    "description": "Emits a workgroup-specific typed value with the given workgroup-relative\n    ordinal.\n\n    This op will be preserved even if the output is not used as it is only for\n    debugging purposes.",
    "inputs": [
      { "name": "buffer", "type": "AnyMemRef" },
      { "name": "workgroupKey", "type": "Index" },
      { "name": "operand", "type": "AnyType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyType" }
    ],
    "attributes": [
      { "name": "ordinal", "type": "AnyI8Attr" }
    ],
    "assemblyFormat": "`` `[` $buffer `:` type($buffer) `for` $workgroupKey `]`\n    $ordinal `=` $operand attr-dict `:` type($operand)"
  },
  {
    "name": "hal.instrument.workgroup",
    "summary": "Emits a dispatch workgroup instrumentation event.",
    "description": "Emits an `iree_instrument_dispatch_workgroup_t` event into the\n    instrumentation stream. The workgroup event identifies the unique dispatch,\n    its workgroup count, and the ID of the emitting workgroup within the\n    dispatch. Optionally targets that support querying the processor ID\n    executing the workgroup can attach that information for tracking purposes.\n\n    On targets such as CPUs where entire workgroups execute as atomic units\n    only one workgroup event should be emitted. On targets such as GPUs where\n    there may be multiple invocations executing as part of a single workgroup\n    only the first invocation within the workgroup should emit the workgroup\n    event (by checking if the LocalInvocationIndex or threadIdx == 0, etc).\n\n    The resulting workgroup key is used by subsequent workgroup-specific\n    instrumentation events.",
    "inputs": [
      { "name": "buffer", "type": "AnyMemRef" },
      { "name": "dispatchId", "type": "I32" }
    ],
    "outputs": [
      { "name": "workgroupKey", "type": "Index" }
    ],
    "assemblyFormat": "`` `[` $buffer `:` type($buffer) `]`\n    `dispatch` `(` $dispatchId `)`\n    attr-dict `:` type($workgroupKey)"
  },
  {
    "name": "hal.interface.binding.subspan",
    "summary": "Returns an alias to a subspan of interface binding data.",
    "description": "Returns a subspan of an interface binding storage buffer in a generic type.\n    The exact shape, type, and alignment of the returned type are defined by\n    the result type (tensor, memref, etc).\n\n    An optional alignment indicates the byte alignment of the base binding\n    resource. Note that the byte offset is added to the base and the alignment\n    will be the minimum of the two.",
    "inputs": [
      { "name": "byte_offset", "type": "Optional" },
      { "name": "dynamic_dims", "type": "HAL_ShapeDynamicDims" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyType" }
    ],
    "attributes": [
      { "name": "layout", "type": "HAL_PipelineLayoutAttr" },
      { "name": "binding", "type": "IndexAttr" },
      { "name": "alignment", "type": "OptionalAttr" },
      { "name": "descriptor_flags", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "`layout` `(` $layout `)`\n    `binding` `(` $binding `)`\n    (`alignment` `(` $alignment^ `)`)?\n    (`offset` `(` $byte_offset^ `)`)?\n    (`flags` `(` $descriptor_flags^ `)`)?\n    attr-dict `:` type($result) (`{` $dynamic_dims^ `}`)?"
  },
  {
    "name": "hal.interface.constant.load",
    "summary": "Loads a constant value from the interface constant block.",
    "description": "Loads a scalar constant value from an executable IO push constant block.\n    The value will be loaded from the given constant offset and will be\n    bitcast (possibly with truncation or zero-extension) to the result type.\n\n    An optional alignment indicates the byte alignment of potential values for\n    the constant when it could be determined from analysis. If omitted the value\n    may be anything and its interpretation is up to the usage. This is intended\n    to provide pointer alignment-like semantics to constants that are used to\n    index into binding resources.\n\n    An optional set of values indicates all possible values that can be passed\n    to the constant from all dispatch sites in the program. If omitted the value\n    may be from an unanalyzable source (outside of the program, indirect, etc)\n    and must be assumed to have any value.",
    "outputs": [
      { "name": "result", "type": "HAL_PrimitiveType" }
    ],
    "attributes": [
      { "name": "layout", "type": "HAL_PipelineLayoutAttr" },
      { "name": "ordinal", "type": "HAL_HostSizeAttr" },
      { "name": "alignment", "type": "OptionalAttr" },
      { "name": "values", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "`layout` `(` $layout `)`\n    `ordinal` `(` $ordinal `)`\n    (`alignment` `(` $alignment^ `)`)?\n    (`values` `(` $values^ `)`)?\n    attr-dict `:` type($result)"
  },
  {
    "name": "hal.interface.workgroup.count",
    "summary": "Returns the total workgroup count of the grid.",
    "description": "The total number of workgroups along each dimension in the dispatch grid.\n    Matches what was passed to the `hal.command_buffer.dispatch` command (or\n    what was indirectly specified).\n\n    Corresponds to the `NumWorkgroups` SPIR-V built-in and the `gridDim` CUDA\n    built-in variable.\n\n    ```mlir\n    %x = hal.interface.workgroup.count[0] : index\n    %y = hal.interface.workgroup.count[1] : index\n    %z = hal.interface.workgroup.count[2] : index\n    ```",
    "outputs": [
      { "name": "result", "type": "HAL_Dim" }
    ],
    "attributes": [
      { "name": "dimension", "type": "IndexAttr" },
      { "name": "upper_bound", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "`[` $dimension `]` (`upper_bound` $upper_bound^)? attr-dict `:` type($result)"
  },
  {
    "name": "hal.interface.workgroup.id",
    "summary": "Returns the index of the current workgroup in the grid.",
    "description": "The global workgroup ID of the current tile in the range of\n    `[0, hal.interface.workgroup.count)` along each XYZ dimension.\n\n    Corresponds to the `WorkgroupId` SPIR-V built-in and the `blockIdx` CUDA\n    built-in variable.\n\n    ```mlir\n    %x = hal.interface.workgroup.id[0] : index\n    %y = hal.interface.workgroup.id[1] : index\n    %z = hal.interface.workgroup.id[2] : index\n    ```",
    "outputs": [
      { "name": "result", "type": "HAL_Dim" }
    ],
    "attributes": [
      { "name": "dimension", "type": "IndexAttr" },
      { "name": "upper_bound", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "`[` $dimension `]` (`upper_bound` $upper_bound^)? attr-dict `:` type($result)"
  },
  {
    "name": "hal.interface.workgroup.size",
    "summary": "Returns the size of each workgroup in invocations.",
    "description": "The number of local invocations within the current workgroup along each\n    dimension. Depending on backend this may map to the SIMT thread count or\n    inner loop nest parameters.\n\n    Corresponds to the `WorkgroupSize` SPIR-V built-in and the `blockDim` CUDA\n    built-in variable.\n\n    ```mlir\n    %x = hal.interface.workgroup.size[0] : index\n    %y = hal.interface.workgroup.size[1] : index\n    %z = hal.interface.workgroup.size[2] : index\n    ```",
    "outputs": [
      { "name": "result", "type": "HAL_Dim" }
    ],
    "attributes": [
      { "name": "dimension", "type": "IndexAttr" },
      { "name": "upper_bound", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "`[` $dimension `]` (`upper_bound` $upper_bound^)? attr-dict `:` type($result)"
  },
  {
    "name": "hal.memory_type",
    "summary": "An iree_hal_memory_type_t for the given memory type bits.",
    "description": "Maps memory type bits to a runtime `iree_hal_memory_type_t` value.",
    "outputs": [
      { "name": "result", "type": "HAL_MemoryType" }
    ],
    "attributes": [
      { "name": "type", "type": "HAL_MemoryTypeBitfieldAttr" }
    ],
    "assemblyFormat": "`<` $type `>`\n    attr-dict\n    `:` type($result)"
  },
  {
    "name": "hal.return",
    "summary": "Return from a hal.* region.",
    "description": "Returns the given values from the region and back to the host code.",
    "inputs": [
      { "name": "operands", "type": "Variadic" }
    ],
    "assemblyFormat": "($operands^ `:` type($operands))? attr-dict"
  },
  {
    "name": "hal.tensor.alias",
    "summary": "Hints that tensor storage should alias a HAL buffer view.",
    "description": "Hints that the backing storage of an entire tensor aliases the given storage\n    buffer. There's no guarantee that the storage will alias and instead only\n    that the tensor contents will be written to the storage as if a copy had\n    occurred. This allows the compiler to avoid copies in the ideal case of a\n    producer that is able to produce directly into the target storage but still\n    handle cases where the producer is not able to be in-place.\n\n    The storage buffer provided must have sufficient space for the tensor once\n    encoded. Dynamically shaped tensors may not consume the entire provided\n    storage. If a buffer view is provided the metadata is ignored and only the\n    backing buffer is used.\n\n    An optional wait fence can be provided in cases where the storage is not\n    immediately available. Producers that may alias the storage will wait until\n    the storage is available before updating the contents.\n\n    Explicit aliasing side-steps any analysis that may be performed by the\n    compiler and requires users to guarantee that the safety of the aliasing.\n    Copy-on-write, alias analysis for overlap detection, and ordering via\n    use-def chains are all ignorant of the aliased buffer memory and only ensure\n    the compiler consumes or produces the aliased memory consistent with itself.\n\n    Example:\n    ```mlir\n    %init = tensor.empty\n    %value = linalg.generic ... outs(%init)\n    %aliased = hal.tensor.alias %value : tensor<...> to %buffer : !hal.buffer\n    ... linalg.generic ins(%aliased) ...\n    ```",
    "inputs": [
      { "name": "source", "type": "AnyTensor" },
      { "name": "source_dims", "type": "HAL_ShapeDynamicDims" },
      { "name": "storage", "type": "AnyTypeOf" },
      { "name": "wait_fence", "type": "Optional" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTensor" }
    ],
    "attributes": [
      { "name": "affinity", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "(`on` `(` $affinity^ `)`)?\n    (`wait` `(` $wait_fence^ `)` `=` `` `>`)?\n    $source `:` type($source) (`{` $source_dims^ `}`)?\n    `to`\n    $storage `:` type($storage)\n    attr-dict"
  },
  {
    "name": "hal.tensor.barrier",
    "summary": "Signals a fence when all tensors are available.",
    "description": "Defines a barrier that is used to indicate availability of an entire set of\n    tensors by signaling a fence. The source tensors are returned for chaining.",
    "inputs": [
      { "name": "sources", "type": "Variadic" },
      { "name": "signal_fence", "type": "HAL_Fence" }
    ],
    "outputs": [
      { "name": "results", "type": "Variadic" }
    ],
    "assemblyFormat": "`join` `` `(` $sources `:` type($sources) `)`\n    `=` `` `>`\n    $signal_fence `:` type($signal_fence)\n    attr-dict-with-keyword"
  },
  {
    "name": "hal.tensor.export",
    "summary": "Exports a tensor to a HAL buffer view.",
    "description": "Defines an export of an SSA-form tensor to an external HAL buffer view.\n\n    The provided `source_encoding`, if different from the `source` type,\n    indicates that the ABI-facing type may differ from the internal\n    representation. The types must be bitcastable (same storage size) and\n    dynamically shaped values must have the same number of dynamic dimensions.\n    This allows for casting between rank-0 and rank-N types, different element\n    types, etc.",
    "inputs": [
      { "name": "source", "type": "AnyTensor" },
      { "name": "source_dims", "type": "HAL_ShapeDynamicDims" }
    ],
    "outputs": [
      { "name": "target", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "source_encoding", "type": "TypeAttr" },
      { "name": "name", "type": "OptionalAttr" },
      { "name": "affinity", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "(`on` `(` $affinity^ `)`)?\n    $source\n    ($name^)?\n    `:`\n    custom<TypeAlias>($source_encoding, type($source)) (`{` $source_dims^ `}`)?\n    `->`\n    type($target)\n    attr-dict"
  },
  {
    "name": "hal.tensor.import",
    "summary": "Imports a tensor from a HAL buffer view.",
    "description": "Defines an import of an external HAL buffer view into a SSA-form tensor.\n    An optional fence can be specified indicating when the buffer view is\n    available for use. If no fence is provided it is assumed the buffer view is\n    immediately available.\n\n    The provided `target_encoding`, if different from the `target` type,\n    indicates that the ABI-facing type may differ from the internal\n    representation. The types must be bitcastable (same storage size) and\n    dynamically shaped values must have the same number of dynamic dimensions.\n    This allows for casting between rank-0 and rank-N types, different element\n    types, etc.\n\n    `consume` can be used to indicate a transfer of ownership. Though the\n    imported value may still have external references when consumed a resource\n    will be conceptually released from its existing owner and retained by the\n    importer atomically.",
    "inputs": [
      { "name": "source", "type": "AnyTypeOf" },
      { "name": "target_dims", "type": "HAL_ShapeDynamicDims" },
      { "name": "wait_fence", "type": "Optional" }
    ],
    "outputs": [
      { "name": "target", "type": "AnyTensor" }
    ],
    "attributes": [
      { "name": "target_encoding", "type": "TypeAttr" },
      { "name": "consume", "type": "UnitAttr" },
      { "name": "name", "type": "OptionalAttr" },
      { "name": "affinity", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "(`on` `(` $affinity^ `)`)?\n    (`wait` `(` $wait_fence^ `)` `=` `` `>`)?\n    (`consume` $consume^)?\n    $source\n    ($name^)?\n    `:` type($source) `->`\n    custom<TypeAlias>($target_encoding, type($target)) (`{` $target_dims^ `}`)?\n    attr-dict"
  },
  {
    "name": "irdl.all_of",
    "summary": "Constraints to the intersection of the provided constraints",
    "description": "`irdl.all_of` defines a constraint that accepts any type or attribute that\n    satisfies all of its provided constraints.\n\n    Example:\n\n    ```mlir\n    irdl.dialect @cmath {\n      irdl.type @complex_f32 {\n        %0 = irdl.is i32\n        %1 = irdl.is f32\n        %2 = irdl.any_of(%0, %1) // is 32-bit\n\n        %3 = irdl.is f32\n        %4 = irdl.is f64\n        %5 = irdl.any_of(%3, %4) // is a float\n\n        %6 = irdl.all_of(%2, %5) // is a 32-bit float\n        irdl.parameters(%6)\n      }\n    }\n    ```\n\n    The above program defines a type `complex` inside the dialect `cmath` that\n    has one parameter that must be 32-bit long and a float (in other\n    words, that must be `f32`).",
    "inputs": [
      { "name": "args", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "output", "type": "IRDL_AttributeType" }
    ],
    "assemblyFormat": "`(` $args `)` ` ` attr-dict"
  },
  {
    "name": "irdl.any",
    "summary": "Accept any type or attribute",
    "description": "`irdl.any` defines a constraint that accepts any type or attribute.\n\n    Example:\n\n    ```mlir\n    irdl.dialect @cmath {\n      irdl.type @complex_flexible {\n        %0 = irdl.any\n        irdl.parameters(%0)\n      }\n    }\n    ```\n\n    The above program defines a type `complex_flexible` inside the dialect\n    `cmath` that has a single parameter that can be any attribute.",
    "outputs": [
      { "name": "output", "type": "IRDL_AttributeType" }
    ],
    "assemblyFormat": "attr-dict"
  },
  {
    "name": "irdl.any_of",
    "summary": "Constraints to the union of the provided constraints",
    "description": "`irdl.any_of` defines a constraint that accepts any type or attribute that\n    satisfies at least one of its provided type constraints.\n\n    Example:\n\n    ```mlir\n    irdl.dialect @cmath {\n      irdl.type @complex {\n        %0 = irdl.is i32\n        %1 = irdl.is i64\n        %2 = irdl.is f32\n        %3 = irdl.is f64\n        %4 = irdl.any_of(%0, %1, %2, %3)\n        irdl.parameters(%4)\n      }\n    }\n    ```\n\n    The above program defines a type `complex` inside the dialect `cmath` that\n    has a single type parameter that can be either `i32`, `i64`, `f32` or\n    `f64`.",
    "inputs": [
      { "name": "args", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "output", "type": "IRDL_AttributeType" }
    ],
    "assemblyFormat": "`(` $args `)` ` ` attr-dict"
  },
  {
    "name": "irdl.attribute",
    "summary": "Define a new attribute",
    "description": "`irdl.attribute` defines a new attribute belonging to the `irdl.dialect`\n    parent.\n\n    The attribute parameters can be defined with an `irdl.parameters` operation\n    in the optional region.\n\n    Example:\n\n    ```mlir\n    irdl.dialect @testd {\n      irdl.attribute @enum_attr {\n        %0 = irdl.is \"foo\"\n        %1 = irdl.is \"bar\"\n        %2 = irdl.any_of(%0, %1)\n        irdl.parameters(%2)\n      }\n    }\n    ```\n\n    The above program defines an `enum_attr` attribute inside the `testd`\n    dialect. The attribute has one `StringAttr` parameter that should be\n    either a `\"foo\"` or a `\"bar\"`.",
    "attributes": [
      { "name": "sym_name", "type": "SymbolNameAttr" }
    ],
    "assemblyFormat": "$sym_name attr-dict-with-keyword custom<SingleBlockRegion>($body)"
  },
  {
    "name": "irdl.attributes",
    "summary": "Define the attributes of an operation",
    "description": "`irdl.attributes` defines the attributes of the `irdl.operation` parent\n    operation definition.\n\n    In the following example, `irdl.attributes` defines the attributes of the\n    `attr_op` operation:\n\n    ```mlir\n    irdl.dialect @example {\n\n      irdl.operation @attr_op {\n        %0 = irdl.any\n        %1 = irdl.is i64\n        irdl.attibutes {\n          \"attr1\" = %0,\n          \"attr2\" = %1\n        }\n      }\n    }\n    ```\n\n    The operation will expect an arbitrary attribute \"attr1\" and an\n    attribute \"attr2\" with value `i64`.",
    "inputs": [
      { "name": "attributeValues", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "attributeValueNames", "type": "StrArrayAttr" }
    ],
    "assemblyFormat": "custom<AttributesOp>($attributeValues, $attributeValueNames) attr-dict"
  },
  {
    "name": "irdl.base",
    "summary": "Constraints an attribute/type base",
    "description": "`irdl.base` defines a constraint that only accepts a single type\n    or attribute base, e.g. an `IntegerType`. The attribute base is defined\n    either by a symbolic reference to the corresponding IRDL definition,\n    or by the name of the base. Named bases are prefixed with `!` or `#`\n    respectively for types and attributes.\n\n    Example:\n\n    ```mlir\n    irdl.dialect @cmath {\n      irdl.type @complex {\n        %0 = irdl.base \"!builtin.integer\"\n        irdl.parameters(%0)\n      }\n\n      irdl.type @complex_wrapper {\n        %0 = irdl.base @cmath::@complex\n        irdl.parameters(%0)\n      }\n    }\n    ```\n\n    The above program defines a `cmath.complex` type that expects a single\n    parameter, which is a type with base name `builtin.integer`, which is the\n    name of an `IntegerType` type.\n    It also defines a `cmath.complex_wrapper` type that expects a single\n    parameter, which is a type of base type `cmath.complex`.",
    "outputs": [
      { "name": "output", "type": "IRDL_AttributeType" }
    ],
    "attributes": [
      { "name": "base_ref", "type": "OptionalAttr" },
      { "name": "base_name", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "($base_ref^)? ($base_name^)? ` ` attr-dict"
  },
  {
    "name": "irdl.c_pred",
    "summary": "Constraints an attribute using a C++ predicate",
    "description": "`irdl.c_pred` defines a constraint that is written in C++.\n\n    Dialects using this operation cannot be registered at runtime, as it relies\n    on C++ code.\n\n    Special placeholders can be used to refer to entities in the context where\n    this predicate is used. They serve as \"hooks\" to the enclosing environment.\n    The following special placeholders are supported in constraints for an op:\n\n    * `$_builder` will be replaced by a mlir::Builder instance.\n    * `$_op` will be replaced by the current operation.\n    * `$_self` will be replaced with the entity this predicate is attached to.\n       Compared to ODS, `$_self` is always of type `mlir::Attribute`, and types\n       are manipulated as `TypeAttr` attributes.\n\n    Example:\n    ```mlir\n    irdl.type @op_with_attr {\n      %0 = irdl.c_pred \"::llvm::isa<::mlir::IntegerAttr>($_self)\"\n      irdl.parameters(%0)\n    }\n    ```\n\n    In this example, @op_with_attr is defined as a type with a single\n    parameter, which is an `IntegerAttr`, as constrained by the C++ predicate.",
    "outputs": [
      { "name": "output", "type": "IRDL_AttributeType" }
    ],
    "attributes": [
      { "name": "pred", "type": "StrAttr" }
    ],
    "assemblyFormat": "$pred ` ` attr-dict"
  },
  {
    "name": "irdl.IRDL_Dialect",
    "summary": "Define a new dialect",
    "description": "The `irdl.dialect` operation defines a dialect. All operations, attributes,\n    and types defined inside its region will be part of the dialect.\n\n    Example:\n\n    ```mlir\n    irdl.dialect @cmath {\n      ...\n    }\n    ```\n\n    The above program defines a `cmath` dialect.",
    "attributes": [
      { "name": "sym_name", "type": "SymbolNameAttr" }
    ],
    "assemblyFormat": "$sym_name attr-dict-with-keyword custom<SingleBlockRegion>($body)"
  },
  {
    "name": "irdl.is",
    "summary": "Constraints an attribute/type to be a specific attribute instance",
    "description": "`irdl.is` defines a constraint that only accepts a specific instance of a\n    type or attribute.\n\n    Example:\n\n    ```mlir\n    irdl.dialect @cmath {\n      irdl.type @complex_i32 {\n        %0 = irdl.is i32\n        irdl.parameters(%0)\n      }\n    }\n    ```\n\n    The above program defines a `complex_i32` type inside the dialect `cmath`\n    that can only have a `i32` as its parameter.",
    "outputs": [
      { "name": "output", "type": "IRDL_AttributeType" }
    ],
    "attributes": [
      { "name": "expected", "type": "AnyAttr" }
    ],
    "assemblyFormat": "$expected ` ` attr-dict"
  },
  {
    "name": "irdl.operands",
    "summary": "Define the operands of an operation",
    "description": "`irdl.operands` define the operands of the `irdl.operation` parent operation\n    definition. Each operand is named after an identifier.\n\n    In the following example, `irdl.operands` defines the operands of the\n    `mul` operation:\n\n    ```mlir\n    irdl.dialect @cmath {\n\n      irdl.type @complex { /* ... */ }\n\n      irdl.operation @mul {\n        %0 = irdl.any\n        %1 = irdl.parametric @cmath::@complex<%0>\n        irdl.results(res: %1)\n        irdl.operands(lhs: %1, rhs: %1)\n      }\n    }\n    ```\n\n    The `mul` operation will expect two operands of type `cmath.complex`, that\n    have the same type, and return a result of the same type.\n\n    The operands can also be marked as variadic or optional:\n    ```mlir\n    irdl.operands(foo: %0, bar: single %1, baz: optional %2, qux: variadic %3)\n    ```\n\n    Here, foo and bar are required single operands, baz is an optional operand,\n    and qux is a variadic operand.\n\n    When more than one operand is marked as optional or variadic, the operation\n    will expect a 'operandSegmentSizes' attribute that defines the number of\n    operands in each segment.",
    "inputs": [
      { "name": "args", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "names", "type": "StrArrayAttr" },
      { "name": "variadicity", "type": "VariadicityArrayAttr" }
    ],
    "assemblyFormat": "`` custom<NamedValueListWithVariadicity>($args, $names, $variadicity) attr-dict"
  },
  {
    "name": "irdl.operation",
    "summary": "Define a new operation",
    "description": "`irdl.operation` defines a new operation belonging to the `irdl.dialect`\n    parent.\n\n    Operations can define constraints on their operands and results with the\n    `irdl.results` and `irdl.operands` operations. If these operations are not\n    present in the region, the results or operands are expected to be empty.\n\n    Example:\n\n    ```mlir\n    irdl.dialect @cmath {\n\n      irdl.type @complex { /* ... */ }\n\n      irdl.operation @norm {\n        %0 = irdl.any\n        %1 = irdl.parametric @cmath::@complex<%0>\n        irdl.results(%0)\n        irdl.operands(%1)\n      }\n    }\n    ```\n\n    The above program defines an operation `norm` inside the dialect `cmath`.\n    The operation expects a single operand of base type `cmath.complex`, and\n    returns a single result of the element type of the operand.",
    "attributes": [
      { "name": "sym_name", "type": "SymbolNameAttr" }
    ],
    "assemblyFormat": "$sym_name attr-dict-with-keyword custom<SingleBlockRegion>($body)"
  },
  {
    "name": "irdl.parameters",
    "summary": "Define the constraints on parameters of a type/attribute definition",
    "description": "`irdl.parameters` defines the constraints on parameters of a type or\n    attribute definition. Each parameter is named after an identifier.\n\n    Example:\n\n    ```mlir\n    irdl.dialect @cmath {\n      irdl.type @complex {\n        %0 = irdl.is i32\n        %1 = irdl.is i64\n        %2 = irdl.any_of(%0, %1)\n        irdl.parameters(elem: %2)\n      }\n    }\n    ```\n\n    The above program defines a type `complex` inside the dialect `cmath`. The\n    type has a single parameter `elem` that should be either `i32` or `i64`.",
    "inputs": [
      { "name": "args", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "names", "type": "StrArrayAttr" }
    ],
    "assemblyFormat": "`` custom<NamedValueList>($args, $names) attr-dict"
  },
  {
    "name": "irdl.parametric",
    "summary": "Constraints an attribute/type base and its parameters",
    "description": "`irdl.parametric` defines a constraint that accepts only a single type\n    or attribute base. The attribute base is defined by a symbolic reference\n    to the corresponding definition. It will additionally constraint the\n    parameters of the type/attribute.\n\n    Example:\n\n    ```mlir\n    irdl.dialect @cmath {\n\n      irdl.type @complex { /* ... */ }\n\n      irdl.operation @norm {\n        %0 = irdl.any\n        %1 = irdl.parametric @cmath::@complex<%0>\n        irdl.operands(%1)\n        irdl.results(%0)\n      }\n    }\n    ```\n\n    The above program defines an operation `norm` inside the dialect `cmath` that\n    for any `T` takes a `cmath.complex` with parameter `T` and returns a `T`.",
    "inputs": [
      { "name": "args", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "output", "type": "IRDL_AttributeType" }
    ],
    "attributes": [
      { "name": "base_type", "type": "SymbolRefAttr" }
    ],
    "assemblyFormat": "$base_type `<` $args `>` ` ` attr-dict"
  },
  {
    "name": "irdl.region",
    "summary": "Define a region of an operation",
    "description": "The irdl.region construct defines a set of characteristics\n    that a region of an operation should satify. Each region is named after\n    an identifier.\n\n    These characteristics include constraints for the entry block arguments\n    of the region and the total number of blocks it contains.\n    The number of blocks must be a non-zero and non-negative integer,\n    and it is optional by default.\n    The set of constraints for the entry block arguments may be optional or\n    empty. If no parentheses are provided, the set is assumed to be optional,\n    and the arguments are not constrained in any way. If parentheses are\n    provided with no arguments, it means that the region must have\n    no entry block arguments\n\n\n    Example:\n\n    ```mlir\n    irdl.dialect @example {\n      irdl.operation @op_with_regions {\n          %r0 = irdl.region\n          %r1 = irdl.region()\n          %v0 = irdl.is i32\n          %v1 = irdl.is i64\n          %r2 = irdl.region(%v0, %v1)\n          %r3 = irdl.region with size 3\n\n          irdl.regions(foo: %r0, bar: %r1, baz: %r2, qux: %r3)\n      }\n    }\n    ```\n\n    The above snippet demonstrates an operation named `@op_with_regions`,\n    which is constrained to have four regions.\n\n    * Region `foo` doesn't have any constraints on the arguments\n      or the number of blocks.\n    * Region `bar` should have an empty set of arguments.\n    * Region `baz` should have two arguments of types `i32` and `i64`.\n    * Region `qux` should contain exactly three blocks.",
    "inputs": [
      { "name": "entryBlockArgs", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "output", "type": "IRDL_RegionType" }
    ],
    "attributes": [
      { "name": "numberOfBlocks", "type": "OptionalAttr" },
      { "name": "constrainedArguments", "type": "UnitAttr" }
    ],
    "assemblyFormat": "``(`(` $entryBlockArgs $constrainedArguments^ `)`)?\n    ``(` ` `with` `size` $numberOfBlocks^)? attr-dict"
  },
  {
    "name": "irdl.regions",
    "summary": "Define the regions of an operation",
    "description": "`irdl.regions` defines the regions of an operation by accepting\n    values produced by `irdl.region` operation as arguments. Each\n    region has an identifier as name.\n\n    Example:\n\n    ```mlir\n    irdl.dialect @example {\n      irdl.operation @op_with_regions {\n        %r1 = irdl.region with size 3\n        %0 = irdl.any\n        %r2 = irdl.region(%0)\n        irdl.regions(foo: %r1, bar: %r2)\n      }\n    }\n    ```\n\n    In the snippet above the operation is constrained to have two regions.\n    The first region (`foo`) should contain three blocks.\n    The second region (`bar`) should have one region with one argument.",
    "inputs": [
      { "name": "args", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "names", "type": "StrArrayAttr" }
    ],
    "assemblyFormat": "`` custom<NamedValueList>($args, $names) attr-dict"
  },
  {
    "name": "irdl.results",
    "summary": "Define the results of an operation",
    "description": "`irdl.results` define the results of the `irdl.operation` parent operation\n    definition. Each result is named after an identifier.\n\n    In the following example, `irdl.results` defines the results of the\n    `get_values` operation:\n\n    ```mlir\n    irdl.dialect @cmath {\n\n      irdl.type @complex { /* ... */ }\n\n      /// Returns the real and imaginary parts of a complex number.\n      irdl.operation @get_values {\n        %0 = irdl.any\n        %1 = irdl.parametric @cmath::@complex<%0>\n        irdl.results(re: %0, im: %0)\n        irdl.operands(complex: %1)\n      }\n    }\n    ```\n\n    The operation will expect one operand of the `cmath.complex` type, and two\n    results that have the underlying type of the `cmath.complex`.\n\n    The results can also be marked as variadic or optional:\n    ```mlir\n    irdl.results(foo: %0, bar: single %1, baz: optional %2, qux: variadic %3)\n    ```\n\n    Here, foo and bar are required single results, baz is an optional result,\n    and qux is a variadic result.\n\n    When more than one result is marked as optional or variadic, the operation\n    will expect a 'resultSegmentSizes' attribute that defines the number of\n    results in each segment.",
    "inputs": [
      { "name": "args", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "names", "type": "StrArrayAttr" },
      { "name": "variadicity", "type": "VariadicityArrayAttr" }
    ],
    "assemblyFormat": "`` custom<NamedValueListWithVariadicity>($args, $names, $variadicity) attr-dict"
  },
  {
    "name": "irdl.type",
    "summary": "Define a new type",
    "description": "`irdl.type` defines a new type belonging to the `irdl.dialect` parent.\n\n    The type parameters can be defined with an `irdl.parameters` operation in\n    the optional region.\n\n    Example:\n\n    ```mlir\n    irdl.dialect @cmath {\n      irdl.type @complex {\n        %0 = irdl.is i32\n        %1 = irdl.is i64\n        %2 = irdl.any_of(%0, %1)\n        irdl.parameters(%2)\n      }\n    }\n    ```\n\n    The above program defines a type `complex` inside the dialect `cmath`. The\n    type has a single parameter that should be either `i32` or `i64`.",
    "attributes": [
      { "name": "sym_name", "type": "SymbolNameAttr" }
    ],
    "assemblyFormat": "$sym_name attr-dict-with-keyword custom<SingleBlockRegion>($body)"
  },
  {
    "name": "linalg.index",
    "summary": "linalg index operation",
    "description": "The `linalg.index` operation returns the iteration index of the immediately\n    enclosing linalg structured operation for the iteration dimension `dim`. The\n    `dim` attribute specifies the position of the accessed dimension in the\n    indexing map domain.\n\n    Example:\n\n    ```mlir\n    #map = affine_map<(i, j) -> (i, j)>\n    linalg.generic {indexing_maps = [#map, #map],\n                    iterator_types = [\"parallel\", \"parallel\"]}\n      outs(%I, %J : memref<?x?xindex>, memref<?x?xindex>) {\n      ^bb0(%arg0 : index, %arg1 : index):\n      // Access the outer iteration dimension i\n      %i = linalg.index 0 : index\n      // Access the inner iteration dimension j\n      %j = linalg.index 1 : index\n      linalg.yield %i, %j : index, index\n    }\n    ```\n\n    This may lower to IR resembling:\n\n    ```mlir\n    %0 = dim %I, %c0 : memref<?x?xindex>\n    %1 = dim %I, %c1 : memref<?x?xindex>\n    scf.for %i = %c0 to %0 step %c1 {\n      scf.for %j = %c0 to %1 step %c1 {\n        store %i, %I[%i, %j] : memref<?x?xindex>\n        store %j, %J[%i, %j] : memref<?x?xindex>\n      }\n    }\n    ```",
    "assemblyFormat": "$dim attr-dict `:` type($result)"
  },
  {
    "name": "linalg.softmax",
    "summary": "Softmax operator",
    "description": "linalg.softmax computes a numerically stable version of softmax.\n\n    For a given input tensor and a specified dimension `d`, compute:\n      1. the max `m` along that dimension `d`\n      2. f(x) = exp(x - m)\n      3. sum f(x) along dimension d to get l(x).\n      4. compute the final result f(x) / l(x).\n\n    This is an aggregate linalg operation that further reduces to a small DAG of\n    structured operations.\n\n    Warning: Regarding the tiling capabilities, the implementation doesn't\n    check that the provided dimensions make sense. This is the responsability\n    of the transformation calling the tiling to ensure that the provided\n    sizes for each dimension make sense with respect to the semantic of\n    softmax.",
    "inputs": [
      { "name": "input", "type": "AnyShaped" },
      { "name": "output", "type": "AnyShaped" }
    ],
    "outputs": [
      { "name": "result", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "dimension", "type": "I64Attr" }
    ],
    "assemblyFormat": "attr-dict\n    `dimension` `(` $dimension `)`\n    `ins` `(` $input `:` type($input) `)`\n    `outs` `(` $output `:` type($output) `)`\n    (`->` type($result)^)?",
    "category": "Activation"
  },
  {
    "name": "linalg.winograd_filter_transform",
    "summary": "Winograd filter transform operator",
    "description": "Winograd Conv2D algorithm will convert linalg Conv2D operator into batched\n    matrix multiply. Before the matrix multiply, it will convert filter and\n    input into a format suitable for batched matrix multiply. After the matrix\n    multiply, it will convert output to the final result tensor.\n\n    The algorithm F(m x m, r x r) is\n\n    Y = A^T x [(G x g x G^T) @ (B^T x d x B)] x A\n\n    The size of output Y is m x m. The size of filter g is r x r. The size of\n    input d is (m + r - 1) x (m + r - 1). A^T, A, G^T, G, B^T, and B are\n    transformation matrices.\n\n    This operator is defined to represent the high level concept of filter\n    transformation (G x g x G^T) in the Winograd Conv2D algorithm.",
    "inputs": [
      { "name": "filter", "type": "TensorRankOf" },
      { "name": "output", "type": "TensorRankOf" },
      { "name": "fmr", "type": "WinogradConv2DFmr" }
    ],
    "outputs": [
      { "name": "result", "type": "TensorRankOf" }
    ],
    "assemblyFormat": "attr-dict\n    `fmr` `(` $fmr `)`\n    `ins` `(` $filter `:` type($filter) `)`\n    `outs` `(` $output `:` type($output) `)`\n    `->` type($result)"
  },
  {
    "name": "linalg.winograd_input_transform",
    "summary": "Winograd input transform operator",
    "description": "Winograd Conv2D algorithm will convert linalg Conv2D operator into batched\n    matrix multiply. Before the matrix multiply, it will convert filter and\n    input into a format suitable for batched matrix multiply. After the matrix\n    multiply, it will convert output to the final result tensor.\n\n    The algorithm F(m x m, r x r) is\n\n    Y = A^T x [(G x g x G^T) @ (B^T x d x B)] x A\n\n    The size of output Y is m x m. The size of filter g is r x r. The size of\n    input d is (m + r - 1) x (m + r - 1). A^T, A, G^T, G, B^T, and B are\n    transformation matrices.\n\n    This operator is defined to represent the high level concept of input\n    transformation (B^T x d x B) in the Winograd Conv2D algorithm.",
    "inputs": [
      { "name": "input", "type": "TensorRankOf" },
      { "name": "output", "type": "TensorRankOf" },
      { "name": "fmr", "type": "WinogradConv2DFmr" }
    ],
    "outputs": [
      { "name": "result", "type": "TensorRankOf" }
    ],
    "assemblyFormat": "attr-dict\n    `fmr` `(` $fmr `)`\n    `ins` `(` $input `:` type($input) `)`\n    `outs` `(` $output `:` type($output) `)`\n    `->` type($result)"
  },
  {
    "name": "linalg.winograd_output_transform",
    "summary": "Winograd output transform operator",
    "description": "Winograd Conv2D algorithm will convert linalg Conv2D operator into batched\n    matrix multiply. Before the matrix multiply, it will convert filter and\n    input into a format suitable for batched matrix multiply. After the matrix\n    multiply, it will convert output to the final result tensor.\n\n    The algorithm F(m x m, r x r) is\n\n    Y = A^T x [(G x g x G^T) @ (B^T x d x B)] x A\n\n    The size of output Y is m x m. The size of filter g is r x r. The size of\n    input d is (m + r - 1) x (m + r - 1). A^T, A, G^T, G, B^T, and B are\n    transformation matrices.\n\n    This operator is defined to represent the high level concept of output\n    transformation (A^T x y x A) in the Winograd Conv2D algorithm.",
    "inputs": [
      { "name": "value", "type": "TensorRankOf" },
      { "name": "output", "type": "TensorRankOf" },
      { "name": "fmr", "type": "WinogradConv2DFmr" }
    ],
    "outputs": [
      { "name": "result", "type": "TensorRankOf" }
    ],
    "assemblyFormat": "attr-dict\n    `fmr` `(` $fmr `)`\n    `ins` `(` $value `:` type($value) `)`\n    `outs` `(` $output `:` type($output) `)`\n    `->` type($result)"
  },
  {
    "name": "linalg.yield",
    "summary": "Linalg yield operation",
    "description": "`linalg.yield` is a special terminator operation for blocks inside regions\n    in `linalg` generic ops. It returns values to the immediately enclosing\n    `linalg` generic op.\n\n    Example:\n\n    ```mlir\n    linalg.yield %f0, %f1 : f32, f32\n    ```"
  },
  {
    "name": "memref.alloc",
    "summary": "memory allocation operation",
    "description": "The `alloc` operation allocates a region of memory, as specified by its\n    memref type.\n\n    Example:\n\n    ```mlir\n    %0 = memref.alloc() : memref<8x64xf32, 1>\n    ```\n\n    The optional list of dimension operands are bound to the dynamic dimensions\n    specified in its memref type. In the example below, the ssa value '%d' is\n    bound to the second dimension of the memref (which is dynamic).\n\n    ```mlir\n    %0 = memref.alloc(%d) : memref<8x?xf32, 1>\n    ```\n\n    The optional list of symbol operands are bound to the symbols of the\n    memrefs affine map. In the example below, the ssa value '%s' is bound to\n    the symbol 's0' in the affine map specified in the allocs memref type.\n\n    ```mlir\n    %0 = memref.alloc()[%s] : memref<8x64xf32,\n                              affine_map<(d0, d1)[s0] -> ((d0 + s0), d1)>, 1>\n    ```\n\n    This operation returns a single ssa value of memref type, which can be used\n    by subsequent load and store operations.\n\n    The optional `alignment` attribute may be specified to ensure that the\n    region of memory that will be indexed is aligned at the specified byte\n    boundary.\n\n    ```mlir\n    %0 = memref.alloc()[%s] {alignment = 8} :\n      memref<8x64xf32, affine_map<(d0, d1)[s0] -> ((d0 + s0), d1)>, 1>\n    ```",
    "inputs": [
      { "name": "dynamicSizes", "type": "Variadic" },
      { "name": "symbolOperands", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "memref", "type": "Res" }
    ],
    "attributes": [
      { "name": "alignment", "type": "ConfinedAttr" }
    ],
    "assemblyFormat": "`(`$dynamicSizes`)` (`` `[` $symbolOperands^ `]`)? attr-dict `:` type($memref)"
  },
  {
    "name": "memref.alloca",
    "summary": "stack memory allocation operation",
    "description": "The `alloca` operation allocates memory on the stack, to be automatically\n    released when control transfers back from the region of its closest\n    surrounding operation with an\n    [`AutomaticAllocationScope`](../Traits/#automaticallocationscope) trait.\n    The amount of memory allocated is specified by its memref and additional\n    operands. For example:\n\n    ```mlir\n    %0 = memref.alloca() : memref<8x64xf32>\n    ```\n\n    The optional list of dimension operands are bound to the dynamic dimensions\n    specified in its memref type. In the example below, the SSA value '%d' is\n    bound to the second dimension of the memref (which is dynamic).\n\n    ```mlir\n    %0 = memref.alloca(%d) : memref<8x?xf32>\n    ```\n\n    The optional list of symbol operands are bound to the symbols of the\n    memref's affine map. In the example below, the SSA value '%s' is bound to\n    the symbol 's0' in the affine map specified in the allocs memref type.\n\n    ```mlir\n    %0 = memref.alloca()[%s] : memref<8x64xf32,\n                               affine_map<(d0, d1)[s0] -> ((d0 + s0), d1)>>\n    ```\n\n    This operation returns a single SSA value of memref type, which can be used\n    by subsequent load and store operations. An optional alignment attribute, if\n    specified, guarantees alignment at least to that boundary. If not specified,\n    an alignment on any convenient boundary compatible with the type will be\n    chosen.",
    "inputs": [
      { "name": "dynamicSizes", "type": "Variadic" },
      { "name": "symbolOperands", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "memref", "type": "Res" }
    ],
    "attributes": [
      { "name": "alignment", "type": "ConfinedAttr" }
    ],
    "assemblyFormat": "`(`$dynamicSizes`)` (`` `[` $symbolOperands^ `]`)? attr-dict `:` type($memref)"
  },
  {
    "name": "memref.alloca_scope",
    "summary": "explicitly delimited scope for stack allocation",
    "description": "The `memref.alloca_scope` operation represents an explicitly-delimited\n    scope for the alloca allocations. Any `memref.alloca` operations that are\n    used within this scope are going to be cleaned up automatically once\n    the control-flow exits the nested region. For example:\n\n    ```mlir\n    memref.alloca_scope {\n      %myalloca = memref.alloca(): memref<4x3xf32>\n      ...\n    }\n    ```\n\n    Here, `%myalloca` memref is valid within the explicitly delimited scope\n    and is automatically deallocated at the end of the given region. Conceptually,\n    `memref.alloca_scope` is a passthrough operation with\n    `AutomaticAllocationScope` that spans the body of the region within the operation.\n\n    `memref.alloca_scope` may also return results that are defined in the nested\n    region. To return a value, one should use `memref.alloca_scope.return`\n    operation:\n\n    ```mlir\n    %result = memref.alloca_scope {\n      ...\n      memref.alloca_scope.return %value\n    }\n    ```\n\n    If `memref.alloca_scope` returns no value, the `memref.alloca_scope.return ` can\n    be left out, and will be inserted implicitly.",
    "outputs": [
      { "name": "results", "type": "Variadic" }
    ]
  },
  {
    "name": "memref.alloca_scope.return",
    "summary": "terminator for alloca_scope operation",
    "description": "`memref.alloca_scope.return` operation returns zero or more SSA values\n    from the region within `memref.alloca_scope`. If no values are returned,\n    the return operation may be omitted. Otherwise, it has to be present\n    to indicate which values are going to be returned. For example:\n\n    ```mlir\n    memref.alloca_scope.return %value\n    ```",
    "inputs": [
      { "name": "results", "type": "Variadic" }
    ],
    "assemblyFormat": "attr-dict ($results^ `:` type($results))?"
  },
  {
    "name": "memref.assume_alignment",
    "summary": "assumption that gives alignment information to the input memref",
    "description": "The `assume_alignment` operation takes a memref and an integer alignment\n      value. It returns a new SSA value of the same memref type, but associated\n      with the assumption that the underlying buffer is aligned to the given\n      alignment.\n\n      If the buffer isn't aligned to the given alignment, its result is poison.\n      This operation doesn't affect the semantics of a program where the\n      alignment assumption holds true. It is intended for optimization purposes,\n      allowing the compiler to generate more efficient code based on the\n      alignment assumption. The optimization is best-effort.",
    "inputs": [
      { "name": "memref", "type": "AnyMemRef" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyMemRef" }
    ],
    "attributes": [
      { "name": "alignment", "type": "ConfinedAttr" }
    ],
    "assemblyFormat": "$memref `,` $alignment attr-dict `:` type($memref)"
  },
  {
    "name": "memref.atomic_rmw",
    "summary": "atomic read-modify-write operation",
    "description": "The `memref.atomic_rmw` operation provides a way to perform a read-modify-write\n    sequence that is free from data races. The kind enumeration specifies the\n    modification to perform. The value operand represents the new value to be\n    applied during the modification. The memref operand represents the buffer\n    that the read and write will be performed against, as accessed by the\n    specified indices. The arity of the indices is the rank of the memref. The\n    result represents the latest value that was stored.\n\n    Example:\n\n    ```mlir\n    %x = memref.atomic_rmw \"addf\" %value, %I[%i] : (f32, memref<10xf32>) -> f32\n    ```",
    "inputs": [
      { "name": "value", "type": "AnyTypeOf" },
      { "name": "memref", "type": "Arg" },
      { "name": "indices", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "kind", "type": "AtomicRMWKindAttr" }
    ],
    "assemblyFormat": "$kind $value `,` $memref `[` $indices `]` attr-dict `:` `(` type($value) `,`\n    type($memref) `)` `->` type($result)"
  },
  {
    "name": "memref.atomic_yield",
    "summary": "yield operation for GenericAtomicRMWOp",
    "description": "\"memref.atomic_yield\" yields an SSA value from a\n    GenericAtomicRMWOp region.",
    "inputs": [
      { "name": "result", "type": "AnyType" }
    ],
    "assemblyFormat": "$result attr-dict `:` type($result)"
  },
  {
    "name": "memref.cast",
    "summary": "memref cast operation",
    "description": "The `memref.cast` operation converts a memref from one type to an equivalent\n    type with a compatible shape. The source and destination types are\n    compatible if:\n\n    a. Both are ranked memref types with the same element type, address space,\n    and rank and:\n      1. Both have the same layout or both have compatible strided layouts.\n      2. The individual sizes (resp. offset and strides in the case of strided\n         memrefs) may convert constant dimensions to dynamic dimensions and\n         vice-versa.\n\n    If the cast converts any dimensions from an unknown to a known size, then it\n    acts as an assertion that fails at runtime if the dynamic dimensions\n    disagree with resultant destination size.\n\n    Example:\n\n    ```mlir\n    // Assert that the input dynamic shape matches the destination static shape.\n    %2 = memref.cast %1 : memref<?x?xf32> to memref<4x4xf32>\n    // Erase static shape information, replacing it with dynamic information.\n    %3 = memref.cast %1 : memref<4xf32> to memref<?xf32>\n\n    // The same holds true for offsets and strides.\n\n    // Assert that the input dynamic shape matches the destination static stride.\n    %4 = memref.cast %1 : memref<12x4xf32, strided<[?, ?], offset: ?>> to\n                          memref<12x4xf32, strided<[4, 1], offset: 5>>\n    // Erase static offset and stride information, replacing it with\n    // dynamic information.\n    %5 = memref.cast %1 : memref<12x4xf32, strided<[4, 1], offset: 5>> to\n                          memref<12x4xf32, strided<[?, ?], offset: ?>>\n    ```\n\n    b. Either or both memref types are unranked with the same element type, and\n    address space.\n\n    Example:\n\n    ```mlir\n    Cast to concrete shape.\n        %4 = memref.cast %1 : memref<*xf32> to memref<4x?xf32>\n\n    Erase rank information.\n        %5 = memref.cast %1 : memref<4x?xf32> to memref<*xf32>\n    ```",
    "inputs": [
      { "name": "source", "type": "AnyRankedOrUnrankedMemRef" }
    ],
    "outputs": [
      { "name": "dest", "type": "AnyRankedOrUnrankedMemRef" }
    ],
    "assemblyFormat": "$source attr-dict `:` type($source) `to` type($dest)"
  },
  {
    "name": "memref.collapse_shape",
    "summary": "operation to produce a memref with a smaller rank.",
    "description": "The `memref.collapse_shape` op produces a new view with a smaller rank\n    whose sizes are a reassociation of the original `view`. The operation is\n    limited to such reassociations, where subsequent, contiguous dimensions are\n    collapsed into a single dimension. Such reassociations never require\n    additional allocs or copies.\n\n    Collapsing non-contiguous dimensions is undefined behavior. When a group of\n    dimensions can be statically proven to be non-contiguous, collapses of such\n    groups are rejected in the verifier on a best-effort basis. In the general\n    case, collapses of dynamically-sized dims with dynamic strides cannot be\n    proven to be contiguous or non-contiguous due to limitations in the memref\n    type.\n\n    A reassociation is defined as a continuous grouping of dimensions and is\n    represented with an array of DenseI64ArrayAttr attribute.\n\n    Note: Only the dimensions within a reassociation group must be contiguous.\n    The remaining dimensions may be non-contiguous.\n\n    The result memref type can be zero-ranked if the source memref type is\n    statically shaped with all dimensions being unit extent. In such a case, the\n    reassociation indices must be empty.\n\n    Examples:\n\n    ```mlir\n    // Dimension collapse (i, j) -> i' and k -> k'\n    %1 = memref.collapse_shape %0 [[0, 1], [2]] :\n        memref<?x?x?xf32, stride_spec> into memref<?x?xf32, stride_spec_2>\n    ```\n\n    For simplicity, this op may not be used to cast dynamicity of dimension\n    sizes and/or strides. I.e., a result dimension must be dynamic if and only\n    if at least one dimension in the corresponding reassociation group is\n    dynamic. Similarly, the stride of a result dimension must be dynamic if and\n    only if the corresponding start dimension in the source type is dynamic.\n\n    Note: This op currently assumes that the inner strides are of the\n    source/result layout map are the faster-varying ones.",
    "inputs": [
      { "name": "src", "type": "AnyStridedMemRef" }
    ],
    "attributes": [
      { "name": "reassociation", "type": "IndexListArrayAttr" }
    ],
    "assemblyFormat": "$src $reassociation attr-dict `:` type($src) `into` type($result)"
  },
  {
    "name": "memref.copy",
    "description": "Copies the data from the source to the destination memref.\n\n    Usage:\n\n    ```mlir\n    memref.copy %arg0, %arg1 : memref<?xf32> to memref<?xf32>\n    ```\n\n    Source and destination are expected to have the same element type and shape.\n    Otherwise, the result is undefined. They may have different layouts.",
    "inputs": [
      { "name": "source", "type": "Arg" },
      { "name": "target", "type": "Arg" }
    ],
    "assemblyFormat": "$source `,` $target attr-dict `:` type($source) `to` type($target)"
  },
  {
    "name": "memref.dealloc",
    "summary": "memory deallocation operation",
    "description": "The `dealloc` operation frees the region of memory referenced by a memref\n    which was originally created by the `alloc` operation.\n    The `dealloc` operation should not be called on memrefs which alias an\n    alloc'd memref (e.g. memrefs returned by `view` operations).\n\n    Example:\n\n    ```mlir\n    %0 = memref.alloc() : memref<8x64xf32, affine_map<(d0, d1) -> (d0, d1), 1>>\n    memref.dealloc %0 : memref<8x64xf32,  affine_map<(d0, d1) -> (d0, d1), 1>>\n    ```",
    "inputs": [
      { "name": "memref", "type": "Arg" }
    ],
    "assemblyFormat": "$memref attr-dict `:` type($memref)"
  },
  {
    "name": "memref.dim",
    "summary": "dimension index operation",
    "description": "The `dim` operation takes a memref and a dimension operand of type `index`.\n    It returns the size of the requested dimension of the given memref.\n    If the dimension index is out of bounds the behavior is undefined.\n\n    The specified memref type is that of the first operand.\n\n    Example:\n\n    ```mlir\n    // Always returns 4, can be constant folded:\n    %c0 = arith.constant 0 : index\n    %x = memref.dim %A, %c0 : memref<4 x ? x f32>\n\n    // Returns the dynamic dimension of %A.\n    %c1 = arith.constant 1 : index\n    %y = memref.dim %A, %c1 : memref<4 x ? x f32>\n\n    // Equivalent generic form:\n    %x = \"memref.dim\"(%A, %c0) : (memref<4 x ? x f32>, index) -> index\n    %y = \"memref.dim\"(%A, %c1) : (memref<4 x ? x f32>, index) -> index\n    ```",
    "inputs": [
      { "name": "source", "type": "AnyNon0RankedOrUnrankedMemRef" },
      { "name": "index", "type": "Index" }
    ],
    "outputs": [
      { "name": "result", "type": "Index" }
    ],
    "assemblyFormat": "attr-dict $source `,` $index `:` type($source)"
  },
  {
    "name": "memref.distinct_objects",
    "summary": "assumption that acesses to specific memrefs will never alias",
    "description": "The `distinct_objects` operation takes a list of memrefs and returns the same\n      memrefs, with the additional assumption that accesses to them will never\n      alias with each other. This means that loads and stores to different\n      memrefs in the list can be safely reordered.\n\n      If the memrefs do alias, the load/store behavior is undefined. This\n      operation doesn't affect the semantics of a valid program. It is\n      intended for optimization purposes, allowing the compiler to generate more\n      efficient code based on the non-aliasing assumption. The optimization is\n      best-effort.\n\n      Example:\n\n      ```mlir\n      %1, %2 = memref.distinct_objects %a, %b : memref<?xf32>, memref<?xf32>\n      ```",
    "inputs": [
      { "name": "operands", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "results", "type": "Variadic" }
    ],
    "assemblyFormat": "$operands attr-dict `:` type($operands)"
  },
  {
    "name": "memref.dma_start",
    "summary": "non-blocking DMA operation that starts a transfer",
    "description": "Syntax:\n\n    ```\n    operation ::= `memref.dma_start` ssa-use`[`ssa-use-list`]` `,`\n                   ssa-use`[`ssa-use-list`]` `,` ssa-use `,`\n                   ssa-use`[`ssa-use-list`]` (`,` ssa-use `,` ssa-use)?\n                  `:` memref-type `,` memref-type `,` memref-type\n    ```\n\n    DmaStartOp starts a non-blocking DMA operation that transfers data from a\n    source memref to a destination memref. The source and destination memref\n    need not be of the same dimensionality, but need to have the same elemental\n    type. The operands include the source and destination memref's each followed\n    by its indices, size of the data transfer in terms of the number of elements\n    (of the elemental type of the memref), a tag memref with its indices, and\n    optionally at the end, a stride and a number_of_elements_per_stride\n    arguments. The tag location is used by a DmaWaitOp to check for completion.\n    The indices of the source memref, destination memref, and the tag memref\n    have the same restrictions as any load/store. The optional stride arguments\n    should be of 'index' type, and specify a stride for the slower memory space\n    (memory space with a lower memory space id), transferring chunks of\n    number_of_elements_per_stride every stride until %num_elements are\n    transferred. Either both or no stride arguments should be specified. If the\n    source and destination locations overlap the behavior of this operation is\n    not defined.\n\n    For example, a DmaStartOp operation that transfers 256 elements of a memref\n    '%src' in memory space 0 at indices [%i, %j] to memref '%dst' in memory\n    space 1 at indices [%k, %l], would be specified as follows:\n\n    ```mlir\n    %num_elements = arith.constant 256\n    %idx = arith.constant 0 : index\n    %tag = memref.alloc() : memref<1 x i32, affine_map<(d0) -> (d0)>, 4>\n    dma_start %src[%i, %j], %dst[%k, %l], %num_elements, %tag[%idx] :\n      memref<40 x 128 x f32>, affine_map<(d0) -> (d0)>, 0>,\n      memref<2 x 1024 x f32>, affine_map<(d0) -> (d0)>, 1>,\n      memref<1 x i32>, affine_map<(d0) -> (d0)>, 2>\n    ```\n\n    If %stride and %num_elt_per_stride are specified, the DMA is expected to\n    transfer %num_elt_per_stride elements every %stride elements apart from\n    memory space 0 until %num_elements are transferred.\n\n    ```mlir\n    dma_start %src[%i, %j], %dst[%k, %l], %num_elements, %tag[%idx], %stride,\n              %num_elt_per_stride :\n    ```\n\n    * TODO: add additional operands to allow source and destination striding, and\n    multiple stride levels.\n    * TODO: Consider replacing src/dst memref indices with view memrefs.",
    "inputs": [
      { "name": "operands", "type": "Variadic" }
    ]
  },
  {
    "name": "memref.dma_wait",
    "summary": "blocking DMA operation that waits for transfer completion",
    "description": "DmaWaitOp blocks until the completion of a DMA operation associated with the\n   tag element '%tag[%index]'. %tag is a memref, and %index has to be an index\n   with the same restrictions as any load/store index. %num_elements is the\n   number of elements associated with the DMA operation.\n\n   Example:\n\n   ```mlir\n    dma_start %src[%i, %j], %dst[%k, %l], %num_elements, %tag[%index] :\n      memref<2048 x f32>, affine_map<(d0) -> (d0)>, 0>,\n      memref<256 x f32>, affine_map<(d0) -> (d0)>, 1>\n      memref<1 x i32>, affine_map<(d0) -> (d0)>, 2>\n    ...\n    ...\n    dma_wait %tag[%index], %num_elements : memref<1 x i32, affine_map<(d0) -> (d0)>, 2>\n    ```",
    "inputs": [
      { "name": "tagMemRef", "type": "AnyMemRef" },
      { "name": "tagIndices", "type": "Variadic" },
      { "name": "numElements", "type": "Index" }
    ],
    "assemblyFormat": "$tagMemRef `[` $tagIndices `]` `,` $numElements attr-dict `:` type($tagMemRef)"
  },
  {
    "name": "memref.expand_shape",
    "summary": "operation to produce a memref with a higher rank.",
    "description": "The `memref.expand_shape` op produces a new view with a higher rank whose\n    sizes are a reassociation of the original `view`. The operation is limited\n    to such reassociations, where a dimension is expanded into one or multiple\n    contiguous dimensions. Such reassociations never require additional allocs\n    or copies.\n\n    A reassociation is defined as a grouping of dimensions and is represented\n    with an array of DenseI64ArrayAttr attributes.\n\n    Example:\n\n    ```mlir\n    %r = memref.expand_shape %0 [[0, 1], [2]] output_shape [%sz0, %sz1, 32]\n        : memref<?x32xf32> into memref<?x?x32xf32>\n    ```\n\n    If an op can be statically proven to be invalid (e.g, an expansion from\n    `memref<10xf32>` to `memref<2x6xf32>`), it is rejected by the verifier. If\n    it cannot statically be proven invalid (e.g., the full example above; it is\n    unclear whether the first source dimension is divisible by 5), the op is\n    accepted by the verifier. However, if the op is in fact invalid at runtime,\n    the behavior is undefined.\n\n    The source memref can be zero-ranked. In that case, the reassociation\n    indices must be empty and the result shape may only consist of unit\n    dimensions.\n\n    For simplicity, this op may not be used to cast dynamicity of dimension\n    sizes and/or strides. I.e., if and only if a source dimension is dynamic,\n    there must be a dynamic result dimension in the corresponding reassociation\n    group. Same for strides.\n\n    The representation for the output shape supports a partially-static\n    specification via attributes specified through the `static_output_shape`\n    argument.  A special sentinel value `ShapedType::kDynamic` encodes that the\n    corresponding entry has a dynamic value.  There must be exactly as many SSA\n    inputs in `output_shape` as there are `ShapedType::kDynamic` entries in\n    `static_output_shape`.\n\n    Note: This op currently assumes that the inner strides are of the\n    source/result layout map are the faster-varying ones.",
    "inputs": [
      { "name": "src", "type": "AnyStridedMemRef" },
      { "name": "output_shape", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "reassociation", "type": "IndexListArrayAttr" },
      { "name": "static_output_shape", "type": "DenseI64ArrayAttr" }
    ],
    "assemblyFormat": "$src $reassociation `output_shape`\n    custom<DynamicIndexList>($output_shape, $static_output_shape) attr-dict `:`\n    type($src) `into` type($result)"
  },
  {
    "name": "memref.extract_aligned_pointer_as_index",
    "summary": "Extracts a memref's underlying aligned pointer as an index",
    "description": "Extracts the underlying aligned pointer as an index.\n\n    This operation is useful for lowering to lower-level dialects while still\n    avoiding the need to define a pointer type in higher-level dialects such as\n    the memref dialect.\n\n    This operation is intended solely as step during lowering, it has no side\n    effects. A reverse operation that creates a memref from an index interpreted\n    as a pointer is explicitly discouraged.\n\n    Example:\n\n    ```\n      %0 = memref.extract_aligned_pointer_as_index %arg : memref<4x4xf32> -> index\n      %1 = arith.index_cast %0 : index to i64\n      %2 = llvm.inttoptr %1 : i64 to !llvm.ptr\n      call @foo(%2) : (!llvm.ptr) ->()\n    ```",
    "inputs": [
      { "name": "source", "type": "AnyRankedOrUnrankedMemRef" }
    ],
    "outputs": [
      { "name": "aligned_pointer", "type": "Index" }
    ],
    "assemblyFormat": "$source `:` type($source) `->` type(results) attr-dict"
  },
  {
    "name": "memref.extract_strided_metadata",
    "summary": "Extracts a buffer base with offset and strides",
    "description": "Extracts a base buffer, offset and strides. This op allows additional layers\n    of transformations and foldings to be added as lowering progresses from\n    higher-level dialect to lower-level dialects such as the LLVM dialect.\n\n    The op requires a strided memref source operand. If the source operand is not\n    a strided memref, then verification fails.\n\n    This operation is also useful for completeness to the existing memref.dim op.\n    While accessing strides, offsets and the base pointer independently is not\n    available, this is useful for composing with its natural complement op:\n    `memref.reinterpret_cast`.\n\n    Intended Use Cases:\n\n    The main use case is to expose the logic for manipulate memref metadata at a\n    higher level than the LLVM dialect.\n    This makes lowering more progressive and brings the following benefits:\n      - not all users of MLIR want to lower to LLVM and the information to e.g.\n        lower to library calls---like libxsmm---or to SPIR-V was not available.\n      - foldings and canonicalizations can happen at a higher level in MLIR:\n        before this op existed, lowering to LLVM would create large amounts of\n        LLVMIR. Even when LLVM does a good job at folding the low-level IR from\n        a performance perspective, it is unnecessarily opaque and inefficient to\n        send unkempt IR to LLVM.\n\n    Example:\n\n    ```mlir\n      %base, %offset, %sizes:2, %strides:2 =\n        memref.extract_strided_metadata %memref :\n          memref<10x?xf32>, index, index, index, index, index\n\n      // After folding, the type of %m2 can be memref<10x?xf32> and further\n      // folded to %memref.\n      %m2 = memref.reinterpret_cast %base to\n          offset: [%offset],\n          sizes: [%sizes#0, %sizes#1],\n          strides: [%strides#0, %strides#1]\n        : memref<f32> to memref<?x?xf32, offset: ?, strides: [?, ?]>\n    ```",
    "inputs": [
      { "name": "source", "type": "AnyStridedMemRef" }
    ],
    "outputs": [
      { "name": "base_buffer", "type": "AnyStridedMemRefOfRank" },
      { "name": "offset", "type": "Index" },
      { "name": "sizes", "type": "Variadic" },
      { "name": "strides", "type": "Variadic" }
    ],
    "assemblyFormat": "$source `:` type($source) `->` type(results) attr-dict"
  },
  {
    "name": "memref.generic_atomic_rmw",
    "summary": "atomic read-modify-write operation with a region",
    "description": "The `memref.generic_atomic_rmw` operation provides a way to perform a\n    read-modify-write sequence that is free from data races. The memref operand\n    represents the buffer that the read and write will be performed against, as\n    accessed by the specified indices. The arity of the indices is the rank of\n    the memref. The result represents the latest value that was stored. The\n    region contains the code for the modification itself. The entry block has\n    a single argument that represents the value stored in `memref[indices]`\n    before the write is performed. No side-effecting ops are allowed in the\n    body of `GenericAtomicRMWOp`.\n\n    Example:\n\n    ```mlir\n    %x = memref.generic_atomic_rmw %I[%i] : memref<10xf32> {\n      ^bb0(%current_value : f32):\n        %c1 = arith.constant 1.0 : f32\n        %inc = arith.addf %c1, %current_value : f32\n        memref.atomic_yield %inc : f32\n    }\n    ```",
    "inputs": [
      { "name": "memref", "type": "Arg" },
      { "name": "indices", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTypeOf" }
    ]
  },
  {
    "name": "memref.get_global",
    "summary": "get the memref pointing to a global variable",
    "description": "The `memref.get_global` operation retrieves the memref pointing to a\n     named global variable. If the global variable is marked constant, writing\n     to the result memref (such as through a `memref.store` operation) is\n     undefined.\n\n     Example:\n\n     ```mlir\n     %x = memref.get_global @foo : memref<2xf32>\n     ```",
    "outputs": [
      { "name": "result", "type": "AnyStaticShapeMemRef" }
    ],
    "attributes": [
      { "name": "name", "type": "FlatSymbolRefAttr" }
    ],
    "assemblyFormat": "$name `:` type($result) attr-dict"
  },
  {
    "name": "memref.global",
    "summary": "declare or define a global memref variable",
    "description": "The `memref.global` operation declares or defines a named global memref\n    variable. The backing memory for the variable is allocated statically and is\n    described by the type of the variable (which should be a statically shaped\n    memref type). The operation is a declaration if no `initial_value` is\n    specified, else it is a definition. The `initial_value` can either be a unit\n    attribute to represent a definition of an uninitialized global variable, or\n    an elements attribute to represent the definition of a global variable with\n    an initial value. The global variable can also be marked constant using the\n    `constant` unit attribute. Writing to such constant global variables is\n    undefined.\n\n    The global variable can be accessed by using the `memref.get_global` to\n    retrieve the memref for the global variable. Note that the memref\n    for such global variable itself is immutable (i.e., memref.get_global for a\n    given global variable will always return the same memref descriptor).\n\n    Example:\n\n    ```mlir\n    // Private variable with an initial value.\n    memref.global \"private\" @x : memref<2xf32> = dense<0.0,2.0>\n\n    // Private variable with an initial value and an alignment (power of 2).\n    memref.global \"private\" @x : memref<2xf32> = dense<0.0,2.0> {alignment = 64}\n\n    // Declaration of an external variable.\n    memref.global \"private\" @y : memref<4xi32>\n\n    // Uninitialized externally visible variable.\n    memref.global @z : memref<3xf16> = uninitialized\n\n    // Externally visible constant variable.\n    memref.global constant @c : memref<2xi32> = dense<1, 4>\n    ```",
    "attributes": [
      { "name": "sym_name", "type": "SymbolNameAttr" },
      { "name": "sym_visibility", "type": "OptionalAttr" },
      { "name": "type", "type": "MemRefTypeAttr" },
      { "name": "initial_value", "type": "OptionalAttr" },
      { "name": "constant", "type": "UnitAttr" },
      { "name": "alignment", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "($sym_visibility^)?\n       (`constant` $constant^)?\n       $sym_name `:`\n       custom<GlobalMemrefOpTypeAndInitialValue>($type, $initial_value)\n       attr-dict"
  },
  {
    "name": "memref.load",
    "summary": "load operation",
    "description": "The `load` op reads an element from a memref at the specified indices.\n\n    The number of indices must match the rank of the memref. The indices must\n    be in-bounds: `0 <= idx < dim_size`.\n\n    Lowerings of `memref.load` may emit attributes, e.g. `inbouds` + `nuw`\n    when converting to LLVM's `llvm.getelementptr`, that would cause undefined\n    behavior if indices are out of bounds or if computing the offset in the\n    memref would cause signed overflow of the `index` type.\n\n    The single result of `memref.load` is a value with the same type as the\n    element type of the memref.\n\n    A set `nontemporal` attribute indicates that this load is not expected to\n    be reused in the cache. For details, refer to the\n    [LLVM load instruction](https://llvm.org/docs/LangRef.html#load-instruction).\n\n    An optional `alignment` attribute allows to specify the byte alignment of the\n    load operation. It must be a positive power of 2. The operation must access\n    memory at an address aligned to this boundary. Violations may lead to\n    architecture-specific faults or performance penalties.\n    A value of 0 indicates no specific alignment requirement.\n    Example:\n\n    ```mlir\n    %0 = memref.load %A[%a, %b] : memref<8x?xi32, #layout, memspace0>\n    ```",
    "inputs": [
      { "name": "memref", "type": "Arg" },
      { "name": "indices", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyType" }
    ],
    "attributes": [
      { "name": "nontemporal", "type": "DefaultValuedOptionalAttr" },
      { "name": "alignment", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "$memref `[` $indices `]` attr-dict `:` type($memref)"
  },
  {
    "name": "memref.memory_space_cast",
    "summary": "memref memory space cast operation",
    "description": "This operation casts memref values between memory spaces.\n    The input and result will be memrefs of the same types and shape that alias\n    the same underlying memory, though, for some casts on some targets,\n    the underlying values of the pointer stored in the memref may be affected\n    by the cast.\n\n    The input and result must have the same shape, element type, rank, and layout.\n\n    If the source and target address spaces are the same, this operation is a noop.\n\n    Finally, if the target memory-space is the generic/default memory-space,\n    then it is assumed this cast can be bubbled down safely. See the docs of\n    `MemorySpaceCastOpInterface` interface for more details.\n\n    Example:\n\n    ```mlir\n    // Cast a GPU private memory attribution into a generic pointer\n    %2 = memref.memory_space_cast %1 : memref<?xf32, 5> to memref<?xf32>\n    // Cast a generic pointer to workgroup-local memory\n    %4 = memref.memory_space_cast %3 : memref<5x4xi32> to memref<5x34xi32, 3>\n    // Cast between two non-default memory spaces\n    %6 = memref.memory_space_cast %5\n      : memref<*xmemref<?xf32>, 5> to memref<*xmemref<?xf32>, 3>\n    ```",
    "inputs": [
      { "name": "source", "type": "AnyRankedOrUnrankedMemRef" }
    ],
    "outputs": [
      { "name": "dest", "type": "AnyRankedOrUnrankedMemRef" }
    ],
    "assemblyFormat": "$source attr-dict `:` type($source) `to` type($dest)"
  },
  {
    "name": "memref.prefetch",
    "summary": "prefetch operation",
    "description": "The \"prefetch\" op prefetches data from a memref location described with\n    subscript indices similar to memref.load, and with three attributes: a\n    read/write specifier, a locality hint, and a cache type specifier as shown\n    below:\n\n    ```mlir\n    memref.prefetch %0[%i, %j], read, locality<3>, data : memref<400x400xi32>\n    ```\n\n    The read/write specifier is either 'read' or 'write', the locality hint\n    ranges from locality<0> (no locality) to locality<3> (extremely local keep\n    in cache). The cache type specifier is either 'data' or 'instr'\n    and specifies whether the prefetch is performed on data cache or on\n    instruction cache.",
    "inputs": [
      { "name": "memref", "type": "Arg" },
      { "name": "indices", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "isWrite", "type": "BoolAttr" },
      { "name": "localityHint", "type": "ConfinedAttr" },
      { "name": "isDataCache", "type": "BoolAttr" }
    ]
  },
  {
    "name": "memref.rank",
    "summary": "rank operation",
    "description": "The `memref.rank` operation takes a memref operand and returns its rank.\n\n    Example:\n\n    ```mlir\n    %0 = memref.rank %arg0 : memref<*xf32>\n    %1 = memref.rank %arg1 : memref<?x?xf32>\n    ```",
    "inputs": [
      { "name": "memref", "type": "AnyRankedOrUnrankedMemRef" }
    ],
    "assemblyFormat": "$memref attr-dict `:` type($memref)"
  },
  {
    "name": "memref.realloc",
    "summary": "memory reallocation operation",
    "description": "The `realloc` operation changes the size of a memory region. The memory\n    region is specified by a 1D source memref and the size of the new memory\n    region is specified by a 1D result memref type and an optional dynamic Value\n    of `Index` type. The source and the result memref must be in the same memory\n    space and have the same element type.\n\n    The operation may move the memory region to a new location. In this case,\n    the content of the memory block is preserved up to the lesser of the new\n    and old sizes. If the new size if larger, the value of the extended memory\n    is undefined. This is consistent with the ISO C realloc.\n\n    The operation returns an SSA value for the memref.\n\n    Example:\n\n    ```mlir\n    %0 = memref.realloc %src : memref<64xf32> to memref<124xf32>\n    ```\n\n    The source memref may have a dynamic shape, in which case, the compiler will\n    generate code to extract its size from the runtime data structure for the\n    memref.\n\n    ```mlir\n    %1 = memref.realloc %src : memref<?xf32> to memref<124xf32>\n    ```\n\n    If the result memref has a dynamic shape, a result dimension operand is\n    needed to spefify its dynamic dimension. In the example below, the ssa value\n    '%d' specifies the unknown dimension of the result memref.\n\n    ```mlir\n    %2 = memref.realloc %src(%d) : memref<?xf32> to memref<?xf32>\n    ```\n\n    An optional `alignment` attribute may be specified to ensure that the\n    region of memory that will be indexed is aligned at the specified byte\n    boundary.  This is consistent with the fact that memref.alloc supports such\n    an optional alignment attribute. Note that in ISO C standard, neither alloc\n    nor realloc supports alignment, though there is aligned_alloc but not\n    aligned_realloc.\n\n    ```mlir\n    %3 = memref.realloc %src {alignment = 8} : memref<64xf32> to memref<124xf32>\n    ```\n\n    Referencing the memref through the old SSA value after realloc is undefined\n    behavior.\n\n    ```mlir\n    %new = memref.realloc %old : memref<64xf32> to memref<124xf32>\n    %4 = memref.load %new[%index]   // ok\n    %5 = memref.load %old[%index]   // undefined behavior\n    ```",
    "inputs": [
      { "name": "source", "type": "Arg" },
      { "name": "dynamicResultSize", "type": "Optional" }
    ],
    "attributes": [
      { "name": "alignment", "type": "ConfinedAttr" }
    ],
    "assemblyFormat": "$source (`(` $dynamicResultSize^ `)`)? attr-dict\n    `:` type($source) `to` type(results)"
  },
  {
    "name": "memref.reinterpret_cast",
    "summary": "memref reinterpret cast operation",
    "description": "Modify offset, sizes and strides of an unranked/ranked memref.\n\n    Example 1:\n\n    Consecutive `reinterpret_cast` operations on memref's with static\n    dimensions.\n\n    We distinguish between *underlying memory* — the sequence of elements as\n    they appear in the contiguous memory of the memref — and the\n    *strided memref*, which refers to the underlying memory interpreted\n    according to specified offsets, sizes, and strides.\n\n    ```mlir\n    %result1 = memref.reinterpret_cast %arg0 to\n      offset: [9],\n      sizes: [4, 4],\n      strides: [16, 2]\n    : memref<8x8xf32, strided<[8, 1], offset: 0>> to\n      memref<4x4xf32, strided<[16, 2], offset: 9>>\n\n    %result2 = memref.reinterpret_cast %result1 to\n      offset: [0],\n      sizes: [2, 2],\n      strides: [4, 2]\n    : memref<4x4xf32, strided<[16, 2], offset: 9>> to\n      memref<2x2xf32, strided<[4, 2], offset: 0>>\n    ```\n\n    The underlying memory of `%arg0` consists of a linear sequence of integers\n    from 1 to 64. Its memref has the following 8x8 elements:\n\n    ```mlir\n    [[1,  2,  3,  4,  5,  6,  7,  8],\n    [9,  10, 11, 12, 13, 14, 15, 16],\n    [17, 18, 19, 20, 21, 22, 23, 24],\n    [25, 26, 27, 28, 29, 30, 31, 32],\n    [33, 34, 35, 36, 37, 38, 39, 40],\n    [41, 42, 43, 44, 45, 46, 47, 48],\n    [49, 50, 51, 52, 53, 54, 55, 56],\n    [57, 58, 59, 60, 61, 62, 63, 64]]\n    ```\n\n    Following the first `reinterpret_cast`, the strided memref elements\n    of `%result1` are:\n\n    ```mlir\n    [[10, 12, 14, 16],\n    [26, 28, 30, 32],\n    [42, 44, 46, 48],\n    [58, 60, 62, 64]]\n    ```\n\n    Note: The offset and strides are relative to the underlying memory of\n    `%arg0`.\n\n    The second `reinterpret_cast` results in the following strided memref\n    for `%result2`:\n\n    ```mlir\n    [[1, 3],\n    [5, 7]]\n    ```\n\n    Notice that it does not matter if you use %result1 or %arg0 as a source\n    for the second `reinterpret_cast` operation. Only the underlying memory\n    pointers will be reused.\n\n    The offset and stride are relative to the base underlying memory of the\n    memref, starting at 1, not at 10 as seen in the output of `%result1`.\n    This behavior contrasts with the `subview` operator, where values are\n    relative to the strided memref (refer to `subview` examples).\n    Consequently, the second `reinterpret_cast` behaves as if `%arg0` were\n    passed directly as its argument.\n\n    Example 2:\n    ```mlir\n    memref.reinterpret_cast %ranked to\n      offset: [0],\n      sizes: [%size0, 10],\n      strides: [1, %stride1]\n    : memref<?x?xf32> to memref<?x10xf32, strided<[1, ?], offset: 0>>\n\n    memref.reinterpret_cast %unranked to\n      offset: [%offset],\n      sizes: [%size0, %size1],\n      strides: [%stride0, %stride1]\n    : memref<*xf32> to memref<?x?xf32, strided<[?, ?], offset: ?>>\n    ```\n\n    This operation creates a new memref descriptor using the base of the\n    source and applying the input arguments to the other metadata.\n    In other words:\n    ```mlir\n    %dst = memref.reinterpret_cast %src to\n      offset: [%offset],\n      sizes: [%sizes],\n      strides: [%strides]\n    ```\n    means that `%dst`'s descriptor will be:\n    ```mlir\n    %dst.base = %src.base\n    %dst.aligned = %src.aligned\n    %dst.offset = %offset\n    %dst.sizes = %sizes\n    %dst.strides = %strides\n    ```",
    "inputs": [
      { "name": "source", "type": "Arg" },
      { "name": "offsets", "type": "Variadic" },
      { "name": "sizes", "type": "Variadic" },
      { "name": "strides", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyStridedMemRef" }
    ],
    "attributes": [
      { "name": "static_offsets", "type": "DenseI64ArrayAttr" },
      { "name": "static_sizes", "type": "DenseI64ArrayAttr" },
      { "name": "static_strides", "type": "DenseI64ArrayAttr" }
    ],
    "assemblyFormat": "$source `to` `offset` `` `:`\n    custom<DynamicIndexList>($offsets, $static_offsets)\n    `` `,` `sizes` `` `:`\n    custom<DynamicIndexList>($sizes, $static_sizes)\n    `` `,` `strides` `` `:`\n    custom<DynamicIndexList>($strides, $static_strides)\n    attr-dict `:` type($source) `to` type($result)"
  },
  {
    "name": "memref.reshape",
    "summary": "memref reshape operation",
    "description": "The `reshape` operation converts a memref from one type to an\n    equivalent type with a provided shape. The data is never copied or\n    modified. The source and destination types are compatible if both have the\n    same element type, same number of elements, address space and identity\n    layout map. The following combinations are possible:\n\n    a. Source type is ranked or unranked. Shape argument has static size.\n    Result type is ranked.\n\n    ```mlir\n    // Reshape statically-shaped memref.\n    %dst = memref.reshape %src(%shape)\n             : (memref<4x1xf32>, memref<1xi32>) to memref<4xf32>\n    %dst0 = memref.reshape %src(%shape0)\n             : (memref<4x1xf32>, memref<2xi32>) to memref<2x2xf32>\n    // Flatten unranked memref.\n    %dst = memref.reshape %src(%shape)\n             : (memref<*xf32>, memref<1xi32>) to memref<?xf32>\n    ```\n\n    b. Source type is ranked or unranked. Shape argument has dynamic size.\n    Result type is unranked.\n\n    ```mlir\n    // Reshape dynamically-shaped 1D memref.\n    %dst = memref.reshape %src(%shape)\n             : (memref<?xf32>, memref<?xi32>) to memref<*xf32>\n    // Reshape unranked memref.\n    %dst = memref.reshape %src(%shape)\n             : (memref<*xf32>, memref<?xi32>) to memref<*xf32>\n    ```",
    "inputs": [
      { "name": "source", "type": "AnyRankedOrUnrankedMemRef" },
      { "name": "shape", "type": "Arg" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyRankedOrUnrankedMemRef" }
    ],
    "assemblyFormat": "$source `(` $shape `)` attr-dict `:` functional-type(operands, results)",
    "category": "Shape"
  },
  {
    "name": "memref.store",
    "summary": "store operation",
    "description": "The `store` op stores an element into a memref at the specified indices.\n\n    The number of indices must match the rank of the memref. The indices must\n    be in-bounds: `0 <= idx < dim_size`.\n\n    Lowerings of `memref.store` may emit attributes, e.g. `inbouds` + `nuw`\n    when converting to LLVM's `llvm.getelementptr`, that would cause undefined\n    behavior if indices are out of bounds or if computing the offset in the\n    memref would cause signed overflow of the `index` type.\n\n    A set `nontemporal` attribute indicates that this store is not expected to\n    be reused in the cache. For details, refer to the\n    [LLVM store instruction](https://llvm.org/docs/LangRef.html#store-instruction).\n\n    An optional `alignment` attribute allows to specify the byte alignment of the\n    store operation. It must be a positive power of 2. The operation must access\n    memory at an address aligned to this boundary. Violations may lead to\n    architecture-specific faults or performance penalties.\n    A value of 0 indicates no specific alignment requirement.\n    Example:\n\n    ```mlir\n    memref.store %val, %A[%a, %b] : memref<8x?xi32, #layout, memspace0>\n    ```",
    "inputs": [
      { "name": "value", "type": "AnyType" },
      { "name": "memref", "type": "Arg" },
      { "name": "indices", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "nontemporal", "type": "DefaultValuedOptionalAttr" },
      { "name": "alignment", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "$value `,` $memref `[` $indices `]` attr-dict `:` type($memref)"
  },
  {
    "name": "memref.subview",
    "summary": "memref subview operation",
    "description": "The `subview` operation converts a memref type to a memref type which\n    represents a reduced-size view of the original memref as specified by the\n    operation's offsets, sizes and strides arguments.\n\n    The `subview` operation supports the following arguments:\n\n    * source: the \"base\" memref on which to create a \"view\" memref.\n    * offsets: memref-rank number of offsets into the \"base\" memref at which to\n               create the \"view\" memref.\n    * sizes: memref-rank number of sizes which specify the sizes of the result\n             \"view\" memref type.\n    * strides: memref-rank number of strides that compose multiplicatively with\n               the base memref strides in each dimension.\n\n    The representation based on offsets, sizes and strides support a\n    partially-static specification via attributes specified through the\n    `static_offsets`, `static_sizes` and `static_strides` arguments. A special\n    sentinel value `ShapedType::kDynamic` encodes that the corresponding entry\n    has a dynamic value.\n\n    A `subview` operation may additionally reduce the rank of the resulting\n    view by removing dimensions that are statically known to be of size 1.\n\n    In the absence of rank reductions, the resulting memref type is computed\n    as follows:\n    ```\n    result_sizes[i] = size_operands[i]\n    result_strides[i] = src_strides[i] * stride_operands[i]\n    result_offset = src_offset + dot_product(offset_operands, src_strides)\n    ```\n\n    The offset, size and stride operands must be in-bounds with respect to the\n    source memref. When possible, the static operation verifier will detect\n    out-of-bounds subviews. Subviews that cannot be confirmed to be in-bounds\n    or out-of-bounds based on compile-time information are valid. However,\n    performing an out-of-bounds subview at runtime is undefined behavior.\n\n    Example 1:\n\n    Consecutive `subview` operations on memref's with static dimensions.\n\n    We distinguish between *underlying memory* — the sequence of elements as\n    they appear in the contiguous memory of the memref — and the\n    *strided memref*, which refers to the underlying memory interpreted\n    according to specified offsets, sizes, and strides.\n\n    ```mlir\n    %result1 = memref.subview %arg0[1, 1][4, 4][2, 2]\n    : memref<8x8xf32, strided<[8, 1], offset: 0>> to\n      memref<4x4xf32, strided<[16, 2], offset: 9>>\n\n    %result2 = memref.subview %result1[1, 1][2, 2][2, 2]\n    : memref<4x4xf32, strided<[16, 2], offset: 9>> to\n      memref<2x2xf32, strided<[32, 4], offset: 27>>\n    ```\n\n    The underlying memory of `%arg0` consists of a linear sequence of integers\n    from 1 to 64. Its memref has the following 8x8 elements:\n\n    ```mlir\n    [[1,  2,  3,  4,  5,  6,  7,  8],\n    [9,  10, 11, 12, 13, 14, 15, 16],\n    [17, 18, 19, 20, 21, 22, 23, 24],\n    [25, 26, 27, 28, 29, 30, 31, 32],\n    [33, 34, 35, 36, 37, 38, 39, 40],\n    [41, 42, 43, 44, 45, 46, 47, 48],\n    [49, 50, 51, 52, 53, 54, 55, 56],\n    [57, 58, 59, 60, 61, 62, 63, 64]]\n    ```\n\n    Following the first `subview`, the strided memref elements of `%result1`\n    are:\n\n    ```mlir\n    [[10, 12, 14, 16],\n    [26, 28, 30, 32],\n    [42, 44, 46, 48],\n    [58, 60, 62, 64]]\n    ```\n\n    Note: The offset and strides are relative to the strided memref of `%arg0`\n    (compare to the corresponding `reinterpret_cast` example).\n\n    The second `subview` results in the following strided memref for\n    `%result2`:\n\n    ```mlir\n    [[28, 32],\n    [60, 64]]\n    ```\n\n    Unlike the `reinterpret_cast`, the values are relative to the strided\n    memref of the input (`%result1` in this case) and not its\n    underlying memory.\n\n    Example 2:\n\n    ```mlir\n    // Subview of static memref with strided layout at static offsets, sizes\n    // and strides.\n    %1 = memref.subview %0[4, 2][8, 2][3, 2]\n        : memref<64x4xf32, strided<[7, 9], offset: 91>> to\n          memref<8x2xf32, strided<[21, 18], offset: 137>>\n    ```\n\n    Example 3:\n\n    ```mlir\n    // Subview of static memref with identity layout at dynamic offsets, sizes\n    // and strides.\n    %1 = memref.subview %0[%off0, %off1][%sz0, %sz1][%str0, %str1]\n        : memref<64x4xf32> to memref<?x?xf32, strided<[?, ?], offset: ?>>\n    ```\n\n    Example 4:\n\n    ```mlir\n    // Subview of dynamic memref with strided layout at dynamic offsets and\n    // strides, but static sizes.\n    %1 = memref.subview %0[%off0, %off1][4, 4][%str0, %str1]\n        : memref<?x?xf32, strided<[?, ?], offset: ?>> to\n          memref<4x4xf32, strided<[?, ?], offset: ?>>\n    ```\n\n    Example 5:\n\n    ```mlir\n    // Rank-reducing subviews.\n    %1 = memref.subview %0[0, 0, 0][1, 16, 4][1, 1, 1]\n        : memref<8x16x4xf32> to memref<16x4xf32>\n    %3 = memref.subview %2[3, 4, 2][1, 6, 3][1, 1, 1]\n        : memref<8x16x4xf32> to memref<6x3xf32, strided<[4, 1], offset: 210>>\n    ```\n\n    Example 6:\n\n    ```mlir\n    // Identity subview. The subview is the full source memref.\n    %1 = memref.subview %0[0, 0, 0] [8, 16, 4] [1, 1, 1]\n        : memref<8x16x4xf32> to memref<8x16x4xf32>\n    ```",
    "inputs": [
      { "name": "source", "type": "AnyMemRef" },
      { "name": "offsets", "type": "Variadic" },
      { "name": "sizes", "type": "Variadic" },
      { "name": "strides", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyMemRef" }
    ],
    "attributes": [
      { "name": "static_offsets", "type": "DenseI64ArrayAttr" },
      { "name": "static_sizes", "type": "DenseI64ArrayAttr" },
      { "name": "static_strides", "type": "DenseI64ArrayAttr" }
    ],
    "assemblyFormat": "$source ``\n    custom<DynamicIndexList>($offsets, $static_offsets)\n    custom<DynamicIndexList>($sizes, $static_sizes)\n    custom<DynamicIndexList>($strides, $static_strides)\n    attr-dict `:` type($source) `to` type($result)"
  },
  {
    "name": "memref.transpose",
    "summary": "`transpose` produces a new strided memref (metadata-only)",
    "description": "The `transpose` op produces a strided memref whose sizes and strides\n    are a permutation of the original `in` memref. This is purely a metadata\n    transformation.\n\n    Example:\n\n    ```mlir\n    %1 = memref.transpose %0 (i, j) -> (j, i) : memref<?x?xf32> to memref<?x?xf32, affine_map<(d0, d1)[s0] -> (d1 * s0 + d0)>>\n    ```",
    "category": "Transform"
  },
  {
    "name": "memref.view",
    "summary": "memref view operation",
    "description": "The \"view\" operation extracts an N-D contiguous memref with empty layout map\n    with arbitrary element type from a 1-D contiguous memref with empty layout\n    map of i8 element  type. The ViewOp supports the following arguments:\n\n    * A single dynamic byte-shift operand must be specified which represents a\n      a shift of the base 1-D memref pointer from which to create the resulting\n      contiguous memref view with identity layout.\n    * A dynamic size operand that must be specified for each dynamic dimension\n      in the resulting view memref type.\n\n    The \"view\" operation gives a structured indexing form to a flat 1-D buffer.\n    Unlike \"subview\" it can perform a type change. The type change behavior\n    requires the op to have special semantics because, e.g. a byte shift of 3\n    cannot be represented as an offset on f64.\n    For now, a \"view\" op:\n\n    1. Only takes a contiguous source memref with 0 offset and empty layout.\n    2. Must specify a byte_shift operand (in the future, a special integer\n       attribute may be added to support the folded case).\n    3. Returns a contiguous memref with 0 offset and empty layout.\n\n    Example:\n\n    ```mlir\n    // Allocate a flat 1D/i8 memref.\n    %0 = memref.alloc() : memref<2048xi8>\n\n    // ViewOp with dynamic offset and static sizes.\n    %1 = memref.view %0[%offset_1024][] : memref<2048xi8> to memref<64x4xf32>\n\n    // ViewOp with dynamic offset and two dynamic size.\n    %2 = memref.view %0[%offset_1024][%size0, %size1] :\n      memref<2048xi8> to memref<?x4x?xf32>\n    ```",
    "inputs": [
      { "name": "source", "type": "MemRefRankOf" },
      { "name": "byte_shift", "type": "Index" },
      { "name": "sizes", "type": "Variadic" }
    ],
    "assemblyFormat": "$source `[` $byte_shift `]` `` `[` $sizes `]` attr-dict\n    `:` type($source) `to` type(results)"
  },
  {
    "name": "mhlo.abs",
    "summary": "Absolute value operator",
    "description": "Returns `abs(operand)` element-wise.\n\n    See\n    https://www.tensorflow.org/xla/operation_semantics#element-wise_unary_functions.",
    "inputs": [
      { "name": "operand", "type": "TensorType" }
    ]
  },
  {
    "name": "mhlo.acos",
    "summary": "Acos operation",
    "description": "Performs element-wise acos operation on `operand` tensor and produces a\n    `result` tensor.\n\n    Example:\n    ```mlir\n    %result = mhlo.acos %operand : tensor<2x2xf32>\n    ```",
    "inputs": [
      { "name": "operand", "type": "OperandType" }
    ],
    "outputs": [
      { "name": "result", "type": "ResultType" }
    ],
    "assemblyFormat": "$operand attr-dict\n      `:` custom<SameOperandsAndResultType>(type($operand), type($result))"
  },
  {
    "name": "mhlo.acosh",
    "summary": "Acosh operation",
    "description": "Performs element-wise acosh operation on `operand` tensor and produces a\n    `result` tensor.\n\n    Example:\n    ```mlir\n    %result = mhlo.acosh %operand : tensor<2x2xf32>\n    ```",
    "inputs": [
      { "name": "operand", "type": "OperandType" }
    ],
    "outputs": [
      { "name": "result", "type": "ResultType" }
    ],
    "assemblyFormat": "$operand attr-dict\n      `:` custom<SameOperandsAndResultType>(type($operand), type($result))"
  },
  {
    "name": "mhlo.add",
    "summary": "Addition operator",
    "description": "Returns `lhs + rhs` element-wise.\n\n    See\n    https://www.tensorflow.org/xla/operation_semantics#element-wise_binary_arithmetic_operations.",
    "inputs": [
      { "name": "lhs", "type": "HLO_Tensor" },
      { "name": "rhs", "type": "HLO_Tensor" }
    ]
  },
  {
    "name": "mhlo.add_dependency",
    "summary": "AddDependency operation",
    "description": "This operation is private to the XLA compiler, so it is does not yet have\n    a specification.\n\n    Informally, this operation two operands: a data operand and a token. The\n    output of the operation is the data operand. When used with AfterAll this\n    operation enables ordering non-side-effecting operations (those that do not\n    produce token values).\n\n    Example:\n    ```mlir\n    %1 = mhlo.add_dependency %arg0, %0 : (tensor<3x4xf32>, !mhlo.token) -> tensor<3x4xf32>\n    ```",
    "inputs": [
      { "name": "operand", "type": "MHLO_TensorOrAnyToken" },
      { "name": "token", "type": "MHLO_AnyToken" }
    ],
    "outputs": [
      { "name": "output", "type": "MHLO_TensorOrAnyToken" }
    ],
    "assemblyFormat": "operands attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "mhlo.after_all",
    "summary": "AfterAll operator",
    "description": "AfterAll takes a variadic number of tokens and produces a single token.\n    Tokens are primitive types which can be threaded between side-effecting\n    operations to enforce ordering. AfterAll can be used as a join of tokens\n    for ordering a operation after a set operations.\n\n    See https://www.tensorflow.org/xla/operation_semantics#afterall.",
    "inputs": [
      { "name": "operands", "type": "Variadic" }
    ]
  },
  {
    "name": "mhlo.all_gather",
    "summary": "AllGather operator",
    "description": "Performs concatenation across replicas.\n\n    See https://www.tensorflow.org/xla/operation_semantics#allgather",
    "inputs": [
      { "name": "operand", "type": "HLO_Tensor" }
    ],
    "attributes": [
      { "name": "all_gather_dim", "type": "I64Attr" },
      { "name": "replica_groups", "type": "I64ElementsAttr" },
      { "name": "channel_handle", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "mhlo.all_reduce",
    "summary": "AllReduce operator",
    "description": "Performs a custom reduction across replicas.\n\n    See https://www.tensorflow.org/xla/operation_semantics#allreduce.",
    "inputs": [
      { "name": "operand", "type": "HLO_Tensor" }
    ],
    "attributes": [
      { "name": "replica_groups", "type": "I64ElementsAttr" },
      { "name": "channel_handle", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "mhlo.all_to_all",
    "inputs": [
      { "name": "operand", "type": "HLO_Tensor" }
    ],
    "attributes": [
      { "name": "split_dimension", "type": "I64Attr" },
      { "name": "concat_dimension", "type": "I64Attr" },
      { "name": "split_count", "type": "I64Attr" },
      { "name": "replica_groups", "type": "I64ElementsAttr" }
    ]
  },
  {
    "name": "mhlo.and",
    "summary": "Logical and",
    "description": "Returns `logical_and(lhs, rhs)` element-wise.\n\n    See\n    https://www.tensorflow.org/xla/operation_semantics#element-wise_binary_arithmetic_operations.",
    "inputs": [
      { "name": "lhs", "type": "HLO_PredOrIntTensor" },
      { "name": "rhs", "type": "HLO_PredOrIntTensor" }
    ]
  },
  {
    "name": "mhlo.asin",
    "summary": "Asin operation",
    "description": "Performs element-wise asin operation on `operand` tensor and produces a\n    `result` tensor.\n\n    Example:\n    ```mlir\n    %result = mhlo.asin %operand : tensor<2x2xf32>\n    ```",
    "inputs": [
      { "name": "operand", "type": "OperandType" }
    ],
    "outputs": [
      { "name": "result", "type": "ResultType" }
    ],
    "assemblyFormat": "$operand attr-dict\n      `:` custom<SameOperandsAndResultType>(type($operand), type($result))"
  },
  {
    "name": "mhlo.asinh",
    "summary": "Asinh operation",
    "description": "Performs element-wise asinh operation on `operand` tensor and produces a\n    `result` tensor.\n\n    Example:\n    ```mlir\n    %result = mhlo.asinh %operand : tensor<2x2xf32>\n    ```",
    "inputs": [
      { "name": "operand", "type": "OperandType" }
    ],
    "outputs": [
      { "name": "result", "type": "ResultType" }
    ],
    "assemblyFormat": "$operand attr-dict\n      `:` custom<SameOperandsAndResultType>(type($operand), type($result))"
  },
  {
    "name": "mhlo.async_done",
    "summary": "AsyncDone operation",
    "description": "This operation is private to the XLA compiler, so it is does not yet have\n    a specification.\n\n    Informally, this operation blocks until the end of an asynchronous computation.\n    It returns the final result of the asynchronous computation.\n\n    See the documentation for AsyncStart for more information.",
    "inputs": [
      { "name": "bundle", "type": "MHLO_AsyncBundle" }
    ]
  },
  {
    "name": "mhlo.async_start",
    "summary": "AsyncStart operation",
    "description": "This operation is private to the XLA compiler, so it is does not yet have\n    a specification.\n\n    Informally, this operation kicks off an asynchronous computation.\n\n    This is used when there are functions that contain both asynchronous waits\n    (such as DMAs) and on-thread computation. For example, a function might\n    consist of a computation, a DMA, another computation, a second DMA, and a\n    final computation. This would be represented as an async_start followed by\n    and async_update and an async_done. The async_start would do the first\n    computation on-thread and then start the DMA. The async_update would wait\n    for the DMA to complete if it wasn't yet done, then execute the second\n    computation in the function, and start the second DMA. Finally, the\n    async_done would wait on this last DMA, and then run the last computation\n    that needs to be run on-thread and return the result of that final\n    computation.\n\n    `operands` are passed to the computation directly\n    `called_computation` is the function that will be run asynchronously\n    `execution_thread` is the name of the thread in which it will be run. The main\n      thread is called \"main\". All threads have names.\n\n    This returns all the state needed between async ops. After buffer\n    assignment, the return values represents the space needed to hold the input,\n    results, and any scratchpads needed or edited by the async op.",
    "inputs": [
      { "name": "inputs", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "called_computation", "type": "FlatSymbolRefAttr" },
      { "name": "execution_thread", "type": "StrAttr" }
    ]
  },
  {
    "name": "mhlo.async_update",
    "summary": "AsyncUpdate operation",
    "description": "This operation is private to the XLA compiler, so it is does not yet have\n    a specification.\n\n    Informally, this operation blocks on an asynchronous computation until a\n    sync barrier. This returns `bundle` after operating on it.\n\n    See the documentation for AsyncStart for more information.",
    "inputs": [
      { "name": "bundle", "type": "MHLO_AsyncBundle" }
    ]
  },
  {
    "name": "mhlo.atan2",
    "summary": "Atan2 operator",
    "description": "Returns `atan2(lhs/rhs)` element-wise.\n\n    See\n    https://www.tensorflow.org/xla/operation_semantics#element-wise_binary_arithmetic_operations.",
    "inputs": [
      { "name": "lhs", "type": "HLO_Tensor" },
      { "name": "rhs", "type": "HLO_Tensor" }
    ]
  },
  {
    "name": "mhlo.atanh",
    "summary": "Atanh operation",
    "description": "Performs element-wise atanh operation on `operand` tensor and produces a\n    `result` tensor.\n\n    Example:\n    ```mlir\n    %result = mhlo.atanh %operand : tensor<2x2xf32>\n    ```",
    "inputs": [
      { "name": "operand", "type": "OperandType" }
    ],
    "outputs": [
      { "name": "result", "type": "ResultType" }
    ],
    "assemblyFormat": "$operand attr-dict\n      `:` custom<SameOperandsAndResultType>(type($operand), type($result))"
  },
  {
    "name": "mhlo.batch_norm_grad",
    "summary": "Batch Normalization Gradient",
    "description": "Calculates gradients of batch norm.\n\n    See https://www.tensorflow.org/xla/operation_semantics#batchnormgrad",
    "inputs": [
      { "name": "operand", "type": "HLO_Tensor" },
      { "name": "scale", "type": "HLO_Tensor" },
      { "name": "mean", "type": "HLO_Tensor" },
      { "name": "variance", "type": "HLO_Tensor" },
      { "name": "grad_output", "type": "HLO_Tensor" }
    ],
    "attributes": [
      { "name": "epsilon", "type": "F32Attr" },
      { "name": "feature_index", "type": "I64Attr" }
    ]
  },
  {
    "name": "mhlo.batch_norm_inference",
    "summary": "Batch Normalization for Inference",
    "description": "Normalizes an array across batch and spatial dimensions.\n\n    See https://www.tensorflow.org/xla/operation_semantics#batchnorminference",
    "inputs": [
      { "name": "operand", "type": "HLO_Tensor" },
      { "name": "scale", "type": "HLO_Tensor" },
      { "name": "offset", "type": "HLO_Tensor" },
      { "name": "mean", "type": "HLO_Tensor" },
      { "name": "variance", "type": "HLO_Tensor" }
    ],
    "attributes": [
      { "name": "epsilon", "type": "F32Attr" },
      { "name": "feature_index", "type": "I64Attr" }
    ]
  },
  {
    "name": "mhlo.batch_norm_training",
    "summary": "Batch Normalization for Training",
    "description": "Normalizes an array across batch and spatial dimensions.\n\n    See https://www.tensorflow.org/xla/operation_semantics#batchnormtraining",
    "inputs": [
      { "name": "operand", "type": "HLO_Tensor" },
      { "name": "scale", "type": "HLO_Tensor" },
      { "name": "offset", "type": "HLO_Tensor" }
    ],
    "attributes": [
      { "name": "epsilon", "type": "F32Attr" },
      { "name": "feature_index", "type": "I64Attr" }
    ]
  },
  {
    "name": "mhlo.bitcast",
    "summary": "Bitcast operator",
    "description": "This op changes the shape of the input in the way that the physical\n    arrangement of elements are unchanged.\n\n    However, the op needs layout information to make sense of \"physical\n    arrangement of elements\". Layout support in MHLO is currently under\n    exploration.",
    "inputs": [
      { "name": "operand", "type": "HLO_Tensor" }
    ]
  },
  {
    "name": "mhlo.bitcast_convert",
    "summary": "BitcastConvert operator",
    "description": "Similar to a 'tf.bitcast' in TensorFlow, performs an element-wise bitcast\n    operation from a data shape to a target shape. The dimensions must match,\n    and the conversion is an element-wise one. Bitcast is implemented as a\n    low-level cast, so machines with different floating-point representations\n    will give different results.\n\n    See https://www.tensorflow.org/xla/operation_semantics#bitcastconverttype.",
    "inputs": [
      { "name": "operand", "type": "HLO_Tensor" }
    ]
  },
  {
    "name": "mhlo.broadcast",
    "summary": "Broadcast a tensor to a higher rank by prepending dimensions",
    "description": "Broadcasts the operand tensor to a higher rank by prepending\n    `broadcast_sizes` to the dimensions. The current values of the operand are\n    copied into the other dimensions.\n\n    This is a more limited form of broadcasting, that corresponds to the XLA\n    client Broadcast method. For a more general form of broadcasting, see the\n    BroadcastInDimOp.\n\n    See https://www.tensorflow.org/xla/operation_semantics#broadcast.",
    "inputs": [
      { "name": "operand", "type": "HLO_Tensor" }
    ],
    "attributes": [
      { "name": "broadcast_sizes", "type": "I64ElementsAttr" }
    ]
  },
  {
    "name": "mhlo.broadcast_in_dim",
    "summary": "Broadcast a tensor into the given shape by adding dimensions.",
    "description": "Broadcasts the `operand` tensor to a higher rank. This is not the limited\n    form of broadcasting exposed as the XLA client broadcast op, but rather the\n    more powerful \"InDim\" broadcasting, which is closer to the HLO broadcast op\n    and exposed in the XLA client BroadcastInDim method.\n\n    `broadcast_dimensions` maps the operand dimension number to the target shape\n    dimension number. It must have the same size as the rank of the operand. The\n    mapped dimensions must either be the same size or the dimension being\n    broadcast from must be size 1 (degenerate broadcasting).\n\n    For a scalar (0D tensor) operand, `broadcast_dimensions` must be empty. The\n    The scalar value will be broadcast to every element in the target shape.\n\n    See https://www.tensorflow.org/xla/broadcasting.",
    "inputs": [
      { "name": "operand", "type": "HLO_Tensor" }
    ],
    "attributes": [
      { "name": "broadcast_dimensions", "type": "BroadcastDimAttr" }
    ]
  },
  {
    "name": "mhlo.case",
    "summary": "Switch-Case operator",
    "description": "Returns the result of executing `branches[index]`. If\n    `index` is < 0 or >= N, then `branches[N-1] is executed as\n    the default branch.\n\n    Each branch `branches[b]` must take in a single argument of same type as\n    `branch_operands[b]` and will be invoked with `branch_operands[b]`. The type\n    of the returned value of each branch must be the same.\n\n    Note that only one of the branches will be executed depending on the value\n    of index.\n    See https://www.tensorflow.org/xla/operation_semantics#conditional.",
    "inputs": [
      { "name": "index", "type": "I32Tensor" }
    ]
  },
  {
    "name": "mhlo.cbrt",
    "summary": "Cubic root operator",
    "description": "Returns element-wise cubic root of the operand.\n\n    See\n    https://www.tensorflow.org/xla/operation_semantics#element-wise_unary_functions.",
    "inputs": [
      { "name": "operand", "type": "TensorType" }
    ]
  },
  {
    "name": "mhlo.ceil",
    "summary": "Ceil operator",
    "description": "Returns `Ceil(operand)` element-wise.\n\n    See\n    https://www.tensorflow.org/xla/operation_semantics#element-wise_unary_functions.",
    "inputs": [
      { "name": "operand", "type": "TensorType" }
    ]
  },
  {
    "name": "mhlo.cholesky",
    "summary": "Cholesky operator",
    "description": "Computes the Cholesky decomposition of a batch of symmetric (Hermitian)\n  positive definite matrices.\n\n  If lower is true, computes lower-triangular matrices l such that\n  `a=l.Transpose(l)`. If lower is false, computes upper-triangular matrices u such\n  that `a=Transpose(u).u`.\n\n  Input data is read only from the lower/upper triangle of a, depending on the\n  value of lower. Values from the other triangle are ignored. Output data is\n  returned in the same triangle; the values in the other triangle are\n  implementation-defined and may be anything.\n\n  If the rank of a is greater than 2, a is treated as a batch of matrices, where\n  all except the minor 2 dimensions are batch dimensions.\n\n  If a is not symmetric (Hermitian) positive definite, the result is\n  implementation-defined.\n\n    See https://www.tensorflow.org/xla/operation_semantics#cholesky.",
    "inputs": [
      { "name": "a", "type": "HLO_FpOrComplexTensor" }
    ],
    "attributes": [
      { "name": "lower", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "mhlo.clamp",
    "summary": "Clamp operator",
    "description": "Clamps an operand to within the range between a minimum and maximum value.\n\n    Note: All three arrays must be the same shape. Alternatively, as a\n          restricted form of broadcasting, min and/or max can be a scalar (0D\n          tensor) of the element type of the tensor operand.\n\n    See https://www.tensorflow.org/xla/operation_semantics#clamp.",
    "inputs": [
      { "name": "min", "type": "HLO_Tensor" },
      { "name": "operand", "type": "HLO_Tensor" },
      { "name": "max", "type": "HLO_Tensor" }
    ]
  },
  {
    "name": "mhlo.collective_broadcast",
    "summary": "CollectiveBroadcast operation",
    "description": "Within each process group in the process grid, send the value of the\n    `operand` tensor from the source process to the target processes and produce a\n    `result` tensor.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#collective_broadcast\n\n    Example:\n    ```mlir\n    %result = \"mhlo.collective_broadcast\"(%operand) {\n      replica_groups = dense<[[0, 1]]> : tensor<1x2xi64>,\n      channel_handle = #mhlo.channel_handle<handle = 0, type = 0>\n    } : (tensor<1x2xi64>) -> tensor<1x2xi64>\n    ```",
    "inputs": [
      { "name": "operand", "type": "MHLO_Tensor" }
    ],
    "attributes": [
      { "name": "replica_groups", "type": "I64ElementsAttr" },
      { "name": "channel_handle", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "mhlo.collective_permute",
    "summary": "CollectivePermute operator",
    "description": "CollectivePermute is a collective operation that sends and receives data\n    cross replicas.\n    Note that there are the following restrictions on the source_target_pair:\n    - Any two pairs should not have the same target replica id, and they should\n    not have the same source replica id.\n    - If a replica id is not a target in any pair, then the output on that\n    replica is a tensor consists of 0(s) with the same shape as the input.\n\n    See https://www.tensorflow.org/xla/operation_semantics#collectivepermute.",
    "inputs": [
      { "name": "operand", "type": "HLO_Tensor" }
    ],
    "attributes": [
      { "name": "source_target_pairs", "type": "I64ElementsAttr" }
    ]
  },
  {
    "name": "mhlo.compare",
    "summary": "Comparison operator",
    "description": "Compares `lhs` and `rhs` elementwise according to `comparison_direction`\n    and `compare_type`. If unspecified, `compare_type` is FLOAT for float element\n    types, SIGNED for signed element types and UNSIGNED for unsigned element\n    types.\n\n    See\n    https://www.tensorflow.org/xla/operation_semantics#element-wise_comparison_operations.",
    "inputs": [
      { "name": "lhs", "type": "HLO_Tensor" },
      { "name": "rhs", "type": "HLO_Tensor" }
    ],
    "attributes": [
      { "name": "comparison_direction", "type": "HLO_ComparisonDirectionAttr" },
      { "name": "compare_type", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "mhlo.complex",
    "summary": "Complex operator",
    "description": "Performs element-wise conversion of a pair of real and imaginary values to\n    a complex value.",
    "inputs": [
      { "name": "lhs", "type": "HLO_FpTensor" },
      { "name": "rhs", "type": "HLO_FpTensor" }
    ]
  },
  {
    "name": "mhlo.composite",
    "summary": "Composite operation",
    "description": "Encapsulates an operation made up (composed) of other StableHLO operations,\n    taking `inputs` and `composite_attributes` and producing `results`. The\n    semantics of the op are implemented by the `decomposition` attribute. The\n    `composite` op can be replaced with its decomposition without changing program\n    semantics. In cases where inlining the decomposition does not provide the same\n    op semantics, prefer using `custom_call`.\n\n    The `version` field (defaults to `0`) is used to denote when a composite's\n    semantics change.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#composite\n\n    Example:\n    ```mlir\n    %results = mhlo.composite \"my.op\" %arg0, %arg1 {\n      decomposition = @my_op,\n      composite_attributes = { my_attribute = \"my_value\" },\n      version = 1 : i32\n    } : (tensor<f32>, tensor<f32>) -> tensor<f32>\n    ```",
    "inputs": [
      { "name": "inputs", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "name", "type": "StrAttr" },
      { "name": "composite_attributes", "type": "DefaultValuedOptionalAttr" },
      { "name": "decomposition", "type": "FlatSymbolRefAttr" },
      { "name": "version", "type": "DefaultValuedOptionalAttr" }
    ],
    "assemblyFormat": "$name $inputs attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "mhlo.compute_reshape_shape",
    "summary": "Compute input for reshape with any dynamic dim resolved",
    "description": "This operation handles the dynamic aspect of a TF/NumPy/CHLO reshape. The\n    dynamic aspect is that a single extent can be -1 and that dimension will\n    instead be computed. This handles the computation and can then be passed to\n    an HLO DynamicReshapeOp to replicate the TF/NumPy reshape behavior.\n\n    This op has undefined behavior if the dimensions do not evenly divide the\n    number of elements, or if there are multiple -1 values. It is an identity op\n    if no dimensions are -1.\n\n    ```\n    %0 = hlo.compute_reshape_shape 12, [2, -1] -> [2, 6]\n    ```",
    "inputs": [
      { "name": "num_elements", "type": "Index" },
      { "name": "dynamic_shape", "type": "DTensorOf" }
    ],
    "outputs": [
      { "name": "result", "type": "DTensorOf" }
    ],
    "assemblyFormat": "$num_elements `,` $dynamic_shape attr-dict `:` type($num_elements) `,` type($dynamic_shape) `->` type($result)"
  },
  {
    "name": "mhlo.concatenate",
    "summary": "XLA's concatenate op",
    "description": "Concatenates a set of tensors along the specified dimension.\n\n     See https://www.tensorflow.org/xla/operation_semantics#concatenate.",
    "inputs": [
      { "name": "val", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "dimension", "type": "I64Attr" }
    ]
  },
  {
    "name": "mhlo.constant",
    "summary": "Constant operator",
    "description": "Represents a constant value.",
    "outputs": [
      { "name": "output", "type": "HLO_StaticShapeTensor" }
    ],
    "attributes": [
      { "name": "value", "type": "ElementsAttr" }
    ],
    "assemblyFormat": "attr-dict $value"
  },
  {
    "name": "mhlo.convert",
    "summary": "Convert operator",
    "description": "Performs element-wise conversion of values from one type to another, e.g.\n    float to int.\n\n    See https://www.tensorflow.org/xla/operation_semantics#convertelementtype.",
    "inputs": [
      { "name": "operand", "type": "TensorType" }
    ]
  },
  {
    "name": "mhlo.convolution",
    "summary": "Convolution operator",
    "description": "Computes a convolution of the kind used in neural networks.\n\n    See https://www.tensorflow.org/xla/operation_semantics#conv_convolution.",
    "assemblyFormat": "`(`operands`)`\n       `dim_numbers` `=` custom<ConvolutionDimensions>($dimension_numbers) `,`\n       `window` `=` `{` custom<WindowAttributes>($window_strides, $padding,\n                                                 $lhs_dilation, $rhs_dilation,\n                                                 $window_reversal) `}`\n       attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "mhlo.copy",
    "summary": "Copy operator",
    "description": "Returns a copy of `operand`."
  },
  {
    "name": "mhlo.cosh",
    "summary": "Cosh operation",
    "description": "Performs element-wise cosh operation on `operand` tensor and produces a\n    `result` tensor.\n\n    Example:\n    ```mlir\n    %result = mhlo.cosh %operand : tensor<2x2xf32>\n    ```",
    "inputs": [
      { "name": "operand", "type": "OperandType" }
    ],
    "outputs": [
      { "name": "result", "type": "ResultType" }
    ],
    "assemblyFormat": "$operand attr-dict\n      `:` custom<SameOperandsAndResultType>(type($operand), type($result))"
  },
  {
    "name": "mhlo.cosine",
    "summary": "Cos operator",
    "description": "Returns `Cos(operand)` element-wise.\n\n    See\n    https://www.tensorflow.org/xla/operation_semantics#element-wise_unary_functions.",
    "inputs": [
      { "name": "operand", "type": "TensorType" }
    ]
  },
  {
    "name": "mhlo.count_leading_zeros",
    "summary": "Count-leading-zeros (Clz) operator",
    "description": "Returns the number of leading zeros in each operand element-wise.\n\n    See\n    https://www.tensorflow.org/xla/operation_semantics#element-wise_unary_functions.",
    "inputs": [
      { "name": "operand", "type": "TensorType" }
    ]
  },
  {
    "name": "mhlo.create_token",
    "summary": "Create Token operator",
    "description": "Produces a HLO token. Tokens are used for ordering side-effecting perations.\n    This is exported to HLO as an AfterAll operation with no operands to\n    generate a token.",
    "outputs": [
      { "name": "output", "type": "HLO_Token" }
    ]
  },
  {
    "name": "mhlo.cross-replica-sum",
    "summary": "Sums input across replicated instances.",
    "description": "For each of the replica groups, operands of the group devices are summed\n     so that each device has the sum.\n\n     For example, suppose there are 8 TPU devices: `[A, B, C, D, E, F, G, H]`.\n     Passing group_assignment=`[[0,2,4,6],[1,3,5,7]]` sets `A, C, E, G` as group 0,\n     and `B, D, F, H` as group 1. Thus we get the outputs:\n     `[A+C+E+G, B+D+F+H, A+C+E+G, B+D+F+H, A+C+E+G, B+D+F+H, A+C+E+G, B+D+F+H]`.\n\n     See https://www.tensorflow.org/xla/operation_semantics#crossreplicasum.",
    "inputs": [
      { "name": "operand", "type": "HLO_Tensor" }
    ],
    "attributes": [
      { "name": "replica_groups", "type": "I64ElementsAttr" }
    ]
  },
  {
    "name": "mhlo.cstr_reshapable",
    "summary": "Compute input for reshape with any dynamic dim resolved",
    "description": "This operation creates a witness on the constraint that a given shape would\n    be a valid reshape for the given number of elements.\n\n    ```\n    %0 = mhlo.cstr_reshapable 12, [2, -1] -> success\n    %1 = mhlo.cstr_reshapable 13, [2, -1] -> failure\n    ```",
    "inputs": [
      { "name": "num_elements", "type": "Index" },
      { "name": "dynamic_shape", "type": "DTensorOf" }
    ],
    "outputs": [
      { "name": "result", "type": "Shape_WitnessType" }
    ],
    "assemblyFormat": "$num_elements `,` $dynamic_shape attr-dict `:` type($num_elements) `,` type($dynamic_shape)"
  },
  {
    "name": "mhlo.custom_call",
    "summary": "CustomCall operator",
    "description": "A custom call invokes code external to XLA. The `args` are passed to the\n    external code, and the external code is expected to produce a result of the\n    given type. The exact mechanism is backend-specific. For example, in the CPU\n    backend, a call instruction is emitted which targets a symbol with the name\n    `call_target_name`.\n\n    `call_target_name` and `backend_config` can be arbitrary strings, but\n    `call_target_name` should be short as it may be used in labels.\n    `backend_config` can encode arbitrarily large amounts of information.\n\n    `has_side_effect` must be true if the custom call has side-effects.\n    `api_version` specifies the version of the API used by the custom call\n    function.\n\n    A custom call may apply functions within the scope of the parent module.\n    They can be referenced using `called_computations` attribute.\n\n    A custom call can also have layout constraints on operands and results which\n    can be specified as optional `operand_layouts` and `result_layouts`\n    attributes. The layout attribute is an array of rank-1 index tensors and the\n    i-th layout attribute specifies the layout for i-th operand/result.\n\n    The `operand_layouts` & `result_layouts` attributes can be specified under\n    the following constraints:\n    1) Either both `operand_layouts` and `result_layouts` are specified or none.\n    2) None of the operands are of tuple type.\n    3) None of the results are of tuple type except the common case of single\n       tuple result packing non-tuple values is allowed. In this case the i-th\n       `result_layouts` attribute specifies the layout of i-th element in the\n       result tuple.\n\n    See https://www.tensorflow.org/xla/operation_semantics#customcall.",
    "inputs": [
      { "name": "args", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "call_target_name", "type": "StrAttr" },
      { "name": "has_side_effect", "type": "DefaultValuedAttr" },
      { "name": "backend_config", "type": "DefaultValuedStrAttr" },
      { "name": "api_version", "type": "DefaultValuedAttr" },
      { "name": "called_computations", "type": "DefaultValuedAttr" },
      { "name": "operand_layouts", "type": "OptionalAttr" },
      { "name": "result_layouts", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "mhlo.dequantize",
    "summary": "Dequantize operator",
    "description": "Dequantize the quantized input of packed uint32 to bfloat16. Only uint8 or\n    uint16 is supported for the original unpacked input.\n\n    Returns a tensor of shape [d0,..., dn * unpack_size] if unpacked input shape\n    is [d0, ..., dn], where unpack_size = sizeof(unit32) / sizeof(T), where T is\n    the unpacked input type. If transpose_output is true, will return a tensor\n    of shape [dn * unpack_size, dn-1, ..., d1, d0]. transpose_output is faster\n    when input's rank higher than 1. The input needs to be transposed to use\n    transpose_output feature.",
    "inputs": [
      { "name": "input", "type": "TensorOf" }
    ],
    "outputs": [
      { "name": "output", "type": "TensorOf" }
    ],
    "attributes": [
      { "name": "min_range", "type": "F32Attr" },
      { "name": "max_range", "type": "F32Attr" },
      { "name": "mode", "type": "HLO_DequantizeModeAttr" },
      { "name": "transpose_output", "type": "BoolAttr" },
      { "name": "is_16bits", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "mhlo.divide",
    "summary": "Division operator",
    "description": "Returns `lhs / rhs` element-wise.\n\n    See\n    https://www.tensorflow.org/xla/operation_semantics#element-wise_binary_arithmetic_operations.",
    "inputs": [
      { "name": "lhs", "type": "HLO_Tensor" },
      { "name": "rhs", "type": "HLO_Tensor" }
    ]
  },
  {
    "name": "mhlo.domain",
    "summary": "Domain operation",
    "description": "This operation is private to the XLA compiler, so it is does not yet have\n    a specification.\n\n    Informally, these operations are used to group instructions with the same\n    DomainMetadata property. ShardingMetadata is the main use case today to\n    group instructions on the same device. Domain instructions provide two\n    major benefits:\n      - Prevent unintentionally optimizing instructions across domains.\n      - Automatically assign the metadata of the instructions created in the domain.\n    Without domain instructions, each HLO optimization pass would have to check\n    and propagate the metadata, which would be easy to miss and also adds\n    complexity to the compiler. Since domain instructions connect two different\n    domains, each domain instruction is associated with two DomainMetadata --\n    one on the operand side and one on the user side of the domain.",
    "inputs": [
      { "name": "operand", "type": "MHLO_TensorOrToken" }
    ],
    "outputs": [
      { "name": "result", "type": "MHLO_TensorOrToken" }
    ],
    "attributes": [
      { "name": "kind", "type": "MHLO_DomainKindAttr" },
      { "name": "entry_metadata", "type": "StrAttr" },
      { "name": "exit_metadata", "type": "StrAttr" }
    ]
  },
  {
    "name": "mhlo.dot",
    "summary": "Dot operator",
    "description": "Performs dot products between vectors, vector/matrix and matrix/matrix\n    multiplication.\n\n    See https://www.tensorflow.org/xla/operation_semantics#dot.",
    "inputs": [
      { "name": "lhs", "type": "HLO_Tensor" },
      { "name": "rhs", "type": "HLO_Tensor" }
    ],
    "attributes": [
      { "name": "precision_config", "type": "HLO_PrecisionConfigAttr" }
    ]
  },
  {
    "name": "mhlo.dot_general",
    "summary": "General Dot operator",
    "description": "Performs general dot products between vectors, vector/matrix and\n    matrix/matrix multiplication.\n\n    See https://www.tensorflow.org/xla/operation_semantics#dotgeneral.",
    "inputs": [
      { "name": "lhs", "type": "HLO_Tensor" },
      { "name": "rhs", "type": "HLO_Tensor" },
      { "name": "dot_dimension_numbers", "type": "DotDimensionNumbers" }
    ],
    "attributes": [
      { "name": "precision_config", "type": "HLO_PrecisionConfigAttr" }
    ]
  },
  {
    "name": "mhlo.dynamic_broadcast_in_dim",
    "summary": "Broadcast a tensor into the given dynamic shape by adding dimensions.",
    "description": "This is a generalization of the BroadcastInDimOp which accepts its output\n    dimensions as an argument. It should eventually supercede the statically\n    shaped original, but is being phased as a separate op in order to support\n    compatibility with lowerings and translations that precede dynamic\n    shapes.",
    "inputs": [
      { "name": "operand", "type": "HLO_Tensor" },
      { "name": "output_dimensions", "type": "HLO_DimensionTensor" }
    ],
    "attributes": [
      { "name": "broadcast_dimensions", "type": "BroadcastDimAttr" }
    ]
  },
  {
    "name": "mhlo.dynamic_conv",
    "summary": "Dynamic Convolution operator",
    "description": "The dynamic shape version of ConvOp. Computes a convolution with dynamic padding."
  },
  {
    "name": "mhlo.dynamic_gather",
    "summary": "Dynamic Gather operator",
    "description": "The dynamic shape version of GatherOp. Stitches together several slices of\n    an input array.",
    "inputs": [
      { "name": "operand", "type": "HLO_Tensor" },
      { "name": "start_indices", "type": "HLO_IntTensor" },
      { "name": "slice_sizes", "type": "HLO_IntTensor" },
      { "name": "dimension_numbers", "type": "GatherDimensionNumbers" }
    ],
    "attributes": [
      { "name": "indices_are_sorted", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "mhlo.dynamic_iota",
    "summary": "Create linear increasing values from 0 to length -1.",
    "description": "Produces an HLO Tensor of the specified shape, with an incremental set of\n    values along the specified dimension starting at 0.\n\n    Requires:\n    - The output length of the tensor result.",
    "inputs": [
      { "name": "output_shape", "type": "HLO_DimensionTensor" }
    ],
    "outputs": [
      { "name": "result", "type": "HLO_Tensor" }
    ],
    "attributes": [
      { "name": "iota_dimension", "type": "I64Attr" }
    ]
  },
  {
    "name": "mhlo.dynamic_pad",
    "summary": "Dynamic Pad operator",
    "description": "Dynamically Pads the `operand`, with amount of padding added at\n    low-end/high-end/interior is passed through input tensors.",
    "inputs": [
      { "name": "operand", "type": "HLO_Tensor" },
      { "name": "padding_value", "type": "HLO_Tensor" },
      { "name": "edge_padding_low", "type": "HLO_DimensionTensor" },
      { "name": "edge_padding_high", "type": "HLO_DimensionTensor" },
      { "name": "interior_padding", "type": "HLO_DimensionTensor" }
    ],
    "outputs": [
      { "name": "result", "type": "HLO_Tensor" }
    ]
  },
  {
    "name": "mhlo.dynamic_reshape",
    "summary": "Reshape a tensor to a given, possibly dynamic, shape.",
    "description": "Reshapes `operand` to `output_shape`.\n\n    Requires:\n    - The length of `output_shape` is equal to the rank of `result`.\n    - The number of elements in `operand` (that is, the product of extents of\n      its shape) is equal to the number of elements in `output_shape` (that is,\n      the product of values in `output_shape`).",
    "inputs": [
      { "name": "operand", "type": "HLO_Tensor" },
      { "name": "output_shape", "type": "HLO_DimensionTensor" }
    ],
    "outputs": [
      { "name": "result", "type": "HLO_Tensor" }
    ]
  },
  {
    "name": "mhlo.dynamic_slice",
    "summary": "DynamicSlice operation",
    "description": "Extracts a slice from the `operand` using dynamically-computed starting\n    indices and produces a `result` tensor.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#dynamic_slice\n\n    Example:\n    ```mlir\n    %result = mhlo.dynamic_slice %operand, %start_indices0, %start_indices1, sizes = [2, 2]\n      : (tensor<4x4xi32>, tensor<i64>, tensor<i64>) -> tensor<2x2xi32>\n    ```",
    "inputs": [
      { "name": "operand", "type": "MHLO_Tensor" },
      { "name": "start_indices", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "result", "type": "MHLO_Tensor" }
    ],
    "attributes": [
      { "name": "slice_sizes", "type": "I64ElementsAttr" }
    ]
  },
  {
    "name": "mhlo.dynamic_update_slice",
    "summary": "DynamicUpdateSlice operation",
    "description": "Produces a `result` tensor which is equal to the `operand` tensor except\n    that the slice starting at `start_indices` is updated with the values in\n    `update`.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#dynamic_update_slice\n\n    Example:\n    ```mlir\n    %result = mhlo.dynamic_update_slice %operand, %update, %start_indices0, %start_indices1\n      : (tensor<4x4xi32>, tensor<2x2xi32>, tensor<i64>, tensor<i64>) -> tensor<4x4xi32>\n    ```",
    "inputs": [
      { "name": "operand", "type": "MHLO_Tensor" },
      { "name": "update", "type": "MHLO_Tensor" },
      { "name": "start_indices", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "result", "type": "MHLO_Tensor" }
    ],
    "assemblyFormat": "operands attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "mhlo.dynamic-slice",
    "summary": "Dynamic Slice operator",
    "description": "Extracts a sub-array from the input array at dynamic start_indices.\n\n    See https://www.tensorflow.org/xla/operation_semantics#dynamicslice.",
    "inputs": [
      { "name": "operand", "type": "HLO_Tensor" },
      { "name": "start_indices", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "result", "type": "HLO_Tensor" }
    ],
    "attributes": [
      { "name": "slice_sizes", "type": "I64ElementsAttr" }
    ]
  },
  {
    "name": "mhlo.dynamic-update-slice",
    "summary": "Dynamic Update Slice operator",
    "description": "DynamicUpdateSlice generates a result which is the value of the input array\n    operand, with a slice update overwritten at start_indices.\n\n    See https://www.tensorflow.org/xla/operation_semantics#dynamicupdateslice.",
    "inputs": [
      { "name": "operand", "type": "HLO_Tensor" },
      { "name": "update", "type": "HLO_Tensor" },
      { "name": "start_indices", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "result", "type": "HLO_Tensor" }
    ]
  },
  {
    "name": "mhlo.einsum",
    "inputs": [
      { "name": "lhs", "type": "HLO_Tensor" },
      { "name": "rhs", "type": "HLO_Tensor" }
    ],
    "attributes": [
      { "name": "einsum_config", "type": "StrAttr" }
    ]
  },
  {
    "name": "mhlo.erf",
    "summary": "Erf operation",
    "description": "Performs element-wise erf operation on `operand` tensor and produces a\n    `result` tensor.\n\n    Example:\n    ```mlir\n    %result = mhlo.erf %operand : tensor<2x2xf32>\n    ```",
    "inputs": [
      { "name": "operand", "type": "OperandType" }
    ],
    "outputs": [
      { "name": "result", "type": "ResultType" }
    ],
    "assemblyFormat": "$operand attr-dict\n      `:` custom<SameOperandsAndResultType>(type($operand), type($result))"
  },
  {
    "name": "mhlo.exponential",
    "summary": "Exponential operator",
    "description": "Returns `e^(operand)` element-wise.\n\n    See\n    https://www.tensorflow.org/xla/operation_semantics#element-wise_unary_functions.",
    "inputs": [
      { "name": "operand", "type": "TensorType" }
    ]
  },
  {
    "name": "mhlo.exponential_minus_one",
    "summary": "Exponential minus one operator",
    "description": "Returns `e^(operand) - 1` element-wise.\n\n    See\n    https://www.tensorflow.org/xla/operation_semantics#element-wise_unary_functions.",
    "inputs": [
      { "name": "operand", "type": "TensorType" }
    ]
  },
  {
    "name": "mhlo.fft",
    "summary": "Fast fourier transform operator",
    "description": "Returns the fast-fourier-transform of the input array.\n\n    See\n    https://www.tensorflow.org/xla/operation_semantics#fft.",
    "inputs": [
      { "name": "operand", "type": "HLO_Tensor" }
    ],
    "attributes": [
      { "name": "fft_type", "type": "HLO_FftTypeAttr" },
      { "name": "fft_length", "type": "I64ElementsAttr" }
    ]
  },
  {
    "name": "mhlo.floor",
    "summary": "Floor operator",
    "description": "Returns `Floor(operand)` element-wise.\n\n    See\n    https://www.tensorflow.org/xla/operation_semantics#element-wise_unary_functions.",
    "inputs": [
      { "name": "operand", "type": "TensorType" }
    ]
  },
  {
    "name": "mhlo.fusion",
    "summary": "Fusion operator",
    "description": "Models the fusion instruction.\n\n    A fusion op is consists of a group of basic ops (represented as a region\n    attached to it). It serves as a hint to the backend that it is beneficial\n    to emit the contained ops into a single loop nest or kernel.",
    "inputs": [
      { "name": "operands", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "results", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "fusion_kind", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "mhlo.gather",
    "summary": "Gather operator",
    "description": "Stitches together several slices of `operand` from offsets specified in\n    `start_indices` (each slice at a potentially different runtime offset).\n\n    See https://www.tensorflow.org/xla/operation_semantics#gather.",
    "inputs": [
      { "name": "operand", "type": "HLO_Tensor" },
      { "name": "start_indices", "type": "HLO_IntTensor" },
      { "name": "dimension_numbers", "type": "GatherDimensionNumbers" }
    ],
    "attributes": [
      { "name": "slice_sizes", "type": "I64ElementsAttr" },
      { "name": "indices_are_sorted", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "mhlo.get_dimension_size",
    "summary": "GetDimensionSize operator",
    "description": "Returns the size of the given dimension of the operand.\n\n    See\n    https://www.tensorflow.org/xla/operation_semantics#getdimensionsize.",
    "inputs": [
      { "name": "operand", "type": "HLO_Tensor" }
    ],
    "attributes": [
      { "name": "dimension", "type": "I64Attr" }
    ]
  },
  {
    "name": "mhlo.get_tuple_element",
    "summary": "GetTupleElement operator",
    "description": "Returns a member of a tuple specified by an index.\n\n    See https://www.tensorflow.org/xla/operation_semantics#gettupleelement.",
    "attributes": [
      { "name": "index", "type": "I32Attr" }
    ]
  },
  {
    "name": "mhlo.if",
    "summary": "If operator",
    "description": "Returns the result of executing either a true or false function depending on\n    the result of a condition function.\n\n    See https://www.tensorflow.org/xla/operation_semantics#conditional.",
    "inputs": [
      { "name": "pred", "type": "HLO_PredTensor" }
    ]
  },
  {
    "name": "mhlo.imag",
    "summary": "Imag operator",
    "description": "Returns `Imag(operand)` element-wise.",
    "inputs": [
      { "name": "operand", "type": "TensorType" }
    ]
  },
  {
    "name": "mhlo.infeed",
    "summary": "Infeed operator",
    "description": "Reads a single data item from the implicit Infeed streaming interface of\n    the device, interpreting the data as the given shape, and returns a XlaOp\n    of the data. Multiple Infeed operations are allowed in a computation, but\n    there must be a total order among the Infeed operations.\n\n    Attributes:\n      layout:  Array attribute. Same shape as the output of the infeed, except\n               that every tensor is replaced by a minor_to_major array for the\n               tensor's layout.\n\n    See https://www.tensorflow.org/xla/operation_semantics#infeed.",
    "inputs": [
      { "name": "token", "type": "HLO_Token" }
    ],
    "attributes": [
      { "name": "infeed_config", "type": "DefaultValuedStrAttr" },
      { "name": "layout", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "mhlo.iota",
    "summary": "Iota operator",
    "description": "Creates a rank 1 array of values starting at zero and incrementing by one.",
    "outputs": [
      { "name": "output", "type": "HLO_IntFpOrComplexTensor" }
    ],
    "attributes": [
      { "name": "iota_dimension", "type": "I64Attr" }
    ]
  },
  {
    "name": "mhlo.is_finite",
    "summary": "IsFinite operator",
    "description": "Tests whether each element of operand is finite, i.e., is not positive or\n    negative infinity, and is not NaN. Returns a tensor of 1-bit integers with\n    the same shape as the input, where each element is nonzero (i.e. true) if\n    and only if the corresponding input element is finite.\n\n    See\n    https://www.tensorflow.org/xla/operation_semantics#element-wise_unary_functions.",
    "inputs": [
      { "name": "x", "type": "HLO_FpTensor" }
    ],
    "outputs": [
      { "name": "y", "type": "HLO_PredTensor" }
    ]
  },
  {
    "name": "mhlo.log",
    "summary": "Logarithm operator",
    "description": "Returns `log(operand)` element-wise.\n\n    See\n    https://www.tensorflow.org/xla/operation_semantics#element-wise_unary_functions.",
    "inputs": [
      { "name": "operand", "type": "TensorType" }
    ]
  },
  {
    "name": "mhlo.log_plus_one",
    "summary": "Log1p operator",
    "description": "Returns `log(operand+1)` element-wise.\n\n    See\n    https://www.tensorflow.org/xla/operation_semantics#element-wise_unary_functions.",
    "inputs": [
      { "name": "operand", "type": "TensorType" }
    ]
  },
  {
    "name": "mhlo.logistic",
    "summary": "Logistic operator",
    "description": "Returns `logistic(operand)` element-wise.\n\n    See\n    https://www.tensorflow.org/xla/operation_semantics#element-wise_unary_functions.",
    "inputs": [
      { "name": "operand", "type": "TensorType" }
    ]
  },
  {
    "name": "mhlo.map",
    "summary": "Map operator",
    "description": "Applies a scalar function over the given operands arrays, producing an array\n  of the same dimensions where each element is the result of the mapped function\n  applied to the corresponding elements in the input arrays.\n\n  The mapped function is an arbitrary computation with the restriction that it\n  has N inputs of scalar type T and a single output with type S. The output has\n  the same dimensions as the operands except that the element type T is replaced\n  with S.\n\n  See https://www.tensorflow.org/xla/operation_semantics#map.",
    "inputs": [
      { "name": "operands", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "dimensions", "type": "I64ElementsAttr" }
    ]
  },
  {
    "name": "mhlo.maximum",
    "summary": "Maximum operator",
    "description": "Returns `max(lhs, rhs)` element-wise.\n\n    See\n    https://www.tensorflow.org/xla/operation_semantics#element-wise_binary_arithmetic_operations.",
    "inputs": [
      { "name": "lhs", "type": "HLO_Tensor" },
      { "name": "rhs", "type": "HLO_Tensor" }
    ]
  },
  {
    "name": "mhlo.minimum",
    "summary": "Minimum operator",
    "description": "Returns `min(lhs, rhs)` element-wise.\n\n    See\n    https://www.tensorflow.org/xla/operation_semantics#element-wise_binary_arithmetic_operations.",
    "inputs": [
      { "name": "lhs", "type": "HLO_Tensor" },
      { "name": "rhs", "type": "HLO_Tensor" }
    ]
  },
  {
    "name": "mhlo.minimum_broadcast_shapes",
    "summary": "Minimizes the rank of two or more shapes to be broadcasted",
    "description": "Given two or more 1D tensors representing shapes, returns one 1D tensor for\n    each operand, where operand `i` corresponds to output `i`.\n\n    The returned tensors have the property that they specify a shape which is a\n    reshape of the corresponding input shape, and the broadcasted output shape\n    (using shape::BroadcastOp) of the returned shapes is a reshape of the\n    broadcasted output shape of the input shapes. Among all possibilities with\n    this property, the one is chosen which minimizes the rank of each returned\n    shape.\n\n    The general idea of this op is that it can be used for ops which have a\n    broadcasting semantic to operate on shapes with a possibly smaller rank\n    while preserving equivalence of the computed values. After computing the\n    result of the op using reshaped operands, the result can be reshaped to the\n    result that would have been originally computed.\n\n    Here is an example with two input shapes:\n\n    ```mlir\n    mhlo.minimum_broadcast_shapes [1, 2, 3, 1, 2, 1],\n                                     [1, 1, 1, 2, 3] -> [6, 2, 1], [2, 3]\n    ```\n\n    The broadcasted output shape of the operands is [1, 2, 3, 1, 2, 3], the\n    broadcasted output shape of the outputs is [6, 2, 3]. These two shapes are\n    reshapes of each other, and also each output is a reshape of the\n    corresponding input.",
    "inputs": [
      { "name": "shapes", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "results", "type": "Variadic" }
    ],
    "assemblyFormat": "$shapes attr-dict `:` type($shapes) `->` type($results)"
  },
  {
    "name": "mhlo.multiply",
    "summary": "Multiplication operator",
    "description": "Returns `lhs * rhs` element-wise.\n\n    See\n    https://www.tensorflow.org/xla/operation_semantics#element-wise_binary_arithmetic_operations.",
    "inputs": [
      { "name": "lhs", "type": "HLO_Tensor" },
      { "name": "rhs", "type": "HLO_Tensor" }
    ]
  },
  {
    "name": "mhlo.negate",
    "summary": "Negation operator",
    "description": "Returns `-operand` element-wise.\n\n    See\n    https://www.tensorflow.org/xla/operation_semantics#element-wise_unary_functions.",
    "inputs": [
      { "name": "operand", "type": "TensorType" }
    ]
  },
  {
    "name": "mhlo.not",
    "summary": "Not operator",
    "description": "Returns `!operand` element-wise.\n\n    See\n    https://www.tensorflow.org/xla/operation_semantics#element-wise_unary_functions.",
    "inputs": [
      { "name": "operand", "type": "TensorType" }
    ]
  },
  {
    "name": "mhlo.optimization_barrier",
    "summary": "The `hlo.optimization_barrier` op blocks optimizations.",
    "description": "Blocks any optimization pass from moving computations across the barrier.\n\n    Ensures that all inputs are evaluated before any operators that depend on the barrier's outputs.\n    See\n    https://www.tensorflow.org/xla/operation_semantics#optimizationbarrier",
    "inputs": [
      { "name": "arg", "type": "Variadic" }
    ]
  },
  {
    "name": "mhlo.or",
    "summary": "Logical or",
    "description": "Returns `logical_or(lhs, rhs)` element-wise.\n\n    See\n    https://www.tensorflow.org/xla/operation_semantics#element-wise_binary_arithmetic_operations.",
    "inputs": [
      { "name": "lhs", "type": "HLO_PredOrIntTensor" },
      { "name": "rhs", "type": "HLO_PredOrIntTensor" }
    ]
  },
  {
    "name": "mhlo.outfeed",
    "summary": "Outfeed operator",
    "description": "Generates outgoing data transfers for the given data. It takes data and a\n    token type operand and produces a token type value. Tokens are used for\n    ordering side-effecting operations.\n\n    See https://www.tensorflow.org/xla/operation_semantics#outfeed.",
    "inputs": [
      { "name": "operand", "type": "HLO_TensorOrTuple" },
      { "name": "token", "type": "HLO_Token" }
    ],
    "attributes": [
      { "name": "outfeed_config", "type": "DefaultValuedStrAttr" }
    ]
  },
  {
    "name": "mhlo.pad",
    "summary": "Pad operator",
    "description": "Pads the `operand` according to TBD.",
    "inputs": [
      { "name": "operand", "type": "HLO_Tensor" },
      { "name": "padding_value", "type": "HLO_Tensor" }
    ],
    "attributes": [
      { "name": "edge_padding_low", "type": "I64ElementsAttr" },
      { "name": "edge_padding_high", "type": "I64ElementsAttr" },
      { "name": "interior_padding", "type": "I64ElementsAttr" }
    ]
  },
  {
    "name": "mhlo.partition_id",
    "summary": "PartitionId operation",
    "description": "Produces `partition_id` of the current process.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#partition_id\n\n    Example:\n    ```mlir\n    %result = mhlo.partition_id : tensor<ui32>\n    ```",
    "assemblyFormat": "attr-dict `:` type(results)"
  },
  {
    "name": "mhlo.popcnt",
    "summary": "PopulationCount operator",
    "description": "Returns the number of bits set in each operand element-wise.\n\n    See\n    https://www.tensorflow.org/xla/operation_semantics#element-wise_unary_functions.",
    "inputs": [
      { "name": "operand", "type": "TensorType" }
    ]
  },
  {
    "name": "mhlo.power",
    "summary": "Power operator",
    "description": "Returns `lhs ^ rhs` element-wise.\n\n    See\n    https://www.tensorflow.org/xla/operation_semantics#element-wise_binary_arithmetic_operations.",
    "inputs": [
      { "name": "lhs", "type": "HLO_Tensor" },
      { "name": "rhs", "type": "HLO_Tensor" }
    ]
  },
  {
    "name": "mhlo.print",
    "summary": "Print operation",
    "description": "PrintOp is used to print tensor, which can be used to debug.",
    "inputs": [
      { "name": "input", "type": "HLO_Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "HLO_Tensor" }
    ]
  },
  {
    "name": "mhlo.ragged_dot",
    "summary": "Ragged matrix multiplication over a single ragged dimension",
    "description": "This operation takes three tensor args---lhs, rhs, and group_sizes---and\n    a \"ragged_dot_dimension_numbers\" attribute. Like dot_general, the lhs and\n    rhs are allowed arbitrary batch and contracting dimensions. Additionally,\n    the lhs is required to have one ragged dimension, and the rhs may have at\n    most one group dimension. The op has three modes, depending on the kind of\n    the lhs ragged dimension.\n\n    In mode 1, the shape-signature is `[b,m,k], [g,b,k,n], [b,g] -> [b,m,n]`.\n    Here the ragged dimension is an lhs non-contracting dimension (`m`). The\n    dimensions `b` and `k` represent batch and contracting dimensions\n    respectively. The rhs is required to have a group dimension (`g`).\n\n    In mode 2, the shape-signature is `[b,m,k], [b,k,n], [b,g] -> [g,b,m,n]`.\n    Here the ragged dimension is an lhs/rhs contracting dimension (`k`).\n\n    In mode 3, the shape-signature is `[b,m,k], [b,k,n], [g] -> [b,m,n]`. Here\n    the ragged dimension is an lhs/rhs batch dimension (`b`).",
    "inputs": [
      { "name": "lhs", "type": "MHLO_Tensor" },
      { "name": "rhs", "type": "MHLO_Tensor" },
      { "name": "group_sizes", "type": "MHLO_Tensor" },
      { "name": "ragged_dot_dimension_numbers", "type": "MHLO_RaggedDotDimensionNumbers" }
    ],
    "outputs": [
      { "name": "result", "type": "HLO_Tensor" }
    ],
    "attributes": [
      { "name": "precision_config", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "mhlo.real",
    "summary": "Real operator",
    "description": "Returns `Real(operand)` element-wise.",
    "inputs": [
      { "name": "operand", "type": "TensorType" }
    ]
  },
  {
    "name": "mhlo.real_dynamic_slice",
    "summary": "Real Dynamic Slice operator",
    "description": "The dynamic shape version of SliceOp. Extracts a sub-array from the input\n    array according to start_indices, limit_indices and strides. Expect\n    start_indices/limit_indices/strides to be statically shaped and matching\n    the rank of the input.",
    "inputs": [
      { "name": "operand", "type": "HLO_Tensor" },
      { "name": "start_indices", "type": "HLO_DimensionTensor" },
      { "name": "limit_indices", "type": "HLO_DimensionTensor" },
      { "name": "strides", "type": "HLO_DimensionTensor" }
    ],
    "outputs": [
      { "name": "result", "type": "HLO_Tensor" }
    ]
  },
  {
    "name": "mhlo.recv",
    "summary": "Recv operator",
    "description": "Receives data of the given shape from a Send instruction in another\n    computation that shares the same channel handle. Returns a tuple containing\n    value for the received data and a token. Recv operation represents\n    synchronous communication. However, the instruction is internally decomposed\n    into 2 HLO instructions (Recv and RecvDone) to enable asynchronous data\n    transfers.\n\n    See https://www.tensorflow.org/xla/operation_semantics#recv.",
    "inputs": [
      { "name": "token", "type": "HLO_Token" },
      { "name": "channel_handle", "type": "ChannelHandle" }
    ],
    "attributes": [
      { "name": "is_host_transfer", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "mhlo.reduce",
    "summary": "Reduce operator",
    "description": "Returns the result of executing a reduction function on one or more arrays\n    in parallel.\n\n    See https://www.tensorflow.org/xla/operation_semantics#reduce.",
    "inputs": [
      { "name": "inputs", "type": "Variadic" },
      { "name": "init_values", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "dimensions", "type": "I64ElementsAttr" }
    ]
  },
  {
    "name": "mhlo.reduce_precision",
    "summary": "Reduce precision operator",
    "description": "Models the effect of converting floating - point values to a lower -\n    precision format(such as IEEE - FP16) and back to the original\n    format. The number of exponent and mantissa bits in the lower -\n    precision format can be specified arbitrarily,\n    although all bit sizes may not be supported on all hardware\n    implementations.\n\n    See https://www.tensorflow.org/xla/operation_semantics#reduceprecision.",
    "inputs": [
      { "name": "operand", "type": "HLO_FpTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "HLO_FpTensor" }
    ],
    "attributes": [
      { "name": "exponent_bits", "type": "I32Attr" },
      { "name": "mantissa_bits", "type": "I32Attr" }
    ]
  },
  {
    "name": "mhlo.reduce_scatter",
    "summary": "ReduceScatter operator",
    "description": "Performs all_reduce followed by a scatter.\n\n     See https://www.tensorflow.org/xla/operation_semantics#reducescatter",
    "inputs": [
      { "name": "operand", "type": "HLO_Tensor" }
    ],
    "attributes": [
      { "name": "scatter_dimension", "type": "I64Attr" },
      { "name": "replica_groups", "type": "I64ElementsAttr" },
      { "name": "channel_handle", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "mhlo.reduce_window",
    "summary": "ReduceWindow operator",
    "description": "Returns the result of executing a reduction function over all elements in\n    each window of one or more arrays in parallel.\n\n    See https://www.tensorflow.org/xla/operation_semantics#reducewindow.",
    "inputs": [
      { "name": "inputs", "type": "Variadic" },
      { "name": "init_values", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "window_dimensions", "type": "I64ElementsAttr" },
      { "name": "window_strides", "type": "OptionalAttr" },
      { "name": "base_dilations", "type": "OptionalAttr" },
      { "name": "window_dilations", "type": "OptionalAttr" },
      { "name": "padding", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "mhlo.remainder",
    "summary": "Remainder operator",
    "description": "Returns `lhs % rhs` element-wise.\n\n    See\n    https://www.tensorflow.org/xla/operation_semantics#element-wise_binary_arithmetic_operations.",
    "inputs": [
      { "name": "lhs", "type": "HLO_Tensor" },
      { "name": "rhs", "type": "HLO_Tensor" }
    ]
  },
  {
    "name": "mhlo.replica_id",
    "summary": "ReplicaId operator",
    "description": "Returns the unique ID (int32 scalar) of the replica.\n\n    The unique ID of each replica is an unsigned integer in the interval [0, N),\n    where N is the number of replicas. Since all the replicas are running the\n    same program, a ReplicaId() call in the program will return a different\n    value on each replica.\n\n    See https://www.tensorflow.org/xla/operation_semantics#replicaid."
  },
  {
    "name": "mhlo.reshape",
    "summary": "Reshape operator",
    "description": "Reshapes the dimensions of `operand` into a new configuration.\n\n    See https://www.tensorflow.org/xla/operation_semantics#reshape.",
    "inputs": [
      { "name": "operand", "type": "HLO_Tensor" }
    ]
  },
  {
    "name": "mhlo.return",
    "summary": "The `hlo.return` operation terminates a region and returns values.",
    "inputs": [
      { "name": "results", "type": "Variadic" }
    ]
  },
  {
    "name": "mhlo.reverse",
    "summary": "Reverse operator",
    "description": "Reverses the specified dimensions of `operand` according to the given\n    `dimensions`.\n\n    See https://www.tensorflow.org/xla/operation_semantics#rev_reverse.",
    "inputs": [
      { "name": "operand", "type": "HLO_Tensor" }
    ],
    "attributes": [
      { "name": "dimensions", "type": "I64ElementsAttr" }
    ]
  },
  {
    "name": "mhlo.rng",
    "summary": "Rng operation",
    "description": "Generates random numbers using the `rng_distribution` algorithm and produces\n    a `result` tensor of a given shape `shape`.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#rng\n\n    Example:\n    ```mlir\n    %result = mhlo.rng %a, %b, %shape, distribution = NORMAL : (tensor<i32>, tensor<i32>, tensor<2xi64>) -> tensor<3x3xi32>\n    ```",
    "inputs": [
      { "name": "a", "type": "DTensorOf" },
      { "name": "b", "type": "DTensorOf" },
      { "name": "shape", "type": "MHLO_DimensionTensor" }
    ],
    "outputs": [
      { "name": "result", "type": "MHLO_PredIntOrFpTensor" }
    ],
    "attributes": [
      { "name": "rng_distribution", "type": "MHLO_RngDistributionAttr" }
    ]
  },
  {
    "name": "mhlo.rng_bit_generator",
    "summary": "Uniform random number generator operator",
    "description": "Returns an output with a given shape filled with uniform random bits using\n    the specified algorithm (or backend default) and returns an updated state\n    (with the same shape as initial state) and the generated random data.\n\n    See\n    https://www.tensorflow.org/xla/operation_semantics#rngbitgenerator.",
    "inputs": [
      { "name": "initial_state", "type": "HLO_IntOrFpTensor" }
    ],
    "attributes": [
      { "name": "rng_algorithm", "type": "I32Attr" }
    ]
  },
  {
    "name": "mhlo.rng_normal",
    "summary": "RNG with normal distribution.",
    "description": "Constructs an output of a given shape with random numbers generated\n    following the normal distribution with parameters `mu` and `sigma`. The\n    parameters and output shape have to have a floating point elemental type.\n    The parameters furthermore have to be scalar valued.\n\n    See https://www.tensorflow.org/xla/operation_semantics#rngnormal.",
    "inputs": [
      { "name": "mu", "type": "HLO_FpTensor" },
      { "name": "sigma", "type": "HLO_FpTensor" },
      { "name": "shape", "type": "HLO_DimensionTensor" }
    ]
  },
  {
    "name": "mhlo.rng_uniform",
    "summary": "RNG with uniform distribution.",
    "description": "Constructs an output of a given shape with random numbers generated\n    following the uniform distribution over the interval `[a,b)`. The parameters\n    and output element type have to be a boolean type, an integral type or a\n    floating point types, and the types have to be consistent.\n\n    See https://www.tensorflow.org/xla/operation_semantics#rnguniform.",
    "inputs": [
      { "name": "a", "type": "HLO_PredIntOrFpTensor" },
      { "name": "b", "type": "HLO_PredIntOrFpTensor" },
      { "name": "shape", "type": "HLO_DimensionTensor" }
    ]
  },
  {
    "name": "mhlo.round_nearest_afz",
    "summary": "Round operator",
    "description": "Returns `Round(operand)` element-wise, rounding to nearest integer with\n    half-way cases rounding away from zero.\n\n    See\n    https://www.tensorflow.org/xla/operation_semantics#element-wise_unary_functions.",
    "inputs": [
      { "name": "operand", "type": "TensorType" }
    ]
  },
  {
    "name": "mhlo.round_nearest_even",
    "summary": "RoundNearestEven operation",
    "description": "Performs element-wise rounding towards the nearest integer, breaking ties\n    towards the even integer, on the `operand` tensor and produces a `result`\n    tensor.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#round_nearest_even\n\n    Example:\n    ```mlir\n    %result = mhlo.round_nearest_even %operand : tensor<5xf32>\n    ```",
    "inputs": [
      { "name": "operand", "type": "OperandType" }
    ],
    "outputs": [
      { "name": "result", "type": "ResultType" }
    ],
    "assemblyFormat": "$operand attr-dict\n      `:` custom<SameOperandsAndResultType>(type($operand), type($result))"
  },
  {
    "name": "mhlo.rsqrt",
    "summary": "Reciprocal Square-root operator",
    "description": "Returns `1.0 / sqrt(operand)` element-wise.\n\n    See\n    https://www.tensorflow.org/xla/operation_semantics#element-wise_unary_functions.",
    "inputs": [
      { "name": "operand", "type": "TensorType" }
    ]
  },
  {
    "name": "mhlo.scatter",
    "summary": "Scatter operator",
    "description": "Generates a result which is the value of the input array `operand`,\n    with several slices (at indices specified by `scatter_indices`)\n    updated with the values in `updates` using `update_computation`.\n\n    See https://www.tensorflow.org/xla/operation_semantics#scatter.",
    "inputs": [
      { "name": "operand", "type": "HLO_Tensor" },
      { "name": "scatter_indices", "type": "HLO_Tensor" },
      { "name": "updates", "type": "HLO_Tensor" },
      { "name": "scatter_dimension_numbers", "type": "ScatterDimensionNumbers" }
    ],
    "attributes": [
      { "name": "indices_are_sorted", "type": "DefaultValuedAttr" },
      { "name": "unique_indices", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "mhlo.select",
    "summary": "Select operator",
    "description": "Constructs an output tensor from the elements of `on_true` and `on_false`\n    based on the values of `pred`. All three operands must be of the same shape\n    with the exception of `pred`, which may also be a scalar in which case it is\n    broadcasted.\n\n    See https://www.tensorflow.org/xla/operation_semantics#select.",
    "inputs": [
      { "name": "pred", "type": "HLO_PredTensor" },
      { "name": "on_true", "type": "HLO_Tensor" },
      { "name": "on_false", "type": "HLO_Tensor" }
    ]
  },
  {
    "name": "mhlo.select_and_scatter",
    "summary": "SelectAndScatter operator",
    "description": "Runs a windowed selection `select` function over `operand` with shape\n    `window_dimensions` and stride `window_strides`. This will produce an amount\n    of selected locations whose shape matches `source`. These are then scattered\n    to the output which is initialized with `init_value`.\n    Multiple scattered elements which land in the same output location are\n    combined using the `scatter` function.\n\n    See https://www.tensorflow.org/xla/operation_semantics#selectandscatter.",
    "inputs": [
      { "name": "operand", "type": "HLO_Tensor" },
      { "name": "source", "type": "HLO_Tensor" },
      { "name": "init_value", "type": "HLO_Tensor" }
    ],
    "attributes": [
      { "name": "window_dimensions", "type": "OptionalAttr" },
      { "name": "window_strides", "type": "OptionalAttr" },
      { "name": "padding", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "mhlo.send",
    "summary": "Send operator",
    "description": "Sends the given operand data to a Recv instruction in another computation\n    that shares the same channel handle. Does not return any data. Similar to\n    the Recv operation, Send operation represents synchronous communication,\n    and is internally decomposed into 2 HLO instructions (Send and SendDone) to\n    enable asynchronous data transfers.\n\n    See https://www.tensorflow.org/xla/operation_semantics#send.",
    "inputs": [
      { "name": "operand", "type": "HLO_TensorOrTuple" },
      { "name": "token", "type": "HLO_Token" },
      { "name": "channel_handle", "type": "ChannelHandle" }
    ],
    "attributes": [
      { "name": "is_host_transfer", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "mhlo.set_dimension_size",
    "summary": "SetDimensionSize operator",
    "description": "Sets the dynamic size of operand's given dimension. Pass through the operand\n    as result, with dynamic dimension tracked by the compiler. Padded values\n    will be ignored by downstream reduction ops.\n\n    See https://www.tensorflow.org/xla/operation_semantics#setdimensionsize.",
    "inputs": [
      { "name": "operand", "type": "HLO_Tensor" },
      { "name": "size", "type": "I32Tensor" }
    ],
    "attributes": [
      { "name": "dimension", "type": "I64Attr" }
    ]
  },
  {
    "name": "mhlo.shift_left",
    "summary": "Shift Left operator",
    "description": "Returns `lhs << rhs` element-wise.\n\n    See\n    https://www.tensorflow.org/xla/operation_semantics#element-wise_binary_arithmetic_operations.",
    "inputs": [
      { "name": "lhs", "type": "HLO_Tensor" },
      { "name": "rhs", "type": "HLO_Tensor" }
    ]
  },
  {
    "name": "mhlo.shift_right_arithmetic",
    "summary": "Shift right arithmetic operator",
    "description": "Returns arithmetic `lhs >> rhs` element-wise.\n\n    See\n    https://www.tensorflow.org/xla/operation_semantics#element-wise_binary_arithmetic_operations.",
    "inputs": [
      { "name": "lhs", "type": "HLO_Tensor" },
      { "name": "rhs", "type": "HLO_Tensor" }
    ]
  },
  {
    "name": "mhlo.shift_right_logical",
    "summary": "Shift right logical operator",
    "description": "Returns logical `lhs >> rhs` element-wise.\n\n    See\n    https://www.tensorflow.org/xla/operation_semantics#element-wise_binary_arithmetic_operations.",
    "inputs": [
      { "name": "lhs", "type": "HLO_Tensor" },
      { "name": "rhs", "type": "HLO_Tensor" }
    ]
  },
  {
    "name": "mhlo.sign",
    "summary": "Sign operator",
    "description": "Returns `sign(operand)` element-wise, where\n\n    ```\n    sign(x) = -1  : x < 0\n            = -0  : x = -0\n            = NaN : x = NaN\n            = +0  : x = +0\n            = 1   : x > 0\n    ```\n\n    See\n    https://www.tensorflow.org/xla/operation_semantics#element-wise_unary_functions.",
    "inputs": [
      { "name": "operand", "type": "TensorType" }
    ]
  },
  {
    "name": "mhlo.sine",
    "summary": "Sin operator",
    "description": "Returns `Sin(operand)` element-wise.\n\n    See\n    https://www.tensorflow.org/xla/operation_semantics#element-wise_unary_functions.",
    "inputs": [
      { "name": "operand", "type": "TensorType" }
    ]
  },
  {
    "name": "mhlo.sinh",
    "summary": "Sinh operation",
    "description": "Performs element-wise sinh operation on `operand` tensor and produces a\n    `result` tensor.\n\n    Example:\n    ```mlir\n    %result = mhlo.sinh %operand : tensor<2x2xf32>\n    ```",
    "inputs": [
      { "name": "operand", "type": "OperandType" }
    ],
    "outputs": [
      { "name": "result", "type": "ResultType" }
    ],
    "assemblyFormat": "$operand attr-dict\n      `:` custom<SameOperandsAndResultType>(type($operand), type($result))"
  },
  {
    "name": "mhlo.slice",
    "inputs": [
      { "name": "operand", "type": "HLO_Tensor" }
    ],
    "attributes": [
      { "name": "start_indices", "type": "I64ElementsAttr" },
      { "name": "limit_indices", "type": "I64ElementsAttr" },
      { "name": "strides", "type": "I64ElementsAttr" }
    ]
  },
  {
    "name": "mhlo.sort",
    "summary": "Sort operator",
    "description": "Sorts the given `operands` at the given `dimension` with the given\n    `comparator`.\n\n    See https://www.tensorflow.org/xla/operation_semantics#sort.",
    "inputs": [
      { "name": "operands", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "dimension", "type": "DefaultValuedAttr" },
      { "name": "is_stable", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "mhlo.sqrt",
    "summary": "Square-root operator",
    "description": "Returns `sqrt(operand)` element-wise.\n\n    See\n    https://www.tensorflow.org/xla/operation_semantics#element-wise_unary_functions.",
    "inputs": [
      { "name": "operand", "type": "TensorType" }
    ]
  },
  {
    "name": "mhlo.stochastic_convert",
    "summary": "StochasticConvert operation",
    "description": "This operation is a work in progress, so it is not yet included in\n    the specification: https://github.com/openxla/stablehlo/issues/295.\n\n    Informally, this operation performs element-wise conversion of values from\n    a bigger type to a smaller one with stochastic rounding using the random\n    number passed in.",
    "inputs": [
      { "name": "operand", "type": "MHLO_FpTensor" },
      { "name": "random", "type": "RankedTensorOf" }
    ],
    "outputs": [
      { "name": "result", "type": "MHLO_Tensor" }
    ]
  },
  {
    "name": "mhlo.subtract",
    "summary": "Subtraction operator",
    "description": "Returns `lhs - rhs` element-wise.\n\n    See\n    https://www.tensorflow.org/xla/operation_semantics#element-wise_binary_arithmetic_operations.",
    "inputs": [
      { "name": "lhs", "type": "HLO_Tensor" },
      { "name": "rhs", "type": "HLO_Tensor" }
    ]
  },
  {
    "name": "mhlo.tan",
    "summary": "Tan operation",
    "description": "This operation is a work in progress, so it is not yet included in\n    the specification: https://github.com/openxla/stablehlo/issues/954.\n\n    Informally, this operation returns `Tan(operand)` element-wise.\n\n    Example:\n    ```mlir\n    %0 = mhlo.tan %arg0 : tensor<2xf32>\n    ```",
    "inputs": [
      { "name": "operand", "type": "MHLO_FpComplexOrQuantizedIntTensor" }
    ],
    "outputs": [
      { "name": "result", "type": "MHLO_FpComplexOrQuantizedIntTensor" }
    ],
    "attributes": [
      { "name": "result_accuracy", "type": "DefaultValuedOptionalAttr" }
    ],
    "assemblyFormat": "$operand attr-dict\n      `:` custom<SameOperandsAndResultType>(type($operand), type($result))"
  },
  {
    "name": "mhlo.tanh",
    "summary": "Tanh operator",
    "description": "Returns `tanh(operand)` element-wise.\n\n    See\n    https://www.tensorflow.org/xla/operation_semantics#element-wise_unary_functions.",
    "inputs": [
      { "name": "operand", "type": "TensorType" }
    ]
  },
  {
    "name": "mhlo.topk",
    "summary": "TopK operation",
    "description": "Returns top `k` values and their indices, along the last\n    dimension of the operand if `largest=true` or the bottom `k` values if\n    `largest=false`.\n\n    See:\n    https://www.tensorflow.org/xla/operation_semantics#top-k\n\n    Example:\n    ```mlir\n    %values, %indices = mhlo.topk(%operand, k=5, largest=true)\n      : tensor<100xf32> -> (tensor<5xf32>, tensor<5xi32>)\n    ```",
    "inputs": [
      { "name": "operand", "type": "MHLO_Tensor" }
    ],
    "outputs": [
      { "name": "values", "type": "MHLO_Tensor" },
      { "name": "indices", "type": "MHLO_Tensor" }
    ],
    "attributes": [
      { "name": "k", "type": "I64Attr" },
      { "name": "largest", "type": "DefaultValuedOptionalAttr" }
    ],
    "assemblyFormat": "`(`$operand `,` `k` `=` $k (`,` `largest` `=` $largest^)? `)` attr-dict `:`\n    type($operand) `->` `(`type($values)`,` type($indices)`)`"
  },
  {
    "name": "mhlo.torch_index_select",
    "inputs": [
      { "name": "input", "type": "HLO_Tensor" },
      { "name": "index", "type": "HLO_Tensor" }
    ],
    "attributes": [
      { "name": "dim", "type": "I64Attr" },
      { "name": "batch_dims", "type": "I64Attr" }
    ]
  },
  {
    "name": "mhlo.trace",
    "summary": "Trace operator",
    "description": "Emits a logging message `tag` with the `operand`.",
    "inputs": [
      { "name": "operand", "type": "HLO_Tensor" }
    ],
    "attributes": [
      { "name": "tag", "type": "StrAttr" }
    ]
  },
  {
    "name": "mhlo.transpose",
    "summary": "Transpose operator",
    "description": "Permutes the dimensions of `operand` according to the given `permutation`.\n\n    `res_dimensions[i] = operand_dimensions[permutation[i]]`\n\n    See https://www.tensorflow.org/xla/operation_semantics#transpose.",
    "inputs": [
      { "name": "operand", "type": "HLO_Tensor" }
    ],
    "attributes": [
      { "name": "permutation", "type": "I64ElementsAttr" }
    ]
  },
  {
    "name": "mhlo.triangular_solve",
    "summary": "TriangularSolve operator",
    "description": "Solves systems of linear equations with lower or upper triangular\n    coefficient matrices by forward- or back-substitution. Broadcasting along\n    leading dimensions, this routine solves one of the matrix systems\n    op(a) * x = b, or x * op(a) = b, for the variable x, given a and b, where\n    op(a) is either op(a) = a, or op(a) = Transpose(a), or\n    op(a) = Conj(Transpose(a)).\n\n    Input data is read only from the lower/upper triangle of a, depending on the\n    value of lower. Values from the other triangle are ignored. Output data is\n    returned in the same triangle; the values in the other triangle are\n    implementation-defined and may be anything.\n\n    If the rank of a and b are greater than 2, they are treated as batches of\n    matrices, where all except the minor 2 dimensions are batch dimensions. a\n    and b must have equal batch dimensions.\n\n    See https://www.tensorflow.org/xla/operation_semantics#triangularsolve.",
    "inputs": [
      { "name": "a", "type": "HLO_FpOrComplexTensor" },
      { "name": "b", "type": "HLO_FpOrComplexTensor" }
    ],
    "attributes": [
      { "name": "left_side", "type": "BoolAttr" },
      { "name": "lower", "type": "BoolAttr" },
      { "name": "unit_diagonal", "type": "BoolAttr" },
      { "name": "transpose_a", "type": "HLO_TransposeAttr" }
    ]
  },
  {
    "name": "mhlo.tuple",
    "summary": "XLA's tuple op",
    "description": "Groups a set of tensor inputs into a single tuple object.\n\n     See https://www.tensorflow.org/xla/operation_semantics#tuple.",
    "inputs": [
      { "name": "val", "type": "Variadic" }
    ]
  },
  {
    "name": "mhlo.unary_einsum",
    "inputs": [
      { "name": "operand", "type": "HLO_Tensor" }
    ],
    "attributes": [
      { "name": "einsum_config", "type": "StrAttr" }
    ]
  },
  {
    "name": "mhlo.uniform_dequantize",
    "summary": "UniformDequantize operation",
    "description": "Performs element-wise conversion of quantized tensor `operand` to a\n    floating-point tensor `result` according to the quantization parameters\n    defined by the `operand` type.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#uniform_dequantize\n\n    Example:\n    ```mlir\n    %result = mhlo.uniform_dequantize %operand : (tensor<16x16x!quant.uniform<i8:f32, 34.0:16>>) -> tensor<16x16xf32>\n    ```",
    "inputs": [
      { "name": "operand", "type": "OperandType" }
    ],
    "outputs": [
      { "name": "result", "type": "ResultType" }
    ],
    "assemblyFormat": "$operand attr-dict\n      `:` custom<SameOperandsAndResultType>(type($operand), type($result))"
  },
  {
    "name": "mhlo.uniform_quantize",
    "summary": "UniformQuantize operation",
    "description": "Performs element-wise conversion of floating-point tensor or quantized\n    tensor `operand` to a quantized tensor `result` according to the\n    quantization parameters defined by the `result` type.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#uniform_quantize\n\n    Example:\n    ```mlir\n    %result = mhlo.uniform_quantize %operand : (tensor<16x16xf32>) -> tensor<16x16x!quant.uniform<ui8:f32, 34.0:16>>\n    ```",
    "inputs": [
      { "name": "operand", "type": "OperandType" }
    ],
    "outputs": [
      { "name": "result", "type": "ResultType" }
    ],
    "assemblyFormat": "$operand attr-dict\n      `:` custom<SameOperandsAndResultType>(type($operand), type($result))"
  },
  {
    "name": "mhlo.while",
    "summary": "While operator",
    "description": "Returns the result of executing a body function until the cond body returns\n    true.\n\n    See https://www.tensorflow.org/xla/operation_semantics#while.",
    "inputs": [
      { "name": "arg", "type": "Variadic" }
    ]
  },
  {
    "name": "mhlo.xla.rng_get_and_update_state",
    "summary": "XlaRngGetAndUpdateState operation",
    "description": "This operation is private to the XLA compiler, so it is does not yet have\n    a specification.\n\n    Informally, this operation represents the change of the global random number\n    generator state for rng instructions. The global state is incremented by\n    delta and the old state is returned.\n\n    The output is currently defined for a single output type. If this changes in\n    the future to support multiple types, lowering to use of a global memref\n    must ensure that a single memref is still used and updated appropriately.",
    "attributes": [
      { "name": "delta", "type": "I64Attr" }
    ],
    "assemblyFormat": "attr-dict"
  },
  {
    "name": "mhlo.xor",
    "summary": "Logical xor",
    "description": "Returns `logical_xor(lhs, rhs)` element-wise.\n\n    See\n    https://www.tensorflow.org/xla/operation_semantics#element-wise_binary_arithmetic_operations.",
    "inputs": [
      { "name": "lhs", "type": "HLO_PredOrIntTensor" },
      { "name": "rhs", "type": "HLO_PredOrIntTensor" }
    ]
  },
  {
    "name": "onnx.Abs",
    "summary": "ONNX Abs operation",
    "description": "Absolute takes one input data (Tensor<T>) and produces one output data\n  (Tensor<T>) where absolute value, y = abs(x), is applied to\n  the tensor elementwise.",
    "inputs": [
      { "name": "X", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "Y", "type": "AnyTypeOf" }
    ]
  },
  {
    "name": "onnx.Acos",
    "summary": "ONNX Acos operation",
    "description": "Calculates the arccosine (inverse of cosine) of the given input tensor, element-wise.",
    "inputs": [
      { "name": "input", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTypeOf" }
    ]
  },
  {
    "name": "onnx.Acosh",
    "summary": "ONNX Acosh operation",
    "description": "Calculates the hyperbolic arccosine of the given input tensor element-wise.",
    "inputs": [
      { "name": "input", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTypeOf" }
    ]
  },
  {
    "name": "onnx.Adagrad",
    "summary": "ONNX Adagrad operation",
    "description": "Compute one iteration of ADAGRAD, a stochastic gradient based optimization\n      algorithm. This operator can conduct the optimization of multiple tensor variables.\n  \n      Let's define the behavior of this operator. As you can imagine, ADAGRAD requires\n      some parameters:\n  \n       - The initial learning-rate \\\"R\\\".\n       - The update count \\\"T\\\". That is, the number of training iterations conducted.\n       - A L2-norm regularization coefficient \\\"norm_coefficient\\\".\n       - A learning-rate decay factor \\\"decay_factor\\\".\n       - A small constant \\\"epsilon\\\" to avoid dividing-by-zero.\n  \n      At each ADAGRAD iteration, the optimized tensors are moved along a direction\n      computed based on their estimated gradient and accumulated squared gradient. Assume\n      that only a single tensor \\\"X\\\" is updated by this operator. We need the value of \\\"X\\\",\n      its gradient \\\"G\\\", and its accumulated squared gradient \\\"H\\\". Therefore, variables in\n      this operator's input list are sequentially \\\"R\\\", \\\"T\\\", \\\"X\\\", \\\"G\\\", and \\\"H\\\". Other\n      parameters are given as attributes because they are usually constants. Also, the\n      corresponding output tensors are the new value of \\\"X\\\" (called \\\"X_new\\\"), and then\n      the new accumulated squared gradient (called \\\"H_new\\\"). Those outputs are computed\n      from the given inputs following the pseudo code below.\n  \n      Let \\\"+\\\", \\\"-\\\", \\\"*\\\", and \\\"/\\\" are all element-wise arithmetic operations with\n      numpy-style broadcasting support. The pseudo code to compute those outputs is:\n  \n        // Compute a scalar learning-rate factor. At the first update of X, T is generally\n        // 0 (0-based update index) or 1 (1-based update index).\n        r = R / (1 + T * decay_factor);\n  \n        // Add gradient of 0.5 * norm_coefficient * ||X||_2^2, where ||X||_2 is the 2-norm.\n        G_regularized = norm_coefficient * X + G;\n  \n        // Compute new accumulated squared gradient.\n        H_new = H + G_regularized * G_regularized;\n  \n        // Compute the adaptive part of per-coordinate learning rate. Note that Sqrt(...)\n        // computes element-wise square-root.\n        H_adaptive = Sqrt(H_new) + epsilon\n  \n        // Compute the new value of \\\"X\\\".\n        X_new = X - r * G_regularized / H_adaptive;\n  \n      If one assign this operators to optimize multiple inputs, for example, \\\"X_1\\\" and \\\"X_2\\\", the same\n      pseudo code may be extended to handle all tensors jointly. More specifically, we can view \\\"X\\\" as a\n      concatenation of \\\"X_1\\\" and \\\"X_2\\\" (of course, their gradient and accumulate gradient should\n      be concatenated too) and then just reuse the entire pseudo code.\n  \n      Note that ADAGRAD was first proposed in http://jmlr.org/papers/volume12/duchi11a/duchi11a.pdf.\n      In that reference paper, this operator is a special case of the Figure 1's composite mirror\n      descent update.",
    "inputs": [
      { "name": "R", "type": "AnyTypeOf" },
      { "name": "T", "type": "TensorOf" },
      { "name": "inputs", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "outputs", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "decay_factor", "type": "DefaultValuedAttr" },
      { "name": "epsilon", "type": "DefaultValuedAttr" },
      { "name": "norm_coefficient", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "onnx.Adam",
    "summary": "ONNX Adam operation",
    "description": "Compute one iteration of Adam, a stochastic gradient based optimization\n      algorithm. This operator can conduct the optimization of multiple tensor variables.\n  \n      Let's define the behavior of this operator. First of all, Adam requires\n      some parameters:\n  \n       - The learning-rate \\\"R\\\".\n       - The update count \\\"T\\\". That is, the number of training iterations conducted.\n       - A L2-norm regularization coefficient \\\"norm_coefficient\\\".\n       - A small constant \\\"epsilon\\\" to avoid dividing-by-zero.\n       - Two coefficients, \\\"alpha\\\" and \\\"beta\\\".\n  \n      At each Adam iteration, the optimized tensors are moved along a direction\n      computed based on their exponentially-averaged historical gradient and\n      exponentially-averaged historical squared gradient. Assume that only a tensor\n      \\\"X\\\" is being optimized. The rest of required information is\n  \n       - the value of \\\"X\\\",\n       - \\\"X\\\"'s gradient (denoted by \\\"G\\\"),\n       - \\\"X\\\"'s exponentially-averaged historical gradient (denoted by \\\"V\\\"), and\n       - \\\"X\\\"'s exponentially-averaged historical squared gradient (denoted by \\\"H\\\").\n  \n      Some of those parameters are passed into this operator as input tensors and others\n      are stored as this operator's attributes. Specifically, this operator's input tensor\n      list is [\\\"R\\\", \\\"T\\\", \\\"X\\\", \\\"G\\\", \\\"V\\\", \\\"H\\\"]. That is, \\\"R\\\" is the first input, \\\"T\\\" is\n      the second input, and so on. Other parameters are given as attributes because they\n      are constants. Moreover, the corresponding output tensors are\n  \n       - the new value of \\\"X\\\" (called \\\"X_new\\\"),\n       - the new exponentially-averaged historical gradient (denoted by \\\"V_new\\\"), and\n       - the new exponentially-averaged historical squared gradient (denoted by \\\"H_new\\\").\n  \n      Those outputs are computed following the pseudo code below.\n  \n      Let \\\"+\\\", \\\"-\\\", \\\"*\\\", and \\\"/\\\" are all element-wise arithmetic operations with\n      numpy-style broadcasting support. The pseudo code to compute those outputs is:\n  \n        // Add gradient of 0.5 * norm_coefficient * ||X||_2^2, where ||X||_2 is the 2-norm.\n        G_regularized = norm_coefficient * X + G\n  \n        // Update exponentially-averaged historical gradient.\n        V_new = alpha * V + (1 - alpha) * G_regularized\n  \n        // Update exponentially-averaged historical squared gradient.\n        H_new = beta * H + (1 - beta) * G_regularized * G_regularized\n  \n        // Compute the element-wise square-root of H_new. V_new will be element-wisely\n        // divided by H_sqrt for a better update direction.\n        H_sqrt = Sqrt(H_new) + epsilon\n  \n        // Compute learning-rate. Note that \\\"alpha**T\\\"/\\\"beta**T\\\" is alpha's/beta's T-th power.\n        R_adjusted = T > 0 ? R * Sqrt(1 - beta**T) / (1 - alpha**T) : R\n  \n        // Compute new value of \\\"X\\\".\n        X_new = X - R_adjusted * V_new / H_sqrt\n  \n        // Post-update regularization.\n        X_final = (1 - norm_coefficient_post) * X_new\n  \n      If there are multiple inputs to be optimized, the pseudo code will be applied\n      independently to each of them.",
    "inputs": [
      { "name": "R", "type": "AnyTypeOf" },
      { "name": "T", "type": "TensorOf" },
      { "name": "inputs", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "outputs", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "alpha", "type": "DefaultValuedAttr" },
      { "name": "beta", "type": "DefaultValuedAttr" },
      { "name": "epsilon", "type": "DefaultValuedAttr" },
      { "name": "norm_coefficient", "type": "DefaultValuedAttr" },
      { "name": "norm_coefficient_post", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "onnx.Add",
    "summary": "ONNX Add operation",
    "description": "Performs element-wise binary addition (with Numpy-style broadcasting support).\n  \n  This operator supports **multidirectional (i.e., Numpy-style) broadcasting**; for more details please check [the doc](Broadcasting.md).\n  \n  (Opset 14 change): Extend supported types to include uint8, int8, uint16, and int16.",
    "inputs": [
      { "name": "A", "type": "AnyTypeOf" },
      { "name": "B", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "C", "type": "AnyTypeOf" }
    ]
  },
  {
    "name": "onnx.And",
    "summary": "ONNX And operation",
    "description": "Returns the tensor resulted from performing the `and` logical operation\n  elementwise on the input tensors `A` and `B` (with Numpy-style broadcasting support).\n  \n  This operator supports **multidirectional (i.e., Numpy-style) broadcasting**; for more details please check [the doc](Broadcasting.md).",
    "inputs": [
      { "name": "A", "type": "TensorOf" },
      { "name": "B", "type": "TensorOf" }
    ],
    "outputs": [
      { "name": "C", "type": "TensorOf" }
    ]
  },
  {
    "name": "onnx.ArgMax",
    "summary": "ONNX ArgMax operation",
    "description": "Computes the indices of the max elements of the input tensor's element along the\n  provided axis. The resulting tensor has the same rank as the input if keepdims equals 1.\n  If keepdims equals 0, then the resulting tensor has the reduced dimension pruned.\n  If select_last_index is True (default False), the index of the last occurrence of the max\n  is selected if the max appears more than once in the input. Otherwise the index of the\n  first occurrence is selected.\n  The type of the output tensor is integer.",
    "inputs": [
      { "name": "data", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "reduced", "type": "TensorOf" }
    ],
    "attributes": [
      { "name": "axis", "type": "DefaultValuedAttr" },
      { "name": "keepdims", "type": "DefaultValuedAttr" },
      { "name": "select_last_index", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "onnx.ArgMin",
    "summary": "ONNX ArgMin operation",
    "description": "Computes the indices of the min elements of the input tensor's element along the\n  provided axis. The resulting tensor has the same rank as the input if keepdims equals 1.\n  If keepdims equals 0, then the resulting tensor has the reduced dimension pruned.\n  If select_last_index is True (default False), the index of the last occurrence of the min\n  is selected if the min appears more than once in the input. Otherwise the index of the\n  first occurrence is selected.\n  The type of the output tensor is integer.",
    "inputs": [
      { "name": "data", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "reduced", "type": "TensorOf" }
    ],
    "attributes": [
      { "name": "axis", "type": "DefaultValuedAttr" },
      { "name": "keepdims", "type": "DefaultValuedAttr" },
      { "name": "select_last_index", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "onnx.ArrayFeatureExtractor",
    "summary": "ONNX ArrayFeatureExtractor operation",
    "description": "Select elements of the input tensor based on the indices passed.<br>\n      The indices are applied to the last axes of the tensor.",
    "inputs": [
      { "name": "X", "type": "AnyTypeOf" },
      { "name": "Y", "type": "TensorOf" }
    ],
    "outputs": [
      { "name": "Z", "type": "AnyTypeOf" }
    ]
  },
  {
    "name": "onnx.Asin",
    "summary": "ONNX Asin operation",
    "description": "Calculates the arcsine (inverse of sine) of the given input tensor, element-wise.",
    "inputs": [
      { "name": "input", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTypeOf" }
    ]
  },
  {
    "name": "onnx.Asinh",
    "summary": "ONNX Asinh operation",
    "description": "Calculates the hyperbolic arcsine of the given input tensor element-wise.",
    "inputs": [
      { "name": "input", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTypeOf" }
    ]
  },
  {
    "name": "onnx.Atan",
    "summary": "ONNX Atan operation",
    "description": "Calculates the arctangent (inverse of tangent) of the given input tensor, element-wise.",
    "inputs": [
      { "name": "input", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTypeOf" }
    ]
  },
  {
    "name": "onnx.Atanh",
    "summary": "ONNX Atanh operation",
    "description": "Calculates the hyperbolic arctangent of the given input tensor element-wise.",
    "inputs": [
      { "name": "input", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTypeOf" }
    ]
  },
  {
    "name": "onnx.AveragePool",
    "summary": "ONNX AveragePool operation",
    "description": "AveragePool consumes an input tensor X and applies average pooling across\n   the tensor according to kernel sizes, stride sizes, and pad lengths.\n   average pooling consisting of computing the average on all values of a\n   subset of the input tensor according to the kernel size and downsampling the\n   data into the output tensor Y for further processing. The output spatial shape is calculated differently\n   depending on whether explicit padding is used, where pads is employed, or auto padding is used, where auto_pad is utilized.\n   With explicit padding (https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html?highlight=maxpool#torch.nn.MaxPool2d):\n   ```\n   output_spatial_shape[i] = floor((input_spatial_shape[i] + pad_shape[i] - dilation[i] * (kernel_shape[i] - 1) - 1) / strides_spatial_shape[i] + 1)\n   ```\n   or\n   ```\n   output_spatial_shape[i] = ceil((input_spatial_shape[i] + pad_shape[i] - dilation[i] * (kernel_shape[i] - 1) - 1) / strides_spatial_shape[i] + 1)\n   ```\n   if ceil_mode is enabled. `pad_shape[i]` is the sum of pads along axis `i`. Sliding windows that would start in the right padded region are ignored.\n  \n   `auto_pad` is a DEPRECATED attribute. If you are using them currently, the output spatial shape will be following when ceil_mode is enabled:\n   ```\n   VALID: output_spatial_shape[i] = ceil((input_spatial_shape[i] - ((kernel_spatial_shape[i] - 1) * dilations[i] + 1) + 1) / strides_spatial_shape[i])\n   SAME_UPPER or SAME_LOWER: output_spatial_shape[i] = ceil(input_spatial_shape[i] / strides_spatial_shape[i])\n   ```\n   or when ceil_mode is disabled (https://www.tensorflow.org/api_docs/python/tf/keras/layers/AveragePooling2D):\n   ```\n   VALID: output_spatial_shape[i] = floor((input_spatial_shape[i] - ((kernel_spatial_shape[i] - 1) * dilations[i] + 1)) / strides_spatial_shape[i]) + 1\n   SAME_UPPER or SAME_LOWER: output_spatial_shape[i] = floor((input_spatial_shape[i] - 1) / strides_spatial_shape[i]) + 1\n   ```\n   And pad shape will be following if `SAME_UPPER` or `SAME_LOWER`:\n   ```\n   pad_shape[i] = (output_spatial_shape[i] - 1) * strides_spatial_shape[i] + ((kernel_spatial_shape[i] - 1) * dilations[i] + 1) - input_spatial_shape[i]\n   ```\n   The output of each pooling window is divided by the number of elements (exclude pad when attribute count_include_pad is zero).",
    "inputs": [
      { "name": "X", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "Y", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "auto_pad", "type": "DefaultValuedStrAttr" },
      { "name": "ceil_mode", "type": "DefaultValuedAttr" },
      { "name": "count_include_pad", "type": "DefaultValuedAttr" },
      { "name": "dilations", "type": "OptionalAttr" },
      { "name": "kernel_shape", "type": "I64ArrayAttr" },
      { "name": "pads", "type": "OptionalAttr" },
      { "name": "strides", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "onnx.BatchNormalization",
    "summary": "ONNX BatchNormalization operation",
    "description": "Carries out batch normalization as described in the paper\n  https://arxiv.org/abs/1502.03167. Depending on the mode it is being run,\n  There are five required inputs 'X', 'scale', 'B', 'input_mean' and\n  'input_var'.\n  Note that 'input_mean' and 'input_var' are expected to be the estimated\n  statistics in inference mode (training_mode=False, default),\n  and the running statistics in training mode (training_mode=True).\n  There are multiple cases for the number of outputs, which we list below:\n  \n  * Output case #1: Y, running_mean, running_var (training_mode=True)\n  * Output case #2: Y (training_mode=False)\n  \n  When training_mode=False, extra outputs are invalid.\n  The outputs are updated as follows when training_mode=True:\n  ```\n  running_mean = input_mean * momentum + current_mean * (1 - momentum)\n  running_var = input_var * momentum + current_var * (1 - momentum)\n  \n  Y = (X - current_mean) / sqrt(current_var + epsilon) * scale + B\n  ```\n  where:\n  ```\n  current_mean = ReduceMean(X, axis=all_except_channel_index)\n  current_var =  ReduceVar(X, axis=all_except_channel_index)\n  ```\n  Notice that `ReduceVar` refers to the population variance, and it equals to\n  `sum(sqrd(x_i - x_avg)) / N`\n  where `N` is the population size (this formula does not use sample size `N - 1`).\n  \n  The computation of ReduceMean and ReduceVar uses float to avoid overflow for float16 inputs.\n  \n  When training_mode=False:\n  ```\n  Y = (X - input_mean) / sqrt(input_var + epsilon) * scale + B\n  ```\n  \n  For previous (depreciated) non-spatial cases, implementors are suggested\n  to flatten the input shape to (N x C * D1 * D2 * ... * Dn) before a BatchNormalization Op.\n  This operator has **optional** inputs/outputs. See [the doc](IR.md) for more details about the representation of optional arguments. An empty string may be used in the place of an actual argument's name to indicate a missing argument. Trailing optional arguments (those not followed by an argument that is present) may also be simply omitted.",
    "inputs": [
      { "name": "X", "type": "AnyTypeOf" },
      { "name": "scale", "type": "AnyTypeOf" },
      { "name": "B", "type": "AnyTypeOf" },
      { "name": "input_mean", "type": "AnyTypeOf" },
      { "name": "input_var", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "Y", "type": "AnyTypeOf" },
      { "name": "running_mean", "type": "AnyTypeOf" },
      { "name": "running_var", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "epsilon", "type": "DefaultValuedAttr" },
      { "name": "momentum", "type": "DefaultValuedAttr" },
      { "name": "training_mode", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "onnx.BatchNormalizationInferenceMode",
    "summary": "ONNX BatchNormalization operation in test mode",
    "description": "Carries out batch normalization as described in the paper\n    https://arxiv.org/abs/1502.03167. Depending on the mode it is being run,\n    there are multiple cases for the number of outputs, which we list below:\n\n    Output case #1: Y, mean, var, saved_mean, saved_var (training mode)\n    Output case #2: Y (test mode)\"\n\n    For previous (depreciated) non-spatial cases, implementors are suggested\n    to flatten the input shape to (N x C*D1*D2 ..*Dn) before a BatchNormalization Op.\n    This operator has **optional** inputs/outputs. See [the doc](IR.md)\n    for more details about the representation of optional arguments.\n    An empty string may be used in the place of an actual argument's name to\n    indicate a missing argument. Trailing optional arguments (those not followed\n    by an argument that is present) may also be simply omitted.\n\n    This operation is not part of the standard and was added to assist onnx-mlir.",
    "inputs": [
      { "name": "X", "type": "AnyTypeOf" },
      { "name": "scale", "type": "AnyTypeOf" },
      { "name": "B", "type": "AnyTypeOf" },
      { "name": "mean", "type": "AnyTypeOf" },
      { "name": "var", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "o_Y", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "epsilon", "type": "DefaultValuedAttr" },
      { "name": "momentum", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "onnx.Bernoulli",
    "summary": "ONNX Bernoulli operation",
    "description": "Draws binary random numbers (0 or 1) from a Bernoulli distribution. The input tensor should be a tensor\n  containing probabilities p (a value in the range [0,1]) to be used for drawing the binary random number,\n  where an output of 1 is produced with probability p and an output of 0 is produced with probability (1-p).\n  \n  This operator is non-deterministic and may not produce the same values in different\n  implementations (even if a seed is specified).",
    "inputs": [
      { "name": "input", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "dtype", "type": "OptionalAttr" },
      { "name": "seed", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "onnx.Binarizer",
    "summary": "ONNX Binarizer operation",
    "description": "Maps the values of the input tensor to either 0 or 1, element-wise, based on the outcome of a comparison against a threshold value.",
    "inputs": [
      { "name": "X", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "Y", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "threshold", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "onnx.BitShift",
    "summary": "ONNX BitShift operation",
    "description": "Bitwise shift operator performs element-wise operation. For each input element, if the\n  attribute \\\"direction\\\" is \\\"RIGHT\\\", this operator moves its binary representation toward\n  the right side so that the input value is effectively decreased. If the attribute \\\"direction\\\"\n  is \\\"LEFT\\\", bits of binary representation moves toward the left side, which results the\n  increase of its actual value. The input X is the tensor to be shifted and another input\n  Y specifies the amounts of shifting. For example, if \\\"direction\\\" is \\\"Right\\\", X is [1, 4],\n  and S is [1, 1], the corresponding output Z would be [0, 2]. If \\\"direction\\\" is \\\"LEFT\\\" with\n  X=[1, 2] and S=[1, 2], the corresponding output Y would be [2, 8].\n  \n  Because this operator supports Numpy-style broadcasting, X's and Y's shapes are\n  not necessarily identical.\n  This operator supports **multidirectional (i.e., Numpy-style) broadcasting**; for more details please check [the doc](Broadcasting.md).",
    "inputs": [
      { "name": "X", "type": "AnyTypeOf" },
      { "name": "Y", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "Z", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "direction", "type": "StrAttr" }
    ]
  },
  {
    "name": "onnx.BitwiseAnd",
    "summary": "ONNX BitwiseAnd operation",
    "description": "Returns the tensor resulting from performing the bitwise `and` operation\n  elementwise on the input tensors `A` and `B` (with Numpy-style broadcasting support).\n  \n  This operator supports **multidirectional (i.e., Numpy-style) broadcasting**; for more details please check [the doc](Broadcasting.md).",
    "inputs": [
      { "name": "A", "type": "AnyTypeOf" },
      { "name": "B", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "C", "type": "AnyTypeOf" }
    ]
  },
  {
    "name": "onnx.BitwiseNot",
    "summary": "ONNX BitwiseNot operation",
    "description": "Returns the bitwise not of the input tensor element-wise.",
    "inputs": [
      { "name": "X", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "Y", "type": "AnyTypeOf" }
    ]
  },
  {
    "name": "onnx.BitwiseOr",
    "summary": "ONNX BitwiseOr operation",
    "description": "Returns the tensor resulting from performing the bitwise `or` operation\n  elementwise on the input tensors `A` and `B` (with Numpy-style broadcasting support).\n  \n  This operator supports **multidirectional (i.e., Numpy-style) broadcasting**; for more details please check [the doc](Broadcasting.md).",
    "inputs": [
      { "name": "A", "type": "AnyTypeOf" },
      { "name": "B", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "C", "type": "AnyTypeOf" }
    ]
  },
  {
    "name": "onnx.BitwiseXor",
    "summary": "ONNX BitwiseXor operation",
    "description": "Returns the tensor resulting from performing the bitwise `xor` operation\n  elementwise on the input tensors `A` and `B` (with Numpy-style broadcasting support).\n  \n  This operator supports **multidirectional (i.e., Numpy-style) broadcasting**; for more details please check [the doc](Broadcasting.md).",
    "inputs": [
      { "name": "A", "type": "AnyTypeOf" },
      { "name": "B", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "C", "type": "AnyTypeOf" }
    ]
  },
  {
    "name": "onnx.BlackmanWindow",
    "summary": "ONNX BlackmanWindow operation",
    "description": "Generates a Blackman window as described in the paper https://ieeexplore.ieee.org/document/1455106.",
    "inputs": [
      { "name": "size", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "output_datatype", "type": "DefaultValuedAttr" },
      { "name": "periodic", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "onnx.Cast",
    "summary": "ONNX Cast operation",
    "description": "The operator casts the elements of a given input tensor to a data type\n  specified by the 'to' argument and returns an output tensor of the same size in\n  the converted type. The 'to' argument must be one of the data types specified\n  in the 'DataType' enum field in the TensorProto message.\n  \n  Casting from string tensor in plain (e.g., \\\"3.14\\\" and \\\"1000\\\") and scientific numeric representations\n  (e.g., \\\"1e-5\\\" and \\\"1E8\\\") to float types is supported. For example, converting string \\\"100.5\\\" to an integer may\n  yield result 100. There are some string literals reserved for special floating-point values;\n  \\\"+INF\\\" (and \\\"INF\\\"), \\\"-INF\\\", and \\\"NaN\\\" are positive infinity, negative infinity, and not-a-number, respectively.\n  Any string which can exactly match \\\"+INF\\\" in a case-insensitive way would be mapped to positive infinite. Similarly,\n  this case-insensitive rule is applied to \\\"INF\\\" and \\\"NaN\\\". When casting from numeric tensors\n  to string tensors, plain floating-point representation (such as \\\"314.15926\\\") would be used.\n  Converting non-numerical-literal string such as \\\"Hello World!\\\" is an undefined behavior. Cases\n  of converting string representing floating-point arithmetic value, such as \\\"2.718\\\", to INT is an undefined behavior.\n  \n  Conversion from a numerical type to any numerical type is always allowed.\n  User must be aware of precision loss and value change caused by range difference between two types.\n  For example, a 64-bit float 3.1415926459 may be round to a 32-bit float 3.141592. Similarly, converting\n  an integer 36 to Boolean may produce 1 because we truncate bits which can't be stored in the targeted type.\n  \n  In more detail, the conversion among numerical types should follow these rules\n  if the destination type is not a float 8 type.\n  \n  * Casting from floating point to:\n    * floating point: +/- infinity if OOR (out of range).\n    * fixed point: undefined if OOR.\n    * bool: +/- 0.0 to False; all else to True.\n  * Casting from fixed point to:\n    * floating point: +/- infinity if OOR. (+ infinity in the case of uint)\n    * fixed point: when OOR, discard higher bits and reinterpret (with respect to two's complement representation for\n      signed types). For example, 200 (int16) -> -56 (int8).\n    * bool: zero to False; nonzero to True.\n  * Casting from bool to:\n    * floating point: `{1.0, 0.0}`.\n    * fixed point: `{1, 0}`.\n    * bool: no change.\n  \n  Float 8 type were introduced to speed up the training of\n  deep models. By default the conversion of a float *x* obeys\n  to the following rules. `[x]` means the value rounded to\n  the target mantissa width.\n  \n  | x | E4M3FN | E4M3FNUZ | E5M2 | E5M2FNUZ |\n  |------|----|----|----|----|\n  | 0 | 0 | 0 | 0 | 0 |\n  |-0 | -0 | 0 | -0 | 0 |\n  | NaN | NaN | NaN | NaN | NaN |\n  | +/- Inf | +/- FLT_MAX | NaN | FLT_MAX | NaN |\n  | [x] > FLT_MAX | FLT_MAX | FLT_MAX | FLT_MAX | FLT_MAX |\n  | [x] < -FLT_MAX | -FLT_MAX | -FLT_MAX | -FLT_MAX | -FLT_MAX |\n  | else | RNE | RNE | RNE | RNE |\n  \n  The behavior changes if the parameter 'saturate' is set to False.\n  The rules then become:\n  \n  | x | E4M3FN | E4M3FNUZ | E5M2 | E5M2FNUZ |\n  |------|----|----|----|----|\n  | 0 | 0 | 0 | 0 | 0 |\n  |-0 | -0 | 0 | -0 | 0 |\n  | NaN | NaN | NaN | NaN | NaN |\n  | +/- Inf | NaN | NaN | +/- Inf | NaN |\n  | [x] > FLT_MAX | NaN | NaN | Inf | NaN |\n  | [x] < -FLT_MAX | NaN | NaN | -Inf | NaN |\n  | else | RNE | RNE | RNE | RNE |",
    "inputs": [
      { "name": "input", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "saturate", "type": "DefaultValuedAttr" },
      { "name": "to", "type": "TypeAttr" }
    ]
  },
  {
    "name": "onnx.CastLike",
    "summary": "ONNX CastLike operation",
    "description": "The operator casts the elements of a given input tensor (the first input) to\n  the same data type as the elements of the second input tensor.\n  See documentation of the Cast operator for further details.",
    "inputs": [
      { "name": "input", "type": "AnyTypeOf" },
      { "name": "target_type", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "saturate", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "onnx.CastMap",
    "summary": "ONNX CastMap operation",
    "description": "Converts a map to a tensor.<br>The map key must be an int64 and the values will be ordered\n      in ascending order based on this key.<br>The operator supports dense packing or sparse packing.\n      If using sparse packing, the key cannot exceed the max_map-1 value.",
    "inputs": [
      { "name": "X", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "Y", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "cast_to", "type": "DefaultValuedStrAttr" },
      { "name": "map_form", "type": "DefaultValuedStrAttr" },
      { "name": "max_map", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "onnx.CategoryMapper",
    "summary": "ONNX CategoryMapper operation",
    "description": "Converts strings to integers and vice versa.<br>\n      Two sequences of equal length are used to map between integers and strings,\n      with strings and integers at the same index detailing the mapping.<br>\n      Each operator converts either integers to strings or strings to integers, depending\n      on which default value attribute is provided. Only one default value attribute\n      should be defined.<br>\n      If the string default value is set, it will convert integers to strings.\n      If the int default value is set, it will convert strings to integers.",
    "inputs": [
      { "name": "X", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "Y", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "cats_int64s", "type": "OptionalAttr" },
      { "name": "cats_strings", "type": "OptionalAttr" },
      { "name": "default_int64", "type": "DefaultValuedAttr" },
      { "name": "default_string", "type": "DefaultValuedStrAttr" }
    ]
  },
  {
    "name": "onnx.Ceil",
    "summary": "ONNX Ceil operation",
    "description": "Ceil takes one input data (Tensor<T>) and produces one output data\n  (Tensor<T>) where the ceil is, y = ceil(x), is applied to\n  the tensor elementwise. If x is integral, +0, -0, NaN,  or infinite, x itself is returned.",
    "inputs": [
      { "name": "X", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "Y", "type": "AnyTypeOf" }
    ]
  },
  {
    "name": "onnx.Celu",
    "summary": "ONNX Celu operation",
    "description": "Continuously Differentiable Exponential Linear Units:\n  Perform the linear unit element-wise on the input tensor X\n  using formula:\n  \n  ```\n  max(0,x) + min(0,alpha*(exp(x/alpha)-1))\n  ```",
    "inputs": [
      { "name": "X", "type": "TensorOf" }
    ],
    "outputs": [
      { "name": "Y", "type": "TensorOf" }
    ],
    "attributes": [
      { "name": "alpha", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "onnx.CenterCropPad",
    "summary": "ONNX CenterCropPad operation",
    "description": "Center crop or pad an input to given dimensions.\n  \n  The crop/pad dimensions can be specified for a subset of the `axes`. Non-specified dimensions will not be\n  cropped or padded.\n  \n  If the input dimensions are bigger than the crop shape, a centered cropping window is extracted from the input.\n  If the input dimensions are smaller than the crop shape, the input is padded on each side equally,\n  so that the input is centered in the output.",
    "inputs": [
      { "name": "input_data", "type": "AnyTypeOf" },
      { "name": "shape", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "output_data", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "axes", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "onnx.Clip",
    "summary": "ONNX Clip operation",
    "description": "Clip operator limits the given input within an interval. The interval is\n  specified by the inputs 'min' and 'max'. They default to\n  numeric_limits::lowest() and numeric_limits::max(), respectively.",
    "inputs": [
      { "name": "input", "type": "AnyTypeOf" },
      { "name": "min", "type": "AnyTypeOf" },
      { "name": "max", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTypeOf" }
    ]
  },
  {
    "name": "onnx.ClipV11",
    "summary": "ONNX Clip operation",
    "description": "Clip operator limits the given input within an interval. The interval is\n  specified by the inputs 'min' and 'max'. They default to\n  numeric_limits::lowest() and numeric_limits::max(), respectively.",
    "inputs": [
      { "name": "input", "type": "AnyTypeOf" },
      { "name": "min", "type": "AnyTypeOf" },
      { "name": "max", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTypeOf" }
    ]
  },
  {
    "name": "onnx.ClipV12",
    "summary": "ONNX Clip operation",
    "description": "Clip operator limits the given input within an interval. The interval is\n  specified by the inputs 'min' and 'max'. They default to\n  numeric_limits::lowest() and numeric_limits::max(), respectively.",
    "inputs": [
      { "name": "input", "type": "AnyTypeOf" },
      { "name": "min", "type": "AnyTypeOf" },
      { "name": "max", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTypeOf" }
    ]
  },
  {
    "name": "onnx.ClipV6",
    "summary": "ONNX Clip operation",
    "description": "Clip operator limits the given input within an interval. The interval is\n  specified with arguments 'min' and 'max'. They default to\n  numeric_limits::lowest() and numeric_limits::max() respectively.",
    "inputs": [
      { "name": "input", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "max", "type": "DefaultValuedAttr" },
      { "name": "min", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "onnx.Col2Im",
    "summary": "ONNX Col2Im operation",
    "description": "The operator rearranges column blocks back into a multidimensional image\n  \n  Col2Im behaves similarly to PyTorch's fold https://pytorch.org/docs/stable/generated/torch.nn.Fold.html,\n  but it only supports *batched* multi-dimensional image tensors.\n  Another implementation in Python with N-dimension support can be found at https://github.com/f-dangel/unfoldNd/.\n  \n  NOTE:\n    Although specifying image_shape looks redundant because it could be calculated from\n    convolution formulas, it is required as input for more advanced scenarios as explained\n    at PyTorch's implementation (https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/Col2Im.cpp#L10)",
    "inputs": [
      { "name": "input", "type": "AnyTypeOf" },
      { "name": "image_shape", "type": "TensorOf" },
      { "name": "block_shape", "type": "TensorOf" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "dilations", "type": "OptionalAttr" },
      { "name": "pads", "type": "OptionalAttr" },
      { "name": "strides", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "onnx.Compress",
    "summary": "ONNX Compress operation",
    "description": "Selects slices from an input tensor along a given axis where condition evaluates to True for each axis index.\n      In case axis is not provided, input is flattened before elements are selected.\n      Compress behaves like numpy.compress: https://docs.scipy.org/doc/numpy/reference/generated/numpy.compress.html",
    "inputs": [
      { "name": "input", "type": "AnyTypeOf" },
      { "name": "condition", "type": "TensorOf" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "axis", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "onnx.Concat",
    "summary": "ONNX Concat operation",
    "description": "Concatenate a list of tensors into a single tensor. All input tensors must have the same shape, except for the dimension size of the axis to concatenate on.",
    "inputs": [
      { "name": "inputs", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "concat_result", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "axis", "type": "SI64Attr" }
    ]
  },
  {
    "name": "onnx.ConcatFromSequence",
    "summary": "ONNX ConcatFromSequence operation",
    "description": "Concatenate a sequence of tensors into a single tensor.\n  All input tensors must have the same shape, except for the dimension size of the axis to concatenate on.\n  By default 'new_axis' is 0, the behavior is similar to numpy.concatenate.\n  When 'new_axis' is 1, the behavior is similar to numpy.stack.",
    "inputs": [
      { "name": "input_sequence", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "concat_result", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "axis", "type": "SI64Attr" },
      { "name": "new_axis", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "onnx.ConcatShapeTranspose",
    "summary": "ONNX merged operation",
    "description": "Merge the following sequence of ops into one op\n    v1 = onnx.concat\n    v2 = onnx.shape(v1)\n    v3 = onnx.transpose(v1)\n\n    This operation is not part of the standard and was added to assist onnx-mlir.",
    "inputs": [
      { "name": "inputs", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "shape", "type": "TensorOf" },
      { "name": "transposed", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "axis", "type": "SI64Attr" },
      { "name": "end", "type": "OptionalAttr" },
      { "name": "start", "type": "DefaultValuedAttr" },
      { "name": "perm", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "onnx.Constant",
    "summary": "ONNX Constant operation",
    "description": "This operator produces a constant tensor. Exactly one of the provided attributes, either value, sparse_value,\n  or value_* must be specified.",
    "outputs": [
      { "name": "output", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "sparse_value", "type": "OptionalAttr" },
      { "name": "value", "type": "OptionalAttr" },
      { "name": "value_float", "type": "OptionalAttr" },
      { "name": "value_floats", "type": "OptionalAttr" },
      { "name": "value_int", "type": "OptionalAttr" },
      { "name": "value_ints", "type": "OptionalAttr" },
      { "name": "value_string", "type": "OptionalAttr" },
      { "name": "value_strings", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "onnx.ConstantOfShape",
    "summary": "ONNX ConstantOfShape operation",
    "description": "Generate a tensor with given value and shape.",
    "inputs": [
      { "name": "input", "type": "TensorOf" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "value", "type": "OptionalAttr" }
    ],
    "category": "Shape"
  },
  {
    "name": "onnx.Conv",
    "summary": "ONNX Conv operation",
    "description": "The convolution operator consumes an input tensor and a filter, and\n  computes the output.",
    "inputs": [
      { "name": "X", "type": "AnyTypeOf" },
      { "name": "W", "type": "AnyTypeOf" },
      { "name": "B", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "Y", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "auto_pad", "type": "DefaultValuedStrAttr" },
      { "name": "dilations", "type": "OptionalAttr" },
      { "name": "group", "type": "DefaultValuedAttr" },
      { "name": "kernel_shape", "type": "OptionalAttr" },
      { "name": "pads", "type": "OptionalAttr" },
      { "name": "strides", "type": "OptionalAttr" }
    ],
    "category": "Layer"
  },
  {
    "name": "onnx.ConvInteger",
    "summary": "ONNX ConvInteger operation",
    "description": "The integer convolution operator consumes an input tensor, its zero-point, a filter, and its zero-point,\n  and computes the output. The production MUST never overflow. The accumulation may overflow if and only if in 32 bits.",
    "inputs": [
      { "name": "x", "type": "AnyTypeOf" },
      { "name": "w", "type": "AnyTypeOf" },
      { "name": "x_zero_point", "type": "AnyTypeOf" },
      { "name": "w_zero_point", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "y", "type": "TensorOf" }
    ],
    "attributes": [
      { "name": "auto_pad", "type": "DefaultValuedStrAttr" },
      { "name": "dilations", "type": "OptionalAttr" },
      { "name": "group", "type": "DefaultValuedAttr" },
      { "name": "kernel_shape", "type": "OptionalAttr" },
      { "name": "pads", "type": "OptionalAttr" },
      { "name": "strides", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "onnx.ConvTranspose",
    "summary": "ONNX ConvTranspose operation",
    "description": "The convolution transpose operator consumes an input tensor and a filter,\n  and computes the output.\n  \n  If the pads parameter is provided the shape of the output is calculated via the following equation:\n  \n    output_shape[i] = stride[i] * (input_size[i] - 1) + output_padding[i] + ((kernel_shape[i] - 1) * dilations[i] + 1) - pads[start_i] - pads[end_i]\n  \n  output_shape can also be explicitly specified in which case pads values are auto generated using these equations:\n  \n    total_padding[i] = stride[i] * (input_size[i] - 1) + output_padding[i] + ((kernel_shape[i] - 1) * dilations[i] + 1) - output_shape[i]\n    If (auto_pads == SAME_UPPER): pads[start_i] = total_padding[i]/2; pads[end_i] = total_padding[i] - (total_padding[i]/2)\n    Else: pads[start_i] = total_padding[i] - (total_padding[i]/2); pads[end_i] = (total_padding[i]/2).",
    "inputs": [
      { "name": "X", "type": "AnyTypeOf" },
      { "name": "W", "type": "AnyTypeOf" },
      { "name": "B", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "Y", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "auto_pad", "type": "DefaultValuedStrAttr" },
      { "name": "dilations", "type": "OptionalAttr" },
      { "name": "group", "type": "DefaultValuedAttr" },
      { "name": "kernel_shape", "type": "OptionalAttr" },
      { "name": "output_padding", "type": "OptionalAttr" },
      { "name": "output_shape", "type": "OptionalAttr" },
      { "name": "pads", "type": "OptionalAttr" },
      { "name": "strides", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "onnx.Cos",
    "summary": "ONNX Cos operation",
    "description": "Calculates the cosine of the given input tensor, element-wise.",
    "inputs": [
      { "name": "input", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTypeOf" }
    ]
  },
  {
    "name": "onnx.Cosh",
    "summary": "ONNX Cosh operation",
    "description": "Calculates the hyperbolic cosine of the given input tensor element-wise.",
    "inputs": [
      { "name": "input", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTypeOf" }
    ]
  },
  {
    "name": "onnx.CumSum",
    "summary": "ONNX CumSum operation",
    "description": "Performs cumulative sum of the input elements along the given axis.\n  By default, it will do the sum inclusively meaning the first element is copied as is.\n  Through an `exclusive` attribute, this behavior can change to exclude the first element.\n  It can also perform summation in the opposite direction of the axis. For that, set `reverse` attribute to 1.\n  \n  Example:\n  ```\n  input_x = [1, 2, 3]\n  axis=0\n  output = [1, 3, 6]\n  exclusive=1\n  output = [0, 1, 3]\n  exclusive=0\n  reverse=1\n  output = [6, 5, 3]\n  exclusive=1\n  reverse=1\n  output = [5, 3, 0]\n  ```",
    "inputs": [
      { "name": "x", "type": "AnyTypeOf" },
      { "name": "axis", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "y", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "exclusive", "type": "DefaultValuedAttr" },
      { "name": "reverse", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "onnx.Custom",
    "summary": "ONNX Custom operation",
    "description": "CustomOp is not an Op defined in onnx standard and was added to support\n    extention of Op that can be transformed or finally call a user-defined\n    external function.\"\n\n    It allows for calling a user-defined operation, with a single required\n    attribute being a string that names the operation. Other inputs are passed\n    to the user operation.\n\n    The number of inputs and outputs can vary.\n\n    NoneType is allowed for both input and output, as the CustomOp may require\n    a fixed number of inputs/outputs for the external function call.\n\n    In addition to the values passed to the user-defined operation, certain\n    attributes are introduced to facilitate the analysis and transformation of\n    CustomOp.\n\n    Since the compiler does not define the semantics of CustomOp, onnx-mlir\n    cannot infer the shape of its output. Consequently, specific attributes are\n    introduced to specify how shape inference should be performed on a CustomOp.\n    These attributes are:\n      'inputs_for_infer':\n           Optional. The index of inputs used for shape inference.\n           The value of index should be [0, the number of inputs).\n           If not specified, all the inputs of the CustomOp will be used for\n           shape inference.\n      'shape_infer_pattern':\n           Optional. Specify how to propagate the shape info from the inputs\n           (may be limited by inputs_for_infer) to output. Current supported\n           patterns are `SameAs`, `MDBroadcast`.\n      'output_element_type':\n           Optional. The element type for the output tensor. If not specified,\n           follow the shape infer pattern behavior. Usually the element type of\n           the first input is used.\n    Each instance of CustomOp can have its own attributes for shape inference,\n    allowing for customization. However, CustomOps with the same function_name\n    typically behave similarly in terms of shape inference, and therefore have\n    the same attributes.\n\n    The existing shape inference patterns for ONNX ops are reused for CustomOp,\n    with the polymorphism in shape inference based on its attribute values.\n    Due to the current implementation for ONNX Ops, a CustomOp with specified\n    shape inference attributes supports only a single output, rather than\n    variadic outputs.\n\n    When attributes for shape inference are not provided, the shape inference\n    for CustomOp will simply pass through.\n\n    All of these additional attributes are optional, designed to be less\n    intrusive. The .mlir file can remain the same when a new attribute is\n    added.",
    "inputs": [
      { "name": "inputs", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "outputs", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "function_name", "type": "StrAttr" },
      { "name": "output_element_type", "type": "OptionalAttr" },
      { "name": "shape_infer_pattern", "type": "OptionalAttr" },
      { "name": "inputs_for_infer", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "onnx.DeformConv",
    "summary": "ONNX DeformConv operation",
    "description": "Performs deformable convolution as described in https://arxiv.org/abs/1703.06211 and https://arxiv.org/abs/1811.11168.\n  This operator specification supports the general N-D case. Note that most common use cases have 2D or 3D data.",
    "inputs": [
      { "name": "X", "type": "AnyTypeOf" },
      { "name": "W", "type": "AnyTypeOf" },
      { "name": "offset", "type": "AnyTypeOf" },
      { "name": "B", "type": "AnyTypeOf" },
      { "name": "mask", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "Y", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "dilations", "type": "OptionalAttr" },
      { "name": "group", "type": "DefaultValuedAttr" },
      { "name": "kernel_shape", "type": "OptionalAttr" },
      { "name": "offset_group", "type": "DefaultValuedAttr" },
      { "name": "pads", "type": "OptionalAttr" },
      { "name": "strides", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "onnx.DepthToSpace",
    "summary": "ONNX DepthToSpace operation",
    "description": "DepthToSpace rearranges (permutes) data from depth into blocks of spatial data.\n  This is the reverse transformation of SpaceToDepth. More specifically, this op outputs a copy of\n  the input tensor where values from the depth dimension are moved in spatial blocks to the height\n  and width dimensions. By default, `mode` = `DCR`.\n  In the DCR mode, elements along the depth dimension from the input tensor are rearranged in the\n  following order: depth, column, and then row. The output y is computed from the input x as below:\n  \n  ```\n  b, c, h, w = x.shape\n  tmp = np.reshape(x, [b, blocksize, blocksize, c // (blocksize**2), h, w])\n  tmp = np.transpose(tmp, [0, 3, 4, 1, 5, 2])\n  y = np.reshape(tmp, [b, c // (blocksize**2), h * blocksize, w * blocksize])\n  ```\n  \n  In the CRD mode, elements along the depth dimension from the input tensor are rearranged in the\n  following order: column, row, and the depth. The output y is computed from the input x as below:\n  \n  ```\n  b, c, h, w = x.shape\n  tmp = np.reshape(x, [b, c // (blocksize ** 2), blocksize, blocksize, h, w])\n  tmp = np.transpose(tmp, [0, 1, 4, 2, 5, 3])\n  y = np.reshape(tmp, [b, c // (blocksize ** 2), h * blocksize, w * blocksize])\n  ```",
    "inputs": [
      { "name": "input", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "blocksize", "type": "SI64Attr" },
      { "name": "mode", "type": "DefaultValuedStrAttr" }
    ]
  },
  {
    "name": "onnx.DequantizeLinear",
    "summary": "ONNX DequantizeLinear operation",
    "description": "The linear dequantization operator. It consumes a quantized tensor, a scale, and a zero point to compute the full precision tensor.\n  The dequantization formula is `y = (x - x_zero_point) * x_scale`. `x_scale` and `x_zero_point` must have same shape, and can be either a scalar\n  for per-tensor / per layer quantization, or a 1-D tensor for per-axis quantization.\n  `x_zero_point` and `x` must have same type. `x` and `y` must have same shape. In the case of dequantizing int32,\n  there's no zero point (zero point is supposed to be 0).\n  `zero-point` is usually not used in the case of float8e4m3fn, float8e4m3fnuz, float8e5m2, float8e5m2fnuz quantization,\n  but the dequantization formula remains the same for consistency and 'x_scale' still determines the output type.",
    "inputs": [
      { "name": "x", "type": "AnyTypeOf" },
      { "name": "x_scale", "type": "AnyTypeOf" },
      { "name": "x_zero_point", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "y", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "axis", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "onnx.Det",
    "summary": "ONNX Det operation",
    "description": "Det calculates determinant of a square matrix or batches of square matrices.\n  Det takes one input tensor of shape `[*, M, M]`, where `*` is zero or more batch dimensions,\n  and the inner-most 2 dimensions form square matrices.\n  The output is a tensor of shape `[*]`, containing the determinants of all input submatrices.\n  e.g., When the input is 2-D, the output is a scalar(shape is empty: `[]`).",
    "inputs": [
      { "name": "X", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "Y", "type": "AnyTypeOf" }
    ]
  },
  {
    "name": "onnx.DFT",
    "summary": "ONNX DFT operation",
    "description": "Computes the discrete Fourier Transform (DFT) of the input.\n  \n  Assuming the input has shape `[M, N]`, where `N` is the dimension over which the\n  DFT is computed and `M` denotes the conceptual \\\"all other dimensions,\\\"\n  the DFT `y[m, k]` of shape `[M, N]` is defined as\n  \n  $$y[m, k] = \\sum_{n=0}^{N-1} e^{-2 \\pi j \\frac{k n}{N} } x[m, n] ,$$\n  \n  and the inverse transform is defined as\n  \n  $$x[m, n] = \\frac{1}{N} \\sum_{k=0}^{N-1} e^{2 \\pi j \\frac{k n}{N} } y[m, k] ,$$\n  \n  where $j$ is the imaginary unit.\n  \n  The actual shape of the output is specified in the \\\"output\\\" section.\n  \n  Reference: https://docs.scipy.org/doc/scipy/tutorial/fft.html",
    "inputs": [
      { "name": "input", "type": "AnyTypeOf" },
      { "name": "dft_length", "type": "AnyTypeOf" },
      { "name": "axis", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "inverse", "type": "DefaultValuedAttr" },
      { "name": "onesided", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "onnx.DFTV17",
    "summary": "ONNX DFT operation",
    "description": "Computes the discrete Fourier transform of input.",
    "inputs": [
      { "name": "input", "type": "AnyTypeOf" },
      { "name": "dft_length", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "axis", "type": "DefaultValuedAttr" },
      { "name": "inverse", "type": "DefaultValuedAttr" },
      { "name": "onesided", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "onnx.DictVectorizer",
    "summary": "ONNX DictVectorizer operation",
    "description": "Uses an index mapping to convert a dictionary to an array.<br>\n      Given a dictionary, each key is looked up in the vocabulary attribute corresponding to\n      the key type. The index into the vocabulary array at which the key is found is then\n      used to index the output 1-D tensor 'Y' and insert into it the value found in the dictionary 'X'.<br>\n      The key type of the input map must correspond to the element type of the defined vocabulary attribute.\n      Therefore, the output array will be equal in length to the index mapping vector parameter.\n      All keys in the input dictionary must be present in the index mapping vector.\n      For each item in the input dictionary, insert its value in the output array.\n      Any keys not present in the input dictionary, will be zero in the output array.<br>\n      For example: if the ``string_vocabulary`` parameter is set to ``[\\\"a\\\", \\\"c\\\", \\\"b\\\", \\\"z\\\"]``,\n      then an input of ``{\\\"a\\\": 4, \\\"c\\\": 8}`` will produce an output of ``[4, 8, 0, 0]``.",
    "inputs": [
      { "name": "X", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "Y", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "int64_vocabulary", "type": "OptionalAttr" },
      { "name": "string_vocabulary", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "onnx.Dim",
    "summary": "ONNX dimensions operation.",
    "description": "This operation is to obtain the dimension of a Tensor;\n\n    ```\n    \"onnx.Dim\"(%tensor) {axis = 0 : si64} : (tensor<?x3x5xf32>) -> tensor<1xi64>\n    ```\n\n    The axis identifies the dimension within the shape which is going to be obtained.\n\n    This operation is not part of the standard and was added to assist onnx-mlir.",
    "inputs": [
      { "name": "data", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "dim", "type": "TensorOf" }
    ],
    "attributes": [
      { "name": "axis", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "onnx.DimGroup",
    "summary": "ONNX dimension group operation.",
    "description": "This operation is to link a compile-time unknown dimension of a Tensor\n    to a group id. Two dimensions that have the same group id are expected\n    to be equal at runtime.\n\n    ```\n    \"onnx.DimGroup\"(%tensor) {axis = 0 : si64, group_id = 1: si64} : (tensor<?x3x5xf32>) -> ()\n    ```\n\n    `axis` identifies the dimension position in the tensor.\n\n    `group_id` identifies the group id of the dimension. It is non-negative.\n    Value -1 for `group_id` means the dimension does not belong to any group.\n\n    This operation is currently used in the pass `--onnx-dim-analysis`\n    for testing the unknown dimension analysis class.\n\n    This operation is not part of the standard and was added to assist onnx-mlir.",
    "inputs": [
      { "name": "data", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "axis", "type": "DefaultValuedAttr" },
      { "name": "group_id", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "onnx.Div",
    "summary": "ONNX Div operation",
    "description": "Performs element-wise binary division (with Numpy-style broadcasting support).\n  \n  This operator supports **multidirectional (i.e., Numpy-style) broadcasting**; for more details please check [the doc](Broadcasting.md).\n  \n  (Opset 14 change): Extend supported types to include uint8, int8, uint16, and int16.",
    "inputs": [
      { "name": "A", "type": "AnyTypeOf" },
      { "name": "B", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "C", "type": "AnyTypeOf" }
    ]
  },
  {
    "name": "onnx.Dropout",
    "summary": "ONNX Dropout operation",
    "description": "Dropout takes an input floating-point tensor, an optional input ratio (floating-point scalar) and an optional input training_mode (boolean scalar). It produces two tensor outputs,\n  output (floating-point tensor) and mask (optional `Tensor<bool>`). If `training_mode` is true then the output Y will be a random dropout;\n  Note that this Dropout scales the masked input data by the following equation, so to convert the trained model into inference mode,\n  the user can simply not pass `training_mode` input or set it to false.\n  ```\n  output = scale * data * mask,\n  ```\n  where\n  ```\n  scale = 1. / (1. - ratio).\n  ```\n  This operator has **optional** inputs/outputs. See [the doc](IR.md) for more details about the representation of optional arguments. An empty string may be used in the place of an actual argument's name to indicate a missing argument. Trailing optional arguments (those not followed by an argument that is present) may also be simply omitted.",
    "inputs": [
      { "name": "data", "type": "AnyTypeOf" },
      { "name": "ratio", "type": "AnyTypeOf" },
      { "name": "training_mode", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTypeOf" },
      { "name": "mask", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "seed", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "onnx.DynamicQuantizeLinear",
    "summary": "ONNX DynamicQuantizeLinear operation",
    "description": "A Function to fuse calculation for Scale, Zero Point and FP32->8Bit conversion of FP32 Input data.\n  Outputs Scale, ZeroPoint and Quantized Input for a given FP32 Input.\n  Scale is calculated as:\n  ```\n  y_scale = (maximum(0, max(x)) - minimum(0, min(x))) / (qmax - qmin)\n  ```\n  \n  * where qmax and qmin are max and min values for quantization range i.e. [0, 255] in case of uint8\n  * data range is adjusted to include 0.\n  \n  Zero point is calculated as:\n  ```\n  intermediate_zero_point = qmin - min(x)/y_scale\n  y_zero_point = cast(round(saturate(itermediate_zero_point)))\n  ```\n  \n  * where qmax and qmin are max and min values for quantization range .i.e [0, 255] in case of uint8\n  * for saturation, it saturates to [0, 255] if it's uint8, or [-127, 127] if it's int8. Right now only uint8 is supported.\n  * rounding to nearest ties to even.\n  \n  Data quantization formula is:\n  ```\n  y = saturate (round (x / y_scale) + y_zero_point)\n  ```\n  \n  * for saturation, it saturates to [0, 255] if it's uint8, or [-127, 127] if it's int8. Right now only uint8 is supported.\n  * rounding to nearest ties to even.",
    "inputs": [
      { "name": "x", "type": "TensorOf" }
    ],
    "outputs": [
      { "name": "y", "type": "TensorOf" },
      { "name": "y_scale", "type": "TensorOf" },
      { "name": "y_zero_point", "type": "TensorOf" }
    ]
  },
  {
    "name": "onnx.Einsum",
    "summary": "ONNX Einsum operation",
    "description": "An einsum of the form `term1, term2 -> output-term` produces an output tensor using the following equation\n  \n  ```\n  output[output-term] = reduce-sum( input1[term1] * input2[term2] )\n  ```\n  \n  where the reduce-sum performs a summation over all the indices occurring in the input terms (term1, term2)\n  that do not occur in the output-term.\n  \n  The Einsum operator evaluates algebraic tensor operations on a sequence of tensors, using the Einstein summation\n  convention. The equation string contains a comma-separated sequence of lower case letters. Each term corresponds to\n  an operand tensor, and the characters within the terms correspond to operands dimensions.\n  \n  This sequence may be followed by \\\"->\\\" to separate the left and right hand side of the equation.\n  If the equation contains \\\"->\\\" followed by the right-hand side, the explicit (not classical) form of the Einstein\n  summation is performed, and the right-hand side indices indicate output tensor dimensions. In other cases,\n  output indices are (implicitly) set to the alphabetically sorted sequence of indices appearing exactly once in the\n  equation.\n  \n  When a dimension character is repeated in the left-hand side, it represents summation along the dimension.\n  \n  The equation may contain ellipsis (\\\"...\\\") to enable broadcasting. Ellipsis must indicate a fixed number of dimensions.\n  Specifically, every occurrence of ellipsis in the equation must represent the same number of dimensions.\n  The right-hand side may contain exactly one ellipsis. In implicit mode, the ellipsis dimensions are set to the\n  beginning of the output. The equation string may contain space (U+0020) character.",
    "inputs": [
      { "name": "Inputs", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "Output", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "equation", "type": "StrAttr" }
    ]
  },
  {
    "name": "onnx.Elu",
    "summary": "ONNX Elu operation",
    "description": "Elu takes one input data (Tensor<T>) and produces one output data\n  (Tensor<T>) where the function `f(x) = alpha * (exp(x) - 1.) for x <\n  0`, `f(x) = x for x >= 0`., is applied to the tensor elementwise.",
    "inputs": [
      { "name": "X", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "Y", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "alpha", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "onnx.EntryPoint",
    "summary": "Indicate ONNX entry point",
    "description": "The \"onnx.EntryPoint\" function indicates the main entry point of ONNX model.\n\n    This operation is not part of the standard and was added to assist onnx-mlir.",
    "attributes": [
      { "name": "func", "type": "SymbolRefAttr" }
    ]
  },
  {
    "name": "onnx.Equal",
    "summary": "ONNX Equal operation",
    "description": "Returns the tensor resulted from performing the `equal` logical operation\n  elementwise on the input tensors `A` and `B` (with Numpy-style broadcasting support).\n  \n  This operator supports **multidirectional (i.e., Numpy-style) broadcasting**; for more details please check [the doc](Broadcasting.md).",
    "inputs": [
      { "name": "A", "type": "AnyTypeOf" },
      { "name": "B", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "C", "type": "TensorOf" }
    ]
  },
  {
    "name": "onnx.Erf",
    "summary": "ONNX Erf operation",
    "description": "Computes the error function of the given input tensor element-wise.",
    "inputs": [
      { "name": "input", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTypeOf" }
    ]
  },
  {
    "name": "onnx.Exp",
    "summary": "ONNX Exp operation",
    "description": "Calculates the exponential of the given input tensor, element-wise.",
    "inputs": [
      { "name": "input", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTypeOf" }
    ]
  },
  {
    "name": "onnx.Expand",
    "summary": "ONNX Expand operation",
    "description": "Broadcast the input tensor following the given shape and the broadcast rule.\n  The broadcast rule is similar to numpy.array(input) * numpy.ones(shape):\n  Dimensions are right alignment;\n  Two corresponding dimensions must have the same value, or one of them is equal to 1.\n  Also, this operator is similar to numpy.broadcast_to(input, shape),\n  but the major difference is numpy.broadcast_to() does not allow shape to be smaller than input.size().\n  It is possible that the output.shape is not equal to shape, when some dimensions in shape is equal to 1,\n  or the shape.ndim < input.shape.ndim.",
    "inputs": [
      { "name": "input", "type": "AnyTypeOf" },
      { "name": "shape", "type": "TensorOf" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTypeOf" }
    ]
  },
  {
    "name": "onnx.EyeLike",
    "summary": "ONNX EyeLike operation",
    "description": "Generate a 2D tensor (matrix) with ones on the diagonal and zeros everywhere else. Only 2D\n  tensors are supported, i.e. input T1 must be of rank 2. The shape of the output tensor is the\n  same as the input tensor. The data type can be specified by the 'dtype' argument. If\n  'dtype' is not specified, then the type of input tensor is used. By default, the main diagonal\n  is populated with ones, but attribute 'k' can be used to populate upper or lower diagonals.\n  The 'dtype' argument must be one of the data types specified in the 'DataType' enum field in the\n  TensorProto message and be valid as an output type.",
    "inputs": [
      { "name": "input", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "dtype", "type": "OptionalAttr" },
      { "name": "k", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "onnx.FeatureVectorizer",
    "summary": "ONNX FeatureVectorizer operation",
    "description": "Concatenates input tensors into one continuous output.<br>\n      All input shapes are 2-D and are concatenated along the second dimension. 1-D tensors are treated as [1,C].\n      Inputs are copied to the output maintaining the order of the input arguments.<br>\n      All inputs must be integers or floats, while the output will be all floating point values.",
    "inputs": [
      { "name": "X", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "Y", "type": "TensorOf" }
    ],
    "attributes": [
      { "name": "inputdimensions", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "onnx.Flatten",
    "summary": "ONNX Flatten operation",
    "description": "Flattens the input tensor into a 2D matrix. If input tensor has shape\n  (d_0, d_1, ... d_n) then the output will have shape\n  (d_0 X d_1 ... d_(axis-1), d_axis X d_(axis+1) ... X dn).",
    "inputs": [
      { "name": "input", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "axis", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "onnx.Floor",
    "summary": "ONNX Floor operation",
    "description": "Floor takes one input data (Tensor<T>) and produces one output data\n  (Tensor<T>) where the floor is, y = floor(x), is applied to\n  the tensor elementwise. If x is integral, +0, -0, NaN,  or infinite, x itself is returned.",
    "inputs": [
      { "name": "X", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "Y", "type": "AnyTypeOf" }
    ]
  },
  {
    "name": "onnx.Gather",
    "summary": "ONNX Gather operation",
    "description": "Given `data` tensor of rank r >= 1, and `indices` tensor of rank q, gather\n  entries of the axis dimension of `data` (by default outer-most one as axis=0) indexed by `indices`, and concatenates\n  them in an output tensor of rank q + (r - 1).\n  \n  If `axis = 0`, let `k = indices[i_{0}, ..., i_{q-1\\}\\]`\n  then `output[i_{0}, ..., i_{q-1}, j_{0}, ..., j_{r-2\\}\\] = input[k , j_{0}, ..., j_{r-2\\}\\]`:\n  \n  ```\n  data = [\n      [1.0, 1.2],\n      [2.3, 3.4],\n      [4.5, 5.7],\n  ]\n  indices = [\n      [0, 1],\n      [1, 2],\n  ]\n  output = [\n      [\n          [1.0, 1.2],\n          [2.3, 3.4],\n      ],\n      [\n          [2.3, 3.4],\n          [4.5, 5.7],\n      ],\n  ]\n  ```\n  \n  If `axis = 1`, let `k = indices[i_{0}, ..., i_{q-1\\}\\]`\n  then `output[j_{0}, i_{0}, ..., i_{q-1}, j_{1}, ..., j_{r-2\\}\\] = input[j_{0}, k, j_{1}, ..., j_{r-2\\}\\]`:\n  \n  ```\n  data = [\n      [1.0, 1.2, 1.9],\n      [2.3, 3.4, 3.9],\n      [4.5, 5.7, 5.9],\n  ]\n  indices = [\n      [0, 2],\n  ]\n  axis = 1,\n  output = [\n          [[1.0, 1.9]],\n          [[2.3, 3.9]],\n          [[4.5, 5.9]],\n  ]\n  ```",
    "inputs": [
      { "name": "data", "type": "AnyTypeOf" },
      { "name": "indices", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "axis", "type": "DefaultValuedAttr" }
    ],
    "category": "Tensor"
  },
  {
    "name": "onnx.GatherElements",
    "summary": "ONNX GatherElements operation",
    "description": "GatherElements takes two inputs `data` and `indices` of the same rank r >= 1\n  and an optional attribute `axis` that identifies an axis of `data`\n  (by default, the outer-most axis, that is axis 0). It is an indexing operation\n  that produces its output by indexing into the input data tensor at index\n  positions determined by elements of the `indices` tensor.\n  Its output shape is the same as the shape of `indices` and consists of one value\n  (gathered from the `data`) for each element in `indices`.\n  \n  For instance, in the 3-D case (r = 3), the output produced is determined\n  by the following equations:\n  ```\n  out[i][j][k] = input[index[i][j][k]][j][k] if axis = 0,\n  out[i][j][k] = input[i][index[i][j][k]][k] if axis = 1,\n  out[i][j][k] = input[i][j][index[i][j][k]] if axis = 2,\n  ```\n  \n  This operator is also the inverse of ScatterElements. It is similar to Torch's gather operation.\n  \n  Example 1:\n  ```\n  data = [\n      [1, 2],\n      [3, 4],\n  ]\n  indices = [\n      [0, 0],\n      [1, 0],\n  ]\n  axis = 1\n  output = [\n      [1, 1],\n      [4, 3],\n  ]\n  ```\n  Example 2:\n  ```\n  data = [\n      [1, 2, 3],\n      [4, 5, 6],\n      [7, 8, 9],\n  ]\n  indices = [\n      [1, 2, 0],\n      [2, 0, 0],\n  ]\n  axis = 0\n  output = [\n      [4, 8, 3],\n      [7, 2, 3],\n  ]\n  ```",
    "inputs": [
      { "name": "data", "type": "AnyTypeOf" },
      { "name": "indices", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "axis", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "onnx.GatherND",
    "summary": "ONNX GatherND operation",
    "description": "Given `data` tensor of rank `r` >= 1, `indices` tensor of rank `q` >= 1, and `batch_dims` integer `b`, this operator gathers\n  slices of `data` into an output tensor of rank `q + r - indices_shape[-1] - 1 - b`.\n  \n  `indices` is an q-dimensional integer tensor, best thought of as a `(q-1)`-dimensional tensor of index-tuples into `data`,\n  where each element defines a slice of `data`\n  \n  `batch_dims` (denoted as `b`) is an integer indicating the number of batch dimensions, i.e the leading `b` number of dimensions of\n  `data` tensor and `indices` are representing the batches, and the gather starts from the `b+1` dimension.\n  \n  Some salient points about the inputs' rank and shape:\n  \n  1) r >= 1 and q >= 1 are to be honored. There is no dependency condition to be met between ranks `r` and `q`\n  \n  2) The first `b` dimensions of the shape of `indices` tensor and `data` tensor must be equal.\n  \n  3) b < min(q, r) is to be honored.\n  \n  4) The `indices_shape[-1]` should have a value between 1 (inclusive) and rank `r-b` (inclusive)\n  \n  5) All values in `indices` are expected to be within bounds [-s, s-1] along axis of size `s` (i.e.) `-data_shape[i] <= indices[...,i] <= data_shape[i] - 1`.\n     It is an error if any of the index values are out of bounds.\n  \n  The output is computed as follows:\n  \n  The output tensor is obtained by mapping each index-tuple in the `indices` tensor to the corresponding slice of the input `data`.\n  \n  1) If `indices_shape[-1] > r-b` => error condition\n  \n  2) If `indices_shape[-1] == r-b`, since the rank of `indices` is `q`, `indices` can be thought of as `N` `(q-b-1)`-dimensional tensors\n     containing 1-D tensors of dimension `r-b`, where `N` is an integer equals to the product of 1 and all the elements in the batch dimensions\n     of the indices_shape. Let us think of each such `r-b` ranked tensor as `indices_slice`. Each *scalar value* corresponding to `data[0:b-1,indices_slice]`\n     is filled into the corresponding location of the `(q-b-1)`-dimensional tensor to form the `output` tensor (Example 1 below)\n  \n  3) If `indices_shape[-1] < r-b`, since the rank of `indices` is `q`, `indices` can be thought of as `N` `(q-b-1)`-dimensional tensor\n     containing 1-D tensors of dimension `< r-b`. Let us think of each such tensors as `indices_slice`. Each *tensor slice* corresponding\n     to `data[0:b-1, indices_slice , :]` is filled into the corresponding location of the `(q-b-1)`-dimensional tensor\n     to form the `output` tensor (Examples 2, 3, 4 and 5 below)\n  \n  This operator is the inverse of `ScatterND`.\n  \n  **Example 1**\n  \n  ```\n  batch_dims = 0\n  data    = [[0,1],[2,3]]   # data_shape    = [2, 2]\n  indices = [[0,0],[1,1]]   # indices_shape = [2, 2]\n  output  = [0,3]           # output_shape  = [2]\n  ```\n  \n  **Example 2**\n  \n  ```\n  batch_dims = 0\n  data    = [[0,1],[2,3]]  # data_shape    = [2, 2]\n  indices = [[1],[0]]      # indices_shape = [2, 1]\n  output  = [[2,3],[0,1]]  # output_shape  = [2, 2]\n  ```\n  \n  **Example 3**\n  \n  ```\n  batch_dims = 0\n  data    = [[[0,1],[2,3]],[[4,5],[6,7]]] # data_shape    = [2, 2, 2]\n  indices = [[0,1],[1,0]]                 # indices_shape = [2, 2]\n  output  = [[2,3],[4,5]]                 # output_shape  = [2, 2]\n  ```\n  \n  **Example 4**\n  \n  ```\n  batch_dims = 0\n  data    = [[[0,1],[2,3]],[[4,5],[6,7]]] # data_shape    = [2, 2, 2]\n  indices = [[[0,1]],[[1,0]]]             # indices_shape = [2, 1, 2]\n  output  = [[[2,3]],[[4,5]]]             # output_shape  = [2, 1, 2]\n  ```\n  \n  **Example 5**\n  \n  ```\n  batch_dims = 1\n  data    = [[[0,1],[2,3]],[[4,5],[6,7]]] # data_shape    = [2, 2, 2]\n  indices = [[1],[0]]                     # indices_shape = [2, 1]\n  output  = [[2,3],[4,5]]                 # output_shape  = [2, 2]\n  ```",
    "inputs": [
      { "name": "data", "type": "AnyTypeOf" },
      { "name": "indices", "type": "TensorOf" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "batch_dims", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "onnx.Gelu",
    "summary": "ONNX Gelu operation",
    "description": "Gelu takes one input data (Tensor<T>) and produces one\n  output data (Tensor<T>) where the gaussian error linear units function,\n  $y = 0.5 * x * (1 + erf(x/sqrt(2)))$ is applied to the tensor elementwise.\n  If the attribute \\\"approximate\\\" is set to \\\"tanh\\\", the function estimation,\n  $y = 0.5 * x * (1 + Tanh(sqrt(2/\\pi) * (x + 0.044715 * x^3)))$ is used and applied\n  to the tensor elementwise.",
    "inputs": [
      { "name": "X", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "Y", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "approximate", "type": "DefaultValuedStrAttr" }
    ]
  },
  {
    "name": "onnx.Gemm",
    "summary": "ONNX Gemm operation",
    "description": "General Matrix multiplication:\n  https://en.wikipedia.org/wiki/Basic_Linear_Algebra_Subprograms#Level_3\n  \n  * A' = transpose(A) if transA else A\n  * B' = transpose(B) if transB else B\n  \n  Compute Y = alpha * A' * B' + beta * C, where input tensor A has shape (M, K) or (K, M),\n  input tensor B has shape (K, N) or (N, K), input tensor C is broadcastable to shape (M, N),\n  and output tensor Y has shape (M, N). A will be transposed before doing the\n  computation if attribute transA is non-zero, same for B and transB.\n  This operator supports **unidirectional broadcasting** (tensor C should be unidirectional broadcastable to tensor A * B); for more details please check [the doc](Broadcasting.md).\n  This operator has **optional** inputs/outputs. See [the doc](IR.md) for more details about the representation of optional arguments. An empty string may be used in the place of an actual argument's name to indicate a missing argument. Trailing optional arguments (those not followed by an argument that is present) may also be simply omitted.",
    "inputs": [
      { "name": "A", "type": "AnyTypeOf" },
      { "name": "B", "type": "AnyTypeOf" },
      { "name": "C", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "Y", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "alpha", "type": "DefaultValuedAttr" },
      { "name": "beta", "type": "DefaultValuedAttr" },
      { "name": "transA", "type": "DefaultValuedAttr" },
      { "name": "transB", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "onnx.GlobalAveragePool",
    "summary": "ONNX GlobalAveragePool operation",
    "description": "GlobalAveragePool consumes an input tensor X and applies average pooling across\n   the values in the same channel. This is equivalent to AveragePool with kernel size\n   equal to the spatial dimension of input tensor.",
    "inputs": [
      { "name": "X", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "Y", "type": "AnyTypeOf" }
    ]
  },
  {
    "name": "onnx.GlobalLpPool",
    "summary": "ONNX GlobalLpPool operation",
    "description": "GlobalLpPool consumes an input tensor X and applies lp pool pooling across\n   the values in the same channel. This is equivalent to LpPool with kernel size\n   equal to the spatial dimension of input tensor.",
    "inputs": [
      { "name": "X", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "Y", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "p", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "onnx.GlobalMaxPool",
    "summary": "ONNX GlobalMaxPool operation",
    "description": "GlobalMaxPool consumes an input tensor X and applies max pooling across\n   the values in the same channel. This is equivalent to MaxPool with kernel size\n   equal to the spatial dimension of input tensor.",
    "inputs": [
      { "name": "X", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "Y", "type": "AnyTypeOf" }
    ]
  },
  {
    "name": "onnx.Gradient",
    "summary": "ONNX Gradient operation",
    "description": "Gradient operator computes the partial derivatives of a specific tensor w.r.t.\n  some other tensors. This operator is widely used in gradient-based training\n  algorithms. To illustrate its use, let's consider a computation graph,\n  \n  ```\n  X -----.\n         |\n         v\n  W --> Conv --> H --> Gemm --> Y\n                        ^\n                        |\n                        Z\n  ```\n  \n  , where W and Z are trainable tensors. Note that operators' attributes are\n  omitted for the sake of simplicity. Let dY/dW (dY/dZ) be the gradient of\n  Y with respect to W (Z). The user can compute gradient by inserting Gradient\n  operator to form another graph shown below.\n  \n  ```\n  W --> Conv --> H --> Gemm --> Y\n  |      ^              ^\n  |      |              |\n  |      X              Z\n  |      |              |\n  |      |   .----------'\n  |      |   |  (W/Z/X is the 1st/2nd/3rd input of Gradient as shown in\n  |      |   |   \\\"xs\\\" followed by \\\"zs\\\")\n  |      v   v\n  '---> Gradient(xs=[\\\"W\\\", \\\"Z\\\"], zs=[\\\"X\\\"], y=\\\"Y\\\")\n         |   |\n         |   '-----------------------------------> dY/dW (1st output of Gradient)\n         |\n         '---------------------------------------> dY/dZ (2nd output of Gradient)\n  ```\n  \n  By definition, the tensor \\\"y\\\" is a function of independent variables in \\\"xs\\\"\n  and \\\"zs\\\". Since we only compute the gradient of \\\"y\\\" w.r.t. the differentiable\n  variables in \\\"xs\\\", this Gradient only outputs dY/dW and dY/dZ. Note that \\\"H\\\"\n  cannot appear in \\\"xs\\\" and \\\"zs\\\". The reason is that \\\"H\\\" can be determined by\n  tensors \\\"W\\\" and \\\"X\\\" and therefore \\\"H\\\" is not an independent variable.\n  \n  All outputs are optional. If needed, for example, user can assign an empty\n  string to the 1st output name of that Gradient to skip the generation of dY/dW.\n  Note that the concept of optional outputs can also be found in ONNX's RNN, GRU,\n  and LSTM.\n  \n  Gradient operator can compute derivative against intermediate tensors. For\n  example, the gradient of Y with respect to H can be done via\n  \n  ```\n  W --> Conv --> H --> Gemm --> Y\n         ^       |      ^\n         |       |      |\n         X       |      Z\n         .-------'      |\n         |   .----------'\n         |   | (H/Z is the 1st/2nd input of Gradient as shown in \\\"xs\\\")\n         v   v\n        Gradient(xs=[\\\"H\\\", \\\"Z\\\"], y=\\\"Y\\\")\n         |   |\n         |   '-----------------------------------> dY/dH (1st output of Gradient)\n         |\n         '---------------------------------------> dY/dZ (2nd output of Gradient)\n  ```\n  \n  It is possible to represent high-order differentiation using Gradient operators.\n  For example, given the following linear model:\n  \n  ```\n  W --> Gemm --> Y --> Loss --> O\n         ^              ^\n         |              |\n         X              L\n  ```\n  \n  To compute the 2nd order derivative of O with respect to W (denoted by\n  d^2O/dW^2), one can do\n  \n  ```\n  W --> Gemm --> Y --> Loss --> O\n  |      ^              ^\n  |      |              |\n  |      X .------------L\n  |      | |            |\n  |      | |            v\n  +------+-+> Gradient(xs=[\\\"X\\\", \\\"W\\\"], zs=[\\\"L\\\"], y=\\\"O\\\") ---> dO/dX (1st output of Gradient)\n  |      | |    |\n  |      | |    '---> dO/dW (2nd output of Gradient)\n  |      v v\n  '---> Gradient(xs=[\\\"X\\\", \\\"W\\\"], zs=[\\\"L\\\"], y=\\\"dO/dW\\\") ---> d(dO/dW)dX (1st output of\n         |                                                  Gradient)\n         |\n         |\n         '---> d^2O/dW^2 (2nd output of Gradient)\n  ```\n  \n  The tensors named in attributes \\\"xs\\\", \\\"zs\\\", and \\\"y\\\" define the differentiated\n  computation graph, and the inputs to Gradient node define the values at\n  which the gradient is computed. We can feed different tensors to the identified\n  graph. For example, one can compute the gradient of Y with respect to H at\n  a specific value of H, H_1, by providing that value as an input to the Gradient\n  node.\n  \n  ```\n  W --> Conv --> H --> Gemm --> Y\n         ^              ^\n         |              |\n         X              Z\n  \n            Z_1 (2nd input of Gradient)\n             |\n             v\n  H_1 --> Gradient(xs=[\\\"H\\\", \\\"Z\\\"], y=\\\"Y\\\") ---> dY/dH when H = H_1 and Y = Y_1.\n             |\n             '------------------------------> dY/dZ (2nd output of Gradient)\n  ```\n  \n  When the inputs of Gradient are the tensors named in \\\"xs\\\" and \\\"zs\\\", the\n  computation can be optimized. More specifically, intermediate variables in\n  forward pass can be reused if the gradient is computed via reverse-mode\n  auto-differentiation.",
    "inputs": [
      { "name": "Inputs", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "Outputs", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "xs", "type": "StrArrayAttr" },
      { "name": "y", "type": "StrAttr" },
      { "name": "zs", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "onnx.Greater",
    "summary": "ONNX Greater operation",
    "description": "Returns the tensor resulted from performing the `greater` logical operation\n  elementwise on the input tensors `A` and `B` (with Numpy-style broadcasting support).\n  \n  This operator supports **multidirectional (i.e., Numpy-style) broadcasting**; for more details please check [the doc](Broadcasting.md).",
    "inputs": [
      { "name": "A", "type": "AnyTypeOf" },
      { "name": "B", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "C", "type": "TensorOf" }
    ]
  },
  {
    "name": "onnx.GreaterOrEqual",
    "summary": "ONNX GreaterOrEqual operation",
    "description": "Returns the tensor resulted from performing the `greater_equal` logical operation\n  elementwise on the input tensors `A` and `B` (with Numpy-style broadcasting support).\n  \n  This operator supports **multidirectional (i.e., Numpy-style) broadcasting**; for more details please check [the doc](Broadcasting.md).",
    "inputs": [
      { "name": "A", "type": "AnyTypeOf" },
      { "name": "B", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "C", "type": "TensorOf" }
    ]
  },
  {
    "name": "onnx.GridSample",
    "summary": "ONNX GridSample operation",
    "description": "Given an input `X` and a flow-field `grid`, computes the output `Y` using `X` values and pixel locations from the `grid`.\n  For spatial input `X` with shape (N, C, H, W), the `grid` will have shape (N, H_out, W_out, 2),\n  the output `Y` will have shape (N, C, H_out, W_out). For volumetric input `X` with shape (N, C, D, H, W),\n  the `grid` will have shape (N, D_out, H_out, W_out, 3), the output `Y` will have shape (N, C, D_out, H_out, W_out).\n  More generally, for an input `X` of rank r+2 with shape (N, C, d1, d2, ..., dr),\n  the `grid` will have shape (N, D1_out, D2_out, ..., Dr_out, r), the output `Y` will have shape (N, C, D1_out, D2_out, ..., Dr_out).\n  \n  The tensor `X` contains values at centers of square pixels (voxels, etc) locations such as (n, c, d1_in, d2_in, ..., dr_in).\n  The (n, d1_out, d2_out, ..., dr_out, :) values from the tensor `grid` are the normalized positions for interpolating the values\n  at the (n, c, d1_out, d2_out, ..., dr_out) locations from the output tensor `Y` using a specified interpolation method (the mode)\n  and a padding mode (for `grid` positions falling outside the 2-dimensional image).\n  \n  For example, the values in `grid[n, h_out, w_out, :]` are size-2 vectors specifying normalized positions in the 2-dimensional space of `X`.\n  They are used to interpolate output values of `Y[n, c, h_out, w_out]`.\n  \n  The GridSample operator is often used in doing grid generator and sampler in the\n  [Spatial Transformer Networks](https://arxiv.org/abs/1506.02025).\n  See also in [torch.nn.functional.grid_sample](https://pytorch.org/docs/stable/generated/torch.nn.functional.grid_sample.html).",
    "inputs": [
      { "name": "X", "type": "AnyTypeOf" },
      { "name": "grid", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "Y", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "align_corners", "type": "DefaultValuedAttr" },
      { "name": "mode", "type": "DefaultValuedStrAttr" },
      { "name": "padding_mode", "type": "DefaultValuedStrAttr" }
    ]
  },
  {
    "name": "onnx.GridSampleV16",
    "summary": "ONNX GridSample operation",
    "description": "Given an input `X` and a flow-field `grid`, computes the output `Y` using `X` values and pixel locations from `grid`.\n  Currently, only spatial (4-D) inputs are supported. For input `X` with shape (N, C, H, W) and `grid` with shape (N, H_out, W_out, 2),\n  the output `Y` will have shape (N, C, H_out, W_out).\n  \n  The tensor `X` contains values at centers of square pixels in a H by W 2-dimensional image.\n  The tensor `grid` describes normalized positions where the output `Y` is to be computed\n  using a specified interpolation method (the mode) and a padding mode (for grid positions falling outside the 2-dimensional image).\n  \n  Elements in `grid[N, H_out, W_out]` are size-2 vectors specifying positions in the 2-dimensional space of `X`.\n  They are used to interpolate output values of `Y[N, C, H_out, W_out]`.\n  \n  The GridSample operator is often used in doing grid generator and sampler in the [Spatial Transformer Networks](https://arxiv.org/abs/1506.02025).\n  See also in [torch.nn.functional.grid_sample](https://pytorch.org/docs/master/generated/torch.nn.functional.grid_sample.html#torch-nn-functional-grid-sample).",
    "inputs": [
      { "name": "X", "type": "AnyTypeOf" },
      { "name": "grid", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "Y", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "align_corners", "type": "DefaultValuedAttr" },
      { "name": "mode", "type": "DefaultValuedStrAttr" },
      { "name": "padding_mode", "type": "DefaultValuedStrAttr" }
    ]
  },
  {
    "name": "onnx.GroupNormalization",
    "summary": "ONNX GroupNormalization operation",
    "description": "A GroupNormalization function. Carries out group normalization as described in\n  the paper https://arxiv.org/abs/1803.08494\n  \n  This operator transforms input according to\n  ```\n  y = scale * (x - mean) / sqrt(variance + epsilon) + bias,\n  ```\n  where the mean and variance are computed per instance per group of channels, and\n  `scale` and `bias` should be specified for each group of channels. The number of\n  groups `num_groups` should be divisible by the number of channels so that there are\n  an equal number of channels per group.\n  \n  The overall computation has two stages: the first stage normalizes the elements to\n  have zero mean and unit variance for each instance in each group, and the second\n  stage scales and shifts the results of the first stage. The floating-point precision\n  used in the first stage is determined by the `stash_type` attribute. For example,\n  if `stash_type` is 1, the operator casts all input variables to 32-bit float,\n  performs the computation, and finally casts the normalized results back to the\n  original type of `X`. The second stage does not depend on `stash_type`.\n  \n  When the number of groups is the same as the number of channels, this operator is\n  equivalent to InstanceNormalization. When there is only one group, this operator\n  is equivalent to LayerNormalization.",
    "inputs": [
      { "name": "X", "type": "AnyTypeOf" },
      { "name": "scale", "type": "AnyTypeOf" },
      { "name": "bias", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "Y", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "epsilon", "type": "DefaultValuedAttr" },
      { "name": "num_groups", "type": "SI64Attr" },
      { "name": "stash_type", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "onnx.GroupNormalizationV18",
    "summary": "ONNX GroupNormalization operation",
    "description": "A GroupNormalization function. Carries out group normalization as described in\n  the paper https://arxiv.org/abs/1803.08494\n  \n  This operator transforms input according to\n  ```\n  y = scale * (x - mean) / sqrt(variance + epsilon) + bias,\n  ```\n  where the mean and variance are computed per instance per group of channels, and\n  `scale` and `bias` should be specified for each group of channels. The number of\n  groups `num_groups` should be divisible by the number of channels so that there are\n  an equal number of channels per group.\n  \n  When the number of groups is the same as the number of channels, this operator is\n  equivalent to InstanceNormalization. When there is only one group, this operator\n  is equivalent to LayerNormalization.",
    "inputs": [
      { "name": "X", "type": "AnyTypeOf" },
      { "name": "scale", "type": "AnyTypeOf" },
      { "name": "bias", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "Y", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "epsilon", "type": "DefaultValuedAttr" },
      { "name": "num_groups", "type": "SI64Attr" }
    ]
  },
  {
    "name": "onnx.GRU",
    "summary": "ONNX GRU operation",
    "description": "Computes an one-layer GRU. This operator is usually supported via some custom\n  implementation such as CuDNN.\n  \n  Notations:\n  \n  * `X` - input tensor\n  * `z` - update gate\n  * `r` - reset gate\n  * `h` - hidden gate\n  * `t` - time step (t-1 means previous time step)\n  * `W[zrh]` - W parameter weight matrix for update, reset, and hidden gates\n  * `R[zrh]` - R recurrence weight matrix for update, reset, and hidden gates\n  * `Wb[zrh]` - W bias vectors for update, reset, and hidden gates\n  * `Rb[zrh]` - R bias vectors for update, reset, and hidden gates\n  * `WB[zrh]` - W parameter weight matrix for backward update, reset, and hidden gates\n  * `RB[zrh]` - R recurrence weight matrix for backward update, reset, and hidden gates\n  * `WBb[zrh]` - W bias vectors for backward update, reset, and hidden gates\n  * `RBb[zrh]` - R bias vectors for backward update, reset, and hidden gates\n  * `H` - Hidden state\n  * `num_directions` - 2 if direction == bidirectional else 1\n  \n  Activation functions:\n  \n  * Relu(x)                - max(0, x)\n  * Tanh(x)                - (1 - e^{-2x})/(1 + e^{-2x})\n  * Sigmoid(x)             - 1/(1 + e^{-x})\n  \n  NOTE:\n    Below are optional\n  \n  * Affine(x)              - alpha * x + beta\n  * LeakyRelu(x)           - x if x >= 0 else alpha * x\n  * ThresholdedRelu(x)     - x if x >= alpha else 0\n  * ScaledTanh(x)          - alpha * Tanh(beta * x)\n  * HardSigmoid(x)         - min(max(alpha * x + beta, 0), 1)\n  * Elu(x)                 - x if x >= 0 else alpha * (e^x - 1)\n  * Softsign(x)            - x/(1 + |x|)\n  * Softplus(x)            - log(1 + e^x)\n  \n  Equations (Default: f=Sigmoid, g=Tanh):\n  \n  * zt = f(Xt*(Wz^T) + Ht-1*(Rz^T) + Wbz + Rbz)\n  * rt = f(Xt*(Wr^T) + Ht-1*(Rr^T) + Wbr + Rbr)\n  * ht = g(Xt*(Wh^T) + (rt (.) Ht-1)*(Rh^T) + Rbh + Wbh) # default, when linear_before_reset = 0\n  * ht = g(Xt*(Wh^T) + (rt (.) (Ht-1*(Rh^T) + Rbh)) + Wbh) # when linear_before_reset != 0\n  * Ht = (1 - zt) (.) ht + zt (.) Ht-1\n  This operator has **optional** inputs/outputs. See [the doc](IR.md) for more details about the representation of optional arguments. An empty string may be used in the place of an actual argument's name to indicate a missing argument. Trailing optional arguments (those not followed by an argument that is present) may also be simply omitted.",
    "inputs": [
      { "name": "X", "type": "AnyTypeOf" },
      { "name": "W", "type": "AnyTypeOf" },
      { "name": "R", "type": "AnyTypeOf" },
      { "name": "B", "type": "AnyTypeOf" },
      { "name": "sequence_lens", "type": "AnyTypeOf" },
      { "name": "initial_h", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "Y", "type": "AnyTypeOf" },
      { "name": "Y_h", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "activation_alpha", "type": "OptionalAttr" },
      { "name": "activation_beta", "type": "OptionalAttr" },
      { "name": "activations", "type": "OptionalAttr" },
      { "name": "clip", "type": "OptionalAttr" },
      { "name": "direction", "type": "DefaultValuedStrAttr" },
      { "name": "hidden_size", "type": "OptionalAttr" },
      { "name": "layout", "type": "DefaultValuedAttr" },
      { "name": "linear_before_reset", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "onnx.HammingWindow",
    "summary": "ONNX HammingWindow operation",
    "description": "Generates a Hamming window as described in the paper https://ieeexplore.ieee.org/document/1455106.",
    "inputs": [
      { "name": "size", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "output_datatype", "type": "DefaultValuedAttr" },
      { "name": "periodic", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "onnx.HannWindow",
    "summary": "ONNX HannWindow operation",
    "description": "Generates a Hann window as described in the paper https://ieeexplore.ieee.org/document/1455106.",
    "inputs": [
      { "name": "size", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "output_datatype", "type": "DefaultValuedAttr" },
      { "name": "periodic", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "onnx.Hardmax",
    "summary": "ONNX Hardmax operation",
    "description": "The operator computes the hardmax values for the given input:\n  \n   Hardmax(element in input, axis) = 1 if the element is the first maximum value along the specified axis, 0 otherwise\n  \n  The \\\"axis\\\" attribute indicates the dimension along which Hardmax\n  will be performed. The output tensor has the same shape\n  and contains the Hardmax values of the corresponding input.",
    "inputs": [
      { "name": "input", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "axis", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "onnx.HardSigmoid",
    "summary": "ONNX HardSigmoid operation",
    "description": "HardSigmoid takes one input data (Tensor<T>) and produces one output data\n  (Tensor<T>) where the HardSigmoid function, y = max(0, min(1, alpha * x + beta)),\n  is applied to the tensor elementwise.",
    "inputs": [
      { "name": "X", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "Y", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "alpha", "type": "DefaultValuedAttr" },
      { "name": "beta", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "onnx.HardSwish",
    "summary": "ONNX HardSwish operation",
    "description": "HardSwish takes one input data (Tensor<T>) and produces one output data (Tensor<T>) where\n  the HardSwish function, y = x * max(0, min(1, alpha * x + beta)) = x * HardSigmoid<alpha, beta>(x),\n  where alpha = 1/6 and beta = 0.5, is applied to the tensor elementwise.",
    "inputs": [
      { "name": "X", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "Y", "type": "AnyTypeOf" }
    ]
  },
  {
    "name": "onnx.Identity",
    "summary": "ONNX Identity operation",
    "description": "Identity operator",
    "inputs": [
      { "name": "input", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTypeOf" }
    ]
  },
  {
    "name": "onnx.If",
    "summary": "ONNX If operation",
    "description": "If conditional",
    "inputs": [
      { "name": "cond", "type": "TensorOf" }
    ],
    "outputs": [
      { "name": "outputs", "type": "Variadic" }
    ]
  },
  {
    "name": "onnx.Imputer",
    "summary": "ONNX Imputer operation",
    "description": "Replaces inputs that equal one value with another, leaving all other elements alone.<br>\n      This operator is typically used to replace missing values in situations where they have a canonical\n      representation, such as -1, 0, NaN, or some extreme value.<br>\n      One and only one of imputed_value_floats or imputed_value_int64s should be defined -- floats if the input tensor\n      holds floats, integers if the input tensor holds integers. The imputed values must all fit within the\n      width of the tensor element type. One and only one of the replaced_value_float or replaced_value_int64 should be defined,\n      which one depends on whether floats or integers are being processed.<br>\n      The imputed_value attribute length can be 1 element, or it can have one element per input feature.<br>In other words, if the input tensor has the shape [*,F], then the length of the attribute array may be 1 or F. If it is 1, then it is broadcast along the last dimension and applied to each feature.",
    "inputs": [
      { "name": "X", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "Y", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "imputed_value_floats", "type": "OptionalAttr" },
      { "name": "imputed_value_int64s", "type": "OptionalAttr" },
      { "name": "replaced_value_float", "type": "DefaultValuedAttr" },
      { "name": "replaced_value_int64", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "onnx.InstanceNormalization",
    "summary": "ONNX InstanceNormalization operation",
    "description": "Carries out instance normalization as described in the paper\n  https://arxiv.org/abs/1607.08022.\n  \n  y = scale * (x - mean) / sqrt(variance + epsilon) + B,\n  where mean and variance are computed per instance per channel.",
    "inputs": [
      { "name": "input", "type": "AnyTypeOf" },
      { "name": "scale", "type": "AnyTypeOf" },
      { "name": "B", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "epsilon", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "onnx.IsInf",
    "summary": "ONNX IsInf operation",
    "description": "Map infinity to true and other values to false.",
    "inputs": [
      { "name": "X", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "Y", "type": "TensorOf" }
    ],
    "attributes": [
      { "name": "detect_negative", "type": "DefaultValuedAttr" },
      { "name": "detect_positive", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "onnx.IsNaN",
    "summary": "ONNX IsNaN operation",
    "description": "Returns which elements of the input are NaN.",
    "inputs": [
      { "name": "X", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "Y", "type": "TensorOf" }
    ]
  },
  {
    "name": "onnx.LabelEncoder",
    "summary": "ONNX LabelEncoder operation",
    "description": "Maps each element in the input tensor to another value.<br>\n      The mapping is determined by the two parallel attributes, 'keys_*' and\n      'values_*' attribute. The i-th value in the specified 'keys_*' attribute\n      would be mapped to the i-th value in the specified 'values_*' attribute. It\n      implies that input's element type and the element type of the specified\n      'keys_*' should be identical while the output type is identical to the\n      specified 'values_*' attribute. If an input element can not be found in the\n      specified 'keys_*' attribute, the 'default_*' that matches the specified\n      'values_*' attribute may be used as its output value.<br>\n      Let's consider an example which maps a string tensor to an integer tensor.\n      Assume and 'keys_strings' is [\\\"Amy\\\", \\\"Sally\\\"], 'values_int64s' is [5, 6],\n      and 'default_int64' is '-1'.  The input [\\\"Dori\\\", \\\"Amy\\\", \\\"Amy\\\", \\\"Sally\\\",\n      \\\"Sally\\\"] would be mapped to [-1, 5, 5, 6, 6].<br>\n      Since this operator is an one-to-one mapping, its input and output shapes\n      are the same. Notice that only one of 'keys_*'/'values_*' can be set.<br>\n      For key look-up, bit-wise comparison is used so even a float NaN can be\n      mapped to a value in 'values_*' attribute.<br>",
    "inputs": [
      { "name": "X", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "Y", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "default_float", "type": "DefaultValuedAttr" },
      { "name": "default_int64", "type": "DefaultValuedAttr" },
      { "name": "default_string", "type": "DefaultValuedStrAttr" },
      { "name": "keys_floats", "type": "OptionalAttr" },
      { "name": "keys_int64s", "type": "OptionalAttr" },
      { "name": "keys_strings", "type": "OptionalAttr" },
      { "name": "values_floats", "type": "OptionalAttr" },
      { "name": "values_int64s", "type": "OptionalAttr" },
      { "name": "values_strings", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "onnx.LayerNormalization",
    "summary": "ONNX LayerNormalization operation",
    "description": "This is layer normalization defined in ONNX as function.\n        The overall computation can be split into two stages.\n        The first stage is standardization, which makes the\n        normalized elements have zero mean and unit variances.\n        The computation required by standardization can be\n        described by the following equations.\n        ```\n        Mean = ReduceMean<axes=normalized_axes>(X)\n        D = Sub(X, Mean)\n        DD = Mul(D, D)\n        Var = ReduceMean<axes=normalized_axes>(DD)\n        VarEps = Add(Var, epsilon)\n        StdDev = Sqrt(VarEps)\n        InvStdDev = Reciprocal(StdDev)\n        Normalized = Mul(D, InvStdDev)\n        ```\n        where `normalized_axes` is `[axis, ..., rank of X - 1]`.\n        The variables `Var` and `StdDev` stand for variance and\n        standard deviation, respectively. The second output is\n        `Mean` and the last one is `InvStdDev`.\n        Depending on `stash_type` attribute, the actual computation\n        must happen in different floating-point precision.\n        For example, if `stash_type` is 1, this operator casts\n        all input variables to 32-bit float, perform the computation, and\n        finally cast `Normalized` back to the original type of `X`.\n        The second stage then scales and shifts the outcome of the\n        first stage using\n        ```\n        NormalizedScaled = Mul(Normalized, Scale)\n        Y = Add(NormalizedScaled, B)\n        ```\n        The second stage doesn't depends on `stash_type`.\n        All equations are in [this syntax](https://github.com/onnx/onnx/blob/main/docs/Syntax.md).\n        The same variable (i.e., input, output, and attribute) uses\n        the same name in the equations above and this operator's definition.\n        Let `d[i]` indicate the i-th dimension of `X`.\n        If `X`'s shape is `[d[0], ..., d[axis-1], d[axis], ..., d[rank-1]]`,\n        the shape of `Mean` and `InvStdDev` is `[d[0], ..., d[axis-1], 1, ..., 1]`.\n        `Y` and `X` have the same shape. This operator supports unidirectional broadcasting\n        (tensors `Scale` and `B` should be unidirectional broadcastable to tensor `X`);\n        for more details please check [the doc](Broadcasting.md).",
    "inputs": [
      { "name": "X", "type": "AnyTypeOf" },
      { "name": "Scale", "type": "AnyTypeOf" },
      { "name": "B", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "Y", "type": "AnyTypeOf" },
      { "name": "Mean", "type": "AnyTypeOf" },
      { "name": "InvStdDev", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "axis", "type": "DefaultValuedAttr" },
      { "name": "epsilon", "type": "DefaultValuedAttr" },
      { "name": "stash_type", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "onnx.LayoutTransform",
    "summary": "An operation that transforms data between different layout formats",
    "description": "An operation that transforms a tensor from a layout to another layout. \n    A layout is defined by an attribute, i.e. `target_layout`, which allows this\n    operation work with an arbitrary layout (e.g. a layout used for accelerators).\n\n    `target_layout` is optional. If it is not given, the input tensor will be\n    transformed to a normal tensor that does not have layout.\n\n    If `target_layout` is the same as the input's layout, this operation will\n    become an no-op by canonicalization. \n\n    The input and output tensors must have the same shape.\n\n    This operation is not part of the standard and was added to assist onnx-mlir.",
    "inputs": [
      { "name": "data", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "target_layout", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "onnx.LeakyRelu",
    "summary": "ONNX LeakyRelu operation",
    "description": "LeakyRelu takes input data (Tensor<T>) and an argument alpha, and produces one\n  output data (Tensor<T>) where the function `f(x) = alpha * x for x < 0`,\n  `f(x) = x for x >= 0`, is applied to the data tensor elementwise.",
    "inputs": [
      { "name": "X", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "Y", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "alpha", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "onnx.Less",
    "summary": "ONNX Less operation",
    "description": "Returns the tensor resulted from performing the `less` logical operation\n  elementwise on the input tensors `A` and `B` (with Numpy-style broadcasting support).\n  \n  This operator supports **multidirectional (i.e., Numpy-style) broadcasting**; for more details please check [the doc](Broadcasting.md).",
    "inputs": [
      { "name": "A", "type": "AnyTypeOf" },
      { "name": "B", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "C", "type": "TensorOf" }
    ]
  },
  {
    "name": "onnx.LessOrEqual",
    "summary": "ONNX LessOrEqual operation",
    "description": "Returns the tensor resulted from performing the `less_equal` logical operation\n  elementwise on the input tensors `A` and `B` (with Numpy-style broadcasting support).\n  \n  This operator supports **multidirectional (i.e., Numpy-style) broadcasting**; for more details please check [the doc](Broadcasting.md).",
    "inputs": [
      { "name": "A", "type": "AnyTypeOf" },
      { "name": "B", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "C", "type": "TensorOf" }
    ]
  },
  {
    "name": "onnx.LinearClassifier",
    "summary": "ONNX LinearClassifier operation",
    "description": "Linear classifier",
    "inputs": [
      { "name": "X", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "Y", "type": "AnyTypeOf" },
      { "name": "Z", "type": "TensorOf" }
    ],
    "attributes": [
      { "name": "classlabels_ints", "type": "OptionalAttr" },
      { "name": "classlabels_strings", "type": "OptionalAttr" },
      { "name": "coefficients", "type": "F32ArrayAttr" },
      { "name": "intercepts", "type": "OptionalAttr" },
      { "name": "multi_class", "type": "DefaultValuedAttr" },
      { "name": "post_transform", "type": "DefaultValuedStrAttr" }
    ]
  },
  {
    "name": "onnx.LinearRegressor",
    "summary": "ONNX LinearRegressor operation",
    "description": "Generalized linear regression evaluation.<br>\n      If targets is set to 1 (default) then univariate regression is performed.<br>\n      If targets is set to M then M sets of coefficients must be passed in as a sequence\n      and M results will be output for each input n in N.<br>\n      The coefficients array is of length n, and the coefficients for each target are contiguous.\n      Intercepts are optional but if provided must match the number of targets.",
    "inputs": [
      { "name": "X", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "Y", "type": "TensorOf" }
    ],
    "attributes": [
      { "name": "coefficients", "type": "OptionalAttr" },
      { "name": "intercepts", "type": "OptionalAttr" },
      { "name": "post_transform", "type": "DefaultValuedStrAttr" },
      { "name": "targets", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "onnx.Log",
    "summary": "ONNX Log operation",
    "description": "Calculates the natural log of the given input tensor, element-wise.",
    "inputs": [
      { "name": "input", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTypeOf" }
    ]
  },
  {
    "name": "onnx.LogSoftmax",
    "summary": "ONNX LogSoftmax operation",
    "description": "The operator computes the log of softmax values for the given input:\n  \n   LogSoftmax(input, axis) = Log(Softmax(input, axis=axis))\n  \n  The \\\"axis\\\" attribute indicates the dimension along which LogSoftmax\n  will be performed. The output tensor has the same shape\n  and contains the LogSoftmax values of the corresponding input.",
    "inputs": [
      { "name": "input", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "axis", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "onnx.Loop",
    "summary": "ONNX Loop operation",
    "description": "Generic Looping construct. This loop has multiple termination conditions:\n  \n  1) Trip count. Iteration count specified at runtime. Set by\n     specifying the input M. Optional. Set to empty string to omit.\n     Note that a static trip count (specified at graph construction time) can be\n     specified by passing in a constant node for input M.\n  2) Loop termination condition. This is an input to the op that determines\n     whether to run the first iteration and also a loop-carried dependency for\n     the body graph. The body graph must yield a value for the condition variable,\n     whether this input is provided or not.\n  \n  This table summarizes the operating modes of this operator with equivalent\n  C-style code:\n  \n  Operator inputs defined as (max_trip_count, condition_var).\n  \n  * input (\\\"\\\", \\\"\\\"):\n          for (int i=0; ; ++i) {\n            cond = ... // Note this value is ignored, but is required in the body\n          }\n  \n  * input (\\\"\\\", cond) // Note this is analogous to a while loop\n          bool cond = ...;\n          for (int i=0; cond; ++i) {\n            cond = ...;\n          }\n  \n  * input (\\\"\\\", 1) // Note this is analogous to a do-while loop\n          bool cond = true\n          for (int i=0; cond; ++i) {\n            cond = ...;\n          }\n  \n  * input (trip_count, \\\"\\\") // Note this is analogous to a for loop\n          int trip_count = ...\n          for (int i=0; i < trip_count; ++i) {\n            cond = ...; // ignored\n          }\n  \n  * input (trip_count, cond)\n          int trip_count = ...;\n          bool cond = ...;\n          for (int i=0; i < trip_count && cond; ++i) {\n            cond = ...;\n          }\n  \n  \n  *Sample usage - cond as well as trip count*\n  \n      graph predict-net {\n        %a = Constant[value = <Scalar Tensor [3]>]()\n        %b = Constant[value = <Scalar Tensor [6]>]()\n        %keepgoing = Constant[value = <Scalar Tensor [1]>]()\n        %max_trip_count = Constant[value = <Scalar Tensor [10]>]()\n        %keepgoing_out, %b_out, %user_defined_vals = Loop[body = <graph body-net>](%max_trip_count, %keepgoing, %b)\n        return\n      }\n  \n      graph body-net (\n        %i[INT32, scalar]           // iteration number\n        %keepgoing_in[BOOL, scalar] // incoming loop-termination-condition; not used\n        %b_in[INT32, scalar]        // incoming value of loop-carried-dependency b\n      ) {\n        %my_local = Add(%a, %b_in)\n        %b_out = Sub(%a, %b_in) // outgoing value of loop-carried-dependency b\n        %keepgoing_out = Greater(%my_local, %b_out) // outgoing loop-termination-condition\n        %user_defined_val = Add(%b_in, %b_in) // scan-output value to be accumulated\n        return %keepgoing_out, %b_out, %user_defined_val\n      }\n  \n  *Sample equivalent C code*\n  \n      {\n        /* User-defined code (enclosing scope) */\n        int a = 3, b = 6;\n        bool keepgoing = true; // Analogous to input cond\n        /* End user-defined code */\n  \n        /* Implicitly-defined code */\n        const int max_trip_count = 10; // Analogous to input M\n        int user_defined_vals[]; // Imagine this is resizable\n        /* End implicitly-defined code */\n        /* initialize loop-carried variables and scan-output variables */\n        bool keepgoing_out = keepgoing\n        int b_out = b\n  \n        for (int i=0; i < max_trip_count && keepgoing_out; ++i) {\n          /* Implicitly-defined code: bind actual parameter values\n             to formal parameter variables of loop-body */\n          bool keepgoing_in = keepgoing_out;\n          bool b_in = b_out;\n  \n          /* User-defined code (loop body) */\n          int my_local = a + b_in; // Reading value \\\"a\\\" from the enclosing scope is fine\n          b_out = a - b_in;\n          keepgoing_out = my_local > b_out;\n          user_defined_val = b_in + b_in; // b_in and b_out are different variables\n          /* End user-defined code */\n  \n          /* Implicitly defined-code */\n          user_defined_vals[i] = user_defined_val // accumulate scan-output values\n        }\n        // int t = my_local; // Can't do this. my_local is not accessible here.\n  \n        // The values below are bound to the output variables of the loop and therefore accessible\n        // b_out; user_defined_vals; keepgoing_out;\n      }\n  \n  There are several things of note in this code snippet:\n  \n  1) Values from the enclosing scope (i.e. variable \\\"a\\\" here) are in scope and can\n     be referenced in the inputs of the loop.\n  2) Any values computed in the loop body that needs to be used in a subsequent\n     iteration or after the loop are modelled using a pair of variables in the loop-body,\n     consisting of an input variable (eg., b_in) and an output variable (eg., b_out).\n     These are referred to as loop-carried dependences. The loop operation node\n     supplies the input value of the input variable for the first iteration, and\n     returns the output value of the output variable produced by the final\n     iteration.\n  3) Scan_output variables are used to implicitly concatenate values computed across\n     all the iterations. In the above example, the value of user_defined_val computed\n     over all iterations are concatenated and returned as the value of user_defined_vals\n     after the loop.\n  4) Values created in the body cannot be accessed in the enclosing scope,\n     except using the mechanism described above.\n  \n  Note that the semantics of this op support \\\"diagonal\\\" or \\\"wavefront\\\" execution.\n  (See Step 3 here for an example:\n  https://devblogs.nvidia.com/optimizing-recurrent-neural-networks-cudnn-5/).\n  Frontends should emit multi-layer RNNs as a series of While operators (with\n  time being the inner looping dimension), with each successive layer consuming\n  the scan_outputs from the previous layer, possibly going through several\n  point-wise operators (e.g. dropout, residual connections, linear layer).\n  \n  The input/output of subgraph (produced by loop node) matching is based on order instead of name. The implementation will figure out the names based on this order.",
    "inputs": [
      { "name": "M", "type": "AnyTypeOf" },
      { "name": "cond", "type": "AnyTypeOf" },
      { "name": "v_initial", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "v_final_and_scan_outputs", "type": "Variadic" }
    ]
  },
  {
    "name": "onnx.LpNormalization",
    "summary": "ONNX LpNormalization operation",
    "description": "Given a matrix, apply Lp-normalization along the provided axis.",
    "inputs": [
      { "name": "input", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "axis", "type": "DefaultValuedAttr" },
      { "name": "p", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "onnx.LpPool",
    "summary": "ONNX LpPool operation",
    "description": "LpPool consumes an input tensor X and applies Lp pooling across\n   the tensor according to kernel sizes, stride sizes, and pad lengths.\n   Lp pooling consisting of computing the Lp norm on all values of a subset\n   of the input tensor according to the kernel size and downsampling the\n   data into the output tensor Y for further processing. The output spatial shape will be following:\n   ```\n   output_spatial_shape[i] = floor((input_spatial_shape[i] + pad_shape[i] - {kernelSpatialShape}) / strides_spatial_shape[i] + 1)\n   ```\n   or\n   ```\n   output_spatial_shape[i] = ceil((input_spatial_shape[i] + pad_shape[i] - {kernelSpatialShape}) / strides_spatial_shape[i] + 1)\n   ```\n   if ceil_mode is enabled `pad_shape[i]` is the sum of pads along axis `i`.\n  \n   `auto_pad` is a DEPRECATED attribute. If you are using them currently, the output spatial shape will be following:\n   ```\n   VALID: output_spatial_shape[i] = ceil((input_spatial_shape[i] - {kernelSpatialShape} + 1) / strides_spatial_shape[i])\n   SAME_UPPER or SAME_LOWER: output_spatial_shape[i] = ceil(input_spatial_shape[i] / strides_spatial_shape[i])\n   ```\n   And pad shape will be following if `SAME_UPPER` or `SAME_LOWER`:\n   ```\n   pad_shape[i] = (output_spatial_shape[i] - 1) * strides_spatial_shape[i] + {kernelSpatialShape} - input_spatial_shape[i]\n   ```",
    "inputs": [
      { "name": "X", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "Y", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "auto_pad", "type": "DefaultValuedStrAttr" },
      { "name": "ceil_mode", "type": "DefaultValuedAttr" },
      { "name": "dilations", "type": "OptionalAttr" },
      { "name": "kernel_shape", "type": "I64ArrayAttr" },
      { "name": "p", "type": "DefaultValuedAttr" },
      { "name": "pads", "type": "OptionalAttr" },
      { "name": "strides", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "onnx.LRN",
    "summary": "ONNX LRN operation",
    "description": "Local Response Normalization proposed in the [AlexNet paper](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf).\n  It normalizes over local input regions.\n  The local region is defined across the channels. For an element `X[n, c, d1, ..., dk]` in a tensor\n  of shape `(N x C x D1 x D2, ..., Dk)`, its region is\n  `{X[n, i, d1, ..., dk] | max(0, c - floor((size - 1) / 2)) <= i <= min(C - 1, c + ceil((size - 1) / 2))}`.\n  \n  `square_sum[n, c, d1, ..., dk] = sum(X[n, i, d1, ..., dk] ^ 2)`,\n  where `max(0, c - floor((size - 1) / 2)) <= i <= min(C - 1, c + ceil((size - 1) / 2))`.\n  \n  `Y[n, c, d1, ..., dk] = X[n, c, d1, ..., dk] / (bias + alpha / size * square_sum[n, c, d1, ..., dk] ) ^ beta`",
    "inputs": [
      { "name": "X", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "Y", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "alpha", "type": "DefaultValuedAttr" },
      { "name": "beta", "type": "DefaultValuedAttr" },
      { "name": "bias", "type": "DefaultValuedAttr" },
      { "name": "size", "type": "SI64Attr" }
    ]
  },
  {
    "name": "onnx.LSTM",
    "summary": "ONNX LSTM operation",
    "description": "Computes an one-layer LSTM. This operator is usually supported via some\n  custom implementation such as CuDNN.\n  \n  Notations:\n  \n  * `X` - input tensor\n  * `i` - input gate\n  * `o` - output gate\n  * `f` - forget gate\n  * `c` - cell gate\n  * `t` - time step (t-1 means previous time step)\n  * `W[iofc]` - W parameter weight matrix for input, output, forget, and cell gates\n  * `R[iofc]` - R recurrence weight matrix for input, output, forget, and cell gates\n  * `Wb[iofc]` - W bias vectors for input, output, forget, and cell gates\n  * `Rb[iofc]` - R bias vectors for input, output, forget, and cell gates\n  * `P[iof]`  - P peephole weight vector for input, output, and forget gates\n  * `WB[iofc]` - W parameter weight matrix for backward input, output, forget, and cell gates\n  * `RB[iofc]` - R recurrence weight matrix for backward input, output, forget, and cell gates\n  * `WBb[iofc]` - W bias vectors for backward input, output, forget, and cell gates\n  * `RBb[iofc]` - R bias vectors for backward input, output, forget, and cell gates\n  * `PB[iof]`  - P peephole weight vector for backward input, output, and forget gates\n  * `H` - Hidden state\n  * `num_directions` - 2 if direction == bidirectional else 1\n  \n  Activation functions:\n  \n  * Relu(x)                - max(0, x)\n  * Tanh(x)                - (1 - e^{-2x})/(1 + e^{-2x})\n  * Sigmoid(x)             - 1/(1 + e^{-x})\n  \n  NOTE: Below are optional\n  \n  * Affine(x)              - alpha*x + beta\n  * LeakyRelu(x)           - x if x >= 0 else alpha * x\n  * ThresholdedRelu(x)     - x if x >= alpha else 0\n  * ScaledTanh(x)          - alpha*Tanh(beta*x)\n  * HardSigmoid(x)         - min(max(alpha*x + beta, 0), 1)\n  * Elu(x)                 - x if x >= 0 else alpha*(e^x - 1)\n  * Softsign(x)            - x/(1 + |x|)\n  * Softplus(x)            - log(1 + e^x)\n  \n  Equations (Default: f=Sigmoid, g=Tanh, h=Tanh):\n  \n  * it = f(Xt*(Wi^T) + Ht-1*(Ri^T) + Pi (.) Ct-1 + Wbi + Rbi)\n  * ft = f(Xt*(Wf^T) + Ht-1*(Rf^T) + Pf (.) Ct-1 + Wbf + Rbf)\n  * ct = g(Xt*(Wc^T) + Ht-1*(Rc^T) + Wbc + Rbc)\n  * Ct = ft (.) Ct-1 + it (.) ct\n  * ot = f(Xt*(Wo^T) + Ht-1*(Ro^T) + Po (.) Ct + Wbo + Rbo)\n  * Ht = ot (.) h(Ct)\n  This operator has **optional** inputs/outputs. See [the doc](IR.md) for more details about the representation of optional arguments. An empty string may be used in the place of an actual argument's name to indicate a missing argument. Trailing optional arguments (those not followed by an argument that is present) may also be simply omitted.",
    "inputs": [
      { "name": "X", "type": "AnyTypeOf" },
      { "name": "W", "type": "AnyTypeOf" },
      { "name": "R", "type": "AnyTypeOf" },
      { "name": "B", "type": "AnyTypeOf" },
      { "name": "sequence_lens", "type": "AnyTypeOf" },
      { "name": "initial_h", "type": "AnyTypeOf" },
      { "name": "initial_c", "type": "AnyTypeOf" },
      { "name": "P", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "Y", "type": "AnyTypeOf" },
      { "name": "Y_h", "type": "AnyTypeOf" },
      { "name": "Y_c", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "activation_alpha", "type": "OptionalAttr" },
      { "name": "activation_beta", "type": "OptionalAttr" },
      { "name": "activations", "type": "OptionalAttr" },
      { "name": "clip", "type": "OptionalAttr" },
      { "name": "direction", "type": "DefaultValuedStrAttr" },
      { "name": "hidden_size", "type": "OptionalAttr" },
      { "name": "input_forget", "type": "DefaultValuedAttr" },
      { "name": "layout", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "onnx.MatMul",
    "summary": "ONNX MatMul operation",
    "description": "Matrix product that behaves like [numpy.matmul](https://numpy.org/doc/stable/reference/generated/numpy.matmul.html).",
    "inputs": [
      { "name": "A", "type": "AnyTypeOf" },
      { "name": "B", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "Y", "type": "AnyTypeOf" }
    ]
  },
  {
    "name": "onnx.MatMulInteger",
    "summary": "ONNX MatMulInteger operation",
    "description": "Matrix product that behaves like [numpy.matmul](https://numpy.org/doc/stable/reference/generated/numpy.matmul.html).\n  The production MUST never overflow. The accumulation may overflow if and only if in 32 bits.",
    "inputs": [
      { "name": "A", "type": "AnyTypeOf" },
      { "name": "B", "type": "AnyTypeOf" },
      { "name": "a_zero_point", "type": "AnyTypeOf" },
      { "name": "b_zero_point", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "Y", "type": "TensorOf" }
    ]
  },
  {
    "name": "onnx.Max",
    "summary": "ONNX Max operation",
    "description": "Element-wise max of each of the input tensors (with Numpy-style broadcasting support).\n  All inputs and outputs must have the same data type.\n  This operator supports **multidirectional (i.e., Numpy-style) broadcasting**; for more details please check [the doc](Broadcasting.md).",
    "inputs": [
      { "name": "data_0", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "max", "type": "AnyTypeOf" }
    ]
  },
  {
    "name": "onnx.MaxPool",
    "summary": "ONNX MaxPool operation",
    "description": "MaxPool consumes an input tensor X and applies max pooling across\n   the tensor according to kernel sizes, stride sizes, and pad lengths.\n   max pooling consisting of computing the max on all values of a\n   subset of the input tensor according to the kernel size and downsampling the\n   data into the output tensor Y for further processing. The output spatial shape is calculated differently\n   depending on whether explicit padding is used, where pads is employed, or auto padding is used, where auto_pad is utilized.\n   With explicit padding (https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html?highlight=maxpool#torch.nn.MaxPool2d):\n   ```\n   output_spatial_shape[i] = floor((input_spatial_shape[i] + pad_shape[i] - dilation[i] * (kernel_shape[i] - 1) - 1) / strides_spatial_shape[i] + 1)\n   ```\n   or\n   ```\n   output_spatial_shape[i] = ceil((input_spatial_shape[i] + pad_shape[i] - dilation[i] * (kernel_shape[i] - 1) - 1) / strides_spatial_shape[i] + 1)\n   ```\n   if ceil_mode is enabled. `pad_shape[i]` is the sum of pads along axis `i`. Sliding windows that would start in the right padded region are ignored.\n  \n   `auto_pad` is a DEPRECATED attribute. If you are using them currently, the output spatial shape will be following when ceil_mode is enabled:\n   ```\n   VALID: output_spatial_shape[i] = ceil((input_spatial_shape[i] - ((kernel_spatial_shape[i] - 1) * dilations[i] + 1) + 1) / strides_spatial_shape[i])\n   SAME_UPPER or SAME_LOWER: output_spatial_shape[i] = ceil(input_spatial_shape[i] / strides_spatial_shape[i])\n   ```\n   or when ceil_mode is disabled (https://www.tensorflow.org/api_docs/python/tf/keras/layers/AveragePooling2D):\n   ```\n   VALID: output_spatial_shape[i] = floor((input_spatial_shape[i] - ((kernel_spatial_shape[i] - 1) * dilations[i] + 1)) / strides_spatial_shape[i]) + 1\n   SAME_UPPER or SAME_LOWER: output_spatial_shape[i] = floor((input_spatial_shape[i] - 1) / strides_spatial_shape[i]) + 1\n   ```\n   And pad shape will be following if `SAME_UPPER` or `SAME_LOWER`:\n   ```\n   pad_shape[i] = (output_spatial_shape[i] - 1) * strides_spatial_shape[i] + ((kernel_spatial_shape[i] - 1) * dilations[i] + 1) - input_spatial_shape[i]\n   ```\n   The output of each pooling window is maximum number of elements exclude pad.",
    "inputs": [
      { "name": "X", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "Y", "type": "AnyTypeOf" },
      { "name": "Indices", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "auto_pad", "type": "DefaultValuedStrAttr" },
      { "name": "ceil_mode", "type": "DefaultValuedAttr" },
      { "name": "dilations", "type": "OptionalAttr" },
      { "name": "kernel_shape", "type": "I64ArrayAttr" },
      { "name": "pads", "type": "OptionalAttr" },
      { "name": "storage_order", "type": "DefaultValuedAttr" },
      { "name": "strides", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "onnx.MaxPoolSingleOut",
    "summary": "ONNX MaxPool operation with a single output.",
    "description": "ONNX MaxPool operation with a single output.\n    See ONNXMaxPoolOp for a full description of the MaxPool semantics.\n\n    This operation is not part of the standard and was added to assist onnx-mlir.",
    "inputs": [
      { "name": "X", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "o_Y", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "auto_pad", "type": "DefaultValuedStrAttr" },
      { "name": "ceil_mode", "type": "DefaultValuedAttr" },
      { "name": "dilations", "type": "OptionalAttr" },
      { "name": "kernel_shape", "type": "DefaultValuedAttr" },
      { "name": "pads", "type": "OptionalAttr" },
      { "name": "storage_order", "type": "DefaultValuedAttr" },
      { "name": "strides", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "onnx.MaxRoiPool",
    "summary": "ONNX MaxRoiPool operation",
    "description": "ROI max pool consumes an input tensor X and region of interests (RoIs) to\n   apply max pooling across each RoI, to produce output 4-D tensor of shape\n   (num_rois, channels, pooled_shape[0], pooled_shape[1]).",
    "inputs": [
      { "name": "X", "type": "AnyTypeOf" },
      { "name": "rois", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "Y", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "pooled_shape", "type": "I64ArrayAttr" },
      { "name": "spatial_scale", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "onnx.MaxUnpool",
    "summary": "ONNX MaxUnpool operation",
    "description": "MaxUnpool essentially computes the partial inverse of the MaxPool op.\n   The input information to this op is typically the output information from a MaxPool op. The first\n   input tensor X is the tensor that needs to be unpooled, which is typically the pooled tensor (first output)\n   from MaxPool. The second input tensor, I, contains the indices to the (locally maximal) elements corresponding\n   to the elements in the first input tensor X. Input tensor I is typically the second output of the MaxPool op.\n   The third (optional) input is a tensor that specifies the output size of the unpooling operation.\n  \n  MaxUnpool is intended to do 'partial' inverse of the MaxPool op. 'Partial' because all the non-maximal\n   values from the original input to MaxPool are set to zero in the output of the MaxUnpool op. Pooling\n   the result of an unpooling operation should give back the original input to the unpooling op.\n  \n  MaxUnpool can produce the same output size for several input sizes, which makes unpooling op ambiguous.\n   The third input argument, output_size, is meant to disambiguate the op and produce output tensor of\n   known/predictable size.\n  \n  In addition to the inputs, MaxUnpool takes three attributes, namely kernel_shape, strides, and pads,\n   which define the exact unpooling op. The attributes typically have the same values as the corresponding\n   pooling op that the unpooling op is trying to invert.",
    "inputs": [
      { "name": "X", "type": "AnyTypeOf" },
      { "name": "I", "type": "TensorOf" },
      { "name": "output_shape", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "kernel_shape", "type": "I64ArrayAttr" },
      { "name": "pads", "type": "OptionalAttr" },
      { "name": "strides", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "onnx.Mean",
    "summary": "ONNX Mean operation",
    "description": "Element-wise mean of each of the input tensors (with Numpy-style broadcasting support).\n  All inputs and outputs must have the same data type.\n  This operator supports **multidirectional (i.e., Numpy-style) broadcasting**; for more details please check [the doc](Broadcasting.md).",
    "inputs": [
      { "name": "data_0", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "mean", "type": "AnyTypeOf" }
    ]
  },
  {
    "name": "onnx.MeanVarianceNormalization",
    "summary": "ONNX MeanVarianceNormalization operation",
    "description": "A MeanVarianceNormalization Function: Perform mean variance normalization\n        on the input tensor X using formula: `(X-EX)/sqrt(E(X-EX)^2)`",
    "inputs": [
      { "name": "X", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "Y", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "axes", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "onnx.MelWeightMatrix",
    "summary": "ONNX MelWeightMatrix operation",
    "description": "Generate a MelWeightMatrix that can be used to re-weight a Tensor containing a linearly sampled frequency spectra (from DFT or STFT) into num_mel_bins frequency information based on the [lower_edge_hertz, upper_edge_hertz] range on the mel scale.\n  This function defines the mel scale in terms of a frequency in hertz according to the following formula:\n  \n      mel(f) = 2595 * log10(1 + f/700)\n  \n  In the returned matrix, all the triangles (filterbanks) have a peak value of 1.0.\n  \n  The returned MelWeightMatrix can be used to right-multiply a spectrogram S of shape [frames, num_spectrogram_bins] of linear scale spectrum values (e.g. STFT magnitudes) to generate a \\\"mel spectrogram\\\" M of shape [frames, num_mel_bins].",
    "inputs": [
      { "name": "num_mel_bins", "type": "AnyTypeOf" },
      { "name": "dft_length", "type": "AnyTypeOf" },
      { "name": "sample_rate", "type": "AnyTypeOf" },
      { "name": "lower_edge_hertz", "type": "AnyTypeOf" },
      { "name": "upper_edge_hertz", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "output_datatype", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "onnx.Min",
    "summary": "ONNX Min operation",
    "description": "Element-wise min of each of the input tensors (with Numpy-style broadcasting support).\n  All inputs and outputs must have the same data type.\n  This operator supports **multidirectional (i.e., Numpy-style) broadcasting**; for more details please check [the doc](Broadcasting.md).",
    "inputs": [
      { "name": "data_0", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "min", "type": "AnyTypeOf" }
    ]
  },
  {
    "name": "onnx.Mish",
    "summary": "ONNX Mish operation",
    "description": "Mish: A Self Regularized Non-Monotonic Neural Activation Function.\n  \n  Perform the linear unit element-wise on the input tensor X using formula:\n  \n  ```\n  mish(x) = x * tanh(softplus(x)) = x * tanh(ln(1 + e^{x}))\n  ```",
    "inputs": [
      { "name": "X", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "Y", "type": "AnyTypeOf" }
    ]
  },
  {
    "name": "onnx.Mod",
    "summary": "ONNX Mod operation",
    "description": "Performs element-wise binary modulus (with Numpy-style broadcasting support).\n    The sign of the remainder is the same as that of the Divisor.\n  \n    Mod operator can also behave like C fmod() or numpy.fmod. In this case, the sign of the remainder however, will be the same as the Dividend\n    (in contrast to integer mod). To force a behavior like numpy.fmod() an 'fmod' Attribute is provided.\n    This attribute is set to 0 by default causing the behavior to be like integer mod.\n    Setting this attribute to 1 causes the remainder to be calculated similar to that of numpy.fmod().\n  \n    If the input type is floating point, then `fmod` attribute must be set to 1.\n  \n    In case of dividend being zero, the results will be platform dependent.\n  \n    This operator supports **multidirectional (i.e., Numpy-style) broadcasting**; for more details please check [the doc](Broadcasting.md).",
    "inputs": [
      { "name": "A", "type": "AnyTypeOf" },
      { "name": "B", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "C", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "fmod", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "onnx.Momentum",
    "summary": "ONNX Momentum operation",
    "description": "Compute one iteration of stochastic gradient update with momentum.\n      This operator can conduct the optimization of multiple tensor variables.\n  \n      Let's define the behavior of this operator. As you can imagine, SG with momentum requires\n      several parameters:\n  \n       - The learning-rate \\\"R\\\".\n       - The update count \\\"T\\\". That is, the number of conducted training iterations. It should\n         be zero in the first training iteration.\n       - A L2-norm regularization coefficient \\\"norm_coefficient\\\".\n       - A decay coefficient of previous accumulated gradient (i.e., momentum) \\\"alpha\\\".\n       - The scaling coefficient of current gradient \\\"beta\\\".\n       - An attribute to choose either standard momentum or Nesterov's momentum \\\"mode\\\" should\n         be used.\n  \n      For the sake of simplicity, assume that there is only one tensor (called \\\"X\\\") to be optimized.\n      Other necessary inputs are \\\"X\\\"'s gradient (called \\\"G\\\") and \\\"X\\\"'s momentum (called \\\"V\\\"). This\n      Momentum operator maps all these inputs to the new value of \\\"X\\\" (called \\\"X_new\\\") and its new\n      momentum (called \\\"V_new\\\").\n  \n      This operator supports two different momentum algorithms. Set the attribute \\\"mode\\\" to\n      \\\"nesterov\\\" if Nesterov's momentum is desired. Otherwise, set the attribute \\\"model\\\" to\n      \\\"standard\\\" to use standard momentum. Computation details are described subsequently.\n  \n      Let \\\"+\\\", \\\"-\\\", \\\"*\\\", and \\\"/\\\" are all element-wise operations with numpy-style broadcasting.\n  \n      Pseudo code for SG with standard momentum:\n  \n        // Add gradient of 0.5 * norm_coefficient * ||X||^2, where ||X|| is the sum of squared\n        // values of all elements in X.\n        G_regularized = norm_coefficient * X + G\n  \n        // In the first training iteration, beta should always be 1.\n        beta_adjusted = T > 0 ? beta : 1\n  \n        // Compute the current momentum based on previous momentum and the current gradient.\n        V_new = alpha * V + beta_adjusted * G_regularized\n  \n        // Update X.\n        X_new = X - R * V_new\n  \n      Pseudo code for SG with Nesterov's momentum:\n  \n        // Add gradient of 0.5 * norm_coefficient * ||X||^2, where ||X|| is the sum of squared\n        // values of all elements in X.\n        G_regularized = norm_coefficient * X + G;\n  \n        // In the first training iteration, beta should always be 1.\n        beta_adjusted = T > 0 ? beta : 1\n  \n        // Compute the current momentum based on previous momentum and the current gradient.\n        V_new = alpha * V + beta_adjusted * G_regularized;\n  \n        // Compute final update direction and then update X.\n        X_new = X - R * (G_regularized + alpha * V_new)\n  \n      If one assign this operators to optimize multiple inputs, for example, \\\"X_1\\\" and \\\"X_2\\\". The same\n      pseudo code would be extended to handle all tensors jointly. More specifically, we can view \\\"X\\\" as a\n      concatenation of \\\"X_1\\\" and \\\"X_2\\\" (of course, their gradient and accumulate gradient should\n      be concatenated too) and then our pseudo code becomes applicable.",
    "inputs": [
      { "name": "R", "type": "AnyTypeOf" },
      { "name": "T", "type": "TensorOf" },
      { "name": "inputs", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "outputs", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "alpha", "type": "F32Attr" },
      { "name": "beta", "type": "F32Attr" },
      { "name": "mode", "type": "StrAttr" },
      { "name": "norm_coefficient", "type": "F32Attr" }
    ]
  },
  {
    "name": "onnx.Mul",
    "summary": "ONNX Mul operation",
    "description": "Performs element-wise binary multiplication (with Numpy-style broadcasting support).\n  \n  This operator supports **multidirectional (i.e., Numpy-style) broadcasting**; for more details please check [the doc](Broadcasting.md).\n  \n  (Opset 14 change): Extend supported types to include uint8, int8, uint16, and int16.",
    "inputs": [
      { "name": "A", "type": "AnyTypeOf" },
      { "name": "B", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "C", "type": "AnyTypeOf" }
    ]
  },
  {
    "name": "onnx.Multinomial",
    "summary": "ONNX Multinomial operation",
    "description": "Generate a tensor of samples from a multinomial distribution according to the probabilities\n  of each of the possible outcomes.",
    "inputs": [
      { "name": "input", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "dtype", "type": "DefaultValuedAttr" },
      { "name": "sample_size", "type": "DefaultValuedAttr" },
      { "name": "seed", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "onnx.Neg",
    "summary": "ONNX Neg operation",
    "description": "Neg takes one input data (Tensor<T>) and produces one output data\n  (Tensor<T>) where each element flipped sign, y = -x, is applied to\n  the tensor elementwise.",
    "inputs": [
      { "name": "X", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "Y", "type": "AnyTypeOf" }
    ]
  },
  {
    "name": "onnx.NegativeLogLikelihoodLoss",
    "summary": "ONNX NegativeLogLikelihoodLoss operation",
    "description": "A NegativeLogLikelihoodLoss operator computes (weighted) negative log likelihood loss.\n  Its \\\"input\\\" tensor has the shape of (N, C, d1, d2, ..., dk) where k >= 0.\n  The \\\"input\\\" tensor contains log-probabilities for input[n, :, d_1, d_2,..., d_k] being in a class of [0, C).\n  The operator's \\\"target\\\" input tensor has the shape of (N, d1, d2, ..., dk). It encodes class labels (one of C classes)\n  or it may contain a special value (indicated by an attribute ignore_index) for N x d1 x d2 x ... x dk samples.\n  The loss value for input[n, :, d_1, d_2,...d_k] being classified as class c = target[n][d_1][d_2]...[d_k] is computed as:\n  \n  ```\n  loss[n][d_1][d_2]...[d_k] = -input[n][c][d_1][d_2]...[d_k].\n  ```\n  \n  When an optional \\\"weight\\\" is provided, the sample loss is calculated as:\n  \n  ```\n  loss[n][d_1][d_2]...[d_k] = -input[n][c][d_1][d_2]...[d_k] * weight[c].\n  ```\n  \n  loss is zero for the case when target-value equals ignore_index.\n  \n  ```\n  loss[n][d_1][d_2]...[d_k] = 0, when target[n][d_1][d_2]...[d_k] = ignore_index\n  ```\n  \n  If \\\"reduction\\\" attribute is set to \\\"none\\\", the operator's output will be the above loss with shape (N, d1, d2, ..., dk).\n  If \\\"reduction\\\" attribute is set to \\\"mean\\\" (the default attribute value), the output loss is (weight) averaged:\n  \n  ```\n  mean(loss), if \\\"weight\\\" is not provided,\n  ```\n  \n  or if weight is provided,\n  \n  ```\n  sum(loss) / sum(weight[target[n][d_1][d_2]...[d_k]]]), for all samples.\n  ```\n  \n  If \\\"reduction\\\" attribute is set to \\\"sum\\\", the output is a scalar: `sum(loss)`.\n  \n  See also https://pytorch.org/docs/stable/nn.html#torch.nn.NLLLoss.\n  \n  Example 1:\n  \n  ```\n  // negative log likelihood loss, \\\"none\\\" reduction\n  N, C, d1 = 2, 3, 2\n  input = [[[1.0, 2.0], [2.0, 2.0], [3.0, 2.0]],\n            [[0.0, 1.0], [2.0, 2.0], [1.0, 2]]]\n  target = [[2, 1], [0, 2]]\n  \n  loss = np.zeros((N, d1))\n  for n in range(N):\n      for d_1 in range(d1):\n          c = target[n][d_1]\n          loss[n][d_1] = -input[n][c][d_1]\n  \n  // print(loss)\n  // [[-3. -2.]\n  //  [-0. -2.]]\n  ```\n  \n  Example 2:\n  \n  ```\n  // weighted negative log likelihood loss, sum reduction\n  N, C, d1 = 2, 3, 2\n  input = [[[1.0, 2.0], [2.0, 2.0], [3.0, 2.0]],\n          [[0.0, 1.0], [2.0, 2.0], [1.0, 2]]]\n  target = [[2, 1], [0, 2]]\n  weight = [0.2, 0.3, 0.1]\n  loss = np.zeros((N, d1))\n  for n in range(N):\n      for d_1 in range(d1):\n          c = target[n][d_1]\n          loss[n][d_1] = -input[n][c][d_1] * weight[c]\n  \n  loss = np.sum(loss)\n  // print(loss)\n  // -1.1\n  ```\n  \n  Example 3:\n  \n  ```\n  // weighted negative log likelihood loss, mean reduction\n  N, C, d1 = 2, 3, 2\n  input = [[[1.0, 2.0], [2.0, 2.0], [3.0, 2.0]],\n          [[0.0, 1.0], [2.0, 2.0], [1.0, 2]]]\n  target = [[2, 1], [0, 2]]\n  weight = [0.2, 0.3, 0.1]\n  loss = np.zeros((N, d1))\n  weight_total = 0\n  for n in range(N):\n      for d_1 in range(d1):\n          c = target[n][d_1]\n          loss[n][d_1] = -input[n][c][d_1] * weight[c]\n          weight_total = weight_total + weight[c]\n  \n  loss = np.sum(loss) / weight_total\n  // print(loss)\n  // -1.57\n  ```",
    "inputs": [
      { "name": "input", "type": "AnyTypeOf" },
      { "name": "target", "type": "AnyTypeOf" },
      { "name": "weight", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "loss", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "ignore_index", "type": "OptionalAttr" },
      { "name": "reduction", "type": "DefaultValuedStrAttr" }
    ]
  },
  {
    "name": "onnx.NonMaxSuppression",
    "summary": "ONNX NonMaxSuppression operation",
    "description": "Filter out boxes that have high intersection-over-union (IOU) overlap with previously selected boxes.\n  Bounding boxes with score less than score_threshold are removed. Bounding box format is indicated by attribute center_point_box.\n  Note that this algorithm is agnostic to where the origin is in the coordinate system and more generally is invariant to\n  orthogonal transformations and translations of the coordinate system; thus translating or reflections of the coordinate system\n  result in the same boxes being selected by the algorithm.\n  The selected_indices output is a set of integers indexing into the input collection of bounding boxes representing the selected boxes.\n  The bounding box coordinates corresponding to the selected indices can then be obtained using the Gather or GatherND operation.",
    "inputs": [
      { "name": "boxes", "type": "TensorOf" },
      { "name": "scores", "type": "TensorOf" },
      { "name": "max_output_boxes_per_class", "type": "AnyTypeOf" },
      { "name": "iou_threshold", "type": "AnyTypeOf" },
      { "name": "score_threshold", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "selected_indices", "type": "TensorOf" }
    ],
    "attributes": [
      { "name": "center_point_box", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "onnx.NonZero",
    "summary": "ONNX NonZero operation",
    "description": "Returns the indices of the elements that are non-zero\n      (in row-major order - by dimension).\n      NonZero behaves similar to numpy.nonzero:\n      https://docs.scipy.org/doc/numpy/reference/generated/numpy.nonzero.html,\n      but for scalar input, NonZero produces output shape (0, N) instead of (1, N), which is different from Numpy's behavior.",
    "inputs": [
      { "name": "X", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "Y", "type": "TensorOf" }
    ]
  },
  {
    "name": "onnx.Normalizer",
    "summary": "ONNX Normalizer operation",
    "description": "Normalize the input.  There are three normalization modes, which have the corresponding formulas,\n      defined using element-wise infix operators '/' and '^' and tensor-wide functions 'max' and 'sum':<br>\n  <br>\n      Max: Y = X / max(X)<br>\n      L1:  Y = X / sum(X)<br>\n      L2:  Y = sqrt(X^2 / sum(X^2)}<br>\n      In all modes, if the divisor is zero, Y == X.\n  <br>\n      For batches, that is, [N,C] tensors, normalization is done along the C axis. In other words, each row\n      of the batch is normalized independently.",
    "inputs": [
      { "name": "X", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "Y", "type": "TensorOf" }
    ],
    "attributes": [
      { "name": "norm", "type": "DefaultValuedStrAttr" }
    ]
  },
  {
    "name": "onnx.Not",
    "summary": "ONNX Not operation",
    "description": "Returns the negation of the input tensor element-wise.",
    "inputs": [
      { "name": "X", "type": "TensorOf" }
    ],
    "outputs": [
      { "name": "Y", "type": "TensorOf" }
    ]
  },
  {
    "name": "onnx.NoValue",
    "summary": "An operation representing the absence of a value.",
    "description": "This operation can be used to represent the absence of a value. It is typically\n    used as an argument to operators that have optional parameters.\n\n    Example:\n    ```MLIR\n      %cst = \"onnx.NoValue\"() {value} : () -> none\n      %0, %1 = \"onnx.Split\"(%arg0, %cst) { axis=1 : si64 } : (tensor<?xf32>, none) -> (tensor<*xf32>, tensor<*xf32>)\n    ```\n\n    This operation is not part of the standard and was added to assist onnx-mlir.",
    "outputs": [
      { "name": "none_val", "type": "NoneType" }
    ],
    "attributes": [
      { "name": "value", "type": "UnitAttr" }
    ]
  },
  {
    "name": "onnx.OneHot",
    "summary": "ONNX OneHot operation",
    "description": "Produces a one-hot tensor based on inputs.\n      The locations represented by the index values in the 'indices' input tensor will have 'on_value'\n      and the other locations will have 'off_value' in the output tensor, where 'on_value' and 'off_value'\n      are specified as part of required input argument 'values', which is a two-element tensor of format\n      [off_value, on_value]. The rank of the output tensor will be one greater than the rank of the\n      input tensor. The additional dimension is for one-hot representation. The additional dimension will\n      be inserted at the position specified by 'axis'. If 'axis' is not specified then then additional\n      dimension will be inserted as the innermost dimension, i.e. axis=-1. The size of the additional\n      dimension is specified by required scalar input 'depth'. The type of the output tensor is the same\n      as the type of the 'values' input. Any entries in the 'indices' input tensor with values outside\n      the range [-depth, depth-1] will result in one-hot representation with all 'off_value' values in the\n      output tensor.\n  \n      when axis = 0:\n      output[input[i, j, k], i, j, k] = 1 for all i, j, k and 0 otherwise.\n  \n      when axis = -1:\n      output[i, j, k, input[i, j, k]] = 1 for all i, j, k and 0 otherwise.",
    "inputs": [
      { "name": "indices", "type": "AnyTypeOf" },
      { "name": "depth", "type": "AnyTypeOf" },
      { "name": "values", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "axis", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "onnx.OneHotEncoder",
    "summary": "ONNX OneHotEncoder operation",
    "description": "Replace each input element with an array of ones and zeros, where a single\n      one is placed at the index of the category that was passed in. The total category count\n      will determine the size of the extra dimension of the output array Y.<br>\n      For example, if we pass a tensor with a single value of 4, and a category count of 8,\n      the output will be a tensor with ``[0,0,0,0,1,0,0,0]``.<br>\n      This operator assumes every input feature is from the same set of categories.<br>\n      If the input is a tensor of float, int32, or double, the data will be cast\n      to integers and the cats_int64s category list will be used for the lookups.",
    "inputs": [
      { "name": "X", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "Y", "type": "TensorOf" }
    ],
    "attributes": [
      { "name": "cats_int64s", "type": "OptionalAttr" },
      { "name": "cats_strings", "type": "OptionalAttr" },
      { "name": "zeros", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "onnx.Optional",
    "summary": "ONNX Optional operation",
    "description": "Constructs an optional-type value containing either an empty optional of a certain type specified by the attribute,\n  or a non-empty value containing the input element.",
    "inputs": [
      { "name": "input", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "type", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "onnx.OptionalGetElement",
    "summary": "ONNX OptionalGetElement operation",
    "description": "If the input is a tensor or sequence type, it returns the input.\n  If the input is an optional type, it outputs the element in the input.\n  It is an error if the input is an empty optional-type (i.e. does not have an element) and the behavior is undefined in this case.",
    "inputs": [
      { "name": "input", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTypeOf" }
    ]
  },
  {
    "name": "onnx.OptionalHasElement",
    "summary": "ONNX OptionalHasElement operation",
    "description": "Returns true if (1) the input is an optional-type and contains an element,\n  or, (2) the input is a tensor or sequence type.\n  If the input is not provided or is an empty optional-type, this op returns false.",
    "inputs": [
      { "name": "input", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "output", "type": "TensorOf" }
    ]
  },
  {
    "name": "onnx.Or",
    "summary": "ONNX Or operation",
    "description": "Returns the tensor resulted from performing the `or` logical operation\n  elementwise on the input tensors `A` and `B` (with Numpy-style broadcasting support).\n  \n  This operator supports **multidirectional (i.e., Numpy-style) broadcasting**; for more details please check [the doc](Broadcasting.md).",
    "inputs": [
      { "name": "A", "type": "TensorOf" },
      { "name": "B", "type": "TensorOf" }
    ],
    "outputs": [
      { "name": "C", "type": "TensorOf" }
    ]
  },
  {
    "name": "onnx.Pad",
    "summary": "ONNX Pad operation",
    "description": "Given a tensor containing the data to be padded (`data`), a tensor containing the number of start and end pad values for axis (`pads`), (optionally) a `mode`, and (optionally) `constant_value`,\n  a padded tensor (`output`) is generated.\n  \n  The three supported `modes` are (similar to corresponding modes supported by `numpy.pad`):\n  \n  1) `constant`(default) - pads with a given constant value as specified by `constant_value` (which defaults to 0, empty string, or False)\n  \n  2) `reflect` - pads with the reflection of the vector mirrored on the first and last values of the vector along each axis\n  \n  3) `edge` - pads with the edge values of array\n  \n  4) `wrap` - wrap-around padding as if the data tensor forms a torus\n  \n  \n  Example 1 (`constant` mode):\n  \n  Insert 0 pads to the beginning of the second dimension.\n  \n  ```\n  data = [\n      [1.0, 1.2],\n      [2.3, 3.4],\n      [4.5, 5.7],\n  ]\n  \n  pads = [0, 2, 0, 0]\n  \n  mode = 'constant'\n  \n  constant_value = 0.0\n  \n  output = [\n      [0.0, 0.0, 1.0, 1.2],\n      [0.0, 0.0, 2.3, 3.4],\n      [0.0, 0.0, 4.5, 5.7],\n  ]\n  ```\n  \n  Example 2 (`reflect` mode):\n  \n  ```\n  data = [\n      [1.0, 1.2],\n      [2.3, 3.4],\n      [4.5, 5.7],\n  ]\n  \n  pads = [0, 2, 0, 0]\n  \n  mode = 'reflect'\n  \n  output = [\n      [1.0, 1.2, 1.0, 1.2],\n      [2.3, 3.4, 2.3, 3.4],\n      [4.5, 5.7, 4.5, 5.7],\n  ]\n  ```\n  \n  Example 3 (`edge` mode):\n  \n  ```\n  data = [\n      [1.0, 1.2],\n      [2.3, 3.4],\n      [4.5, 5.7],\n  ]\n  \n  pads = [0, 2, 0, 0]\n  \n  mode = 'edge'\n  \n  output = [\n      [1.0, 1.0, 1.0, 1.2],\n      [2.3, 2.3, 2.3, 3.4],\n      [4.5, 4.5, 4.5, 5.7],\n  ]\n  ```\n  \n  Example 4 (`wrap` mode):\n  \n  ```\n  data = [\n      [1.0, 1.2],\n      [2.3, 3.4],\n      [4.5, 5.7],\n  ]\n  \n  pads = [2, 1, 1, 1]\n  \n  mode = 'wrap'\n  \n  output = [\n      [3.4, 2.3, 3.4, 2.3],\n      [5.7, 4.5, 5.7, 4.5],\n      [1.2, 1.0, 1.2, 1.0],\n      [3.4, 2.3, 3.4, 2.3],\n      [5.7, 4.5, 5.7, 4.5],\n      [1.2, 1.0, 1.2, 1.0],\n  ]\n  ```",
    "inputs": [
      { "name": "data", "type": "AnyTypeOf" },
      { "name": "pads", "type": "TensorOf" },
      { "name": "constant_value", "type": "AnyTypeOf" },
      { "name": "axes", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "mode", "type": "DefaultValuedStrAttr" }
    ],
    "category": "Transform"
  },
  {
    "name": "onnx.PadV11",
    "summary": "ONNX Pad operation",
    "description": "Given a tensor containing the data to be padded (`data`), a tensor containing the number of start and end pad values for axis (`pads`), (optionally) a `mode`, and (optionally) `constant_value`,\n  a padded tensor (`output`) is generated.\n  \n  The three supported `modes` are (similar to corresponding modes supported by `numpy.pad`):\n  \n  1) `constant`(default) - pads with a given constant value as specified by `constant_value` (which defaults to 0)\n  \n  2) `reflect` - pads with the reflection of the vector mirrored on the first and last values of the vector along each axis\n  \n  3) `edge` - pads with the edge values of array\n  \n  \n  Example 1 (`constant` mode):\n    Insert 0 pads to the beginning of the second dimension.\n  \n    data =\n    [\n        [1.0, 1.2],\n        [2.3, 3.4],\n        [4.5, 5.7],\n    ]\n  \n    pads = [0, 2, 0, 0]\n  \n    mode = 'constant'\n  \n    constant_value = 0.0\n  \n    output =\n    [\n        [0.0, 0.0, 1.0, 1.2],\n        [0.0, 0.0, 2.3, 3.4],\n        [0.0, 0.0, 4.5, 5.7],\n    ]\n  \n  \n  Example 2 (`reflect` mode):\n    data =\n    [\n        [1.0, 1.2],\n        [2.3, 3.4],\n        [4.5, 5.7],\n    ]\n  \n    pads = [0, 2, 0, 0]\n  \n    mode = 'reflect'\n  \n    output =\n    [\n        [1.0, 1.2, 1.0, 1.2],\n        [2.3, 3.4, 2.3, 3.4],\n        [4.5, 5.7, 4.5, 5.7],\n    ]\n  \n  \n  Example 3 (`edge` mode):\n    data =\n    [\n        [1.0, 1.2],\n        [2.3, 3.4],\n        [4.5, 5.7],\n    ]\n  \n    pads = [0, 2, 0, 0]\n  \n    mode = 'edge'\n  \n    output =\n    [\n        [1.0, 1.0, 1.0, 1.2],\n        [2.3, 2.3, 2.3, 3.4],\n        [4.5, 4.5, 4.5, 5.7],\n    ]",
    "inputs": [
      { "name": "data", "type": "AnyTypeOf" },
      { "name": "pads", "type": "TensorOf" },
      { "name": "constant_value", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "mode", "type": "DefaultValuedStrAttr" }
    ]
  },
  {
    "name": "onnx.PadV13",
    "summary": "ONNX Pad operation",
    "description": "Given a tensor containing the data to be padded (`data`), a tensor containing the number of start and end pad values for axis (`pads`), (optionally) a `mode`, and (optionally) `constant_value`,\n  a padded tensor (`output`) is generated.\n  \n  The three supported `modes` are (similar to corresponding modes supported by `numpy.pad`):\n  \n  1) `constant`(default) - pads with a given constant value as specified by `constant_value` (which defaults to 0, empty string, or False)\n  \n  2) `reflect` - pads with the reflection of the vector mirrored on the first and last values of the vector along each axis\n  \n  3) `edge` - pads with the edge values of array\n  \n  \n  Example 1 (`constant` mode):\n    Insert 0 pads to the beginning of the second dimension.\n  \n    data =\n    [\n        [1.0, 1.2],\n        [2.3, 3.4],\n        [4.5, 5.7],\n    ]\n  \n    pads = [0, 2, 0, 0]\n  \n    mode = 'constant'\n  \n    constant_value = 0.0\n  \n    output =\n    [\n        [0.0, 0.0, 1.0, 1.2],\n        [0.0, 0.0, 2.3, 3.4],\n        [0.0, 0.0, 4.5, 5.7],\n    ]\n  \n  \n  Example 2 (`reflect` mode):\n    data =\n    [\n        [1.0, 1.2],\n        [2.3, 3.4],\n        [4.5, 5.7],\n    ]\n  \n    pads = [0, 2, 0, 0]\n  \n    mode = 'reflect'\n  \n    output =\n    [\n        [1.0, 1.2, 1.0, 1.2],\n        [2.3, 3.4, 2.3, 3.4],\n        [4.5, 5.7, 4.5, 5.7],\n    ]\n  \n  \n  Example 3 (`edge` mode):\n    data =\n    [\n        [1.0, 1.2],\n        [2.3, 3.4],\n        [4.5, 5.7],\n    ]\n  \n    pads = [0, 2, 0, 0]\n  \n    mode = 'edge'\n  \n    output =\n    [\n        [1.0, 1.0, 1.0, 1.2],\n        [2.3, 2.3, 2.3, 3.4],\n        [4.5, 4.5, 4.5, 5.7],\n    ]",
    "inputs": [
      { "name": "data", "type": "AnyTypeOf" },
      { "name": "pads", "type": "TensorOf" },
      { "name": "constant_value", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "mode", "type": "DefaultValuedStrAttr" }
    ]
  },
  {
    "name": "onnx.PadV18",
    "summary": "ONNX Pad operation",
    "description": "Given a tensor containing the data to be padded (`data`), a tensor containing the number of start and end pad values for axis (`pads`), (optionally) a `mode`, and (optionally) `constant_value`,\n  a padded tensor (`output`) is generated.\n  \n  The three supported `modes` are (similar to corresponding modes supported by `numpy.pad`):\n  \n  1) `constant`(default) - pads with a given constant value as specified by `constant_value` (which defaults to 0, empty string, or False)\n  \n  2) `reflect` - pads with the reflection of the vector mirrored on the first and last values of the vector along each axis\n  \n  3) `edge` - pads with the edge values of array\n  \n  \n  Example 1 (`constant` mode):\n  \n  Insert 0 pads to the beginning of the second dimension.\n  \n  ```\n  data = [\n      [1.0, 1.2],\n      [2.3, 3.4],\n      [4.5, 5.7],\n  ]\n  \n  pads = [0, 2, 0, 0]\n  \n  mode = 'constant'\n  \n  constant_value = 0.0\n  \n  output = [\n      [0.0, 0.0, 1.0, 1.2],\n      [0.0, 0.0, 2.3, 3.4],\n      [0.0, 0.0, 4.5, 5.7],\n  ]\n  ```\n  \n  Example 2 (`reflect` mode):\n  \n  ```\n  data = [\n      [1.0, 1.2],\n      [2.3, 3.4],\n      [4.5, 5.7],\n  ]\n  \n  pads = [0, 2, 0, 0]\n  \n  mode = 'reflect'\n  \n  output = [\n      [1.0, 1.2, 1.0, 1.2],\n      [2.3, 3.4, 2.3, 3.4],\n      [4.5, 5.7, 4.5, 5.7],\n  ]\n  ```\n  \n  Example 3 (`edge` mode):\n  \n  ```\n  data = [\n      [1.0, 1.2],\n      [2.3, 3.4],\n      [4.5, 5.7],\n  ]\n  \n  pads = [0, 2, 0, 0]\n  \n  mode = 'edge'\n  \n  output = [\n      [1.0, 1.0, 1.0, 1.2],\n      [2.3, 2.3, 2.3, 3.4],\n      [4.5, 4.5, 4.5, 5.7],\n  ]\n  ```",
    "inputs": [
      { "name": "data", "type": "AnyTypeOf" },
      { "name": "pads", "type": "TensorOf" },
      { "name": "constant_value", "type": "AnyTypeOf" },
      { "name": "axes", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "mode", "type": "DefaultValuedStrAttr" }
    ]
  },
  {
    "name": "onnx.PadV2",
    "summary": "ONNX Pad operation",
    "description": "Given `data` tensor, pads, mode, and value.\n  Example:\n    Insert 0 pads to the beginning of the second dimension.\n    data = [\n        [1.0, 1.2],\n        [2.3, 3.4],\n        [4.5, 5.7],\n    ]\n    pads = [0, 2, 0, 0]\n    output = [\n        [\n            [0.0, 0.0, 1.0, 1.2],\n            [0.0, 0.0, 2.3, 3.4],\n            [0.0, 0.0, 4.5, 5.7],\n        ],\n    ]",
    "inputs": [
      { "name": "data", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "mode", "type": "DefaultValuedStrAttr" },
      { "name": "pads", "type": "I64ArrayAttr" },
      { "name": "value", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "onnx.Pow",
    "summary": "ONNX Pow operation",
    "description": "Pow takes input data (Tensor<T>) and exponent Tensor, and\n  produces one output data (Tensor<T>) where the function `f(x) = x^exponent`,\n  is applied to the data tensor elementwise.\n  This operator supports **multidirectional (i.e., Numpy-style) broadcasting**; for more details please check [the doc](Broadcasting.md).",
    "inputs": [
      { "name": "X", "type": "AnyTypeOf" },
      { "name": "Y", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "Z", "type": "AnyTypeOf" }
    ]
  },
  {
    "name": "onnx.PRelu",
    "summary": "ONNX PRelu operation",
    "description": "PRelu takes input data (Tensor<T>) and slope tensor as input, and produces one\n  output data (Tensor<T>) where the function `f(x) = slope * x for x < 0`,\n  `f(x) = x for x >= 0`., is applied to the data tensor elementwise.\n  This operator supports **unidirectional broadcasting** (tensor slope should be unidirectional broadcastable to input tensor X); for more details please check [the doc](Broadcasting.md).",
    "inputs": [
      { "name": "X", "type": "AnyTypeOf" },
      { "name": "slope", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "Y", "type": "AnyTypeOf" }
    ]
  },
  {
    "name": "onnx.PrintSignature",
    "summary": "ONNX Op to print type signature or data of its input operands",
    "description": "Print type signature or data of the input operands of this op.\n    The parameter op_name specifies a string to be printed before the tensors.\n    and usually the op_name and onnx_node_name are used.\n    This operation is introduced early so as to preserve the name of the original ONNX op.\n    The argument print_data control whether the data of the tensors to be printed.\n    When print_data == 1, the data of the tensor will be printed. Otherwise, just shape.\n    The argument input specifies the tensor to be printed. They could be a list\n    of the inputs and outputs of an ONNX op.\n\n    This operation is not part of the standard and was added to assist onnx-mlir.",
    "inputs": [
      { "name": "input", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "op_name", "type": "StrAttr" },
      { "name": "print_data", "type": "SI64Attr" }
    ]
  },
  {
    "name": "onnx.QLinearConv",
    "summary": "ONNX QLinearConv operation",
    "description": "The convolution operator consumes a quantized input tensor, its scale and zero point,\n  a quantized filter, its scale and zero point, and output's scale and zero point,\n  and computes the quantized output. Each scale and zero-point pair must have same shape.\n  It means they must be either scalars (per tensor) or 1-D tensors (per output channel).\n  Each input or output and its related zero point must have same type.\n  When bias is present it must be quantized using scale = input scale * weight scale and\n  zero point as 0.",
    "inputs": [
      { "name": "x", "type": "AnyTypeOf" },
      { "name": "x_scale", "type": "TensorOf" },
      { "name": "x_zero_point", "type": "AnyTypeOf" },
      { "name": "w", "type": "AnyTypeOf" },
      { "name": "w_scale", "type": "TensorOf" },
      { "name": "w_zero_point", "type": "AnyTypeOf" },
      { "name": "y_scale", "type": "TensorOf" },
      { "name": "y_zero_point", "type": "AnyTypeOf" },
      { "name": "B", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "y", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "auto_pad", "type": "DefaultValuedStrAttr" },
      { "name": "dilations", "type": "OptionalAttr" },
      { "name": "group", "type": "DefaultValuedAttr" },
      { "name": "kernel_shape", "type": "OptionalAttr" },
      { "name": "pads", "type": "OptionalAttr" },
      { "name": "strides", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "onnx.QLinearMatMul",
    "summary": "ONNX QLinearMatMul operation",
    "description": "Matrix product that behaves like [numpy.matmul](https://numpy.org/doc/stable/reference/generated/numpy.matmul.html).\n  It consumes two quantized input tensors, their scales and zero points, scale and zero point of output,\n  and computes the quantized output. The quantization formula is y = saturate((x / y_scale) + y_zero_point).\n  For (x / y_scale), it is rounding to nearest ties to even. Refer to https://en.wikipedia.org/wiki/Rounding for details.\n  Scale and zero point must have same shape. They must be either scalar (per tensor) or N-D tensor\n  (per row for 'a' and per column for 'b'). Scalar refers to per tensor quantization whereas N-D refers to per row\n  or per column quantization. If the input is 2D of shape [M, K] then zero point and scale tensor may be\n  an M element vector [v_1, v_2, ..., v_M] for per row quantization and K element vector of shape [v_1, v_2, ..., v_K]\n  for per column quantization. If the input is N-D tensor with shape [D1, D2, M, K] then zero point and scale tensor may\n  have shape [D1, D2, M, 1] for per row quantization and shape [D1, D2, 1, K] for per column quantization.\n  Production must never overflow, and accumulation may overflow if and only if in 32 bits.",
    "inputs": [
      { "name": "a", "type": "AnyTypeOf" },
      { "name": "a_scale", "type": "TensorOf" },
      { "name": "a_zero_point", "type": "AnyTypeOf" },
      { "name": "b", "type": "AnyTypeOf" },
      { "name": "b_scale", "type": "TensorOf" },
      { "name": "b_zero_point", "type": "AnyTypeOf" },
      { "name": "y_scale", "type": "TensorOf" },
      { "name": "y_zero_point", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "y", "type": "AnyTypeOf" }
    ]
  },
  {
    "name": "onnx.QuantizeLinear",
    "summary": "ONNX QuantizeLinear operation",
    "description": "The linear quantization operator. It consumes a high precision tensor, a scale, and a zero point to compute the low precision / quantized tensor.\n  The scale factor and zero point must have same shape, and can be either a scalar for per-tensor / per layer quantization, or a 1-D tensor for per-axis quantization.\n  The quantization formula is `y = saturate ((x / y_scale) + y_zero_point)`.\n  For saturation, it saturates to [0, 255] if it's uint8, or [-128, 127] if it's int8.\n  For (x / y_scale), it's rounding to the nearest even. Refer to https://en.wikipedia.org/wiki/Rounding for details.\n  'y_zero_point' and 'y' must have same type.\n  'y_zero_point' is usually not used for quantization to float8e4m3fn, float8e4m3fnuz, float8e5m2, float8e5m2fnuz,\n  but the quantization formula remains the same for consistency and\n  the type of the attribute 'y_zero_point' still determines the quantization type.",
    "inputs": [
      { "name": "x", "type": "AnyTypeOf" },
      { "name": "y_scale", "type": "AnyTypeOf" },
      { "name": "y_zero_point", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "y", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "axis", "type": "DefaultValuedAttr" },
      { "name": "saturate", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "onnx.RandomNormal",
    "summary": "ONNX RandomNormal operation",
    "description": "Generate a tensor with random values drawn from a normal distribution. The shape\n  of the tensor is specified by the `shape` argument and the parameter of the normal distribution\n  specified by `mean` and `scale`.\n  \n  The data type is specified by the 'dtype' argument. The 'dtype' argument must\n  be one of the data types specified in the 'DataType' enum field in the\n  TensorProto message.",
    "outputs": [
      { "name": "output", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "dtype", "type": "DefaultValuedAttr" },
      { "name": "mean", "type": "DefaultValuedAttr" },
      { "name": "scale", "type": "DefaultValuedAttr" },
      { "name": "seed", "type": "OptionalAttr" },
      { "name": "shape", "type": "I64ArrayAttr" }
    ]
  },
  {
    "name": "onnx.RandomNormalLike",
    "summary": "ONNX RandomNormalLike operation",
    "description": "Generate a tensor with random values drawn from a normal distribution.\n  The shape of the output tensor is copied from the shape of the input tensor,\n  and the parameters of the normal distribution are specified by `mean` and `scale`.\n  \n  The data type is specified by the 'dtype' argument, or copied from the input tensor if not provided.\n  The 'dtype' argument must be one of the data types specified in the 'DataType' enum field in the\n  TensorProto message, and be valid as an output type.",
    "inputs": [
      { "name": "input", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "dtype", "type": "OptionalAttr" },
      { "name": "mean", "type": "DefaultValuedAttr" },
      { "name": "scale", "type": "DefaultValuedAttr" },
      { "name": "seed", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "onnx.RandomUniform",
    "summary": "ONNX RandomUniform operation",
    "description": "Generate a tensor with random values drawn from a uniform distribution. The shape\n  of the tensor is specified by the `shape` argument and the range by `low` and `high`.\n  \n  The data type is specified by the 'dtype' argument. The 'dtype' argument must\n  be one of the data types specified in the 'DataType' enum field in the\n  TensorProto message.",
    "outputs": [
      { "name": "output", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "dtype", "type": "DefaultValuedAttr" },
      { "name": "high", "type": "DefaultValuedAttr" },
      { "name": "low", "type": "DefaultValuedAttr" },
      { "name": "seed", "type": "OptionalAttr" },
      { "name": "shape", "type": "I64ArrayAttr" }
    ]
  },
  {
    "name": "onnx.RandomUniformLike",
    "summary": "ONNX RandomUniformLike operation",
    "description": "Generate a tensor with random values drawn from a uniform distribution.\n  The shape of the output tensor is copied from the shape of the input tensor,\n  and the parameters of the uniform distribution are specified by `low` and `high`.\n  \n  The data type is specified by the 'dtype' argument, or copied from the input tensor if not provided.\n  The 'dtype' argument must be one of the data types specified in the 'DataType' enum field in the\n  TensorProto message and be valid as an output type.",
    "inputs": [
      { "name": "input", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "dtype", "type": "OptionalAttr" },
      { "name": "high", "type": "DefaultValuedAttr" },
      { "name": "low", "type": "DefaultValuedAttr" },
      { "name": "seed", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "onnx.Range",
    "summary": "ONNX Range operation",
    "description": "Generate a tensor containing a sequence of numbers that begin at `start` and extends by increments of `delta`\n  up to `limit` (exclusive).\n  \n  The number of elements in the output of range is computed as below:\n  \n  ```\n  number_of_elements = max( ceil( (limit - start) / delta ) , 0 )\n  ```\n  \n  The pseudocode determining the contents of the output is shown below:\n  \n  ```\n  for(int i=0; i<number_of_elements; ++i) {\n    output[i] =  start + (i * delta);\n  }\n  ```\n  \n  Example 1\n  \n  ```\n  Inputs: start = 3, limit = 9, delta = 3\n  Output: [3, 6]\n  ```\n  \n  Example 2\n  \n  ```\n  Inputs: start = 10, limit = 4, delta = -2\n  Output: [10, 8, 6]\n  ```",
    "inputs": [
      { "name": "start", "type": "AnyTypeOf" },
      { "name": "limit", "type": "AnyTypeOf" },
      { "name": "delta", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTypeOf" }
    ]
  },
  {
    "name": "onnx.Reciprocal",
    "summary": "ONNX Reciprocal operation",
    "description": "Reciprocal takes one input data (Tensor<T>) and produces one output data\n  (Tensor<T>) where the reciprocal is, y = 1/x, is applied to\n  the tensor elementwise.",
    "inputs": [
      { "name": "X", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "Y", "type": "AnyTypeOf" }
    ]
  },
  {
    "name": "onnx.ReduceL1",
    "summary": "ONNX ReduceL1 operation",
    "description": "Computes the L1 norm of the input tensor's elements along the provided axes. The resulting\n  tensor has the same rank as the input if `keepdims` equals 1. If `keepdims` equals 0, then\n  the resulting tensor has the reduced dimension pruned. Input tensors of rank zero are\n  valid. Reduction over an empty set of values yields 0.\n  \n  \n  The above behavior is similar to numpy, with the exception that numpy defaults `keepdims`\n  to `False` instead of `True`.",
    "inputs": [
      { "name": "data", "type": "AnyTypeOf" },
      { "name": "axes", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "reduced", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "keepdims", "type": "DefaultValuedAttr" },
      { "name": "noop_with_empty_axes", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "onnx.ReduceL1V13",
    "summary": "ONNX ReduceL1 operation",
    "description": "Computes the L1 norm of the input tensor's elements along the provided axes. The resulting\n  tensor has the same rank as the input if `keepdims` equals 1. If `keepdims` equals 0, then\n  the resulting tensor has the reduced dimension pruned. Input tensors of rank zero are\n  valid. Reduction over an empty set of values yields 0.\n  \n  \n  The above behavior is similar to numpy, with the exception that numpy defaults `keepdims`\n  to `False` instead of `True`.",
    "inputs": [
      { "name": "data", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "reduced", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "axes", "type": "OptionalAttr" },
      { "name": "keepdims", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "onnx.ReduceL2",
    "summary": "ONNX ReduceL2 operation",
    "description": "Computes the L2 norm of the input tensor's elements along the provided axes. The resulting\n  tensor has the same rank as the input if `keepdims` equals 1. If `keepdims` equals 0, then\n  the resulting tensor has the reduced dimension pruned. Input tensors of rank zero are\n  valid. Reduction over an empty set of values yields 0.\n  \n  \n  The above behavior is similar to numpy, with the exception that numpy defaults `keepdims`\n  to `False` instead of `True`.",
    "inputs": [
      { "name": "data", "type": "AnyTypeOf" },
      { "name": "axes", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "reduced", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "keepdims", "type": "DefaultValuedAttr" },
      { "name": "noop_with_empty_axes", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "onnx.ReduceL2V13",
    "summary": "ONNX ReduceL2 operation",
    "description": "Computes the L2 norm of the input tensor's elements along the provided axes. The resulting\n  tensor has the same rank as the input if `keepdims` equals 1. If `keepdims` equals 0, then\n  the resulting tensor has the reduced dimension pruned. Input tensors of rank zero are\n  valid. Reduction over an empty set of values yields 0.\n  \n  \n  The above behavior is similar to numpy, with the exception that numpy defaults `keepdims`\n  to `False` instead of `True`.",
    "inputs": [
      { "name": "data", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "reduced", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "axes", "type": "OptionalAttr" },
      { "name": "keepdims", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "onnx.ReduceLogSum",
    "summary": "ONNX ReduceLogSum operation",
    "description": "Computes the log sum of the input tensor's elements along the provided axes. The resulting\n  tensor has the same rank as the input if `keepdims` equals 1. If `keepdims` equals 0, then\n  the resulting tensor has the reduced dimension pruned. Input tensors of rank zero are\n  valid. Reduction over an empty set of values yields minus infinity (if supported by the datatype) or undefined otherwise.\n  \n  \n  The above behavior is similar to numpy, with the exception that numpy defaults `keepdims`\n  to `False` instead of `True`.",
    "inputs": [
      { "name": "data", "type": "AnyTypeOf" },
      { "name": "axes", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "reduced", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "keepdims", "type": "DefaultValuedAttr" },
      { "name": "noop_with_empty_axes", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "onnx.ReduceLogSumExp",
    "summary": "ONNX ReduceLogSumExp operation",
    "description": "Computes the log sum exponent of the input tensor's elements along the provided axes. The resulting\n  tensor has the same rank as the input if `keepdims` equals 1. If `keepdims` equals 0, then\n  the resulting tensor has the reduced dimension pruned. Input tensors of rank zero are\n  valid. Reduction over an empty set of values yields minus infinity (if supported by the datatype) or undefined otherwise.\n  \n  \n  The above behavior is similar to numpy, with the exception that numpy defaults `keepdims`\n  to `False` instead of `True`.",
    "inputs": [
      { "name": "data", "type": "AnyTypeOf" },
      { "name": "axes", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "reduced", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "keepdims", "type": "DefaultValuedAttr" },
      { "name": "noop_with_empty_axes", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "onnx.ReduceLogSumExpV13",
    "summary": "ONNX ReduceLogSumExp operation",
    "description": "Computes the log sum exponent of the input tensor's elements along the provided axes. The resulting\n  tensor has the same rank as the input if `keepdims` equals 1. If `keepdims` equals 0, then\n  the resulting tensor has the reduced dimension pruned. Input tensors of rank zero are\n  valid. Reduction over an empty set of values yields minus infinity (if supported by the datatype) or undefined otherwise.\n  \n  \n  The above behavior is similar to numpy, with the exception that numpy defaults `keepdims`\n  to `False` instead of `True`.",
    "inputs": [
      { "name": "data", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "reduced", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "axes", "type": "OptionalAttr" },
      { "name": "keepdims", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "onnx.ReduceLogSumV13",
    "summary": "ONNX ReduceLogSum operation",
    "description": "Computes the log sum of the input tensor's elements along the provided axes. The resulting\n  tensor has the same rank as the input if `keepdims` equals 1. If `keepdims` equals 0, then\n  the resulting tensor has the reduced dimension pruned. Input tensors of rank zero are\n  valid. Reduction over an empty set of values yields minus infinity (if supported by the datatype) or undefined otherwise.\n  \n  \n  The above behavior is similar to numpy, with the exception that numpy defaults `keepdims`\n  to `False` instead of `True`.",
    "inputs": [
      { "name": "data", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "reduced", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "axes", "type": "OptionalAttr" },
      { "name": "keepdims", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "onnx.ReduceMax",
    "summary": "ONNX ReduceMax operation",
    "description": "Computes the max of the input tensor's elements along the provided axes. The resulting\n  tensor has the same rank as the input if `keepdims` equals 1. If `keepdims` equals 0, then\n  the resulting tensor has the reduced dimension pruned. Input tensors of rank zero are\n  valid. Reduction over an empty set of values yields minus infinity (if supported by the datatype) or the minimum value of the data type otherwise.\n  \n  \n  If the input data type is Boolean, the comparison should consider `False < True`.\n  \n  The above behavior is similar to numpy, with the exception that numpy defaults `keepdims`\n  to `False` instead of `True`.",
    "inputs": [
      { "name": "data", "type": "AnyTypeOf" },
      { "name": "axes", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "reduced", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "keepdims", "type": "DefaultValuedAttr" },
      { "name": "noop_with_empty_axes", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "onnx.ReduceMaxV13",
    "summary": "ONNX ReduceMax operation",
    "description": "Computes the max of the input tensor's elements along the provided axes. The resulting\n  tensor has the same rank as the input if `keepdims` equals 1. If `keepdims` equals 0, then\n  the resulting tensor has the reduced dimension pruned. Input tensors of rank zero are\n  valid. Reduction over an empty set of values yields minus infinity (if supported by the datatype) or the minimum value of the data type otherwise.\n  \n  \n  The above behavior is similar to numpy, with the exception that numpy defaults `keepdims`\n  to `False` instead of `True`.",
    "inputs": [
      { "name": "data", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "reduced", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "axes", "type": "OptionalAttr" },
      { "name": "keepdims", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "onnx.ReduceMaxV18",
    "summary": "ONNX ReduceMax operation",
    "description": "Computes the max of the input tensor's elements along the provided axes. The resulting\n  tensor has the same rank as the input if `keepdims` equals 1. If `keepdims` equals 0, then\n  the resulting tensor has the reduced dimension pruned. Input tensors of rank zero are\n  valid. Reduction over an empty set of values yields minus infinity (if supported by the datatype) or the minimum value of the data type otherwise.\n  \n  \n  The above behavior is similar to numpy, with the exception that numpy defaults `keepdims`\n  to `False` instead of `True`.",
    "inputs": [
      { "name": "data", "type": "AnyTypeOf" },
      { "name": "axes", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "reduced", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "keepdims", "type": "DefaultValuedAttr" },
      { "name": "noop_with_empty_axes", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "onnx.ReduceMean",
    "summary": "ONNX ReduceMean operation",
    "description": "Computes the mean of the input tensor's elements along the provided axes. The resulting\n  tensor has the same rank as the input if `keepdims` equals 1. If `keepdims` equals 0, then\n  the resulting tensor has the reduced dimension pruned. Input tensors of rank zero are\n  valid. Reduction over an empty set of values yields undefined.\n  \n  \n  The above behavior is similar to numpy, with the exception that numpy defaults `keepdims`\n  to `False` instead of `True`.",
    "inputs": [
      { "name": "data", "type": "AnyTypeOf" },
      { "name": "axes", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "reduced", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "keepdims", "type": "DefaultValuedAttr" },
      { "name": "noop_with_empty_axes", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "onnx.ReduceMeanV13",
    "summary": "ONNX ReduceMean operation",
    "description": "Computes the mean of the input tensor's elements along the provided axes. The resulting\n  tensor has the same rank as the input if `keepdims` equals 1. If `keepdims` equals 0, then\n  the resulting tensor has the reduced dimension pruned. Input tensors of rank zero are\n  valid. Reduction over an empty set of values yields undefined.\n  \n  \n  The above behavior is similar to numpy, with the exception that numpy defaults `keepdims`\n  to `False` instead of `True`.",
    "inputs": [
      { "name": "data", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "reduced", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "axes", "type": "OptionalAttr" },
      { "name": "keepdims", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "onnx.ReduceMin",
    "summary": "ONNX ReduceMin operation",
    "description": "Computes the min of the input tensor's elements along the provided axes. The resulting\n  tensor has the same rank as the input if `keepdims` equals 1. If `keepdims` equals 0, then\n  the resulting tensor has the reduced dimension pruned. Input tensors of rank zero are\n  valid. Reduction over an empty set of values yields plus infinity (if supported by the datatype) or the maximum value of the data type otherwise.\n  \n  \n  If the input data type is Boolean, the comparison should consider `False < True`.\n  \n  The above behavior is similar to numpy, with the exception that numpy defaults `keepdims`\n  to `False` instead of `True`.",
    "inputs": [
      { "name": "data", "type": "AnyTypeOf" },
      { "name": "axes", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "reduced", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "keepdims", "type": "DefaultValuedAttr" },
      { "name": "noop_with_empty_axes", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "onnx.ReduceMinV13",
    "summary": "ONNX ReduceMin operation",
    "description": "Computes the min of the input tensor's elements along the provided axes. The resulting\n  tensor has the same rank as the input if `keepdims` equals 1. If `keepdims` equals 0, then\n  the resulting tensor has the reduced dimension pruned. Input tensors of rank zero are\n  valid. Reduction over an empty set of values yields plus infinity (if supported by the datatype) or the maximum value of the data type otherwise.\n  \n  \n  The above behavior is similar to numpy, with the exception that numpy defaults `keepdims`\n  to `False` instead of `True`.",
    "inputs": [
      { "name": "data", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "reduced", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "axes", "type": "OptionalAttr" },
      { "name": "keepdims", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "onnx.ReduceMinV18",
    "summary": "ONNX ReduceMin operation",
    "description": "Computes the min of the input tensor's elements along the provided axes. The resulting\n  tensor has the same rank as the input if `keepdims` equals 1. If `keepdims` equals 0, then\n  the resulting tensor has the reduced dimension pruned. Input tensors of rank zero are\n  valid. Reduction over an empty set of values yields plus infinity (if supported by the datatype) or the maximum value of the data type otherwise.\n  \n  \n  The above behavior is similar to numpy, with the exception that numpy defaults `keepdims`\n  to `False` instead of `True`.",
    "inputs": [
      { "name": "data", "type": "AnyTypeOf" },
      { "name": "axes", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "reduced", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "keepdims", "type": "DefaultValuedAttr" },
      { "name": "noop_with_empty_axes", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "onnx.ReduceProd",
    "summary": "ONNX ReduceProd operation",
    "description": "Computes the product of the input tensor's elements along the provided axes. The resulting\n  tensor has the same rank as the input if `keepdims` equals 1. If `keepdims` equals 0, then\n  the resulting tensor has the reduced dimension pruned. Input tensors of rank zero are\n  valid. Reduction over an empty set of values yields 1.\n  \n  \n  The above behavior is similar to numpy, with the exception that numpy defaults `keepdims`\n  to `False` instead of `True`.",
    "inputs": [
      { "name": "data", "type": "AnyTypeOf" },
      { "name": "axes", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "reduced", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "keepdims", "type": "DefaultValuedAttr" },
      { "name": "noop_with_empty_axes", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "onnx.ReduceProdV13",
    "summary": "ONNX ReduceProd operation",
    "description": "Computes the product of the input tensor's elements along the provided axes. The resulting\n  tensor has the same rank as the input if `keepdims` equals 1. If `keepdims` equals 0, then\n  the resulting tensor has the reduced dimension pruned. Input tensors of rank zero are\n  valid. Reduction over an empty set of values yields 1.\n  \n  \n  The above behavior is similar to numpy, with the exception that numpy defaults `keepdims`\n  to `False` instead of `True`.",
    "inputs": [
      { "name": "data", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "reduced", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "axes", "type": "OptionalAttr" },
      { "name": "keepdims", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "onnx.ReduceSum",
    "summary": "ONNX ReduceSum operation",
    "description": "Computes the sum of the input tensor's elements along the provided axes. The resulting\n  tensor has the same rank as the input if `keepdims` equals 1. If `keepdims` equals 0, then\n  the resulting tensor has the reduced dimension pruned. Input tensors of rank zero are\n  valid. Reduction over an empty set of values yields 0.\n  \n  \n  The above behavior is similar to numpy, with the exception that numpy defaults `keepdims`\n  to `False` instead of `True`.",
    "inputs": [
      { "name": "data", "type": "AnyTypeOf" },
      { "name": "axes", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "reduced", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "keepdims", "type": "DefaultValuedAttr" },
      { "name": "noop_with_empty_axes", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "onnx.ReduceSumSquare",
    "summary": "ONNX ReduceSumSquare operation",
    "description": "Computes the sum square of the input tensor's elements along the provided axes. The resulting\n  tensor has the same rank as the input if `keepdims` equals 1. If `keepdims` equals 0, then\n  the resulting tensor has the reduced dimension pruned. Input tensors of rank zero are\n  valid. Reduction over an empty set of values yields 0.\n  \n  \n  The above behavior is similar to numpy, with the exception that numpy defaults `keepdims`\n  to `False` instead of `True`.",
    "inputs": [
      { "name": "data", "type": "AnyTypeOf" },
      { "name": "axes", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "reduced", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "keepdims", "type": "DefaultValuedAttr" },
      { "name": "noop_with_empty_axes", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "onnx.ReduceSumSquareV13",
    "summary": "ONNX ReduceSumSquare operation",
    "description": "Computes the sum square of the input tensor's elements along the provided axes. The resulting\n  tensor has the same rank as the input if `keepdims` equals 1. If `keepdims` equals 0, then\n  the resulting tensor has the reduced dimension pruned. Input tensors of rank zero are\n  valid. Reduction over an empty set of values yields 0.\n  \n  \n  The above behavior is similar to numpy, with the exception that numpy defaults `keepdims`\n  to `False` instead of `True`.",
    "inputs": [
      { "name": "data", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "reduced", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "axes", "type": "OptionalAttr" },
      { "name": "keepdims", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "onnx.ReduceSumV11",
    "summary": "ONNX ReduceSum operation",
    "description": "Computes the sum of the input tensor's element along the provided axes. The resulting\n  tensor has the same rank as the input if keepdims equals 1. If keepdims equal 0, then\n  the resulted tensor have the reduced dimension pruned.\n  \n  The above behavior is similar to numpy, with the exception that numpy defaults keepdims to\n  False instead of True.",
    "inputs": [
      { "name": "data", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "reduced", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "axes", "type": "OptionalAttr" },
      { "name": "keepdims", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "onnx.Relu",
    "summary": "ONNX Relu operation",
    "description": "Relu takes one input data (Tensor<T>) and produces one output data\n  (Tensor<T>) where the rectified linear function, y = max(0, x), is applied to\n  the tensor elementwise.",
    "inputs": [
      { "name": "X", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "Y", "type": "AnyTypeOf" }
    ],
    "category": "Activation"
  },
  {
    "name": "onnx.Reshape",
    "summary": "ONNX Reshape operation",
    "description": "Reshape the input tensor similar to numpy.reshape.\n  First input is the data tensor, second input is a shape tensor which specifies the output shape. It outputs the reshaped tensor.\n  At most one dimension of the new shape can be -1. In this case, the value is\n  inferred from the size of the tensor and the remaining dimensions. A dimension\n  could also be 0, in which case the actual dimension value is unchanged (i.e. taken\n  from the input tensor). If 'allowzero' is set, and the new shape includes 0, the\n  dimension will be set explicitly to zero (i.e. not taken from input tensor).\n  Shape (second input) could be an empty shape, which means converting to a scalar.\n  The input tensor's shape and the output tensor's shape are required to have the same number of elements.\n  \n  If the attribute 'allowzero' is set, it is invalid for the specified shape to\n  contain both a zero value and -1, as the value of the dimension corresponding\n  to -1 cannot be determined uniquely.",
    "inputs": [
      { "name": "data", "type": "AnyTypeOf" },
      { "name": "shape", "type": "TensorOf" }
    ],
    "outputs": [
      { "name": "reshaped", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "allowzero", "type": "DefaultValuedAttr" }
    ],
    "category": "Shape"
  },
  {
    "name": "onnx.Resize",
    "summary": "ONNX Resize operation",
    "description": "Resize the input tensor. In general, it calculates every value in the output tensor as a weighted average of neighborhood (a.k.a. sampling locations) in the input tensor.\n  Each dimension value of the output tensor is:\n  ```\n  output_dimension = floor(input_dimension * (roi_end - roi_start) * scale)\n  ```\n  if input \\\\\"sizes\\\\\" is not specified.",
    "inputs": [
      { "name": "X", "type": "AnyTypeOf" },
      { "name": "roi", "type": "AnyTypeOf" },
      { "name": "scales", "type": "AnyTypeOf" },
      { "name": "sizes", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "Y", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "antialias", "type": "DefaultValuedAttr" },
      { "name": "axes", "type": "OptionalAttr" },
      { "name": "coordinate_transformation_mode", "type": "DefaultValuedStrAttr" },
      { "name": "cubic_coeff_a", "type": "DefaultValuedAttr" },
      { "name": "exclude_outside", "type": "DefaultValuedAttr" },
      { "name": "extrapolation_value", "type": "DefaultValuedAttr" },
      { "name": "keep_aspect_ratio_policy", "type": "DefaultValuedStrAttr" },
      { "name": "mode", "type": "DefaultValuedStrAttr" },
      { "name": "nearest_mode", "type": "DefaultValuedStrAttr" }
    ]
  },
  {
    "name": "onnx.ResizeV10",
    "summary": "ONNX Resize operation",
    "description": "Resize the input tensor.\n  Each dimension value of the output tensor is:\n    output_dimension = floor(input_dimension * scale).",
    "inputs": [
      { "name": "X", "type": "AnyTypeOf" },
      { "name": "scales", "type": "TensorOf" }
    ],
    "outputs": [
      { "name": "Y", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "mode", "type": "DefaultValuedStrAttr" }
    ]
  },
  {
    "name": "onnx.ResizeV11",
    "summary": "ONNX Resize operation",
    "description": "Resize the input tensor. In general, it calculates every value in the output tensor as a weighted average of neighborhood (a.k.a. sampling locations) in the input tensor.\n  Each dimension value of the output tensor is:\n    output_dimension = floor(input_dimension * (roi_end - roi_start) * scale) if input \\\\\"sizes\\\\\" is not specified.",
    "inputs": [
      { "name": "X", "type": "AnyTypeOf" },
      { "name": "roi", "type": "AnyTypeOf" },
      { "name": "scales", "type": "TensorOf" },
      { "name": "sizes", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "Y", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "coordinate_transformation_mode", "type": "DefaultValuedStrAttr" },
      { "name": "cubic_coeff_a", "type": "DefaultValuedAttr" },
      { "name": "exclude_outside", "type": "DefaultValuedAttr" },
      { "name": "extrapolation_value", "type": "DefaultValuedAttr" },
      { "name": "mode", "type": "DefaultValuedStrAttr" },
      { "name": "nearest_mode", "type": "DefaultValuedStrAttr" }
    ]
  },
  {
    "name": "onnx.ResizeV13",
    "summary": "ONNX Resize operation",
    "description": "Resize the input tensor. In general, it calculates every value in the output tensor as a weighted average of neighborhood (a.k.a. sampling locations) in the input tensor.\n  Each dimension value of the output tensor is:\n    output_dimension = floor(input_dimension * (roi_end - roi_start) * scale) if input \\\\\"sizes\\\\\" is not specified.",
    "inputs": [
      { "name": "X", "type": "AnyTypeOf" },
      { "name": "roi", "type": "AnyTypeOf" },
      { "name": "scales", "type": "AnyTypeOf" },
      { "name": "sizes", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "Y", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "coordinate_transformation_mode", "type": "DefaultValuedStrAttr" },
      { "name": "cubic_coeff_a", "type": "DefaultValuedAttr" },
      { "name": "exclude_outside", "type": "DefaultValuedAttr" },
      { "name": "extrapolation_value", "type": "DefaultValuedAttr" },
      { "name": "mode", "type": "DefaultValuedStrAttr" },
      { "name": "nearest_mode", "type": "DefaultValuedStrAttr" }
    ]
  },
  {
    "name": "onnx.ResizeV18",
    "summary": "ONNX Resize operation",
    "description": "Resize the input tensor. In general, it calculates every value in the output tensor as a weighted average of neighborhood (a.k.a. sampling locations) in the input tensor.\n  Each dimension value of the output tensor is: <br/>\n    `output_dimension = floor(input_dimension * (roi_end - roi_start) * scale)` <br/>\n  if input \\\\\"sizes\\\\\" is not specified.",
    "inputs": [
      { "name": "X", "type": "AnyTypeOf" },
      { "name": "roi", "type": "AnyTypeOf" },
      { "name": "scales", "type": "AnyTypeOf" },
      { "name": "sizes", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "Y", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "antialias", "type": "DefaultValuedAttr" },
      { "name": "axes", "type": "OptionalAttr" },
      { "name": "coordinate_transformation_mode", "type": "DefaultValuedStrAttr" },
      { "name": "cubic_coeff_a", "type": "DefaultValuedAttr" },
      { "name": "exclude_outside", "type": "DefaultValuedAttr" },
      { "name": "extrapolation_value", "type": "DefaultValuedAttr" },
      { "name": "keep_aspect_ratio_policy", "type": "DefaultValuedStrAttr" },
      { "name": "mode", "type": "DefaultValuedStrAttr" },
      { "name": "nearest_mode", "type": "DefaultValuedStrAttr" }
    ]
  },
  {
    "name": "onnx.Return",
    "summary": "Function return operation",
    "description": "The `onnx.Return` operation represents a return operation within a function.\n    The operation takes variable number of operands and produces no results.\n    The operand number and types must match the signature of the function\n    that contains the operation, with the exception that shaped types may have\n    more specific shapes than the function signature result types, which allows\n    rewrites of defining ops of operands to make their result shapes more specific.\n    This operation terminates a func::FuncOp in the ONNX dialect and is replaced\n    by func::ReturnOp in StandardFuncReturnPass before lowering to Krnl or other\n    dialects.",
    "inputs": [
      { "name": "operands", "type": "Variadic" }
    ],
    "assemblyFormat": "attr-dict ($operands^ `:` type($operands))?"
  },
  {
    "name": "onnx.ReverseSequence",
    "summary": "ONNX ReverseSequence operation",
    "description": "Reverse batch of sequences having different lengths specified by `sequence_lens`.\n  \n  For each slice i iterating on batch axis, the operator reverses the first sequence_lens[i] elements on time axis,\n  and copies elements whose index's beyond sequence_lens[i] to the output. So the output slice i contains reversed\n  sequences on the first sequence_lens[i] elements, then have original values copied for the other elements.\n  \n  Example 1:\n    input = [[0.0, 4.0, 8.0,  12.0],\n             [1.0, 5.0, 9.0,  13.0],\n             [2.0, 6.0, 10.0, 14.0],\n             [3.0, 7.0, 11.0, 15.0]]\n    sequence_lens = [4, 3, 2, 1]\n    time_axis = 0\n    batch_axis = 1\n  \n    output = [[3.0, 6.0, 9.0,  12.0],\n              [2.0, 5.0, 8.0,  13.0],\n              [1.0, 4.0, 10.0, 14.0],\n              [0.0, 7.0, 11.0, 15.0]]\n  \n  Example 2:\n    input = [[0.0,  1.0,  2.0,  3.0 ],\n             [4.0,  5.0,  6.0,  7.0 ],\n             [8.0,  9.0,  10.0, 11.0],\n             [12.0, 13.0, 14.0, 15.0]]\n    sequence_lens = [1, 2, 3, 4]\n    time_axis = 1\n    batch_axis = 0\n  \n    output = [[0.0,  1.0,  2.0,  3.0 ],\n              [5.0,  4.0,  6.0,  7.0 ],\n              [10.0, 9.0,  8.0,  11.0],\n              [15.0, 14.0, 13.0, 12.0]]",
    "inputs": [
      { "name": "input", "type": "AnyTypeOf" },
      { "name": "sequence_lens", "type": "TensorOf" }
    ],
    "outputs": [
      { "name": "Y", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "batch_axis", "type": "DefaultValuedAttr" },
      { "name": "time_axis", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "onnx.RMSLayerNormalization",
    "summary": "ONNX RMSLayerNormalization operation",
    "description": "This is RMS layer normalization defined in ONNX as function.\n        The overall computation can be split into two stages.\n        The first stage is an approximate standardization, which makes the\n        normalized elements have zero mean and unit variances.\n        See Equation (4) in [this paper](https://arxiv.org/pdf/1910.07467.pdf).\n        The computation required by standardization can be\n        described by the following equations.\n        ```\n        DD = Mul(X, X)\n        Var = ReduceMean<axes=normalized_axes>(DD)\n        VarEps = Add(Var, epsilon)\n        StdDev = Sqrt(VarEps)\n        InvStdDev = Reciprocal(StdDev)\n        Normalized = Mul(X, InvStdDev)\n        ```\n        where `normalized_axes` is `[axis, ..., rank of X - 1]`.\n        The variables `Var` and `StdDev` stand for approximate variance and\n        standard deviation, respectively.\n        Depending on `stash_type` attribute, the actual computation\n        must happen in different floating-point precision.\n        For example, if `stash_type` is 1, this operator casts\n        all input variables to 32-bit float, perform the computation, and\n        finally cast `Normalized` back to the original type of `X`.\n        The second stage then scales and shifts the outcome of the\n        first stage using\n        ```\n        NormalizedScaled = Mul(Normalized, Scale)\n        Y = Add(NormalizedScaled, B)\n        ```\n        The second stage doesn't depends on `stash_type`.\n        All equations are in [this syntax](https://github.com/onnx/onnx/blob/main/docs/Syntax.md).\n        The same variable (i.e., input, output, and attribute) uses\n        the same name in the equations above and this operator's definition.\n        Let `d[i]` indicate the i-th dimension of `X`.\n        If `X`'s shape is `[d[0], ..., d[axis-1], d[axis], ..., d[rank-1]]`,\n        the shape of `Mean` and `InvStdDev` is `[d[0], ..., d[axis-1], 1, ..., 1]`.\n        `Y` and `X` have the same shape.",
    "inputs": [
      { "name": "X", "type": "AnyTypeOf" },
      { "name": "Scale", "type": "AnyTypeOf" },
      { "name": "B", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "Y", "type": "AnyTypeOf" },
      { "name": "InvStdDev", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "axis", "type": "DefaultValuedAttr" },
      { "name": "epsilon", "type": "DefaultValuedAttr" },
      { "name": "stash_type", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "onnx.RNN",
    "summary": "ONNX RNN operation",
    "description": "Computes an one-layer simple RNN. This operator is usually supported\n  via some custom implementation such as CuDNN.\n  \n  Notations:\n  \n  * `X` - input tensor\n  * `i` - input gate\n  * `t` - time step (t-1 means previous time step)\n  * `Wi` - W parameter weight matrix for input gate\n  * `Ri` - R recurrence weight matrix for input gate\n  * `Wbi` - W parameter bias vector for input gate\n  * `Rbi` - R parameter bias vector for input gate\n  * `WBi` - W parameter weight matrix for backward input gate\n  * `RBi` - R recurrence weight matrix for backward input gate\n  * `WBbi` - WR bias vectors for backward input gate\n  * `RBbi` - RR bias vectors for backward input gate\n  * `H` - Hidden state\n  * `num_directions` - 2 if direction == bidirectional else 1\n  \n  Activation functions:\n  \n  * Relu(x)                - max(0, x)\n  * Tanh(x)                - (1 - e^{-2x})/(1 + e^{-2x})\n  * Sigmoid(x)             - 1/(1 + e^{-x})\n  \n  NOTE: Below are optional\n  \n  * Affine(x)              - alpha*x + beta\n  * LeakyRelu(x)           - x if x >= 0 else alpha * x\n  * ThresholdedRelu(x)     - x if x >= alpha else 0\n  * ScaledTanh(x)          - alpha*Tanh(beta*x)\n  * HardSigmoid(x)         - min(max(alpha*x + beta, 0), 1)\n  * Elu(x)                 - x if x >= 0 else alpha*(e^x - 1)\n  * Softsign(x)            - x/(1 + |x|)\n  * Softplus(x)            - log(1 + e^x)\n  \n  Equations (Default: f=Tanh):\n  \n  * Ht = f(Xt*(Wi^T) + Ht-1*(Ri^T) + Wbi + Rbi)\n  This operator has **optional** inputs/outputs. See [the doc](IR.md) for more details about the representation of optional arguments. An empty string may be used in the place of an actual argument's name to indicate a missing argument. Trailing optional arguments (those not followed by an argument that is present) may also be simply omitted.",
    "inputs": [
      { "name": "X", "type": "AnyTypeOf" },
      { "name": "W", "type": "AnyTypeOf" },
      { "name": "R", "type": "AnyTypeOf" },
      { "name": "B", "type": "AnyTypeOf" },
      { "name": "sequence_lens", "type": "AnyTypeOf" },
      { "name": "initial_h", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "Y", "type": "AnyTypeOf" },
      { "name": "Y_h", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "activation_alpha", "type": "OptionalAttr" },
      { "name": "activation_beta", "type": "OptionalAttr" },
      { "name": "activations", "type": "DefaultValuedAttr" },
      { "name": "clip", "type": "OptionalAttr" },
      { "name": "direction", "type": "DefaultValuedStrAttr" },
      { "name": "hidden_size", "type": "OptionalAttr" },
      { "name": "layout", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "onnx.RoiAlign",
    "summary": "ONNX RoiAlign operation",
    "description": "Region of Interest (RoI) align operation described in the\n  [Mask R-CNN paper](https://arxiv.org/abs/1703.06870).\n  RoiAlign consumes an input tensor X and region of interests (rois)\n  to apply pooling across each RoI; it produces a 4-D tensor of shape\n  (num_rois, C, output_height, output_width).\n  \n  RoiAlign is proposed to avoid the misalignment by removing\n  quantizations while converting from original image into feature\n  map and from feature map into RoI feature; in each ROI bin,\n  the value of the sampled locations are computed directly\n  through bilinear interpolation.",
    "inputs": [
      { "name": "X", "type": "AnyTypeOf" },
      { "name": "rois", "type": "AnyTypeOf" },
      { "name": "batch_indices", "type": "TensorOf" }
    ],
    "outputs": [
      { "name": "Y", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "coordinate_transformation_mode", "type": "DefaultValuedStrAttr" },
      { "name": "mode", "type": "DefaultValuedStrAttr" },
      { "name": "output_height", "type": "DefaultValuedAttr" },
      { "name": "output_width", "type": "DefaultValuedAttr" },
      { "name": "sampling_ratio", "type": "DefaultValuedAttr" },
      { "name": "spatial_scale", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "onnx.Round",
    "summary": "ONNX Round operation",
    "description": "Round takes one input Tensor and rounds the values, element-wise, meaning\n  it finds the nearest integer for each value.\n  In case of halves, the rule is to round them to the nearest even integer.\n  If input x is integral, +0, -0, NaN,  or infinite, x itself is returned.\n  The output tensor has the same shape and type as the input.\n  \n  Examples:\n  ```\n  round([0.9]) = [1.0]\n  round([2.5]) = [2.0]\n  round([2.3]) = [2.0]\n  round([1.5]) = [2.0]\n  round([-4.5]) = [-4.0]\n  ```",
    "inputs": [
      { "name": "X", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "Y", "type": "AnyTypeOf" }
    ]
  },
  {
    "name": "onnx.Scaler",
    "summary": "ONNX Scaler operation",
    "description": "Rescale input data, for example to standardize features by removing the mean and scaling to unit variance.",
    "inputs": [
      { "name": "X", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "Y", "type": "TensorOf" }
    ],
    "attributes": [
      { "name": "offset", "type": "OptionalAttr" },
      { "name": "scale", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "onnx.Scan",
    "summary": "ONNX Scan operation",
    "description": "Scan can be used to iterate over one or more scan_input tensors,\n  constructing zero or more scan_output tensors. It combines ideas from general recurrences,\n  functional programming constructs such as scan, fold, map, and zip, and is intended to enable\n  generalizations of RNN-like constructs for sequence-to-sequence processing.\n  Other tensors (referred to as state_variables here) can be used to carry a state\n  when iterating from one element to another (similar to hidden-state in RNNs, also referred\n  to as loop-carried dependences in the context of loops).\n  Many common usages involve a single scan_input tensor (where functionality\n  similar to scan, fold and map can be obtained). When more than one scan_input is used,\n  a behavior similar to zip is obtained.\n  \n  The attribute body must be a graph, specifying the computation to be performed in\n  every iteration. It takes as input the current values of the state_variables and\n  the current iterated element of the scan_inputs. It must return the (updated) values\n  of the state_variables and zero or more scan_output_element tensors. The values of the\n  scan_output_element tensors are concatenated over all the iterations to produce the\n  scan_output values of the scan construct (similar to the concatenated intermediate\n  hidden-state values of RNN-like constructs). All the output tensors (state_variables as\n  well as scan_output_element tensors) are required to have the same shape in each iteration\n  of the loop (a restriction imposed to enable efficient memory allocation).\n  \n  Note that the iterated element passed to the body subgraph does not have a sequence\n  axis. It will have a rank one less than the rank of the corresponding scan_input.\n  \n  The scan operation returns the final values of the state_variables as well as the\n  scan_outputs.\n  \n  The optional attribute scan_input_directions specifies the direction (forward or backward)\n  for each scan input. If this attribute is omitted, all sequences are scanned in the forward\n  direction. A bidirectional scan may be performed by specifying the same tensor input twice\n  in the scan_inputs, once with a forward direction, and once with a backward direction.\n  \n  The scan_output of the operation is produced by concatenating the scan_output_element\n  values produced by the body in each iteration.  The optional attribute scan_output_directions\n  specifies the direction in which scan_output is constructed (by appending or prepending the\n  scan_output_element to scan_output in each iteration) for each scan_output. If this attribute\n  is omitted, the scan_output_element is appended to the scan_output in each iteration.\n  \n  The optional attribute scan_input_axes specifies the axis to be scanned for each scan_input.\n  If omitted, every scan_input will be scanned in axis 0. For example, if axis 0 is the\n  batch axis and axis 1 is the time axis (to be scanned), specify an axis value of 1.\n  Note that scanning a non-zero axis may be less efficient than scanning axis zero.\n  \n  The optional attribute scan_output_axes specifies the axis along which the scan_outputs\n  are accumulated for each scan_output. For example, if axis 1 is the time axis (to be\n  scanned) for both inputs and outputs, specify a scan_input axis and scan_output axis\n  value of 1.\n  \n  Note that because of the ONNX restriction that only the last parameter of an operator can\n  be variadic, the initial-states and scan-inputs are listed together as one input parameter.\n  Similarly, the final-states and scan-outputs are listed together as one output parameter.\n  The attribute num_scan_inputs indicates the number M of scan-inputs.\n  \n  The behavior of\n  \n      Scan <\n          num_scan_inputs = m,\n          body = loop-body,\n          scan_input_axes = [axis_1, ..., axis_m]\n      > (init_1, ..., init_n, scan_1, ..., scan_m)\n  \n  is equivalent to the following pseudo-code:\n  \n      // scan_i.shape[axis_i] denotes the (max) sequence-length of scan_i\n      // scan_i.shape[axis_i] is required to be equal to scan_j.shape[axis_j] for all i,j.\n      sequence_length = scan_1.shape[axis_1];\n  \n      // initialize state-variables\n      st_1 = init_1; ... st_n = init_n;\n      // initialize scan-output variables: [] denotes an empty tensor\n      scan_out_1 = []; ...; scan_out_k = [];\n      // identify number of iterations:\n  \n      // execute loop\n      for (int t = 0; t < sequence_length; ++t) {\n          // generate the scan-input elements: the notation T<axis=k>[t] indicates the sub-tensor\n          // of rank one less than T obtained by indexing T at position t along axis k.\n          si_1 = scan_1<axis=axis_1>[t];\n          ... ;\n          si_m = scan_m<axis=axis_m>[t];\n          // execute loop-body\n          st_1, ..., st_n, so_1, ..., so_k = loop-body(st_1, ..., st_n, si_1, ..., si_m)\n          // accumulate the scan-output elements\n          scan_out_1 = Concat<axis=0>(scan_out_1, so_1); ... ; scan_out_k = Concat<axis=0>(scan_out_k, so_k);\n      }\n  \n      return st_1, ..., st_n, scan_out_1, ..., scan_out_k;\n  \n  *Sample usage: Encoding RNN using a Scan*\n  \n  The following example shows how a simple RNN over an input tensor %X, with weight tensor %Wi,\n  recurrence weight tensor %Ri, bias tensors %Wbi and %Rbi, and initial hidden-state %H_0 can\n  be encoded as a ScanLoop. Note that the loop-body is a nested graph, and it directly computes\n  %Wi, %Ri, %Wbi, and %Rbi (typically constants or initializers in the body graph). If these\n  values are computed in the outer graph, they need to be passed in as extra state_variables.\n  \n      graph rnn-encoding {\n        %H_0 = ...\n        %X = ...\n        %Y_h, %Y = Scan[body = <graph rnn-cell-1>, num_scan_inputs=1](%H_0, %X)\n        return %Y, %Y_h\n      }\n  \n      graph rnn-cell-1 (\n        %H_tminus1[FLOAT, tensor]\n        %X_t[FLOAT, tensor]\n      ) {\n        %Wi = ...\n        %Ri = ...\n        %Wbi = ...\n        %Rbi = ...\n        %t1 = X_t * (Wi^T)\n        %t2 = H_tminus1*(Ri^T)\n        %t3 = Add(%t1, %t2)\n        %t4 = Add(%t3, %Wbi)\n        %t5 = Add(%t4, %Rbi)\n        %Ht = Tanh(%t5)\n        %Accumulate = Identity(%Ht)\n        return %Ht, %Accumulate\n      }",
    "inputs": [
      { "name": "initial_state_and_scan_inputs", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "final_state_and_scan_outputs", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "num_scan_inputs", "type": "SI64Attr" },
      { "name": "scan_input_axes", "type": "OptionalAttr" },
      { "name": "scan_input_directions", "type": "OptionalAttr" },
      { "name": "scan_output_axes", "type": "OptionalAttr" },
      { "name": "scan_output_directions", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "onnx.Scatter",
    "summary": "ONNX Scatter operation",
    "description": "This operator is deprecated. Please use ScatterElements, which provides the same functionality.\n  \n  Scatter takes three inputs `data`, `updates`, and `indices` of the same\n  rank r >= 1 and an optional attribute axis that identifies an axis of `data`\n  (by default, the outer-most axis, that is axis 0). The output of the operation\n  is produced by creating a copy of the input `data`, and then updating its value\n  to values specified by `updates` at specific index positions specified by\n  `indices`. Its output shape is the same as the shape of `data`.\n  \n  For each entry in `updates`, the target index in `data` is obtained by combining\n  the corresponding entry in `indices` with the index of the entry itself: the\n  index-value for dimension = axis is obtained from the value of the corresponding\n  entry in `indices` and the index-value for dimension != axis is obtained from the\n  index of the entry itself.\n  \n  For instance, in a 2-D tensor case, the update corresponding to the [i][j] entry\n  is performed as below:\n  ```\n    output[indices[i][j]][j] = updates[i][j] if axis = 0,\n    output[i][indices[i][j]] = updates[i][j] if axis = 1,\n  ```\n  \n  This operator is the inverse of GatherElements. It is similar to Torch's Scatter operation.\n  \n  Example 1:\n  ```\n    data = [\n        [0.0, 0.0, 0.0],\n        [0.0, 0.0, 0.0],\n        [0.0, 0.0, 0.0],\n    ]\n    indices = [\n        [1, 0, 2],\n        [0, 2, 1],\n    ]\n    updates = [\n        [1.0, 1.1, 1.2],\n        [2.0, 2.1, 2.2],\n    ]\n    output = [\n        [2.0, 1.1, 0.0]\n        [1.0, 0.0, 2.2]\n        [0.0, 2.1, 1.2]\n    ]\n  ```\n  Example 2:\n  ```\n    data = [[1.0, 2.0, 3.0, 4.0, 5.0]]\n    indices = [[1, 3]]\n    updates = [[1.1, 2.1]]\n    axis = 1\n    output = [[1.0, 1.1, 3.0, 2.1, 5.0]]\n  ```",
    "inputs": [
      { "name": "data", "type": "AnyTypeOf" },
      { "name": "indices", "type": "AnyTypeOf" },
      { "name": "updates", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "axis", "type": "DefaultValuedAttr" }
    ],
    "category": "Tensor"
  },
  {
    "name": "onnx.ScatterElements",
    "summary": "ONNX ScatterElements operation",
    "description": "ScatterElements takes three inputs `data`, `updates`, and `indices` of the same\n  rank r >= 1 and an optional attribute axis that identifies an axis of `data`\n  (by default, the outer-most axis, that is axis 0). The output of the operation\n  is produced by creating a copy of the input `data`, and then updating its value\n  to values specified by `updates` at specific index positions specified by\n  `indices`. Its output shape is the same as the shape of `data`.\n  \n  For each entry in `updates`, the target index in `data` is obtained by combining\n  the corresponding entry in `indices` with the index of the entry itself: the\n  index-value for dimension = axis is obtained from the value of the corresponding\n  entry in `indices` and the index-value for dimension != axis is obtained from the\n  index of the entry itself.\n  \n  `reduction` allows specification of an optional reduction operation, which is applied to all values in `updates`\n  tensor into `output` at the specified `indices`.\n  In cases where `reduction` is set to \\\"none\\\", indices should not have duplicate entries: that is, if idx1 != idx2,\n  then indices[idx1] != indices[idx2]. For instance, in a 2-D tensor case, the update\n  corresponding to the [i][j] entry is performed as below:\n  ```\n  output[indices[i][j]][j] = updates[i][j] if axis = 0,\n  output[i][indices[i][j]] = updates[i][j] if axis = 1,\n  ```\n  When `reduction` is set to some reduction function `f`, the update corresponding to the [i][j] entry is performed as below:\n  ```\n  output[indices[i][j]][j] = f(output[indices[i][j]][j], updates[i][j]) if axis = 0,\n  output[i][indices[i][j]] = f(output[i][indices[i][j]], updates[i][j]) if axis = 1,\n  ```\n  where the `f` is `+`, `*`, `max` or `min` as specified.\n  \n  This operator is the inverse of GatherElements. It is similar to Torch's Scatter operation.\n  \n  (Opset 18 change): Adds max/min to the set of allowed reduction ops.\n  \n  Example 1:\n  ```\n  data = [\n      [0.0, 0.0, 0.0],\n      [0.0, 0.0, 0.0],\n      [0.0, 0.0, 0.0],\n  ]\n  indices = [\n      [1, 0, 2],\n      [0, 2, 1],\n  ]\n  updates = [\n      [1.0, 1.1, 1.2],\n      [2.0, 2.1, 2.2],\n  ]\n  output = [\n      [2.0, 1.1, 0.0]\n      [1.0, 0.0, 2.2]\n      [0.0, 2.1, 1.2]\n  ]\n  ```\n  Example 2:\n  ```\n  data = [[1.0, 2.0, 3.0, 4.0, 5.0]]\n  indices = [[1, 3]]\n  updates = [[1.1, 2.1]]\n  axis = 1\n  output = [[1.0, 1.1, 3.0, 2.1, 5.0]]\n  ```",
    "inputs": [
      { "name": "data", "type": "AnyTypeOf" },
      { "name": "indices", "type": "AnyTypeOf" },
      { "name": "updates", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "axis", "type": "DefaultValuedAttr" },
      { "name": "reduction", "type": "DefaultValuedStrAttr" }
    ]
  },
  {
    "name": "onnx.ScatterND",
    "summary": "ONNX ScatterND operation",
    "description": "ScatterND takes three inputs `data` tensor of rank r >= 1, `indices` tensor of rank q >= 1,\n  and `updates` tensor of rank q + r - indices.shape[-1] - 1. The output of the operation\n  is produced by creating a copy of the input `data`, and then updating its value to values\n  specified by `updates` at specific index positions specified by `indices`. Its output shape\n  is the same as the shape of `data`.\n  \n  `indices` is an integer tensor. Let k denote indices.shape[-1], the last dimension in the shape of `indices`.\n  `indices` is treated as a (q-1)-dimensional tensor of k-tuples, where each k-tuple is a partial-index into `data`.\n  Hence, k can be a value at most the rank of `data`. When k equals rank(data), each update entry specifies an\n  update to a single element of the tensor. When k is less than rank(data) each update entry specifies an\n  update to a slice of the tensor. Index values are allowed to be negative, as per the usual\n  convention for counting backwards from the end, but are expected in the valid range.\n  \n  `updates` is treated as a (q-1)-dimensional tensor of replacement-slice-values. Thus, the\n  first (q-1) dimensions of updates.shape must match the first (q-1) dimensions of indices.shape.\n  The remaining dimensions of `updates` correspond to the dimensions of the\n  replacement-slice-values. Each replacement-slice-value is a (r-k) dimensional tensor,\n  corresponding to the trailing (r-k) dimensions of `data`.  Thus, the shape of `updates`\n  must equal indices.shape[0:q-1] ++ data.shape[k:r-1], where ++ denotes the concatenation\n  of shapes.\n  \n  The `output` is calculated via the following equation:\n  \n  ```\n  output = np.copy(data)\n  update_indices = indices.shape[:-1]\n  for idx in np.ndindex(update_indices):\n      output[indices[idx]] = updates[idx]\n  ```\n  \n  The order of iteration in the above loop is not specified.\n  In particular, indices should not have duplicate entries: that is, if idx1 != idx2, then indices[idx1] != indices[idx2].\n  This ensures that the output value does not depend on the iteration order.\n  \n  `reduction` allows specification of an optional reduction operation, which is applied to all values in `updates`\n  tensor into `output` at the specified `indices`.\n  In cases where `reduction` is set to \\\"none\\\", indices should not have duplicate entries: that is, if idx1 != idx2,\n  then indices[idx1] != indices[idx2]. This ensures that the output value does not depend on the iteration order.\n  When `reduction` is set to some reduction function `f`, `output` is calculated as follows:\n  \n  ```\n  output = np.copy(data)\n  update_indices = indices.shape[:-1]\n  for idx in np.ndindex(update_indices):\n      output[indices[idx]] = f(output[indices[idx]], updates[idx])\n  ```\n  \n  where the `f` is `+`, `*`, `max` or `min` as specified.\n  \n  This operator is the inverse of GatherND.\n  \n  (Opset 18 change): Adds max/min to the set of allowed reduction ops.\n  \n  Example 1:\n  ```\n  data    = [1, 2, 3, 4, 5, 6, 7, 8]\n  indices = [[4], [3], [1], [7]]\n  updates = [9, 10, 11, 12]\n  output  = [1, 11, 3, 10, 9, 6, 7, 12]\n  ```\n  \n  Example 2:\n  ```\n  data    = [[[1, 2, 3, 4], [5, 6, 7, 8], [8, 7, 6, 5], [4, 3, 2, 1]],\n              [[1, 2, 3, 4], [5, 6, 7, 8], [8, 7, 6, 5], [4, 3, 2, 1]],\n              [[8, 7, 6, 5], [4, 3, 2, 1], [1, 2, 3, 4], [5, 6, 7, 8]],\n              [[8, 7, 6, 5], [4, 3, 2, 1], [1, 2, 3, 4], [5, 6, 7, 8]]]\n  indices = [[0], [2]]\n  updates = [[[5, 5, 5, 5], [6, 6, 6, 6], [7, 7, 7, 7], [8, 8, 8, 8]],\n              [[1, 1, 1, 1], [2, 2, 2, 2], [3, 3, 3, 3], [4, 4, 4, 4]]]\n  output  = [[[5, 5, 5, 5], [6, 6, 6, 6], [7, 7, 7, 7], [8, 8, 8, 8]],\n              [[1, 2, 3, 4], [5, 6, 7, 8], [8, 7, 6, 5], [4, 3, 2, 1]],\n              [[1, 1, 1, 1], [2, 2, 2, 2], [3, 3, 3, 3], [4, 4, 4, 4]],\n              [[8, 7, 6, 5], [4, 3, 2, 1], [1, 2, 3, 4], [5, 6, 7, 8]]]\n  ```",
    "inputs": [
      { "name": "data", "type": "AnyTypeOf" },
      { "name": "indices", "type": "TensorOf" },
      { "name": "updates", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "reduction", "type": "DefaultValuedStrAttr" }
    ]
  },
  {
    "name": "onnx.Selu",
    "summary": "ONNX Selu operation",
    "description": "Selu takes one input data (Tensor<T>) and produces one output data\n  (Tensor<T>) where the scaled exponential linear unit function,\n  `y = gamma * (alpha * e^x - alpha) for x <= 0`, `y = gamma * x for x > 0`,\n  is applied to the tensor elementwise.",
    "inputs": [
      { "name": "X", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "Y", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "alpha", "type": "DefaultValuedAttr" },
      { "name": "gamma", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "onnx.SequenceAt",
    "summary": "ONNX SequenceAt operation",
    "description": "Outputs a tensor copy from the tensor at 'position' in 'input_sequence'.\n  Accepted range for 'position' is in `[-n, n - 1]`, where `n` is the number of tensors in 'input_sequence'.\n  Negative value means counting positions from the back.",
    "inputs": [
      { "name": "input_sequence", "type": "AnyTypeOf" },
      { "name": "position", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "tensor", "type": "AnyTypeOf" }
    ]
  },
  {
    "name": "onnx.SequenceConstruct",
    "summary": "ONNX SequenceConstruct operation",
    "description": "Construct a tensor sequence containing 'inputs' tensors.\n  All tensors in 'inputs' must have the same data type.",
    "inputs": [
      { "name": "inputs", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "output_sequence", "type": "AnyTypeOf" }
    ]
  },
  {
    "name": "onnx.SequenceEmpty",
    "summary": "ONNX SequenceEmpty operation",
    "description": "Construct an empty tensor sequence, with given data type.",
    "outputs": [
      { "name": "output", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "dtype", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "onnx.SequenceErase",
    "summary": "ONNX SequenceErase operation",
    "description": "Outputs a tensor sequence that removes the tensor at 'position' from 'input_sequence'.\n  Accepted range for 'position' is in `[-n, n - 1]`, where `n` is the number of tensors in 'input_sequence'.\n  Negative value means counting positions from the back.\n  'position' is optional, by default it erases the last tensor from 'input_sequence'.",
    "inputs": [
      { "name": "input_sequence", "type": "AnyTypeOf" },
      { "name": "position", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "output_sequence", "type": "AnyTypeOf" }
    ]
  },
  {
    "name": "onnx.SequenceInsert",
    "summary": "ONNX SequenceInsert operation",
    "description": "Outputs a tensor sequence that inserts 'tensor' into 'input_sequence' at 'position'.\n  'tensor' must have the same data type as 'input_sequence'.\n  Accepted range for 'position' is in `[-n, n]`, where `n` is the number of tensors in 'input_sequence'.\n  Negative value means counting positions from the back.\n  'position' is optional, by default it inserts 'tensor' to the back of 'input_sequence'.",
    "inputs": [
      { "name": "input_sequence", "type": "AnyTypeOf" },
      { "name": "tensor", "type": "AnyTypeOf" },
      { "name": "position", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "output_sequence", "type": "AnyTypeOf" }
    ]
  },
  {
    "name": "onnx.SequenceLength",
    "summary": "ONNX SequenceLength operation",
    "description": "Produces a scalar(tensor of empty shape) containing the number of tensors in 'input_sequence'.",
    "inputs": [
      { "name": "input_sequence", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "length", "type": "TensorOf" }
    ]
  },
  {
    "name": "onnx.SequenceMap",
    "summary": "ONNX SequenceMap operation",
    "description": "Applies a sub-graph to each sample in the input sequence(s).\n  \n  Inputs can be either tensors or sequences, with the exception of the first input which must\n  be a sequence. The length of the first input sequence will determine the number of samples in the\n  outputs. Any other sequence inputs should have the same number of samples. The number of inputs\n  and outputs, should match the one of the subgraph.\n  \n  For each i-th element in the output, a sample will be extracted from the input sequence(s) at\n  the i-th position and the sub-graph will be applied to it.\n  The outputs will contain the outputs of the sub-graph for each sample, in the same order as in\n  the input.\n  \n  This operator assumes that processing each sample is independent and could executed in parallel\n  or in any order. Users cannot expect any specific ordering in which each subgraph is computed.",
    "inputs": [
      { "name": "input_sequence", "type": "AnyTypeOf" },
      { "name": "additional_inputs", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "out_sequence", "type": "Variadic" }
    ]
  },
  {
    "name": "onnx.Shape",
    "summary": "ONNX Shape operation",
    "description": "Takes a tensor as input and outputs an 1D int64 tensor containing the shape of the input tensor.\n  Optional attributes start and end can be used to compute a slice of the input tensor's shape.\n  If start axis is omitted, the slice starts from axis 0.\n  The end axis, if specified, is exclusive (and the returned value will not include the size of that axis).\n  If the end axis is omitted, the axes upto the last one will be included.\n  Negative axes indicate counting back from the last axis.\n  Note that axes will be clamped to the range [0, r-1], where r is the\n  rank of the input tensor if they are out-of-range (after adding r in the case of\n  negative axis). Thus, specifying any end value > r is equivalent to specifying an end\n  value of r, and specifying any start value < -r is equivalent to specifying a start\n  value of 0.\n  \n  Examples:\n  \n  ```\n  Input tensor with shape: [2, 3, 4]\n  No attributes specified.\n  Output: [2, 3, 4]\n  ```\n  \n  ```\n  Input tensor with shape: [2, 3, 4]\n  start: -1\n  Output: [4]\n  ```\n  \n  ```\n  Input tensor with shape: [2, 3, 4]\n  end: -1\n  Output: [2, 3]\n  ```\n  \n  ```\n  Input tensor with shape: [2, 3, 4]\n  start: 1\n  end: 2\n  Output: [3]\n  ```",
    "inputs": [
      { "name": "data", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "shape", "type": "TensorOf" }
    ],
    "attributes": [
      { "name": "end", "type": "OptionalAttr" },
      { "name": "start", "type": "DefaultValuedAttr" }
    ],
    "category": "Shape"
  },
  {
    "name": "onnx.ShapeTransform",
    "summary": "ONNX Element-wise shape transformation operation",
    "description": "This operator transforms a tensor into another tensor whose shape is changed\n    by a given affine map. This is elemement-wise transformation, so each element\n    in the input will be copied to an element in the output via the affine map.\n    The affine map must be bijective.\n\n    For example, the following code is using `onnx.ShapeTransform` to reshape\n    a tensor from 2D to 4D.\n    ```mlir\n    #reshape = affine_map(d0, d1) -> (d0/32, d0%32, d1/64, d1%64)\n    %Y = onnx.ShapeTransform(%arg0) {index_map = #reshape} :  (tensor<128x128xf32>) -> tensor<4x32x2x64xf32>\n    ```\n\n    `onnx.ShapeTransform` will be finally materialized into an `affine.for` via\n    lowering to `krnl` dialect, e.g.\n    ```mlir\n    %alloc = memref.alloc() {alignment = 16 : i64} : memref<4x32x2x64xf32>\n    affine.for %arg1 = 0 to 128 {\n      affine.for %arg2 = 0 to 128 {\n        %0 = affine.load %arg0[%arg1, %arg2] : memref< 128x128xf32 >\n        affine.store %0, %alloc[%arg1 / 32, %arg1 % 32, %arg2 / 64, %arg2 % 64] : memref<4x32x2x64xf32>\n      }\n    }\n    ```\n\n    When being canonicalized, ShapeTransform operations are composed into\n    a new ShapeTransform operation by composing their affine maps.\n\n    At this moment, this operation only supports static dimensions.\n\n    This operation is not part of the standard and was added to assist onnx-mlir.",
    "inputs": [
      { "name": "input", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "index_map", "type": "AffineMapAttr" }
    ]
  },
  {
    "name": "onnx.Shrink",
    "summary": "ONNX Shrink operation",
    "description": "Shrink takes one input data (Tensor<numeric>) and produces one Tensor output,\n  having same datatype and shape with input. It has two attributes, lambd and\n  bias. The formula of this operator is: If x < -lambd, y = x + bias;\n  If x > lambd, y = x - bias; Otherwise, y = 0.",
    "inputs": [
      { "name": "input", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "bias", "type": "DefaultValuedAttr" },
      { "name": "lambd", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "onnx.Sigmoid",
    "summary": "ONNX Sigmoid operation",
    "description": "Sigmoid takes one input data (Tensor<T>) and produces one output data\n  (Tensor<T>) where the sigmoid function, y = 1 / (1 + exp(-x)), is applied to the\n  tensor elementwise.",
    "inputs": [
      { "name": "X", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "Y", "type": "AnyTypeOf" }
    ],
    "category": "Activation"
  },
  {
    "name": "onnx.Sign",
    "summary": "ONNX Sign operation",
    "description": "Calculate the sign of the given input tensor element-wise.\n  If input > 0, output 1. if input < 0, output -1. if input == 0, output 0.",
    "inputs": [
      { "name": "input", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTypeOf" }
    ]
  },
  {
    "name": "onnx.Sin",
    "summary": "ONNX Sin operation",
    "description": "Calculates the sine of the given input tensor, element-wise.",
    "inputs": [
      { "name": "input", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTypeOf" }
    ]
  },
  {
    "name": "onnx.Sinh",
    "summary": "ONNX Sinh operation",
    "description": "Calculates the hyperbolic sine of the given input tensor element-wise.",
    "inputs": [
      { "name": "input", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTypeOf" }
    ]
  },
  {
    "name": "onnx.Size",
    "summary": "ONNX Size operation",
    "description": "Takes a tensor as input and outputs a int64 scalar that equals to the total number of elements of the input tensor.",
    "inputs": [
      { "name": "data", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "size", "type": "TensorOf" }
    ],
    "category": "Shape"
  },
  {
    "name": "onnx.Slice",
    "summary": "ONNX Slice operation",
    "description": "Produces a slice of the input tensor along multiple axes. Similar to numpy:\n  https://numpy.org/doc/stable/user/basics.indexing.html?highlight=slice#slicing-and-striding\n  \n  Slice uses the `starts`, `ends`, `axes` and `steps` inputs to select a sub-tensor\n  of its input `data` tensor.\n  \n  An effective `starts[i]`, `ends[i]`, and `steps[i]` must be computed for each `i`\n  in `[0, ... r-1]` where `r = rank(input)` as follows:\n  \n  If `axes` are omitted, they are set to `[0, ..., r-1]`.\n  If `steps` are omitted, they are set to `[1, ..., 1]` of length `len(starts)`\n  \n  The effective values are initialized as `start[i] = 0`, `ends[i] = dims[i]` where\n  `dims` are the dimensions of `input` and `steps[i] = 1`.\n  \n  All negative elements of `axes` are made non-negative by adding `r` to them, where\n  `r =rank(input)`.\n  \n  All negative values in `starts[i]` and `ends[i]` have `dims[axes[i]]` added to them,\n  where `dims` are the dimensions of `input`. Then `start[axes[i]]` is the adjusted\n  `starts[i]` is clamped into the range `[0, dims[axes[i]]]` for positive stepping\n  and `[0, dims[axes[i]]-1]` for negative stepping.\n  \n  The clamping for the adjusted `ends[i]` depends on the sign of `steps[i]` and must\n  accommodate copying 0 through `dims[axes[i]]` elements, so for positive stepping\n  `ends[axes[i]]` is clamped to `[0, dims[axes[i]]]`, while for negative stepping it\n  is clamped to `[-1, dims[axes[i]]-1]`.\n  \n  Finally, `steps[axes[i]] = steps[i]`.\n  \n  For slicing to the end of a dimension with unknown size, it is recommended to pass\n  in `INT_MAX` when slicing forward and 'INT_MIN' when slicing backward.\n  \n  Example 1:\n  \n  ```\n  data = [\n      [1, 2, 3, 4],\n      [5, 6, 7, 8],\n  ]\n  axes = [0, 1]\n  starts = [1, 0]\n  ends = [2, 3]\n  steps = [1, 2]\n  result = [\n      [5, 7],\n  ]\n  ```\n  \n  Example 2:\n  \n  ```\n  data = [\n      [1, 2, 3, 4],\n      [5, 6, 7, 8],\n  ]\n  starts = [0, 1]\n  ends = [-1, 1000]\n  result = [\n      [2, 3, 4],\n  ]\n  ```",
    "inputs": [
      { "name": "data", "type": "AnyTypeOf" },
      { "name": "starts", "type": "AnyTypeOf" },
      { "name": "ends", "type": "AnyTypeOf" },
      { "name": "axes", "type": "AnyTypeOf" },
      { "name": "steps", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTypeOf" }
    ],
    "category": "Tensor"
  },
  {
    "name": "onnx.Softmax",
    "summary": "ONNX Softmax operation",
    "description": "The operator computes the normalized exponential values for the given input:\n  \n   Softmax(input, axis) = Exp(input) / ReduceSum(Exp(input), axis=axis, keepdims=1) \n  \n  The \\\"axis\\\" attribute indicates the dimension along which Softmax\n  will be performed. The output tensor has the same shape\n  and contains the Softmax values of the corresponding input.",
    "inputs": [
      { "name": "input", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "axis", "type": "DefaultValuedAttr" }
    ],
    "category": "Activation"
  },
  {
    "name": "onnx.SoftmaxCrossEntropyLoss",
    "summary": "ONNX SoftmaxCrossEntropyLoss operation",
    "description": "Loss function that measures the softmax cross entropy\n  between 'scores' and 'labels'.\n  This operator first computes a loss tensor whose shape is identical to the labels input.\n  If the input is 2-D with shape (N, C), the loss tensor may be a N-element vector L = (l_1, l_2, ..., l_N).\n  If the input is N-D tensor with shape (N, C, D1, D2, ..., Dk),\n  the loss tensor L may have (N, D1, D2, ..., Dk) as its shape and L[i,][j_1][j_2]...[j_k] denotes a scalar element in L.\n  After L is available, this operator can optionally do a reduction operator.\n  \n  * shape(scores): (N, C) where C is the number of classes, or (N, C, D1, D2,..., Dk),\n    with K >= 1 in case of K-dimensional loss.\n  * shape(labels): (N) where each value is 0 <= labels[i] <= C-1, or (N, D1, D2,..., Dk),\n    with K >= 1 in case of K-dimensional loss.\n  \n  The loss for one sample, l_i, can calculated as follows:\n  ```\n  l[i][d1][d2]...[dk] = -y[i][c][d1][d2]..[dk], where i is the index of classes.\n  ```\n  or\n  ```\n  l[i][d1][d2]...[dk] = -y[i][c][d1][d2]..[dk] * weights[c], if 'weights' is provided.\n  ```\n  \n  loss is zero for the case when label-value equals ignore_index.\n  ```\n  l[i][d1][d2]...[dk]  = 0, when labels[n][d1][d2]...[dk] = ignore_index\n  ```\n  \n  where:\n  ```\n  p = Softmax(scores)\n  y = Log(p)\n  c = labels[i][d1][d2]...[dk]\n  ```\n  \n  Finally, L is optionally reduced:\n  \n  * If reduction = 'none', the output is L with shape (N, D1, D2, ..., Dk).\n  * If reduction = 'sum', the output is scalar: Sum(L).\n  * If reduction = 'mean', the output is scalar: ReduceMean(L), or if weight is provided: `ReduceSum(L) / ReduceSum(W)`,\n    where tensor W is of shape `(N, D1, D2, ..., Dk)` and `W[n][d1][d2]...[dk] = weights[labels[i][d1][d2]...[dk]]`.",
    "inputs": [
      { "name": "scores", "type": "AnyTypeOf" },
      { "name": "labels", "type": "AnyTypeOf" },
      { "name": "weights", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTypeOf" },
      { "name": "log_prob", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "ignore_index", "type": "OptionalAttr" },
      { "name": "reduction", "type": "DefaultValuedStrAttr" }
    ]
  },
  {
    "name": "onnx.SoftmaxV11",
    "summary": "ONNX Softmax operation",
    "description": "The operator computes the softmax (normalized exponential) values for each layer in the batch\n   of the given input.\n  \n  The input does not need to explicitly be a 2D vector; rather, it will be\n  coerced into one. For an arbitrary n-dimensional tensor\n  input \\in [a_0, a_1, ..., a_{k-1}, a_k, ..., a_{n-1\\}\\] and k is\n  the axis provided, then input will be coerced into a 2-dimensional tensor with\n  dimensions [a_0 * ... * a_{k-1}, a_k * ... * a_{n-1\\}\\]. For the default\n  case where axis=1, this means the input tensor will be coerced into a 2D tensor\n  of dimensions [a_0, a_1 * ... * a_{n-1\\}\\], where a_0 is often the batch size.\n  In this situation, we must have a_0 = N and a_1 * ... * a_{n-1} = D.\n  Each of these dimensions must be matched correctly, or else the operator\n  will throw errors. The output tensor has the same shape\n  and contains the softmax values of the corresponding input.",
    "inputs": [
      { "name": "input", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "axis", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "onnx.Softplus",
    "summary": "ONNX Softplus operation",
    "description": "Softplus takes one input data (Tensor<T>) and produces one output data\n  (Tensor<T>) where the softplus function, y = ln(exp(x) + 1), is applied to\n  the tensor elementwise.",
    "inputs": [
      { "name": "X", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "Y", "type": "AnyTypeOf" }
    ]
  },
  {
    "name": "onnx.Softsign",
    "summary": "ONNX Softsign operation",
    "description": "Calculates the softsign (x/(1+|x|)) of the given input tensor element-wise.",
    "inputs": [
      { "name": "input", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTypeOf" }
    ]
  },
  {
    "name": "onnx.SpaceToDepth",
    "summary": "ONNX SpaceToDepth operation",
    "description": "SpaceToDepth rearranges blocks of spatial data into depth. More specifically,\n  this op outputs a copy of the input tensor where values from the height and width dimensions\n  are moved to the depth dimension.",
    "inputs": [
      { "name": "input", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "blocksize", "type": "SI64Attr" }
    ]
  },
  {
    "name": "onnx.Split",
    "summary": "ONNX Split operation",
    "description": "Split a tensor into a list of tensors, along the specified 'axis'.\n  Either input 'split' or the attribute 'num_outputs' should be specified, but not both.\n  If the attribute 'num_outputs' is specified, then the tensor is split into equal sized parts.\n  If the tensor is not evenly splittable into `num_outputs`, the last chunk will be smaller.\n  If the input 'split' is specified, it indicates the sizes of each output in the split.",
    "inputs": [
      { "name": "input", "type": "AnyTypeOf" },
      { "name": "split", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "outputs", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "axis", "type": "DefaultValuedAttr" },
      { "name": "num_outputs", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "onnx.SplitToSequence",
    "summary": "ONNX SplitToSequence operation",
    "description": "Split a tensor into a sequence of tensors, along the specified 'axis'.\n  Lengths of the parts can be specified using the optional argument 'split'.\n  If the argument `split' is not specified, a default scalar value of 1\n  is used as the value of `split'.\n  'split' must contain only positive numbers.\n  'split' is either a scalar (tensor of empty shape), or a 1-D tensor.\n  If 'split' is a scalar, then 'input' will be split into chunks all of size 'split'\n  if possible. The last chunk alone may be smaller than 'split' if the 'input' size\n  along the given axis 'axis' is not divisible by 'split'.\n  If 'split' is a 1-dimensional tensor, the input tensor is split into 'size(split)' chunks,\n  with lengths of the parts on 'axis' specified in 'split'. In this scenario, the sum of entries\n  in 'split' must be equal to the dimension size of input tensor on 'axis'.",
    "inputs": [
      { "name": "input", "type": "AnyTypeOf" },
      { "name": "split", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "output_sequence", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "axis", "type": "DefaultValuedAttr" },
      { "name": "keepdims", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "onnx.SplitV11",
    "summary": "ONNX Split operation",
    "description": "Split a tensor into a list of tensors, along the specified\n  'axis'. Lengths of the parts can be specified using argument 'split'.\n  Otherwise, the tensor is split to equal sized parts.",
    "inputs": [
      { "name": "input", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "outputs", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "axis", "type": "DefaultValuedAttr" },
      { "name": "split", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "onnx.SplitV13",
    "summary": "ONNX Split operation",
    "description": "Split a tensor into a list of tensors, along the specified\n  'axis'. Lengths of the parts can be specified using input 'split'.\n  Otherwise, the tensor is split to equal sized parts.",
    "inputs": [
      { "name": "input", "type": "AnyTypeOf" },
      { "name": "split", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "outputs", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "axis", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "onnx.Sqrt",
    "summary": "ONNX Sqrt operation",
    "description": "Square root takes one input data (Tensor<T>) and produces one output data\n  (Tensor<T>) where the square root is, y = x^0.5, is applied to\n  the tensor elementwise. If x is negative, then it will return NaN.",
    "inputs": [
      { "name": "X", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "Y", "type": "AnyTypeOf" }
    ]
  },
  {
    "name": "onnx.Squeeze",
    "summary": "ONNX Squeeze operation",
    "description": "Remove single-dimensional entries from the shape of a tensor.\n  Takes an input `axes` with a list of axes to squeeze.\n  If `axes` is not provided, all the single dimensions will be removed from\n  the shape. If an axis is selected with shape entry not equal to one, an error is raised.",
    "inputs": [
      { "name": "data", "type": "AnyTypeOf" },
      { "name": "axes", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "squeezed", "type": "AnyTypeOf" }
    ]
  },
  {
    "name": "onnx.SqueezeV11",
    "summary": "ONNX Squeeze operation",
    "description": "Remove single-dimensional entries from the shape of a tensor.\n  Takes a  parameter `axes` with a list of axes to squeeze.\n  If `axes` is not provided, all the single dimensions will be removed from\n  the shape. If an axis is selected with shape entry not equal to one, an error is raised.",
    "inputs": [
      { "name": "data", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "squeezed", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "axes", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "onnx.STFT",
    "summary": "ONNX STFT operation",
    "description": "Computes the Short-time Fourier Transform of the signal.",
    "inputs": [
      { "name": "signal", "type": "AnyTypeOf" },
      { "name": "frame_step", "type": "AnyTypeOf" },
      { "name": "window", "type": "AnyTypeOf" },
      { "name": "frame_length", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "onesided", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "onnx.StringNormalizer",
    "summary": "ONNX StringNormalizer operation",
    "description": "StringNormalization performs string operations for basic cleaning.\n  This operator has only one input (denoted by X) and only one output\n  (denoted by Y). This operator first examines the elements in the X,\n  and removes elements specified in \\\"stopwords\\\" attribute.\n  After removing stop words, the intermediate result can be further lowercased,\n  uppercased, or just returned depending the \\\"case_change_action\\\" attribute.\n  This operator only accepts [C]- and [1, C]-tensor.\n  If all elements in X are dropped, the output will be the empty value of string tensor with shape [1]\n  if input shape is [C] and shape [1, 1] if input shape is [1, C].",
    "inputs": [
      { "name": "X", "type": "TensorOf" }
    ],
    "outputs": [
      { "name": "Y", "type": "TensorOf" }
    ],
    "attributes": [
      { "name": "case_change_action", "type": "DefaultValuedStrAttr" },
      { "name": "is_case_sensitive", "type": "DefaultValuedAttr" },
      { "name": "locale", "type": "OptionalAttr" },
      { "name": "stopwords", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "onnx.Sub",
    "summary": "ONNX Sub operation",
    "description": "Performs element-wise binary subtraction (with Numpy-style broadcasting support).\n  \n  This operator supports **multidirectional (i.e., Numpy-style) broadcasting**; for more details please check [the doc](Broadcasting.md).\n  \n  (Opset 14 change): Extend supported types to include uint8, int8, uint16, and int16.",
    "inputs": [
      { "name": "A", "type": "AnyTypeOf" },
      { "name": "B", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "C", "type": "AnyTypeOf" }
    ]
  },
  {
    "name": "onnx.Sum",
    "summary": "ONNX Sum operation",
    "description": "Element-wise sum of each of the input tensors (with Numpy-style broadcasting support).\n  All inputs and outputs must have the same data type.\n  This operator supports **multidirectional (i.e., Numpy-style) broadcasting**; for more details please check [the doc](Broadcasting.md).",
    "inputs": [
      { "name": "data_0", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "sum", "type": "AnyTypeOf" }
    ]
  },
  {
    "name": "onnx.SVMClassifier",
    "summary": "ONNX SVMClassifier operation",
    "description": "Support Vector Machine classifier",
    "inputs": [
      { "name": "X", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "Y", "type": "AnyTypeOf" },
      { "name": "Z", "type": "TensorOf" }
    ],
    "attributes": [
      { "name": "classlabels_ints", "type": "OptionalAttr" },
      { "name": "classlabels_strings", "type": "OptionalAttr" },
      { "name": "coefficients", "type": "OptionalAttr" },
      { "name": "kernel_params", "type": "OptionalAttr" },
      { "name": "kernel_type", "type": "DefaultValuedStrAttr" },
      { "name": "post_transform", "type": "DefaultValuedStrAttr" },
      { "name": "prob_a", "type": "OptionalAttr" },
      { "name": "prob_b", "type": "OptionalAttr" },
      { "name": "rho", "type": "OptionalAttr" },
      { "name": "support_vectors", "type": "OptionalAttr" },
      { "name": "vectors_per_class", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "onnx.SVMRegressor",
    "summary": "ONNX SVMRegressor operation",
    "description": "Support Vector Machine regression prediction and one-class SVM anomaly detection.",
    "inputs": [
      { "name": "X", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "Y", "type": "TensorOf" }
    ],
    "attributes": [
      { "name": "coefficients", "type": "OptionalAttr" },
      { "name": "kernel_params", "type": "OptionalAttr" },
      { "name": "kernel_type", "type": "DefaultValuedStrAttr" },
      { "name": "n_supports", "type": "DefaultValuedAttr" },
      { "name": "one_class", "type": "DefaultValuedAttr" },
      { "name": "post_transform", "type": "DefaultValuedStrAttr" },
      { "name": "rho", "type": "OptionalAttr" },
      { "name": "support_vectors", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "onnx.Tan",
    "summary": "ONNX Tan operation",
    "description": "Calculates the tangent of the given input tensor, element-wise.",
    "inputs": [
      { "name": "input", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTypeOf" }
    ]
  },
  {
    "name": "onnx.Tanh",
    "summary": "ONNX Tanh operation",
    "description": "Calculates the hyperbolic tangent of the given input tensor element-wise.",
    "inputs": [
      { "name": "input", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTypeOf" }
    ],
    "category": "Activation"
  },
  {
    "name": "onnx.TfIdfVectorizer",
    "summary": "ONNX TfIdfVectorizer operation",
    "description": "This transform extracts n-grams from the input sequence and save them as a vector. Input can\n  be either a 1-D or 2-D tensor. For 1-D input, output is the n-gram representation of that input.\n  For 2-D input, the output is also a  2-D tensor whose i-th row is the n-gram representation of the i-th input row.\n  More specifically, if input shape is [C], the corresponding output shape would be [max(ngram_indexes) + 1].\n  If input shape is [N, C], this operator produces a [N, max(ngram_indexes) + 1]-tensor.\n  \n  In contrast to standard n-gram extraction, here, the indexes of extracting an n-gram from the original\n  sequence are not necessarily consecutive numbers. The discontinuity between indexes are controlled by the number of skips.\n  If the number of skips is 2, we should skip two tokens when scanning through the original sequence.\n  Let's consider an example. Assume that input sequence is [94, 17, 36, 12, 28] and the number of skips is 2.\n  The associated 2-grams are [94, 12] and [17, 28] respectively indexed by [0, 3] and [1, 4].\n  If the number of skips becomes 0, the 2-grams generated are [94, 17], [17, 36], [36, 12], [12, 28]\n  indexed by [0, 1], [1, 2], [2, 3], [3, 4], respectively.\n  \n  The output vector (denoted by Y) stores the count of each n-gram;\n  Y[ngram_indexes[i]] indicates the times that the i-th n-gram is found. The attribute ngram_indexes is used to determine the mapping\n  between index i and the corresponding n-gram's output coordinate. If pool_int64s is [94, 17, 17, 36], ngram_indexes is [1, 0],\n  ngram_counts=[0, 0], then the Y[0] (first element in Y) and Y[1] (second element in Y) are the counts of [17, 36] and [94, 17],\n  respectively. An n-gram which cannot be found in pool_strings/pool_int64s should be ignored and has no effect on the output.\n  Note that we may consider all skips up to S when generating the n-grams.\n  \n  The examples used above are true if mode is \\\"TF\\\". If mode is \\\"IDF\\\", all the counts larger than 1 would be truncated to 1 and\n  the i-th element in weights would be used to scale (by multiplication) the count of the i-th n-gram in pool. If mode is \\\"TFIDF\\\",\n  this operator first computes the counts of all n-grams and then scale them by the associated values in the weights attribute.\n  \n  Only one of pool_strings and pool_int64s can be set. If pool_int64s is set, the input should be an integer tensor.\n  If pool_strings is set, the input must be a string tensor.",
    "inputs": [
      { "name": "X", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "Y", "type": "TensorOf" }
    ],
    "attributes": [
      { "name": "max_gram_length", "type": "SI64Attr" },
      { "name": "max_skip_count", "type": "SI64Attr" },
      { "name": "min_gram_length", "type": "SI64Attr" },
      { "name": "mode", "type": "StrAttr" },
      { "name": "ngram_counts", "type": "I64ArrayAttr" },
      { "name": "ngram_indexes", "type": "I64ArrayAttr" },
      { "name": "pool_int64s", "type": "OptionalAttr" },
      { "name": "pool_strings", "type": "OptionalAttr" },
      { "name": "weights", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "onnx.ThresholdedRelu",
    "summary": "ONNX ThresholdedRelu operation",
    "description": "ThresholdedRelu takes one input data (Tensor<T>) and produces one output data\n  (Tensor<T>) where the rectified linear function, y = x for x > alpha, y = 0 otherwise,\n  is applied to the tensor elementwise.",
    "inputs": [
      { "name": "X", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "Y", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "alpha", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "onnx.Tile",
    "summary": "ONNX Tile operation",
    "description": "Constructs a tensor by tiling a given tensor.\n  This is the same as function `tile` in Numpy, but no broadcast.\n  For example A = [[1, 2], [3, 4]], B = [1, 2], tile(A, B) = [[1, 2, 1, 2], [3, 4, 3, 4]]",
    "inputs": [
      { "name": "input", "type": "AnyTypeOf" },
      { "name": "repeats", "type": "TensorOf" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTypeOf" }
    ]
  },
  {
    "name": "onnx.TopK",
    "summary": "ONNX TopK operation",
    "description": "Retrieve the top-K largest or smallest elements along a specified axis. Given an input tensor of\n  shape [a_0, a_1, ..., a_{n-1\\}\\] and integer argument k, return two outputs:\n  \n  * Value tensor of shape [a_0, a_1, ..., a_{axis-1}, k, a_{axis+1}, ... a_{n-1\\}\\]\n    which contains the values of the top k elements along the specified axis\n  * Index tensor of shape [a_0, a_1, ..., a_{axis-1}, k, a_{axis+1}, ... a_{n-1\\}\\] which\n    contains the indices of the top k elements (original indices from the input\n    tensor).\n  \n  * If \\\"largest\\\" is 1 (the default value) then the k largest elements are returned.\n  * If \\\"sorted\\\" is 1 (the default value) then the resulting k elements will be sorted.\n  * If \\\"sorted\\\" is 0, order of returned 'Values' and 'Indices' are undefined.\n  \n  Given two equivalent values, this operator uses the indices along the axis as\n  a tiebreaker. That is, the element with the lower index will appear first.",
    "inputs": [
      { "name": "X", "type": "AnyTypeOf" },
      { "name": "K", "type": "TensorOf" }
    ],
    "outputs": [
      { "name": "Values", "type": "AnyTypeOf" },
      { "name": "Indices", "type": "TensorOf" }
    ],
    "attributes": [
      { "name": "axis", "type": "DefaultValuedAttr" },
      { "name": "largest", "type": "DefaultValuedAttr" },
      { "name": "sorted", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "onnx.Transpose",
    "summary": "ONNX Transpose operation",
    "description": "Transpose the input tensor similar to numpy.transpose. For example, when\n  perm=(1, 0, 2), given an input tensor of shape (1, 2, 3), the output shape\n  will be (2, 1, 3).",
    "inputs": [
      { "name": "data", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "transposed", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "perm", "type": "OptionalAttr" }
    ],
    "category": "Transform"
  },
  {
    "name": "onnx.TreeEnsembleClassifier",
    "summary": "ONNX TreeEnsembleClassifier operation",
    "description": "Tree Ensemble classifier.  Returns the top class for each of N inputs.<br>\n      The attributes named 'nodes_X' form a sequence of tuples, associated by\n      index into the sequences, which must all be of equal length. These tuples\n      define the nodes.<br>\n      Similarly, all fields prefixed with 'class_' are tuples of votes at the leaves.\n      A leaf may have multiple votes, where each vote is weighted by\n      the associated class_weights index.<br>\n      One and only one of classlabels_strings or classlabels_int64s\n      will be defined. The class_ids are indices into this list.",
    "inputs": [
      { "name": "X", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "Y", "type": "AnyTypeOf" },
      { "name": "Z", "type": "TensorOf" }
    ],
    "attributes": [
      { "name": "base_values", "type": "OptionalAttr" },
      { "name": "class_ids", "type": "OptionalAttr" },
      { "name": "class_nodeids", "type": "OptionalAttr" },
      { "name": "class_treeids", "type": "OptionalAttr" },
      { "name": "class_weights", "type": "OptionalAttr" },
      { "name": "classlabels_int64s", "type": "OptionalAttr" },
      { "name": "classlabels_strings", "type": "OptionalAttr" },
      { "name": "nodes_falsenodeids", "type": "OptionalAttr" },
      { "name": "nodes_featureids", "type": "OptionalAttr" },
      { "name": "nodes_hitrates", "type": "OptionalAttr" },
      { "name": "nodes_missing_value_tracks_true", "type": "OptionalAttr" },
      { "name": "nodes_modes", "type": "OptionalAttr" },
      { "name": "nodes_nodeids", "type": "OptionalAttr" },
      { "name": "nodes_treeids", "type": "OptionalAttr" },
      { "name": "nodes_truenodeids", "type": "OptionalAttr" },
      { "name": "nodes_values", "type": "OptionalAttr" },
      { "name": "post_transform", "type": "DefaultValuedStrAttr" }
    ]
  },
  {
    "name": "onnx.TreeEnsembleRegressor",
    "summary": "ONNX TreeEnsembleRegressor operation",
    "description": "Tree Ensemble regressor.  Returns the regressed values for each input in N.<br>\n      All args with nodes_ are fields of a tuple of tree nodes, and\n      it is assumed they are the same length, and an index i will decode the\n      tuple across these inputs.  Each node id can appear only once\n      for each tree id.<br>\n      All fields prefixed with target_ are tuples of votes at the leaves.<br>\n      A leaf may have multiple votes, where each vote is weighted by\n      the associated target_weights index.<br>\n      All trees must have their node ids start at 0 and increment by 1.<br>\n      Mode enum is BRANCH_LEQ, BRANCH_LT, BRANCH_GTE, BRANCH_GT, BRANCH_EQ, BRANCH_NEQ, LEAF",
    "inputs": [
      { "name": "X", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "Y", "type": "TensorOf" }
    ],
    "attributes": [
      { "name": "aggregate_function", "type": "DefaultValuedStrAttr" },
      { "name": "base_values", "type": "OptionalAttr" },
      { "name": "n_targets", "type": "OptionalAttr" },
      { "name": "nodes_falsenodeids", "type": "OptionalAttr" },
      { "name": "nodes_featureids", "type": "OptionalAttr" },
      { "name": "nodes_hitrates", "type": "OptionalAttr" },
      { "name": "nodes_missing_value_tracks_true", "type": "OptionalAttr" },
      { "name": "nodes_modes", "type": "OptionalAttr" },
      { "name": "nodes_nodeids", "type": "OptionalAttr" },
      { "name": "nodes_treeids", "type": "OptionalAttr" },
      { "name": "nodes_truenodeids", "type": "OptionalAttr" },
      { "name": "nodes_values", "type": "OptionalAttr" },
      { "name": "post_transform", "type": "DefaultValuedStrAttr" },
      { "name": "target_ids", "type": "OptionalAttr" },
      { "name": "target_nodeids", "type": "OptionalAttr" },
      { "name": "target_treeids", "type": "OptionalAttr" },
      { "name": "target_weights", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "onnx.Trilu",
    "summary": "ONNX Trilu operation",
    "description": "Given a 2-D matrix or batches of 2-D matrices, returns the upper or lower triangular part of the tensor(s).\n  The attribute \\\"upper\\\" determines whether the upper or lower part is retained. If set to true,\n  the upper triangular matrix is retained. Lower triangular matrix is retained otherwise.\n  Default value for the \\\"upper\\\" attribute is true.\n  Trilu takes one input tensor of shape [*, N, M], where * is zero or more batch dimensions. The upper triangular part consists\n  of the elements on and above the given diagonal (k). The lower triangular part consists of elements on and below the diagonal.\n  All other elements in the matrix are set to zero.\n  If k = 0, the triangular part on and above/below the main diagonal is retained.\n  If upper is set to true, a positive k retains the upper triangular matrix excluding the main diagonal and (k-1) diagonals above it.\n  A negative k value retains the main diagonal and |k| diagonals below it.\n  If upper is set to false, a positive k retains the lower triangular matrix including the main diagonal and k diagonals above it.\n  A negative k value excludes the main diagonal and (|k|-1) diagonals below it.",
    "inputs": [
      { "name": "input", "type": "AnyTypeOf" },
      { "name": "k", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "upper", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "onnx.Unique",
    "summary": "ONNX Unique operation",
    "description": "Find the unique elements of a tensor. When an optional attribute 'axis' is provided, unique subtensors sliced along the 'axis' are returned.\n  Otherwise the input tensor is flattened and unique values of the flattened tensor are returned.\n  \n  This operator returns the unique values or sliced unique subtensors of the input tensor and three optional outputs.\n  The first output tensor 'Y' contains all unique values or subtensors of the input.\n  The second optional output tensor 'indices' contains indices of 'Y' elements' first occurrence in 'X'.\n  The third optional output tensor 'inverse_indices' contains, for elements of 'X', its corresponding indices in 'Y'.\n  The fourth optional output tensor 'counts' contains the count of each element of 'Y' in the input.\n  \n  Outputs are either sorted in ascending order or optionally in the order of the first occurrence of the values in the input.\n  \n  https://docs.scipy.org/doc/numpy/reference/generated/numpy.unique.html\n  \n  Example 1:\n  ```\n  input_X = [2, 1, 1, 3, 4, 3]\n  attribute_sorted = 0\n  attribute_axis = None\n  output_Y = [2, 1, 3, 4]\n  output_indices = [0, 1, 3, 4]\n  output_inverse_indices = [0, 1, 1, 2, 3, 2]\n  output_counts = [1, 2, 2, 1]\n  ```\n  \n  Example 2:\n  ```\n  input_X = [[1, 3], [2, 3]]\n  attribute_sorted = 1\n  attribute_axis = None\n  output_Y = [1, 2, 3]\n  output_indices = [0, 2, 1]\n  output_inverse_indices = [0, 2, 1, 2]\n  output_counts = [1, 1, 2]\n  ```\n  \n  Example 3:\n  ```\n  input_X = [[1, 0, 0], [1, 0, 0], [2, 3, 4]]\n  attribute_sorted = 1\n  attribute_axis = 0\n  output_Y = [[1, 0, 0], [2, 3, 4]]\n  output_indices = [0, 2]\n  output_inverse_indices = [0, 0, 1]\n  output_counts = [2, 1]\n  ```\n  \n  Example 4:\n  ```\n  input_x = [[[1., 1.], [0., 1.], [2., 1.], [0., 1.]],\n              [[1., 1.], [0., 1.], [2., 1.], [0., 1.]]]\n  attribute_sorted = 1\n  attribute_axis = 1\n  ```\n  \n  intermediate data are presented below for better understanding:\n  there are 4 subtensors sliced along axis 1 of input_x (shape = (2, 4, 2)):\n  ```\n  A: [[1, 1], [1, 1]],\n     [[0, 1], [0, 1]],\n     [[2, 1], [2, 1]],\n     [[0, 1], [0, 1]].\n  ```\n  \n  there are 3 unique subtensors:\n  ```\n  [[1, 1], [1, 1]],\n  [[0, 1], [0, 1]],\n  [[2, 1], [2, 1]].\n  ```\n  \n  sorted unique subtensors:\n  ```\n  B: [[0, 1], [0, 1]],\n     [[1, 1], [1, 1]],\n     [[2, 1], [2, 1]].\n  ```\n  \n  output_Y is constructed from B:\n  ```\n  [[[0. 1.], [1. 1.], [2. 1.]],\n   [[0. 1.], [1. 1.], [2. 1.]]]\n  ```\n  \n  output_indices is to map from B to A:\n  ```\n  [1, 0, 2]\n  ```\n  \n  output_inverse_indices is to map from A to B:\n  ```\n  [1, 0, 2, 0]\n  ```\n  \n  output_counts:\n  ```\n  [2, 1, 1]\n  ```",
    "inputs": [
      { "name": "X", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "Y", "type": "AnyTypeOf" },
      { "name": "indices", "type": "AnyTypeOf" },
      { "name": "inverse_indices", "type": "AnyTypeOf" },
      { "name": "counts", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "axis", "type": "OptionalAttr" },
      { "name": "sorted", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "onnx.Unsqueeze",
    "summary": "ONNX Unsqueeze operation",
    "description": "Insert single-dimensional entries to the shape of an input tensor (`data`).\n  Takes one required input `axes` - which contains a list of dimension indices and this operator will insert a dimension of value `1` into the corresponding index of the output tensor (`expanded`).\n  \n  For example, given an input tensor (`data`) of shape [3, 4, 5], then\n  Unsqueeze(data, axes=[0, 4]) outputs a tensor (`expanded`) containing same data as `data` but with shape [1, 3, 4, 5, 1].\n  \n  The input `axes` should not contain any duplicate entries. It is an error if it contains duplicates.\n  The rank of the output tensor (`output_rank`) is the rank of the input tensor (`data`) plus the number of values in `axes`.\n  Each value in `axes` should be within the (inclusive) range [-output_rank , output_rank - 1].\n  The order of values in `axes` does not matter and can come in any order.",
    "inputs": [
      { "name": "data", "type": "AnyTypeOf" },
      { "name": "axes", "type": "TensorOf" }
    ],
    "outputs": [
      { "name": "expanded", "type": "AnyTypeOf" }
    ]
  },
  {
    "name": "onnx.UnsqueezeV11",
    "summary": "ONNX Unsqueeze operation",
    "description": "Insert single-dimensional entries to the shape of an input tensor (`data`).\n  Takes one required argument `axes` - which contains a list of dimension indices and this operator will insert a dimension of value `1` into the corresponding index of the output tensor (`expanded`).\n  \n  For example:\n    Given an input tensor (`data`) of shape [3, 4, 5], then\n    Unsqueeze(data, axes=[0, 4]) outputs a tensor (`expanded`) containing same data as `data` but with shape [1, 3, 4, 5, 1].\n  \n  The attribute `axes` should not contain any duplicate entries. It is an error if it contains duplicates.\n  The rank of the output tensor (`output_rank`) is the rank of the input tensor (`data`) plus the number of values in `axes`.\n  Each value in `axes` should be within the (inclusive) range [-output_rank , output_rank - 1].\n  The order of values in `axes` does not matter and can come in any order.",
    "inputs": [
      { "name": "data", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "expanded", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "axes", "type": "I64ArrayAttr" }
    ]
  },
  {
    "name": "onnx.Upsample",
    "summary": "ONNX Upsample operation",
    "description": "Upsample the input tensor.\n  Each dimension value of the output tensor is:\n    output_dimension = floor(input_dimension * scale).",
    "inputs": [
      { "name": "X", "type": "AnyTypeOf" },
      { "name": "scales", "type": "TensorOf" }
    ],
    "outputs": [
      { "name": "Y", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "mode", "type": "DefaultValuedStrAttr" }
    ]
  },
  {
    "name": "onnx.UpsampleV7",
    "summary": "ONNX Upsample operation",
    "description": "Upsample the input tensor.\n  Each dimension value of the output tensor is:\n    output_dimension = floor(input_dimension * scale).",
    "inputs": [
      { "name": "X", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "Y", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "mode", "type": "DefaultValuedStrAttr" },
      { "name": "scales", "type": "F32ArrayAttr" }
    ]
  },
  {
    "name": "onnx.Where",
    "summary": "ONNX Where operation",
    "description": "Return elements, either from X or Y, depending on condition.\n  Where behaves like\n  [numpy.where](https://docs.scipy.org/doc/numpy/reference/generated/numpy.where.html)\n  with three parameters.\n  \n  This operator supports **multidirectional (i.e., Numpy-style) broadcasting**; for more details please check [the doc](Broadcasting.md).",
    "inputs": [
      { "name": "condition", "type": "TensorOf" },
      { "name": "X", "type": "AnyTypeOf" },
      { "name": "Y", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTypeOf" }
    ]
  },
  {
    "name": "onnx.Xor",
    "summary": "ONNX Xor operation",
    "description": "Returns the tensor resulted from performing the `xor` logical operation\n  elementwise on the input tensors `A` and `B` (with Numpy-style broadcasting support).\n  \n  This operator supports **multidirectional (i.e., Numpy-style) broadcasting**; for more details please check [the doc](Broadcasting.md).",
    "inputs": [
      { "name": "A", "type": "TensorOf" },
      { "name": "B", "type": "TensorOf" }
    ],
    "outputs": [
      { "name": "C", "type": "TensorOf" }
    ]
  },
  {
    "name": "onnx.Yield",
    "summary": "ONNX yield operation",
    "description": "The `onnx.Yield` operation represents a yield operation within an ONNX subgraph.\n    The operation takes variable number of operands and produces no results.\n\n    This operation is not part of the standard and was added to assist onnx-mlir.\n    It terminates a ONNXLoop/Scan/IfOp region.",
    "inputs": [
      { "name": "operands", "type": "Variadic" }
    ],
    "assemblyFormat": "attr-dict ($operands^ `:` type($operands))?"
  },
  {
    "name": "onnx.ZipMap",
    "summary": "ONNX ZipMap operation",
    "description": "Creates a map from the input and the attributes.<br>\n      The values are provided by the input tensor, while the keys are specified by the attributes.\n      Must provide keys in either classlabels_strings or classlabels_int64s (but not both).<br>\n      The columns of the tensor correspond one-by-one to the keys specified by the attributes. There must be as many columns as keys.<br>",
    "inputs": [
      { "name": "X", "type": "TensorOf" }
    ],
    "outputs": [
      { "name": "Z", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "classlabels_int64s", "type": "OptionalAttr" },
      { "name": "classlabels_strings", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "quant.dcast",
    "summary": "Dequantize cast operation",
    "description": "Convert an input quantized value into its expressed floating-point value.\n    The dequantization process consists of the following steps:\n\n    ```\n    def dequantize(quantizedValue: quantizedType) -> expressedType:\n        storedValue = reinterpretCast(quantizedValue, storageType)\n        storedValueFloat = convertIntToFloat(storedValue, expressedType)\n        zeroPointFloat = convertIntToFloat(zeroPoint, expressedType)\n        expressedValue = (storedValueFloat - zeroPointFloat) * scale\n        return expressedValue\n    ```\n\n    Here, `storageType`, `expressedType`, `scale`, and `zeroPoint` are obtained\n    from the corresponding parameters encoded in `quantizedType`. For\n    per-channel quantization, the appropriate `scale` and `zeroPoint` values\n    are used for each tensor element computation according to the channel the\n    element belongs to.\n    \n    The numerical results produced by the algorithm above may vary depending on\n    the rounding methods used by `convertIntToFloat()`, subtraction (`-`), and\n    multiplication (`*`). This operation does not define specific rounding\n    methods; instead, it is the responsibility of a transform pipeline to\n    determine which rounding method to apply when this operation is broken down\n    into lower-level dialects.\n\n    The operation must satisfy the following syntactic constraints:\n\n    - Operand `input` must be a scalar or tensor of type `!quant.uniform`.\n\n    - The result type must be a floating-point scalar or tensor.\n\n    - The `expressedType` parameter of the `!quant.uniform` type of the input\n      must match the floating-point type of the result.\n\n    - The operand and result types must be both scalars or both tensors. If\n      tensors, they must be both ranked or both unranked. If ranked, both must\n      have the same shape, including matching static and dynamic dimensions.\n\n    - If the operand uses per-channel quantization, its `!quant.uniform` type\n      must adhere to the [Per-axis quantization\n      integrity](#per-axis-quantization-integrity) guidelines.\n\n    Examples:\n\n    ```\n    // Dequantize a scalar quantized value\n    %result = quant.dcast %input : !quant.uniform<i8:f32, 2.0> to f32\n\n    // Dequantize a dynamically shaped tensor of quantized values\n    %result = quant.dcast %input : tensor<?x!quant.uniform<i8:f32, 2.0>> to tensor<?xf32>\n\n    // Dequantize an unranked tensor using per-axis quantization information\n    %result = quant.dcast %input : tensor<*x!quant.uniform<i8:f32:1, {2.0, 3.0}>> to tensor<*xf32>\n    ```",
    "inputs": [
      { "name": "input", "type": "quant_QuantizedScalarOrTensor" }
    ],
    "outputs": [
      { "name": "result", "type": "quant_FloatScalarOrTensor" }
    ],
    "assemblyFormat": "$input attr-dict `:` type($input) `to` type($result)"
  },
  {
    "name": "quant.qcast",
    "summary": "Quantize cast operation",
    "description": "Convert a floating-point value to a quantized type. The quantization\n    process consists of the following steps:\n\n    ```\n    def quantize(expressedValue: expressedType) -> quantizedType:\n        zeroPointFloat = convertIntToFloat(zeroPoint, expressedType)\n        scaledValue = expressedValue / scale\n        storedValueFloat = scaledValue + zeroPointFloat\n        storedValue = convertFloatToInt(storedValueFloat, storageType)\n        storedValueClamped = clamp(storedValue, storageMin, storageMax)\n        quantizedValue = reinterpretCast(storedValueClamped, quantizedType)\n        return quantizedValue\n    ```\n\n    Here, `storageType`, `storageMin`, `storageMax`, `expressedType`, `scale`,\n    and `zeroPoint` are obtained from the corresponding parameters encoded in\n    `quantizedType`. For per-channel quantization, the appropriate `scale` and\n    `zeroPoint` values are used for each tensor element computation according\n    to the channel the element belongs to.\n\n    The numerical results produced by the algorithm above may vary depending on\n    the rounding methods used by `convertIntToFloat()`, `convertFloatToInt()`,\n    `clamp()`, division (`/`), and addition (`+`). This operation does not\n    define specific rounding methods; instead, it is the responsibility of a\n    transform pipeline to determine which rounding method to apply when this\n    operation is broken down into lower-level dialects.\n\n    The operation must satisfy the following syntactic constraints:\n\n    - Operand `input` must be a floating-point scalar or tensor.\n\n    - The result type must be a scalar or tensor of type `!quant.uniform`.\n\n    - The `expressedType` parameter in the `!quant.uniform` type of the result\n      must match the floating-point type of the input.\n\n    - The operand and result types must be both scalars or both tensors. If\n      tensors, they must be both ranked or both unranked. If ranked, both must\n      have the same shape, including matching static and dynamic dimensions.\n\n    - If the result uses per-channel quantization, its `!quant.uniform` type\n      must adhere to the [Per-axis quantization\n      integrity](#per-axis-quantization-integrity) guidelines.\n\n    Examples:\n\n    ```\n    // Quantize a scalar floating-point value\n    %result = quant.qcast %input : f32 to !quant.uniform<i8:f32, 2.0>\n\n    // Quantize a dynamically shaped tensor of quantized values\n    %result = quant.qcast %input : tensor<?xf32> to tensor<?x!quant.uniform<i8:f32, 2.0>>\n\n    // Quantize an unranked tensor using per-axis quantization information\n    %result = quant.qcast %input : tensor<*xf32> to tensor<*x!quant.uniform<i8:f32:1, {2.0, 3.0}>>\n    ```",
    "inputs": [
      { "name": "input", "type": "quant_FloatScalarOrTensor" }
    ],
    "outputs": [
      { "name": "result", "type": "quant_QuantizedScalarOrTensor" }
    ],
    "assemblyFormat": "$input attr-dict `:` type($input) `to` type($result)"
  },
  {
    "name": "quant.scast",
    "summary": "Storage cast operation",
    "description": "Convert a value from a quantized type to the corresponding signless integer\n    storage type, or vice versa. This conversion simply involves a\n    reinterpretation of the input bits and does not involve any data\n    manipulation.\n\n    The following syntactic restrictions must be met:\n\n    - Operand `input` must be a scalar or tensor of a signless integer or\n      `!quant.uniform` type.\n\n    - The result must be a scalar or tensor of a signless integer or\n      `!quant.uniform` type.\n\n    - If the operand is a scalar or tensor of type integer, the result must be\n      a scalar or tensor of type `!quant.uniform`, and vice versa.\n\n    - The operand and result must be both scalars or both tensors. If tensors,\n      they must be both ranked or both unranked. If ranked, both must have the\n      same shape, including matching static and dynamic dimensions.\n\n    - The width of the `storageType` parameter of the quantized type of the\n      operand or result must match the width of the signless integer type of\n      the operand or result.\n\n    - If the operand or result uses per-channel quantization, its\n      `!quant.uniform` type must adhere to the [Per-axis quantization\n      integrity](#per-axis-quantization-integrity) guidelines.\n\n    Examples:\n\n    ```\n    // Cast a scalar quantized value into its storage type\n    %result = quant.scast %input : !quant.uniform<i8:f32, 2.0> to i8\n\n    // Cast a dynamically shaped tensor of quantized values into their storage type\n    %result = quant.scast %input : tensor<?x!quant.uniform<i8:f32, 2.0>> to tensor<?xi8>\n\n    // Cast an unranked tensor of signless integers into a quantized type using\n    // per-channel quantization\n    %result = quant.scast %input : tensor<*xi8> to tensor<*x!quant.uniform<i8:f32:1, {2.0, 3.0}>>\n    ```",
    "inputs": [
      { "name": "input", "type": "quant_IntegerOrQuantizedScalarOrTensor" }
    ],
    "outputs": [
      { "name": "result", "type": "quant_IntegerOrQuantizedScalarOrTensor" }
    ],
    "assemblyFormat": "$input attr-dict `:` type($input) `to` type($result)"
  },
  {
    "name": "spirv.AtomicAnd",
    "summary": "Perform the following steps atomically with respect to any other atomic\n    accesses within Scope to the same location:",
    "description": "1) load through Pointer to get an Original Value,\n\n    2) get a New Value by the bitwise AND of Original Value and Value, and\n\n    3) store the New Value back through Pointer.\n\n    The instruction’s result is the Original Value.\n\n    Result Type must be an integer type scalar.\n\n     The type of Value must be the same as Result Type.  The type of the\n    value pointed to by Pointer must be the same as Result Type.\n\n    Memory must be a valid memory Scope.\n\n    <!-- End of AutoGen section -->\n\n    #### Example:\n\n    ```mlir\n    %0 = spirv.AtomicAnd <Device> <None> %pointer, %value :\n                       !spirv.ptr<i32, StorageBuffer>\n    ```",
    "inputs": [
      { "name": "pointer", "type": "SPIRV_AnyPtr" },
      { "name": "value", "type": "SPIRV_Integer" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_Integer" }
    ],
    "attributes": [
      { "name": "memory_scope", "type": "SPIRV_ScopeAttr" },
      { "name": "semantics", "type": "SPIRV_MemorySemanticsAttr" }
    ],
    "assemblyFormat": "$memory_scope $semantics operands attr-dict `:` type($pointer)"
  },
  {
    "name": "spirv.AtomicCompareExchange",
    "summary": "Perform the following steps atomically with respect to any other atomic\n    accesses within Scope to the same location:",
    "description": "1) load through Pointer to get an Original Value,\n\n    2) get a New Value from Value only if Original Value equals Comparator,\n    and\n\n    3) store the New Value back through Pointer'only if 'Original Value\n    equaled Comparator.\n\n    The instruction's result is the Original Value.\n\n    Result Type must be an integer type scalar.\n\n    Use Equal for the memory semantics of this instruction when Value and\n    Original Value compare equal.\n\n    Use Unequal for the memory semantics of this instruction when Value and\n    Original Value compare unequal. Unequal must not be set to Release or\n    Acquire and Release. In addition, Unequal cannot be set to a stronger\n    memory-order then Equal.\n\n     The type of Value must be the same as Result Type.  The type of the\n    value pointed to by Pointer must be the same as Result Type.  This type\n    must also match the type of Comparator.\n\n    Memory is a memory Scope.\n\n    <!-- End of AutoGen section -->\n\n    #### Example:\n\n    ```\n    %0 = spirv.AtomicCompareExchange <Workgroup> <Acquire> <None>\n                                    %pointer, %value, %comparator\n                                    : !spirv.ptr<i32, WorkGroup>\n    ```",
    "inputs": [
      { "name": "pointer", "type": "SPIRV_AnyPtr" },
      { "name": "value", "type": "SPIRV_Integer" },
      { "name": "comparator", "type": "SPIRV_Integer" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_Integer" }
    ],
    "attributes": [
      { "name": "memory_scope", "type": "SPIRV_ScopeAttr" },
      { "name": "equal_semantics", "type": "SPIRV_MemorySemanticsAttr" },
      { "name": "unequal_semantics", "type": "SPIRV_MemorySemanticsAttr" }
    ],
    "assemblyFormat": "$memory_scope $equal_semantics $unequal_semantics operands attr-dict `:`\n      type($pointer)"
  },
  {
    "name": "spirv.AtomicCompareExchangeWeak",
    "summary": "Deprecated (use OpAtomicCompareExchange).",
    "description": "Has the same semantics as OpAtomicCompareExchange.\n\n    Memory must be a valid memory Scope.\n\n    <!-- End of AutoGen section -->\n\n    #### Example:\n\n    ```mlir\n    %0 = spirv.AtomicCompareExchangeWeak <Workgroup> <Acquire> <None>\n                                       %pointer, %value, %comparator\n                                       : !spirv.ptr<i32, WorkGroup>\n    ```",
    "inputs": [
      { "name": "pointer", "type": "SPIRV_AnyPtr" },
      { "name": "value", "type": "SPIRV_Integer" },
      { "name": "comparator", "type": "SPIRV_Integer" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_Integer" }
    ],
    "attributes": [
      { "name": "memory_scope", "type": "SPIRV_ScopeAttr" },
      { "name": "equal_semantics", "type": "SPIRV_MemorySemanticsAttr" },
      { "name": "unequal_semantics", "type": "SPIRV_MemorySemanticsAttr" }
    ],
    "assemblyFormat": "$memory_scope $equal_semantics $unequal_semantics operands attr-dict `:`\n      type($pointer)"
  },
  {
    "name": "spirv.AtomicExchange",
    "summary": "Perform the following steps atomically with respect to any other atomic\n    accesses within Scope to the same location:",
    "description": "1) load through Pointer to get an Original Value,\n\n    2) get a New Value from copying Value, and\n\n    3) store the New Value back through Pointer.\n\n    The instruction's result is the Original Value.\n\n    Result Type must be a scalar of integer type or floating-point type.\n\n     The type of Value must be the same as Result Type.  The type of the\n    value pointed to by Pointer must be the same as Result Type.\n\n    Memory is a memory Scope.\n\n    <!-- End of AutoGen section -->\n\n    #### Example:\n\n    ```mlir\n    %0 = spirv.AtomicExchange <Workgroup> <Acquire> %pointer, %value,\n                            : !spirv.ptr<i32, WorkGroup>\n    ```",
    "inputs": [
      { "name": "pointer", "type": "SPIRV_AnyPtr" },
      { "name": "value", "type": "SPIRV_Numerical" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_Numerical" }
    ],
    "attributes": [
      { "name": "memory_scope", "type": "SPIRV_ScopeAttr" },
      { "name": "semantics", "type": "SPIRV_MemorySemanticsAttr" }
    ],
    "assemblyFormat": "$memory_scope $semantics operands attr-dict `:` type($pointer)"
  },
  {
    "name": "spirv.AtomicIAdd",
    "summary": "Perform the following steps atomically with respect to any other atomic\n    accesses within Scope to the same location:",
    "description": "1) load through Pointer to get an Original Value,\n\n    2) get a New Value by integer addition of Original Value and Value, and\n\n    3) store the New Value back through Pointer.\n\n    The instruction’s result is the Original Value.\n\n    Result Type must be an integer type scalar.\n\n     The type of Value must be the same as Result Type.  The type of the\n    value pointed to by Pointer must be the same as Result Type.\n\n    Memory must be a valid memory Scope.\n\n    <!-- End of AutoGen section -->\n\n    #### Example:\n\n    ```mlir\n    %0 = spirv.AtomicIAdd <Device> <None> %pointer, %value :\n                        !spirv.ptr<i32, StorageBuffer>\n    ```",
    "inputs": [
      { "name": "pointer", "type": "SPIRV_AnyPtr" },
      { "name": "value", "type": "SPIRV_Integer" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_Integer" }
    ],
    "attributes": [
      { "name": "memory_scope", "type": "SPIRV_ScopeAttr" },
      { "name": "semantics", "type": "SPIRV_MemorySemanticsAttr" }
    ],
    "assemblyFormat": "$memory_scope $semantics operands attr-dict `:` type($pointer)"
  },
  {
    "name": "spirv.AtomicIDecrement",
    "summary": "Perform the following steps atomically with respect to any other atomic\n    accesses within Scope to the same location:",
    "description": "1) load through Pointer to get an Original Value,\n\n    2) get a New Value through integer subtraction of 1 from Original Value,\n    and\n\n    3) store the New Value back through Pointer.\n\n    The instruction’s result is the Original Value.\n\n    Result Type must be an integer type scalar.  The type of the value\n    pointed to by Pointer must be the same as Result Type.\n\n    Memory must be a valid memory Scope.\n\n    <!-- End of AutoGen section -->\n\n    #### Example:\n\n    ```mlir\n    %0 = spirv.AtomicIDecrement <Device> <None> %pointer :\n                              !spirv.ptr<i32, StorageBuffer>\n    ```",
    "inputs": [
      { "name": "pointer", "type": "SPIRV_AnyPtr" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_Integer" }
    ],
    "attributes": [
      { "name": "memory_scope", "type": "SPIRV_ScopeAttr" },
      { "name": "semantics", "type": "SPIRV_MemorySemanticsAttr" }
    ],
    "assemblyFormat": "$memory_scope $semantics operands attr-dict `:` type($pointer)"
  },
  {
    "name": "spirv.AtomicIIncrement",
    "summary": "Perform the following steps atomically with respect to any other atomic\n    accesses within Scope to the same location:",
    "description": "1) load through Pointer to get an Original Value,\n\n    2) get a New Value through integer addition of 1 to Original Value, and\n\n    3) store the New Value back through Pointer.\n\n    The instruction’s result is the Original Value.\n\n    Result Type must be an integer type scalar.  The type of the value\n    pointed to by Pointer must be the same as Result Type.\n\n    Memory must be a valid memory Scope.\n\n    <!-- End of AutoGen section -->\n\n    #### Example:\n\n    ```mlir\n    %0 = spirv.AtomicIncrement <Device> <None> %pointer :\n                             !spirv.ptr<i32, StorageBuffer>\n    ```",
    "inputs": [
      { "name": "pointer", "type": "SPIRV_AnyPtr" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_Integer" }
    ],
    "attributes": [
      { "name": "memory_scope", "type": "SPIRV_ScopeAttr" },
      { "name": "semantics", "type": "SPIRV_MemorySemanticsAttr" }
    ],
    "assemblyFormat": "$memory_scope $semantics operands attr-dict `:` type($pointer)"
  },
  {
    "name": "spirv.AtomicISub",
    "summary": "Perform the following steps atomically with respect to any other atomic\n    accesses within Scope to the same location:",
    "description": "1) load through Pointer to get an Original Value,\n\n    2) get a New Value by integer subtraction of Value from Original Value,\n    and\n\n    3) store the New Value back through Pointer.\n\n    The instruction’s result is the Original Value.\n\n    Result Type must be an integer type scalar.\n\n     The type of Value must be the same as Result Type.  The type of the\n    value pointed to by Pointer must be the same as Result Type.\n\n    Memory must be a valid memory Scope.\n\n    <!-- End of AutoGen section -->\n\n    #### Example:\n\n    ```mlir\n    %0 = spirv.AtomicISub <Device> <None> %pointer, %value :\n                        !spirv.ptr<i32, StorageBuffer>\n    ```",
    "inputs": [
      { "name": "pointer", "type": "SPIRV_AnyPtr" },
      { "name": "value", "type": "SPIRV_Integer" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_Integer" }
    ],
    "attributes": [
      { "name": "memory_scope", "type": "SPIRV_ScopeAttr" },
      { "name": "semantics", "type": "SPIRV_MemorySemanticsAttr" }
    ],
    "assemblyFormat": "$memory_scope $semantics operands attr-dict `:` type($pointer)"
  },
  {
    "name": "spirv.AtomicOr",
    "summary": "Perform the following steps atomically with respect to any other atomic\n    accesses within Scope to the same location:",
    "description": "1) load through Pointer to get an Original Value,\n\n    2) get a New Value by the bitwise OR of Original Value and Value, and\n\n    3) store the New Value back through Pointer.\n\n    The instruction’s result is the Original Value.\n\n    Result Type must be an integer type scalar.\n\n     The type of Value must be the same as Result Type.  The type of the\n    value pointed to by Pointer must be the same as Result Type.\n\n    Memory must be a valid memory Scope.\n\n    <!-- End of AutoGen section -->\n\n    #### Example:\n\n    ```mlir\n    %0 = spirv.AtomicOr <Device> <None> %pointer, %value :\n                      !spirv.ptr<i32, StorageBuffer>\n    ```",
    "inputs": [
      { "name": "pointer", "type": "SPIRV_AnyPtr" },
      { "name": "value", "type": "SPIRV_Integer" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_Integer" }
    ],
    "attributes": [
      { "name": "memory_scope", "type": "SPIRV_ScopeAttr" },
      { "name": "semantics", "type": "SPIRV_MemorySemanticsAttr" }
    ],
    "assemblyFormat": "$memory_scope $semantics operands attr-dict `:` type($pointer)"
  },
  {
    "name": "spirv.AtomicSMax",
    "summary": "Perform the following steps atomically with respect to any other atomic\n    accesses within Scope to the same location:",
    "description": "1) load through Pointer to get an Original Value,\n\n    2) get a New Value by finding the largest signed integer of Original\n    Value and Value, and\n\n    3) store the New Value back through Pointer.\n\n    The instruction’s result is the Original Value.\n\n    Result Type must be an integer type scalar.\n\n     The type of Value must be the same as Result Type.  The type of the\n    value pointed to by Pointer must be the same as Result Type.\n\n    Memory must be a valid memory Scope.\n\n    <!-- End of AutoGen section -->\n\n    #### Example:\n\n    ```mlir\n    %0 = spirv.AtomicSMax <Device> <None> %pointer, %value :\n                        !spirv.ptr<i32, StorageBuffer>\n    ```",
    "inputs": [
      { "name": "pointer", "type": "SPIRV_AnyPtr" },
      { "name": "value", "type": "SPIRV_Integer" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_Integer" }
    ],
    "attributes": [
      { "name": "memory_scope", "type": "SPIRV_ScopeAttr" },
      { "name": "semantics", "type": "SPIRV_MemorySemanticsAttr" }
    ],
    "assemblyFormat": "$memory_scope $semantics operands attr-dict `:` type($pointer)"
  },
  {
    "name": "spirv.AtomicSMin",
    "summary": "Perform the following steps atomically with respect to any other atomic\n    accesses within Scope to the same location:",
    "description": "1) load through Pointer to get an Original Value,\n\n    2) get a New Value by finding the smallest signed integer of Original\n    Value and Value, and\n\n    3) store the New Value back through Pointer.\n\n    The instruction’s result is the Original Value.\n\n    Result Type must be an integer type scalar.\n\n     The type of Value must be the same as Result Type.  The type of the\n    value pointed to by Pointer must be the same as Result Type.\n\n    Memory must be a valid memory Scope.\n\n    <!-- End of AutoGen section -->\n\n    #### Example:\n\n    ```mlir\n    %0 = spirv.AtomicSMin <Device> <None> %pointer, %value :\n                        !spirv.ptr<i32, StorageBuffer>\n    ```",
    "inputs": [
      { "name": "pointer", "type": "SPIRV_AnyPtr" },
      { "name": "value", "type": "SPIRV_Integer" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_Integer" }
    ],
    "attributes": [
      { "name": "memory_scope", "type": "SPIRV_ScopeAttr" },
      { "name": "semantics", "type": "SPIRV_MemorySemanticsAttr" }
    ],
    "assemblyFormat": "$memory_scope $semantics operands attr-dict `:` type($pointer)"
  },
  {
    "name": "spirv.AtomicUMax",
    "summary": "Perform the following steps atomically with respect to any other atomic\n    accesses within Scope to the same location:",
    "description": "1) load through Pointer to get an Original Value,\n\n    2) get a New Value by finding the largest unsigned integer of Original\n    Value and Value, and\n\n    3) store the New Value back through Pointer.\n\n    The instruction’s result is the Original Value.\n\n    Result Type must be an integer type scalar.\n\n     The type of Value must be the same as Result Type.  The type of the\n    value pointed to by Pointer must be the same as Result Type.\n\n    Memory must be a valid memory Scope.\n\n    <!-- End of AutoGen section -->\n\n    #### Example:\n\n    ```mlir\n    %0 = spirv.AtomicUMax <Device> <None> %pointer, %value :\n                        !spirv.ptr<i32, StorageBuffer>\n    ```",
    "inputs": [
      { "name": "pointer", "type": "SPIRV_AnyPtr" },
      { "name": "value", "type": "SPIRV_Integer" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_Integer" }
    ],
    "attributes": [
      { "name": "memory_scope", "type": "SPIRV_ScopeAttr" },
      { "name": "semantics", "type": "SPIRV_MemorySemanticsAttr" }
    ],
    "assemblyFormat": "$memory_scope $semantics operands attr-dict `:` type($pointer)"
  },
  {
    "name": "spirv.AtomicUMin",
    "summary": "Perform the following steps atomically with respect to any other atomic\n    accesses within Scope to the same location:",
    "description": "1) load through Pointer to get an Original Value,\n\n    2) get a New Value by finding the smallest unsigned integer of Original\n    Value and Value, and\n\n    3) store the New Value back through Pointer.\n\n    The instruction’s result is the Original Value.\n\n    Result Type must be an integer type scalar.\n\n     The type of Value must be the same as Result Type.  The type of the\n    value pointed to by Pointer must be the same as Result Type.\n\n    Memory must be a valid memory Scope.\n\n    <!-- End of AutoGen section -->\n\n    #### Example:\n\n    ```mlir\n    %0 = spirv.AtomicUMin <Device> <None> %pointer, %value :\n                        !spirv.ptr<i32, StorageBuffer>\n    ```",
    "inputs": [
      { "name": "pointer", "type": "SPIRV_AnyPtr" },
      { "name": "value", "type": "SPIRV_Integer" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_Integer" }
    ],
    "attributes": [
      { "name": "memory_scope", "type": "SPIRV_ScopeAttr" },
      { "name": "semantics", "type": "SPIRV_MemorySemanticsAttr" }
    ],
    "assemblyFormat": "$memory_scope $semantics operands attr-dict `:` type($pointer)"
  },
  {
    "name": "spirv.AtomicXor",
    "summary": "Perform the following steps atomically with respect to any other atomic\n    accesses within Scope to the same location:",
    "description": "1) load through Pointer to get an Original Value,\n\n    2) get a New Value by the bitwise exclusive OR of Original Value and\n    Value, and\n\n    3) store the New Value back through Pointer.\n\n    The instruction’s result is the Original Value.\n\n    Result Type must be an integer type scalar.\n\n     The type of Value must be the same as Result Type.  The type of the\n    value pointed to by Pointer must be the same as Result Type.\n\n    Memory must be a valid memory Scope.\n\n    <!-- End of AutoGen section -->\n\n    #### Example:\n\n    ```mlir\n    %0 = spirv.AtomicXor <Device> <None> %pointer, %value :\n                       !spirv.ptr<i32, StorageBuffer>\n    ```",
    "inputs": [
      { "name": "pointer", "type": "SPIRV_AnyPtr" },
      { "name": "value", "type": "SPIRV_Integer" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_Integer" }
    ],
    "attributes": [
      { "name": "memory_scope", "type": "SPIRV_ScopeAttr" },
      { "name": "semantics", "type": "SPIRV_MemorySemanticsAttr" }
    ],
    "assemblyFormat": "$memory_scope $semantics operands attr-dict `:` type($pointer)"
  },
  {
    "name": "spirv.Bitcast",
    "summary": "Bit pattern-preserving type conversion.",
    "description": "Result Type must be an OpTypePointer, or a scalar or vector of\n    numerical-type.\n\n    Operand must have a type of OpTypePointer, or a scalar or vector of\n    numerical-type. It must be a different type than Result Type.\n\n    If either Result Type or Operand is a pointer, the other must be a\n    pointer (diverges from the SPIR-V spec).\n\n    If Result Type has a different number of components than Operand, the\n    total number of bits in Result Type must equal the total number of bits\n    in Operand. Let L be the type, either Result Type or Operand's type,\n    that has the larger number of components. Let S be the other type, with\n    the smaller number of components. The number of components in L must be\n    an integer multiple of the number of components in S. The first\n    component (that is, the only or lowest-numbered component) of S maps to\n    the first components of L, and so on,  up to the last component of S\n    mapping to the last components of L. Within this mapping, any single\n    component of S (mapping to multiple components of L) maps its lower-\n    ordered bits to the lower-numbered components of L.\n\n    #### Example:\n\n    ```mlir\n    %1 = spirv.Bitcast %0 : f32 to i32\n    %1 = spirv.Bitcast %0 : vector<2xf32> to i64\n    %1 = spirv.Bitcast %0 : !spirv.ptr<f32, Function> to !spirv.ptr<i32, Function>\n    ```",
    "inputs": [
      { "name": "operand", "type": "SPIRV_ScalarOrVectorOrPtr" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_ScalarOrVectorOrPtr" }
    ],
    "assemblyFormat": "$operand attr-dict `:` type($operand) `to` type($result)"
  },
  {
    "name": "spirv.BitCount",
    "summary": "Count the number of set bits in an object.",
    "description": "Results are computed per component.\n\n    Result Type must be a scalar or vector of integer type.  The components\n    must be wide enough to hold the unsigned Width of Base as an unsigned\n    value. That is, no sign bit is needed or counted when checking for a\n    wide enough result width.\n\n    Base must be a scalar or vector of integer type.  It must have the same\n    number of components as Result Type.\n\n    The result is the unsigned value that is the number of bits in Base that\n    are 1.\n\n    #### Example:\n\n    ```mlir\n    %2 = spirv.BitCount %0: i32\n    %3 = spirv.BitCount %1: vector<4xi32>\n    ```",
    "inputs": [
      { "name": "operand", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "assemblyFormat": "$operand `:` type($operand) attr-dict"
  },
  {
    "name": "spirv.BitFieldInsert",
    "summary": "Make a copy of an object, with a modified bit field that comes from\n    another object.",
    "description": "Results are computed per component.\n\n    Result Type must be a scalar or vector of integer type.\n\n    The type of Base and Insert must be the same as Result Type.\n\n    Any result bits numbered outside [Offset, Offset + Count -  1]\n    (inclusive) will come from the corresponding bits in Base.\n\n    Any result bits numbered in [Offset, Offset + Count -  1] come, in\n    order, from the bits numbered [0, Count - 1] of Insert.\n\n    Count  must be an integer type scalar. Count is the number of bits taken\n    from Insert. It will be consumed as an unsigned value. Count can be 0,\n    in which case the result will be Base.\n\n    Offset  must be an integer type scalar. Offset is the lowest-order bit\n    of the bit field.  It will be consumed as an unsigned value.\n\n    The resulting value is undefined if Count or Offset or their sum is\n    greater than the number of bits in the result.\n\n    #### Example:\n\n    ```mlir\n    %0 = spirv.BitFieldInsert %base, %insert, %offset, %count : vector<3xi32>, i8, i8\n    ```",
    "inputs": [
      { "name": "base", "type": "SPIRV_ScalarOrVectorOf" },
      { "name": "insert", "type": "SPIRV_ScalarOrVectorOf" },
      { "name": "offset", "type": "SPIRV_Integer" },
      { "name": "count", "type": "SPIRV_Integer" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "assemblyFormat": "operands attr-dict `:` type($base) `,` type($offset) `,` type($count)"
  },
  {
    "name": "spirv.BitFieldSExtract",
    "summary": "Extract a bit field from an object, with sign extension.",
    "description": "Results are computed per component.\n\n    Result Type must be a scalar or vector of integer type.\n\n    The type of Base must be the same as Result Type.\n\n    If Count is greater than 0: The bits of Base numbered in [Offset, Offset\n    + Count -  1] (inclusive) become the bits numbered [0, Count - 1] of the\n    result. The remaining bits of the result will all be the same as bit\n    Offset + Count -  1 of Base.\n\n    Count  must be an integer type scalar. Count is the number of bits\n    extracted from Base. It will be consumed as an unsigned value. Count can\n    be 0, in which case the result will be 0.\n\n    Offset  must be an integer type scalar. Offset is the lowest-order bit\n    of the bit field to extract from Base.  It will be consumed as an\n    unsigned value.\n\n    The resulting value is undefined if Count or Offset or their sum is\n    greater than the number of bits in the result.\n\n    #### Example:\n\n    ```mlir\n    %0 = spirv.BitFieldSExtract %base, %offset, %count : vector<3xi32>, i8, i8\n    ```",
    "inputs": [
      { "name": "base", "type": "SPIRV_ScalarOrVectorOf" },
      { "name": "offset", "type": "SPIRV_Integer" },
      { "name": "count", "type": "SPIRV_Integer" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "assemblyFormat": "operands attr-dict `:` type($base) `,` type($offset) `,` type($count)"
  },
  {
    "name": "spirv.BitFieldUExtract",
    "summary": "Extract a bit field from an object, without sign extension.",
    "description": "The semantics are the same as with OpBitFieldSExtract with the exception\n    that there is no sign extension. The remaining bits of the result will\n    all be 0.\n\n    #### Example:\n\n    ```mlir\n    %0 = spirv.BitFieldUExtract %base, %offset, %count : vector<3xi32>, i8, i8\n    ```",
    "inputs": [
      { "name": "base", "type": "SPIRV_ScalarOrVectorOf" },
      { "name": "offset", "type": "SPIRV_Integer" },
      { "name": "count", "type": "SPIRV_Integer" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "assemblyFormat": "operands attr-dict `:` type($base) `,` type($offset) `,` type($count)"
  },
  {
    "name": "spirv.BitReverse",
    "summary": "Reverse the bits in an object.",
    "description": "Results are computed per component.\n\n    Result Type must be a scalar or vector of integer type.\n\n    The type of Base must be the same as Result Type.\n\n    The bit-number n of the result will be taken from bit-number Width - 1 -\n    n of Base, where Width is the OpTypeInt operand of the Result Type.\n\n    #### Example:\n\n    ```mlir\n    %2 = spirv.BitReverse %0 : i32\n    %3 = spirv.BitReverse %1 : vector<4xi32>\n    ```",
    "inputs": [
      { "name": "operand", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "assemblyFormat": "$operand `:` type($operand) attr-dict"
  },
  {
    "name": "spirv.BitwiseAnd",
    "summary": "Result is 1 if both Operand 1 and Operand 2 are 1. Result is 0 if either\n    Operand 1 or Operand 2 are 0.",
    "description": "Results are computed per component, and within each component, per bit.\n\n    Result Type must be a scalar or vector of integer type.  The type of\n    Operand 1 and Operand 2  must be a scalar or vector of integer type.\n    They must have the same number of components as Result Type. They must\n    have the same component width as Result Type.\n\n    #### Example:\n\n    ```mlir\n    %2 = spirv.BitwiseAnd %0, %1 : i32\n    %2 = spirv.BitwiseAnd %0, %1 : vector<4xi32>\n    ```",
    "inputs": [
      { "name": "operand1", "type": "SPIRV_ScalarOrVectorOf" },
      { "name": "operand2", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "assemblyFormat": "operands attr-dict `:` type($result)"
  },
  {
    "name": "spirv.BitwiseOr",
    "summary": "Result is 1 if either Operand 1 or Operand 2 is 1. Result is 0 if both\n    Operand 1 and Operand 2 are 0.",
    "description": "Results are computed per component, and within each component, per bit.\n\n    Result Type must be a scalar or vector of integer type.  The type of\n    Operand 1 and Operand 2  must be a scalar or vector of integer type.\n    They must have the same number of components as Result Type. They must\n    have the same component width as Result Type.\n\n    #### Example:\n\n    ```mlir\n    %2 = spirv.BitwiseOr %0, %1 : i32\n    %2 = spirv.BitwiseOr %0, %1 : vector<4xi32>\n    ```",
    "inputs": [
      { "name": "operand1", "type": "SPIRV_ScalarOrVectorOf" },
      { "name": "operand2", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "assemblyFormat": "operands attr-dict `:` type($result)"
  },
  {
    "name": "spirv.BitwiseXor",
    "summary": "Result is 1 if exactly one of Operand 1 or Operand 2 is 1. Result is 0\n    if Operand 1 and Operand 2 have the same value.",
    "description": "Results are computed per component, and within each component, per bit.\n\n    Result Type must be a scalar or vector of integer type.  The type of\n    Operand 1 and Operand 2  must be a scalar or vector of integer type.\n    They must have the same number of components as Result Type. They must\n    have the same component width as Result Type.\n\n    #### Example:\n\n    ```mlir\n    %2 = spirv.BitwiseXor %0, %1 : i32\n    %2 = spirv.BitwiseXor %0, %1 : vector<4xi32>\n    ```",
    "inputs": [
      { "name": "operand1", "type": "SPIRV_ScalarOrVectorOf" },
      { "name": "operand2", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "assemblyFormat": "operands attr-dict `:` type($result)"
  },
  {
    "name": "spirv.Branch",
    "summary": "Unconditional branch to target block.",
    "description": "This instruction must be the last instruction in a block.\n\n    #### Example:\n\n    ```mlir\n    spirv.Branch ^target\n    spirv.Branch ^target(%0, %1: i32, f32)\n    ```",
    "inputs": [
      { "name": "targetOperands", "type": "Variadic" }
    ],
    "assemblyFormat": "$target (`(` $targetOperands^ `:` type($targetOperands) `)`)? attr-dict"
  },
  {
    "name": "spirv.BranchConditional",
    "summary": "If Condition is true, branch to true block, otherwise branch to false\n    block.",
    "description": "Condition must be a Boolean type scalar.\n\n    Branch weights are unsigned 32-bit integer literals. There must be\n    either no Branch Weights or exactly two branch weights. If present, the\n    first is the weight for branching to True Label, and the second is the\n    weight for branching to False Label. The implied probability that a\n    branch is taken is its weight divided by the sum of the two Branch\n    weights. At least one weight must be non-zero. A weight of zero does not\n    imply a branch is dead or permit its removal; branch weights are only\n    hints. The two weights must not overflow a 32-bit unsigned integer when\n    added together.\n\n    This instruction must be the last instruction in a block.\n\n    <!-- End of AutoGen section -->\n\n    ```\n    branch-conditional-op ::= `spirv.BranchConditional` ssa-use\n                              (`[` integer-literal, integer-literal `]`)?\n                              `,` successor `,` successor\n    successor ::= bb-id branch-use-list?\n    branch-use-list ::= `(` ssa-use-list `:` type-list-no-parens `)`\n    ```\n\n    #### Example:\n\n    ```mlir\n    spirv.BranchConditional %condition, ^true_branch, ^false_branch\n    spirv.BranchConditional %condition, ^true_branch(%0: i32), ^false_branch(%1: i32)\n    ```",
    "inputs": [
      { "name": "condition", "type": "SPIRV_Bool" },
      { "name": "trueTargetOperands", "type": "Variadic" },
      { "name": "falseTargetOperands", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "branch_weights", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "spirv.CompositeConstruct",
    "summary": "Construct a new composite object from a set of constituent objects.",
    "description": "Result Type must be a composite type, whose top-level\n    members/elements/components/columns have the same type as the types of\n    the operands, with one exception. The exception is that for constructing\n    a vector, the operands may also be vectors with the same component type\n    as the Result Type component type. When constructing a vector, the total\n    number of components in all the operands must equal the number of\n    components in Result Type.\n\n    Constituents will become members of a structure, or elements of an\n    array, or components of a vector, or columns of a matrix. There must be\n    exactly one Constituent for each top-level\n    member/element/component/column of the result, with one exception. The\n    exception is that for constructing a vector, a contiguous subset of the\n    scalars consumed can be represented by a vector operand instead. The\n    Constituents must appear in the order needed by the definition of the\n    type of the result. When constructing a vector, there must be at least\n    two Constituent operands.\n\n    #### Example:\n\n    ```mlir\n    %a = spirv.CompositeConstruct %1, %2, %3 : vector<3xf32>\n    %b = spirv.CompositeConstruct %a, %1 : (vector<3xf32>, f32) -> vector<4xf32>\n\n    %c = spirv.CompositeConstruct %1 :\n      (f32) -> !spirv.coopmatrix<4x4xf32, Subgroup, MatrixA>\n\n    %d = spirv.CompositeConstruct %a, %4, %5 :\n      (vector<3xf32>, !spirv.array<4xf32>, !spirv.struct<(f32)>) ->\n        !spirv.struct<(vector<3xf32>, !spirv.array<4xf32>, !spirv.struct<(f32)>)>\n    ```",
    "inputs": [
      { "name": "constituents", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_Composite" }
    ],
    "assemblyFormat": "$constituents attr-dict `:` `(` type(operands) `)` `->` type($result)"
  },
  {
    "name": "spirv.CompositeExtract",
    "summary": "Extract a part of a composite object.",
    "description": "Result Type must be the type of object selected by the last provided\n    index.  The instruction result is the extracted object.\n\n    Composite is the composite to extract from.\n\n    Indexes walk the type hierarchy, potentially down to component\n    granularity, to select the part to extract. All indexes must be in\n    bounds.  All composite constituents use zero-based numbering, as\n    described by their OpType… instruction.\n\n    <!-- End of AutoGen section -->\n\n    ```\n    composite-extract-op ::= ssa-id `=` `spirv.CompositeExtract` ssa-use\n                             `[` integer-literal (',' integer-literal)* `]`\n                             `:` composite-type\n    ```\n\n    #### Example:\n\n    ```mlir\n    %0 = spirv.Variable : !spirv.ptr<!spirv.array<4x!spirv.array<4xf32>>, Function>\n    %1 = spirv.Load \"Function\" %0 [\"Volatile\"] : !spirv.array<4x!spirv.array<4xf32>>\n    %2 = spirv.CompositeExtract %1[1 : i32] : !spirv.array<4x!spirv.array<4xf32>>\n    ```",
    "inputs": [
      { "name": "composite", "type": "SPIRV_Composite" }
    ],
    "outputs": [
      { "name": "component", "type": "SPIRV_Type" }
    ],
    "attributes": [
      { "name": "indices", "type": "I32ArrayAttr" }
    ]
  },
  {
    "name": "spirv.CompositeInsert",
    "summary": "Make a copy of a composite object, while modifying one part of it.",
    "description": "Result Type must be the same type as Composite.\n\n    Object is the object to use as the modified part.\n\n    Composite is the composite to copy all but the modified part from.\n\n    Indexes walk the type hierarchy of Composite to the desired depth,\n    potentially down to component granularity, to select the part to modify.\n    All indexes must be in bounds. All composite constituents use zero-based\n    numbering, as described by their OpType… instruction. The type of the\n    part selected to modify must match the type of Object.\n\n    <!-- End of AutoGen section -->\n\n    ```\n    composite-insert-op ::= ssa-id `=` `spirv.CompositeInsert` ssa-use, ssa-use\n                            `[` integer-literal (',' integer-literal)* `]`\n                            `:` object-type `into` composite-type\n    ```\n\n    #### Example:\n\n    ```mlir\n    %0 = spirv.CompositeInsert %object, %composite[1 : i32] : f32 into !spirv.array<4xf32>\n    ```",
    "inputs": [
      { "name": "object", "type": "SPIRV_Type" },
      { "name": "composite", "type": "SPIRV_Composite" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_Composite" }
    ],
    "attributes": [
      { "name": "indices", "type": "I32ArrayAttr" }
    ]
  },
  {
    "name": "spirv.Constant",
    "summary": "Declare a new integer-type or floating-point-type scalar constant.",
    "description": "This op declares a SPIR-V normal constant. SPIR-V has multiple constant\n    instructions covering different constant types:\n\n    * `OpConstantTrue` and `OpConstantFalse` for boolean constants\n    * `OpConstant` for scalar constants\n    * `OpConstantComposite` for composite constants\n    * `OpConstantNull` for null constants\n    * ...\n\n    Having such a plethora of constant instructions renders IR transformations\n    more tedious. Therefore, we use a single `spirv.Constant` op to represent\n    them all. Note that conversion between those SPIR-V constant instructions\n    and this op is purely mechanical; so it can be scoped to the binary\n    (de)serialization process.\n\n    <!-- End of AutoGen section -->\n\n    ```\n    spirv.Constant-op ::= ssa-id `=` `spirv.Constant` attribute-value\n                        (`:` spirv-type)?\n    ```\n\n    #### Example:\n\n    ```mlir\n    %0 = spirv.Constant true\n    %1 = spirv.Constant dense<[2.0, 3.0]> : vector<2xf32>\n    %2 = spirv.Constant [dense<3.0> : vector<2xf32>] : !spirv.array<1xvector<2xf32>>\n    ```\n\n    TODO: support constant structs",
    "outputs": [
      { "name": "constant", "type": "SPIRV_Type" }
    ],
    "attributes": [
      { "name": "value", "type": "AnyAttr" }
    ]
  },
  {
    "name": "spirv.ControlBarrier",
    "summary": "Wait for other invocations of this module to reach the current point of\n    execution.",
    "description": "All invocations of this module within Execution scope must reach this\n    point of execution before any invocation will proceed beyond it.\n\n    When Execution is Workgroup or larger, behavior is undefined if this\n    instruction is used in control flow that is non-uniform within\n    Execution. When Execution is Subgroup or Invocation, the behavior of\n    this instruction in non-uniform control flow is defined by the client\n    API.\n\n    If Semantics is not None, this instruction also serves as an\n    OpMemoryBarrier instruction, and must also perform and adhere to the\n    description and semantics of an OpMemoryBarrier instruction with the\n    same Memory and Semantics operands.  This allows atomically specifying\n    both a control barrier and a memory barrier (that is, without needing\n    two instructions). If Semantics is None, Memory is ignored.\n\n    Before version 1.3, it is only valid to use this instruction with\n    TessellationControl, GLCompute, or Kernel execution models. There is no\n    such restriction starting with version 1.3.\n\n    When used with the TessellationControl execution model, it also\n    implicitly synchronizes the Output Storage Class:  Writes to Output\n    variables performed by any invocation executed prior to a\n    OpControlBarrier will be visible to any other invocation after return\n    from that OpControlBarrier.\n\n    #### Example:\n\n    ```mlir\n    spirv.ControlBarrier <Workgroup>, <Device>, <Acquire|UniformMemory>\n    ```",
    "attributes": [
      { "name": "execution_scope", "type": "SPIRV_ScopeAttr" },
      { "name": "memory_scope", "type": "SPIRV_ScopeAttr" },
      { "name": "memory_semantics", "type": "SPIRV_MemorySemanticsAttr" }
    ],
    "assemblyFormat": "$execution_scope `,` $memory_scope `,` $memory_semantics attr-dict"
  },
  {
    "name": "spirv.ConvertFToS",
    "summary": "Convert value numerically from floating point to signed integer, with\n    round toward 0.0.",
    "description": "Result Type must be a scalar or vector of integer type.\n\n    Float Value must be a scalar or vector of floating-point type.  It must\n    have the same number of components as Result Type.\n\n    Results are computed per component.\n\n    #### Example:\n\n    ```mlir\n    %1 = spirv.ConvertFToS %0 : f32 to i32\n    %3 = spirv.ConvertFToS %2 : vector<3xf32> to vector<3xi32>\n    ```",
    "inputs": [
      { "name": "operand", "type": "SPIRV_ScalarOrVectorOrCoopMatrixOf" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_ScalarOrVectorOrCoopMatrixOf" }
    ],
    "assemblyFormat": "$operand attr-dict `:` type($operand) `to` type($result)"
  },
  {
    "name": "spirv.ConvertFToU",
    "summary": "Convert value numerically from floating point to unsigned integer, with\n    round toward 0.0.",
    "description": "Result Type must be a scalar or vector of integer type, whose Signedness\n    operand is 0.\n\n    Float Value must be a scalar or vector of floating-point type.  It must\n    have the same number of components as Result Type.\n\n    Results are computed per component.\n\n    #### Example:\n\n    ```mlir\n    %1 = spirv.ConvertFToU %0 : f32 to i32\n    %3 = spirv.ConvertFToU %2 : vector<3xf32> to vector<3xi32>\n    ```",
    "inputs": [
      { "name": "operand", "type": "SPIRV_ScalarOrVectorOrCoopMatrixOf" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_ScalarOrVectorOrCoopMatrixOf" }
    ],
    "assemblyFormat": "$operand attr-dict `:` type($operand) `to` type($result)"
  },
  {
    "name": "spirv.ConvertPtrToU",
    "summary": "Bit pattern-preserving conversion of a pointer to\n    an unsigned scalar integer of possibly different bit width.",
    "description": "Result Type must be a scalar of integer type, whose Signedness operand is 0.\n\n    Pointer must be a physical pointer type. If the bit width of Pointer is\n    smaller than that of Result Type, the conversion zero extends Pointer.\n    If the bit width of Pointer is larger than that of Result Type,\n    the conversion truncates Pointer.\n\n    For same bit width Pointer and Result Type, this is the same as OpBitcast.\n\n    #### Example:\n\n    ```mlir\n    %1 = spirv.ConvertPtrToU %0 : !spirv.ptr<i32, Generic> to i32\n    ```",
    "inputs": [
      { "name": "pointer", "type": "SPIRV_AnyPtr" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_Integer" }
    ],
    "assemblyFormat": "$pointer attr-dict `:` type($pointer) `to` type($result)"
  },
  {
    "name": "spirv.ConvertSToF",
    "summary": "Convert value numerically from signed integer to floating point.",
    "description": "Result Type must be a scalar or vector of floating-point type.\n\n    Signed Value must be a scalar or vector of integer type.  It must have\n    the same number of components as Result Type.\n\n    Results are computed per component.\n\n    #### Example:\n\n    ```mlir\n    %1 = spirv.ConvertSToF %0 : i32 to f32\n    %3 = spirv.ConvertSToF %2 : vector<3xi32> to vector<3xf32>\n    ```",
    "inputs": [
      { "name": "operand", "type": "SPIRV_ScalarOrVectorOrCoopMatrixOf" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_ScalarOrVectorOrCoopMatrixOf" }
    ],
    "assemblyFormat": "$operand attr-dict `:` type($operand) `to` type($result)"
  },
  {
    "name": "spirv.ConvertUToF",
    "summary": "Convert value numerically from unsigned integer to floating point.",
    "description": "Result Type must be a scalar or vector of floating-point type.\n\n    Unsigned Value must be a scalar or vector of integer type.  It must have\n    the same number of components as Result Type.\n\n    Results are computed per component.\n\n    #### Example:\n\n    ```mlir\n    %1 = spirv.ConvertUToF %0 : i32 to f32\n    %3 = spirv.ConvertUToF %2 : vector<3xi32> to vector<3xf32>\n    ```",
    "inputs": [
      { "name": "operand", "type": "SPIRV_ScalarOrVectorOrCoopMatrixOf" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_ScalarOrVectorOrCoopMatrixOf" }
    ],
    "assemblyFormat": "$operand attr-dict `:` type($operand) `to` type($result)"
  },
  {
    "name": "spirv.ConvertUToPtr",
    "summary": "Bit pattern-preserving conversion of an unsigned scalar integer\n    to a pointer.",
    "description": "Result Type must be a physical pointer type.\n\n    Integer Value must be a scalar of integer type, whose Signedness\n    operand is 0. If the bit width of Integer Value is smaller\n    than that of Result Type, the conversion zero extends Integer Value.\n    If the bit width of Integer Value is larger than that of Result Type,\n    the conversion truncates Integer Value.\n\n    For same-width Integer Value and Result Type, this is the same as OpBitcast.\n\n    #### Example:\n\n    ```mlir\n    %1 = spirv.ConvertUToPtr %0 :  i32 to !spirv.ptr<i32, Generic>\n    ```",
    "inputs": [
      { "name": "operand", "type": "SPIRV_Integer" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_AnyPtr" }
    ],
    "assemblyFormat": "$operand attr-dict `:` type($operand) `to` type($result)"
  },
  {
    "name": "spirv.Dot",
    "summary": "Dot product of Vector 1 and Vector 2",
    "description": "Result Type must be a floating point scalar.\n\n    Vector 1 and Vector 2 must be vectors of the same type, and their component\n    type must be Result Type.\n\n    #### Example:\n\n    ```mlir\n    %0 = spirv.Dot %v1, %v2 : vector<4xf32> -> f32\n    ```",
    "inputs": [
      { "name": "vector1", "type": "SPIRV_VectorOf" },
      { "name": "vector2", "type": "SPIRV_VectorOf" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_AnyFloat" }
    ],
    "assemblyFormat": "operands attr-dict `:` type($vector1) `->` type($result)"
  },
  {
    "name": "spirv.EntryPoint",
    "summary": "Declare an entry point, its execution model, and its interface.",
    "description": "Execution Model is the execution model for the entry point and its\n    static call tree. See Execution Model.\n\n    Entry Point must be the Result <id> of an OpFunction instruction.\n\n    Name is a name string for the entry point. A module cannot have two\n    OpEntryPoint instructions with the same Execution Model and the same\n    Name string.\n\n    Interface is a list of symbol references to `spirv.GlobalVariable`\n    operations. These declare the set of global variables from a\n    module that form the interface of this entry point. The set of\n    Interface symbols must be equal to or a superset of the\n    `spirv.GlobalVariable`s referenced by the entry point’s static call\n    tree, within the interface’s storage classes.  Before version 1.4,\n    the interface’s storage classes are limited to the Input and\n    Output storage classes. Starting with version 1.4, the interface’s\n    storage classes are all storage classes used in declaring all\n    global variables referenced by the entry point’s call tree.\n\n    <!-- End of AutoGen section -->\n\n    ```\n    execution-model ::= \"Vertex\" | \"TesellationControl\" |\n                        <and other SPIR-V execution models...>\n\n    entry-point-op ::= ssa-id `=` `spirv.EntryPoint` execution-model\n                       symbol-reference (`, ` symbol-reference)*\n    ```\n\n    #### Example:\n\n    ```mlir\n    spirv.EntryPoint \"GLCompute\" @foo\n    spirv.EntryPoint \"Kernel\" @foo, @var1, @var2\n\n    ```",
    "attributes": [
      { "name": "execution_model", "type": "SPIRV_ExecutionModelAttr" },
      { "name": "fn", "type": "FlatSymbolRefAttr" },
      { "name": "interface", "type": "SymbolRefArrayAttr" }
    ]
  },
  {
    "name": "spirv.ExecutionMode",
    "summary": "Declare an execution mode for an entry point.",
    "description": "Entry Point must be the Entry Point <id> operand of an OpEntryPoint\n    instruction.\n\n    Mode is the execution mode. See Execution Mode.\n\n    This instruction is only valid when the Mode operand is an execution\n    mode that takes no Extra Operands, or takes Extra Operands that are not\n    <id> operands.\n\n    <!-- End of AutoGen section -->\n\n    ```\n    execution-mode ::= \"Invocations\" | \"SpacingEqual\" |\n                       <and other SPIR-V execution modes...>\n\n    execution-mode-op ::= `spirv.ExecutionMode ` ssa-use execution-mode\n                          (integer-literal (`, ` integer-literal)* )?\n    ```\n\n    #### Example:\n\n    ```mlir\n    spirv.ExecutionMode @foo \"ContractionOff\"\n    spirv.ExecutionMode @bar \"LocalSizeHint\", 3, 4, 5\n    ```",
    "attributes": [
      { "name": "fn", "type": "FlatSymbolRefAttr" },
      { "name": "execution_mode", "type": "SPIRV_ExecutionModeAttr" },
      { "name": "values", "type": "I32ArrayAttr" }
    ]
  },
  {
    "name": "spirv.FAdd",
    "summary": "Floating-point addition of Operand 1 and Operand 2.",
    "description": "Result Type must be a scalar or vector of floating-point type.\n\n    The types of Operand 1 and Operand 2 both must be the same as Result\n    Type.\n\n    Results are computed per component.\n\n    #### Example:\n\n    ```mlir\n    %4 = spirv.FAdd %0, %1 : f32\n    %5 = spirv.FAdd %2, %3 : vector<4xf32>\n    ```",
    "inputs": [
      { "name": "operand1", "type": "SPIRV_ScalarOrVectorOrCoopMatrixOf" },
      { "name": "operand2", "type": "SPIRV_ScalarOrVectorOrCoopMatrixOf" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_ScalarOrVectorOrCoopMatrixOf" }
    ],
    "assemblyFormat": "operands attr-dict `:` type($result)"
  },
  {
    "name": "spirv.FConvert",
    "summary": "Convert value numerically from one floating-point width to another\n    width.",
    "description": "Result Type must be a scalar or vector of floating-point type.\n\n    Float Value must be a scalar or vector of floating-point type.  It must\n    have the same number of components as Result Type.  The component width\n    cannot equal the component width in Result Type.\n\n    Results are computed per component.\n\n    #### Example:\n\n    ```mlir\n    %1 = spirv.FConvertOp %0 : f32 to f64\n    %3 = spirv.FConvertOp %2 : vector<3xf32> to vector<3xf64>\n    ```",
    "inputs": [
      { "name": "operand", "type": "SPIRV_ScalarOrVectorOrCoopMatrixOf" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_ScalarOrVectorOrCoopMatrixOf" }
    ],
    "assemblyFormat": "$operand attr-dict `:` type($operand) `to` type($result)"
  },
  {
    "name": "spirv.FDiv",
    "summary": "Floating-point division of Operand 1 divided by Operand 2.",
    "description": "Result Type must be a scalar or vector of floating-point type.\n\n    The types of Operand 1 and Operand 2 both must be the same as Result\n    Type.\n\n    Results are computed per component.  The resulting value is undefined\n    if Operand 2 is 0.\n\n    #### Example:\n\n    ```mlir\n    %4 = spirv.FDiv %0, %1 : f32\n    %5 = spirv.FDiv %2, %3 : vector<4xf32>\n    ```",
    "inputs": [
      { "name": "operand1", "type": "SPIRV_ScalarOrVectorOrCoopMatrixOf" },
      { "name": "operand2", "type": "SPIRV_ScalarOrVectorOrCoopMatrixOf" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_ScalarOrVectorOrCoopMatrixOf" }
    ],
    "assemblyFormat": "operands attr-dict `:` type($result)"
  },
  {
    "name": "spirv.FMod",
    "summary": "The floating-point remainder whose sign matches the sign of Operand 2.",
    "description": "Result Type must be a scalar or vector of floating-point type.\n\n    The types of Operand 1 and Operand 2 both must be the same as Result\n    Type.\n\n    Results are computed per component.  The resulting value is undefined\n    if Operand 2 is 0.  Otherwise, the result is the remainder r of Operand\n    1 divided by Operand 2 where if r ≠ 0, the sign of r is the same as the\n    sign of Operand 2.\n\n    #### Example:\n\n    ```mlir\n    %4 = spirv.FMod %0, %1 : f32\n    %5 = spirv.FMod %2, %3 : vector<4xf32>\n    ```",
    "inputs": [
      { "name": "operand1", "type": "SPIRV_ScalarOrVectorOf" },
      { "name": "operand2", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "assemblyFormat": "operands attr-dict `:` type($result)"
  },
  {
    "name": "spirv.FMul",
    "summary": "Floating-point multiplication of Operand 1 and Operand 2.",
    "description": "Result Type must be a scalar or vector of floating-point type.\n\n    The types of Operand 1 and Operand 2 both must be the same as Result\n    Type.\n\n    Results are computed per component.\n\n    #### Example:\n\n    ```mlir\n    %4 = spirv.FMul %0, %1 : f32\n    %5 = spirv.FMul %2, %3 : vector<4xf32>\n    ```",
    "inputs": [
      { "name": "operand1", "type": "SPIRV_ScalarOrVectorOrCoopMatrixOf" },
      { "name": "operand2", "type": "SPIRV_ScalarOrVectorOrCoopMatrixOf" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_ScalarOrVectorOrCoopMatrixOf" }
    ],
    "assemblyFormat": "operands attr-dict `:` type($result)"
  },
  {
    "name": "spirv.FNegate",
    "summary": "Inverts the sign bit of Operand. (Note, however, that OpFNegate is still\n    considered a floating-point instruction, and so is subject to the\n    general floating-point rules regarding, for example, subnormals and NaN\n    propagation).",
    "description": "Result Type must be a scalar or vector of floating-point type.\n\n    The type of Operand must be the same as Result Type.\n\n    Results are computed per component.\n\n    #### Example:\n\n    ```mlir\n    %1 = spirv.FNegate %0 : f32\n    %3 = spirv.FNegate %2 : vector<4xf32>\n    ```",
    "inputs": [
      { "name": "operand", "type": "SPIRV_ScalarOrVectorOrCoopMatrixOf" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_ScalarOrVectorOrCoopMatrixOf" }
    ],
    "assemblyFormat": "operands attr-dict `:` type($result)"
  },
  {
    "name": "spirv.FOrdEqual",
    "summary": "Floating-point comparison for being ordered and equal.",
    "description": "Result Type must be a scalar or vector of Boolean type.\n\n    The type of Operand 1 and Operand 2  must be a scalar or vector of\n    floating-point type.  They must have the same type, and they must have\n    the same number of components as Result Type.\n\n    Results are computed per component.\n\n    #### Example:\n\n    ```mlir\n    %4 = spirv.FOrdEqual %0, %1 : f32\n    %5 = spirv.FOrdEqual %2, %3 : vector<4xf32>\n    ```",
    "inputs": [
      { "name": "operand1", "type": "SPIRV_ScalarOrVectorOf" },
      { "name": "operand2", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "assemblyFormat": "$operand1 `,` $operand2 `:` type($operand1) attr-dict"
  },
  {
    "name": "spirv.FOrdGreaterThan",
    "summary": "Floating-point comparison if operands are ordered and Operand 1 is\n    greater than  Operand 2.",
    "description": "Result Type must be a scalar or vector of Boolean type.\n\n    The type of Operand 1 and Operand 2  must be a scalar or vector of\n    floating-point type.  They must have the same type, and they must have\n    the same number of components as Result Type.\n\n    Results are computed per component.\n\n    #### Example:\n\n    ```mlir\n    %4 = spirv.FOrdGreaterThan %0, %1 : f32\n    %5 = spirv.FOrdGreaterThan %2, %3 : vector<4xf32>\n    ```",
    "inputs": [
      { "name": "operand1", "type": "SPIRV_ScalarOrVectorOf" },
      { "name": "operand2", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "assemblyFormat": "$operand1 `,` $operand2 `:` type($operand1) attr-dict"
  },
  {
    "name": "spirv.FOrdGreaterThanEqual",
    "summary": "Floating-point comparison if operands are ordered and Operand 1 is\n    greater than or equal to Operand 2.",
    "description": "Result Type must be a scalar or vector of Boolean type.\n\n    The type of Operand 1 and Operand 2  must be a scalar or vector of\n    floating-point type.  They must have the same type, and they must have\n    the same number of components as Result Type.\n\n    Results are computed per component.\n\n    #### Example:\n\n    ```mlir\n    %4 = spirv.FOrdGreaterThanEqual %0, %1 : f32\n    %5 = spirv.FOrdGreaterThanEqual %2, %3 : vector<4xf32>\n    ```",
    "inputs": [
      { "name": "operand1", "type": "SPIRV_ScalarOrVectorOf" },
      { "name": "operand2", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "assemblyFormat": "$operand1 `,` $operand2 `:` type($operand1) attr-dict"
  },
  {
    "name": "spirv.FOrdLessThan",
    "summary": "Floating-point comparison if operands are ordered and Operand 1 is less\n    than Operand 2.",
    "description": "Result Type must be a scalar or vector of Boolean type.\n\n    The type of Operand 1 and Operand 2  must be a scalar or vector of\n    floating-point type.  They must have the same type, and they must have\n    the same number of components as Result Type.\n\n    Results are computed per component.\n\n    #### Example:\n\n    ```mlir\n    %4 = spirv.FOrdLessThan %0, %1 : f32\n    %5 = spirv.FOrdLessThan %2, %3 : vector<4xf32>\n    ```",
    "inputs": [
      { "name": "operand1", "type": "SPIRV_ScalarOrVectorOf" },
      { "name": "operand2", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "assemblyFormat": "$operand1 `,` $operand2 `:` type($operand1) attr-dict"
  },
  {
    "name": "spirv.FOrdLessThanEqual",
    "summary": "Floating-point comparison if operands are ordered and Operand 1 is less\n    than or equal to Operand 2.",
    "description": "Result Type must be a scalar or vector of Boolean type.\n\n    The type of Operand 1 and Operand 2  must be a scalar or vector of\n    floating-point type.  They must have the same type, and they must have\n    the same number of components as Result Type.\n\n    Results are computed per component.\n\n    #### Example:\n\n    ```mlir\n    %4 = spirv.FOrdLessThanEqual %0, %1 : f32\n    %5 = spirv.FOrdLessThanEqual %2, %3 : vector<4xf32>\n    ```",
    "inputs": [
      { "name": "operand1", "type": "SPIRV_ScalarOrVectorOf" },
      { "name": "operand2", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "assemblyFormat": "$operand1 `,` $operand2 `:` type($operand1) attr-dict"
  },
  {
    "name": "spirv.FOrdNotEqual",
    "summary": "Floating-point comparison for being ordered and not equal.",
    "description": "Result Type must be a scalar or vector of Boolean type.\n\n    The type of Operand 1 and Operand 2  must be a scalar or vector of\n    floating-point type.  They must have the same type, and they must have\n    the same number of components as Result Type.\n\n    Results are computed per component.\n\n    #### Example:\n\n    ```mlir\n    %4 = spirv.FOrdNotEqual %0, %1 : f32\n    %5 = spirv.FOrdNotEqual %2, %3 : vector<4xf32>\n    ```",
    "inputs": [
      { "name": "operand1", "type": "SPIRV_ScalarOrVectorOf" },
      { "name": "operand2", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "assemblyFormat": "$operand1 `,` $operand2 `:` type($operand1) attr-dict"
  },
  {
    "name": "spirv.FRem",
    "summary": "The floating-point remainder whose sign matches the sign of Operand 1.",
    "description": "Result Type must be a scalar or vector of floating-point type.\n\n    The types of Operand 1 and Operand 2 both must be the same as Result\n    Type.\n\n    Results are computed per component.  The resulting value is undefined\n    if Operand 2 is 0.  Otherwise, the result is the remainder r of Operand\n    1 divided by Operand 2 where if r ≠ 0, the sign of r is the same as the\n    sign of Operand 1.\n\n    #### Example:\n\n    ```mlir\n    %4 = spirv.FRemOp %0, %1 : f32\n    %5 = spirv.FRemOp %2, %3 : vector<4xf32>\n    ```",
    "inputs": [
      { "name": "operand1", "type": "SPIRV_ScalarOrVectorOf" },
      { "name": "operand2", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "assemblyFormat": "operands attr-dict `:` type($result)"
  },
  {
    "name": "spirv.FSub",
    "summary": "Floating-point subtraction of Operand 2 from Operand 1.",
    "description": "Result Type must be a scalar or vector of floating-point type.\n\n    The types of Operand 1 and Operand 2 both must be the same as Result\n    Type.\n\n    Results are computed per component.\n\n    #### Example:\n\n    ```mlir\n    %4 = spirv.FRemOp %0, %1 : f32\n    %5 = spirv.FRemOp %2, %3 : vector<4xf32>\n    ```",
    "inputs": [
      { "name": "operand1", "type": "SPIRV_ScalarOrVectorOrCoopMatrixOf" },
      { "name": "operand2", "type": "SPIRV_ScalarOrVectorOrCoopMatrixOf" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_ScalarOrVectorOrCoopMatrixOf" }
    ],
    "assemblyFormat": "operands attr-dict `:` type($result)"
  },
  {
    "name": "spirv.func",
    "summary": "Declare or define a function",
    "description": "This op declares or defines a SPIR-V function using one region, which\n    contains one or more blocks.\n\n    Different from the SPIR-V binary format, this op is not allowed to\n    implicitly capture global values, and all external references must use\n    function arguments or symbol references. This op itself defines a symbol\n    that is unique in the enclosing module op.\n\n    This op itself takes no operands and generates no results. Its region\n    can take zero or more arguments and return zero or one values.\n\n    From `SPV_KHR_physical_storage_buffer`:\n    If a parameter of function is\n    - a pointer (or contains a pointer) in the PhysicalStorageBuffer storage\n      class, the function parameter must be decorated with exactly one of\n      `Aliased` or `Restrict`.\n    - a pointer (or contains a pointer) and the type it points to is a pointer\n      in the PhysicalStorageBuffer storage class, the function parameter must\n      be decorated with exactly one of `AliasedPointer` or `RestrictPointer`.\n\n    <!-- End of AutoGen section -->\n\n    ```\n    spv-function-control ::= \"None\" | \"Inline\" | \"DontInline\" | ...\n    spv-function-op ::= `spirv.func` function-signature\n                         spv-function-control region\n    ```\n\n    #### Example:\n\n    ```mlir\n    spirv.func @foo() -> () \"None\" { ... }\n    spirv.func @bar() -> () \"Inline|Pure\" { ... }\n\n    spirv.func @aliased_pointer(%arg0: !spirv.ptr<i32, PhysicalStorageBuffer>,\n        { spirv.decoration = #spirv.decoration<Aliased> }) -> () \"None\" { ... }\n\n    spirv.func @restrict_pointer(%arg0: !spirv.ptr<i32, PhysicalStorageBuffer>,\n        { spirv.decoration = #spirv.decoration<Restrict> }) -> () \"None\" { ... }\n\n    spirv.func @aliased_pointee(%arg0: !spirv.ptr<!spirv.ptr<i32,\n        PhysicalStorageBuffer>, Generic> { spirv.decoration =\n        #spirv.decoration<AliasedPointer> }) -> () \"None\" { ... }\n\n    spirv.func @restrict_pointee(%arg0: !spirv.ptr<!spirv.ptr<i32,\n        PhysicalStorageBuffer>, Generic> { spirv.decoration =\n        #spirv.decoration<RestrictPointer> }) -> () \"None\" { ... }\n    ```",
    "attributes": [
      { "name": "function_type", "type": "TypeAttrOf" },
      { "name": "arg_attrs", "type": "OptionalAttr" },
      { "name": "res_attrs", "type": "OptionalAttr" },
      { "name": "sym_name", "type": "StrAttr" },
      { "name": "function_control", "type": "SPIRV_FunctionControlAttr" },
      { "name": "linkage_attributes", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "spirv.FunctionCall",
    "summary": "Call a function.",
    "description": "Result Type is the type of the return value of the function. It must be\n    the same as the Return Type operand of the Function Type operand of the\n    Function operand.\n\n    Function is an OpFunction instruction.  This could be a forward\n    reference.\n\n    Argument N is the object to copy to parameter N of Function.\n\n    Note: A forward call is possible because there is no missing type\n    information: Result Type must match the Return Type of the function, and\n    the calling argument types must match the formal parameter types.\n\n    #### Example:\n\n    ```mlir\n    spirv.FunctionCall @f_void(%arg0) : (i32) ->  ()\n    %0 = spirv.FunctionCall @f_iadd(%arg0, %arg1) : (i32, i32) -> i32\n    ```",
    "inputs": [
      { "name": "arguments", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "return_value", "type": "Optional" }
    ],
    "attributes": [
      { "name": "callee", "type": "FlatSymbolRefAttr" },
      { "name": "arg_attrs", "type": "OptionalAttr" },
      { "name": "res_attrs", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "$callee `(` $arguments `)` attr-dict `:`\n      functional-type($arguments, results)"
  },
  {
    "name": "spirv.FUnordEqual",
    "summary": "Floating-point comparison for being unordered or equal.",
    "description": "Result Type must be a scalar or vector of Boolean type.\n\n    The type of Operand 1 and Operand 2  must be a scalar or vector of\n    floating-point type.  They must have the same type, and they must have\n    the same number of components as Result Type.\n\n    Results are computed per component.\n\n    #### Example:\n\n    ```mlir\n    %4 = spirv.FUnordEqual %0, %1 : f32\n    %5 = spirv.FUnordEqual %2, %3 : vector<4xf32>\n    ```",
    "inputs": [
      { "name": "operand1", "type": "SPIRV_ScalarOrVectorOf" },
      { "name": "operand2", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "assemblyFormat": "$operand1 `,` $operand2 `:` type($operand1) attr-dict"
  },
  {
    "name": "spirv.FUnordGreaterThan",
    "summary": "Floating-point comparison if operands are unordered or Operand 1 is\n    greater than  Operand 2.",
    "description": "Result Type must be a scalar or vector of Boolean type.\n\n    The type of Operand 1 and Operand 2  must be a scalar or vector of\n    floating-point type.  They must have the same type, and they must have\n    the same number of components as Result Type.\n\n    Results are computed per component.\n\n    #### Example:\n\n    ```mlir\n    %4 = spirv.FUnordGreaterThan %0, %1 : f32\n    %5 = spirv.FUnordGreaterThan %2, %3 : vector<4xf32>\n    ```",
    "inputs": [
      { "name": "operand1", "type": "SPIRV_ScalarOrVectorOf" },
      { "name": "operand2", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "assemblyFormat": "$operand1 `,` $operand2 `:` type($operand1) attr-dict"
  },
  {
    "name": "spirv.FUnordGreaterThanEqual",
    "summary": "Floating-point comparison if operands are unordered or Operand 1 is\n    greater than or equal to Operand 2.",
    "description": "Result Type must be a scalar or vector of Boolean type.\n\n    The type of Operand 1 and Operand 2  must be a scalar or vector of\n    floating-point type.  They must have the same type, and they must have\n    the same number of components as Result Type.\n\n    Results are computed per component.\n\n    #### Example:\n\n    ```mlir\n    %4 = spirv.FUnordGreaterThanEqual %0, %1 : f32\n    %5 = spirv.FUnordGreaterThanEqual %2, %3 : vector<4xf32>\n    ```",
    "inputs": [
      { "name": "operand1", "type": "SPIRV_ScalarOrVectorOf" },
      { "name": "operand2", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "assemblyFormat": "$operand1 `,` $operand2 `:` type($operand1) attr-dict"
  },
  {
    "name": "spirv.FUnordLessThan",
    "summary": "Floating-point comparison if operands are unordered or Operand 1 is less\n    than Operand 2.",
    "description": "Result Type must be a scalar or vector of Boolean type.\n\n    The type of Operand 1 and Operand 2  must be a scalar or vector of\n    floating-point type.  They must have the same type, and they must have\n    the same number of components as Result Type.\n\n    Results are computed per component.\n\n    #### Example:\n\n    ```mlir\n    %4 = spirv.FUnordLessThan %0, %1 : f32\n    %5 = spirv.FUnordLessThan %2, %3 : vector<4xf32>\n    ```",
    "inputs": [
      { "name": "operand1", "type": "SPIRV_ScalarOrVectorOf" },
      { "name": "operand2", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "assemblyFormat": "$operand1 `,` $operand2 `:` type($operand1) attr-dict"
  },
  {
    "name": "spirv.FUnordLessThanEqual",
    "summary": "Floating-point comparison if operands are unordered or Operand 1 is less\n    than or equal to Operand 2.",
    "description": "Result Type must be a scalar or vector of Boolean type.\n\n    The type of Operand 1 and Operand 2  must be a scalar or vector of\n    floating-point type.  They must have the same type, and they must have\n    the same number of components as Result Type.\n\n    Results are computed per component.\n\n    #### Example:\n\n    ```mlir\n    %4 = spirv.FUnordLessThanEqual %0, %1 : f32\n    %5 = spirv.FUnordLessThanEqual %2, %3 : vector<4xf32>\n    ```",
    "inputs": [
      { "name": "operand1", "type": "SPIRV_ScalarOrVectorOf" },
      { "name": "operand2", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "assemblyFormat": "$operand1 `,` $operand2 `:` type($operand1) attr-dict"
  },
  {
    "name": "spirv.FUnordNotEqual",
    "summary": "Floating-point comparison for being unordered or not equal.",
    "description": "Result Type must be a scalar or vector of Boolean type.\n\n    The type of Operand 1 and Operand 2  must be a scalar or vector of\n    floating-point type.  They must have the same type, and they must have\n    the same number of components as Result Type.\n\n    Results are computed per component.\n\n    #### Example:\n\n    ```mlir\n    %4 = spirv.FUnordNotEqual %0, %1 : f32\n    %5 = spirv.FUnordNotEqual %2, %3 : vector<4xf32>\n    ```",
    "inputs": [
      { "name": "operand1", "type": "SPIRV_ScalarOrVectorOf" },
      { "name": "operand2", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "assemblyFormat": "$operand1 `,` $operand2 `:` type($operand1) attr-dict"
  },
  {
    "name": "spirv.GenericCastToPtr",
    "summary": "Convert a pointer’s Storage Class to a non-Generic class.",
    "description": "Result Type must be an OpTypePointer. Its Storage Class must be\n    Workgroup, CrossWorkgroup, or Function.\n\n    Pointer must point to the Generic Storage Class.\n\n    Result Type and Pointer must point to the same type.\n\n    <!-- End of AutoGen section -->\n\n    #### Example:\n\n    ```mlir\n       %1 = spirv.GenericCastToPtrOp %0 : !spirv.ptr<f32, Generic> to\n       !spirv.ptr<f32, CrossWorkGroup>\n    ```",
    "inputs": [
      { "name": "pointer", "type": "SPIRV_AnyPtr" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_AnyPtr" }
    ],
    "assemblyFormat": "$pointer attr-dict `:` type($pointer) `to` type($result)"
  },
  {
    "name": "spirv.GenericCastToPtrExplicit",
    "summary": "Attempts to explicitly convert Pointer to Storage storage-class pointer\n    value.",
    "description": "Result Type must be an OpTypePointer. Its Storage Class must be Storage.\n\n    Pointer must have a type of OpTypePointer whose Type is the same as the\n    Type of Result Type.Pointer must point to the Generic Storage Class. If\n    the cast fails, the instruction result is an OpConstantNull pointer in\n    the Storage Storage Class.\n\n    Storage must be one of the following literal values from Storage Class:\n    Workgroup, CrossWorkgroup, or Function.\n\n    <!-- End of AutoGen section -->\n\n    #### Example:\n\n    ```mlir\n       %1 = spirv.GenericCastToPtrExplicitOp %0 : !spirv.ptr<f32, Generic> to\n       !spirv.ptr<f32, CrossWorkGroup>\n    ```",
    "inputs": [
      { "name": "pointer", "type": "SPIRV_AnyPtr" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_AnyPtr" }
    ],
    "assemblyFormat": "$pointer attr-dict `:` type($pointer) `to` type($result)"
  },
  {
    "name": "spirv.GlobalVariable",
    "summary": "Allocate an object in memory at module scope. The object is\n    referenced using a symbol name.",
    "description": "The variable type must be an OpTypePointer. Its type operand is the type of\n    object in memory.\n\n    Storage Class is the Storage Class of the memory holding the object. It\n    cannot be Generic. It must be the same as the Storage Class operand of\n    the variable types. Only those storage classes that are valid at module\n    scope (like Input, Output, StorageBuffer, etc.) are valid.\n\n    Initializer is optional.  If Initializer is present, it will be\n    the initial value of the variable’s memory content. Initializer\n    must be an symbol defined from a constant instruction or other\n    `spirv.GlobalVariable` operation in module scope. Initializer must\n    have the same type as the type of the defined symbol.\n\n    <!-- End of AutoGen section -->\n\n    ```\n    variable-op ::= `spirv.GlobalVariable` spirv-type symbol-ref-id\n                    (`initializer(` symbol-ref-id `)`)?\n                    (`bind(` integer-literal, integer-literal `)`)?\n                    (`built_in(` string-literal `)`)?\n                    attribute-dict?\n    ```\n\n    where `initializer` specifies initializer and `bind` specifies the\n    descriptor set and binding number. `built_in` specifies SPIR-V\n    BuiltIn decoration associated with the op.\n\n    #### Example:\n\n    ```mlir\n    spirv.GlobalVariable @var0 : !spirv.ptr<f32, Input> @var0\n    spirv.GlobalVariable @var1 initializer(@var0) : !spirv.ptr<f32, Output>\n    spirv.GlobalVariable @var2 bind(1, 2) : !spirv.ptr<f32, Uniform>\n    spirv.GlobalVariable @var3 built_in(\"GlobalInvocationId\") : !spirv.ptr<vector<3xi32>, Input>\n    ```",
    "attributes": [
      { "name": "type", "type": "TypeAttr" },
      { "name": "sym_name", "type": "StrAttr" },
      { "name": "initializer", "type": "OptionalAttr" },
      { "name": "location", "type": "OptionalAttr" },
      { "name": "binding", "type": "OptionalAttr" },
      { "name": "descriptor_set", "type": "OptionalAttr" },
      { "name": "built_in", "type": "OptionalAttr" },
      { "name": "linkage_attributes", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "spirv.GroupBroadcast",
    "summary": "Broadcast the Value of the invocation identified by the local id LocalId\n    to the result of all invocations in the group.",
    "description": "All invocations of this module within Execution must reach this point of\n    execution.\n\n    Behavior is undefined if this instruction is used in control flow that\n    is non-uniform within Execution.\n\n    Result Type  must be a scalar or vector of floating-point type, integer\n    type, or Boolean type.\n\n    Execution must be Workgroup or Subgroup Scope.\n\n    The type of Value must be the same as Result Type.\n\n    LocalId must be an integer datatype. It can be a scalar, or a vector\n    with 2 components or a vector with 3 components. LocalId must be the\n    same for all invocations in the group.\n\n    #### Example:\n\n    ```mlir\n    %scalar_value = ... : f32\n    %vector_value = ... : vector<4xf32>\n    %scalar_localid = ... : i32\n    %vector_localid = ... : vector<3xi32>\n    %0 = spirv.GroupBroadcast \"Subgroup\" %scalar_value, %scalar_localid : f32, i32\n    %1 = spirv.GroupBroadcast \"Workgroup\" %vector_value, %vector_localid :\n      vector<4xf32>, vector<3xi32>\n    ```",
    "inputs": [
      { "name": "value", "type": "SPIRV_Type" },
      { "name": "localid", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_Type" }
    ],
    "attributes": [
      { "name": "execution_scope", "type": "SPIRV_ScopeAttr" }
    ],
    "assemblyFormat": "$execution_scope operands attr-dict `:` type($value) `,` type($localid)"
  },
  {
    "name": "spirv.GroupFAdd",
    "summary": "A floating-point add group operation specified for all values of X\n    specified by invocations in the group.",
    "description": "Behavior is undefined if not all invocations of this module within\n    Execution reach this point of execution.\n\n    Behavior is undefined unless all invocations within Execution execute\n    the same dynamic instance of this instruction.\n\n    Result Type  must be a scalar or vector of floating-point type.\n\n    Execution is a Scope. It must be either Workgroup or Subgroup.\n\n    The identity I for Operation is 0.\n\n    The type of X must be the same as Result Type.\n\n    #### Example:\n\n    ```mlir\n    %0 = spirv.GroupFAdd <Workgroup> <Reduce> %value : f32\n    ```",
    "inputs": [
      { "name": "x", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "attributes": [
      { "name": "execution_scope", "type": "SPIRV_ScopeAttr" },
      { "name": "group_operation", "type": "SPIRV_GroupOperationAttr" }
    ],
    "assemblyFormat": "$execution_scope $group_operation operands attr-dict `:` type($x)"
  },
  {
    "name": "spirv.GroupFMax",
    "summary": "A floating-point maximum group operation specified for all values of X\n    specified by invocations in the group.",
    "description": "Behavior is undefined if not all invocations of this module within\n    Execution reach this point of execution.\n\n    Behavior is undefined unless all invocations within Execution execute\n    the same dynamic instance of this instruction.\n\n    Result Type  must be a scalar or vector of floating-point type.\n\n    Execution is a Scope. It must be either Workgroup or Subgroup.\n\n    The identity I for Operation is -INF.\n\n    The type of X must be the same as Result Type.\n\n    #### Example:\n\n    ```mlir\n    %0 = spirv.GroupFMax <Workgroup> <Reduce> %value : f32\n    ```",
    "inputs": [
      { "name": "x", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "attributes": [
      { "name": "execution_scope", "type": "SPIRV_ScopeAttr" },
      { "name": "group_operation", "type": "SPIRV_GroupOperationAttr" }
    ],
    "assemblyFormat": "$execution_scope $group_operation operands attr-dict `:` type($x)"
  },
  {
    "name": "spirv.GroupFMin",
    "summary": "A floating-point minimum group operation specified for all values of X\n    specified by invocations in the group.",
    "description": "Behavior is undefined if not all invocations of this module within\n    Execution reach this point of execution.\n\n    Behavior is undefined unless all invocations within Execution execute\n    the same dynamic instance of this instruction.\n\n    Result Type  must be a scalar or vector of floating-point type.\n\n    Execution is a Scope. It must be either Workgroup or Subgroup.\n\n    The identity I for Operation is +INF.\n\n    The type of X must be the same as Result Type.\n\n    #### Example:\n\n    ```mlir\n    %0 = spirv.GroupFMin <Workgroup> <Reduce> %value : f32\n    ```",
    "inputs": [
      { "name": "x", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "attributes": [
      { "name": "execution_scope", "type": "SPIRV_ScopeAttr" },
      { "name": "group_operation", "type": "SPIRV_GroupOperationAttr" }
    ],
    "assemblyFormat": "$execution_scope $group_operation operands attr-dict `:` type($x)"
  },
  {
    "name": "spirv.GroupIAdd",
    "summary": "An integer add group operation specified for all values of X specified\n    by invocations in the group.",
    "description": "Behavior is undefined if not all invocations of this module within\n    Execution reach this point of execution.\n\n    Behavior is undefined unless all invocations within Execution execute\n    the same dynamic instance of this instruction.\n\n    Result Type  must be a scalar or vector of integer type.\n\n    Execution is a Scope. It must be either Workgroup or Subgroup.\n\n    The identity I for Operation is 0.\n\n    The type of X must be the same as Result Type.\n\n    #### Example:\n\n    ```mlir\n    %0 = spirv.GroupIAdd <Workgroup> <Reduce> %value : i32\n    ```",
    "inputs": [
      { "name": "x", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "attributes": [
      { "name": "execution_scope", "type": "SPIRV_ScopeAttr" },
      { "name": "group_operation", "type": "SPIRV_GroupOperationAttr" }
    ],
    "assemblyFormat": "$execution_scope $group_operation operands attr-dict `:` type($x)"
  },
  {
    "name": "spirv.GroupSMax",
    "summary": "A signed integer maximum group operation specified for all values of X\n    specified by invocations in the group.",
    "description": "Behavior is undefined if not all invocations of this module within\n    Execution reach this point of execution.\n\n    Behavior is undefined unless all invocations within Execution execute\n    the same dynamic instance of this instruction.\n\n    Result Type  must be a scalar or vector of integer type.\n\n    Execution is a Scope. It must be either Workgroup or Subgroup.\n\n    The identity I for Operation is INT_MIN when X is 32 bits wide and\n    LONG_MIN when X is 64 bits wide.\n\n    The type of X must be the same as Result Type.\n\n    #### Example:\n\n    ```mlir\n    %0 = spirv.GroupSMax <Workgroup> <Reduce> %value : i32\n    ```",
    "inputs": [
      { "name": "x", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "attributes": [
      { "name": "execution_scope", "type": "SPIRV_ScopeAttr" },
      { "name": "group_operation", "type": "SPIRV_GroupOperationAttr" }
    ],
    "assemblyFormat": "$execution_scope $group_operation operands attr-dict `:` type($x)"
  },
  {
    "name": "spirv.GroupSMin",
    "summary": "A signed integer minimum group operation specified for all values of X\n    specified by invocations in the group.",
    "description": "Behavior is undefined if not all invocations of this module within\n    Execution reach this point of execution.\n\n    Behavior is undefined unless all invocations within Execution execute\n    the same dynamic instance of this instruction.\n\n    Result Type  must be a scalar or vector of integer type.\n\n    Execution is a Scope. It must be either Workgroup or Subgroup.\n\n    The identity I for Operation is INT_MAX when X is 32 bits wide and\n    LONG_MAX when X is 64 bits wide.\n\n    The type of X must be the same as Result Type.\n\n    #### Example:\n\n    ```mlir\n    %0 = spirv.GroupSMin <Workgroup> <Reduce> %value : i32\n    ```",
    "inputs": [
      { "name": "x", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "attributes": [
      { "name": "execution_scope", "type": "SPIRV_ScopeAttr" },
      { "name": "group_operation", "type": "SPIRV_GroupOperationAttr" }
    ],
    "assemblyFormat": "$execution_scope $group_operation operands attr-dict `:` type($x)"
  },
  {
    "name": "spirv.GroupUMax",
    "summary": "An unsigned integer maximum group operation specified for all values of\n    X specified by invocations in the group.",
    "description": "Behavior is undefined if not all invocations of this module within\n    Execution reach this point of execution.\n\n    Behavior is undefined unless all invocations within Execution execute\n    the same dynamic instance of this instruction.\n\n    Result Type  must be a scalar or vector of integer type.\n\n    Execution is a Scope. It must be either Workgroup or Subgroup.\n\n    The identity I for Operation is 0.\n\n    The type of X must be the same as Result Type.\n\n    #### Example:\n\n    ```mlir\n    %0 = spirv.GroupUMax <Workgroup> <Reduce> %value : i32\n    ```",
    "inputs": [
      { "name": "x", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "attributes": [
      { "name": "execution_scope", "type": "SPIRV_ScopeAttr" },
      { "name": "group_operation", "type": "SPIRV_GroupOperationAttr" }
    ],
    "assemblyFormat": "$execution_scope $group_operation operands attr-dict `:` type($x)"
  },
  {
    "name": "spirv.GroupUMin",
    "summary": "An unsigned integer minimum group operation specified for all values of\n    X specified by invocations in the group.",
    "description": "Behavior is undefined if not all invocations of this module within\n    Execution reach this point of execution.\n\n    Behavior is undefined unless all invocations within Execution execute\n    the same dynamic instance of this instruction.\n\n    Result Type  must be a scalar or vector of integer type.\n\n    Execution is a Scope. It must be either Workgroup or Subgroup.\n\n    The identity I for Operation is UINT_MAX when X is 32 bits wide and\n    ULONG_MAX when X is 64 bits wide.\n\n    The type of X must be the same as Result Type.\n\n    #### Example:\n\n    ```mlir\n    %0 = spirv.GroupUMin <Workgroup> <Reduce> %value : i32\n    ```",
    "inputs": [
      { "name": "x", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "attributes": [
      { "name": "execution_scope", "type": "SPIRV_ScopeAttr" },
      { "name": "group_operation", "type": "SPIRV_GroupOperationAttr" }
    ],
    "assemblyFormat": "$execution_scope $group_operation operands attr-dict `:` type($x)"
  },
  {
    "name": "spirv.IAdd",
    "summary": "Integer addition of Operand 1 and Operand 2.",
    "description": "Result Type must be a scalar or vector of integer type.\n\n    The type of Operand 1 and Operand 2  must be a scalar or vector of\n    integer type.  They must have the same number of components as Result\n    Type. They must have the same component width as Result Type.\n\n    The resulting value will equal the low-order N bits of the correct\n    result R, where N is the component width and R is computed with enough\n    precision to avoid overflow and underflow.\n\n    Results are computed per component.\n\n    #### Example:\n\n    ```mlir\n    %4 = spirv.IAdd %0, %1 : i32\n    %5 = spirv.IAdd %2, %3 : vector<4xi32>\n\n    ```",
    "inputs": [
      { "name": "operand1", "type": "SPIRV_ScalarOrVectorOrCoopMatrixOf" },
      { "name": "operand2", "type": "SPIRV_ScalarOrVectorOrCoopMatrixOf" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_ScalarOrVectorOrCoopMatrixOf" }
    ],
    "assemblyFormat": "operands attr-dict `:` type($result)"
  },
  {
    "name": "spirv.IAddCarry",
    "summary": "Integer addition of Operand 1 and Operand 2, including the carry.",
    "description": "Result Type must be from OpTypeStruct.  The struct must have two\n    members, and the two members must be the same type.  The member type\n    must be a scalar or vector of integer type, whose Signedness operand is\n    0.\n\n    Operand 1 and Operand 2 must have the same type as the members of Result\n    Type. These are consumed as unsigned integers.\n\n     Results are computed per component.\n\n    Member 0 of the result gets the low-order bits (full component width) of\n    the addition.\n\n    Member 1 of the result gets the high-order (carry) bit of the result of\n    the addition. That is, it gets the value 1 if the addition overflowed\n    the component width, and 0 otherwise.\n\n    <!-- End of AutoGen section -->\n\n    #### Example:\n\n    ```mlir\n    %2 = spirv.IAddCarry %0, %1 : !spirv.struct<(i32, i32)>\n    %2 = spirv.IAddCarry %0, %1 : !spirv.struct<(vector<2xi32>, vector<2xi32>)>\n    ```",
    "inputs": [
      { "name": "operand1", "type": "SPIRV_ScalarOrVectorOf" },
      { "name": "operand2", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_AnyStruct" }
    ]
  },
  {
    "name": "spirv.IEqual",
    "summary": "Integer comparison for equality.",
    "description": "Result Type must be a scalar or vector of Boolean type.\n\n    The type of Operand 1 and Operand 2  must be a scalar or vector of\n    integer type.  They must have the same component width, and they must\n    have the same number of components as Result Type.\n\n    Results are computed per component.\n\n    #### Example:\n\n    ```mlir\n    %4 = spirv.IEqual %0, %1 : i32\n    %5 = spirv.IEqual %2, %3 : vector<4xi32>\n    ```",
    "inputs": [
      { "name": "operand1", "type": "SPIRV_ScalarOrVectorOf" },
      { "name": "operand2", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "assemblyFormat": "$operand1 `,` $operand2 `:` type($operand1) attr-dict"
  },
  {
    "name": "spirv.IMul",
    "summary": "Integer multiplication of Operand 1 and Operand 2.",
    "description": "Result Type must be a scalar or vector of integer type.\n\n    The type of Operand 1 and Operand 2  must be a scalar or vector of\n    integer type.  They must have the same number of components as Result\n    Type. They must have the same component width as Result Type.\n\n    The resulting value will equal the low-order N bits of the correct\n    result R, where N is the component width and R is computed with enough\n    precision to avoid overflow and underflow.\n\n    Results are computed per component.\n\n    #### Example:\n\n    ```mlir\n    %4 = spirv.IMul %0, %1 : i32\n    %5 = spirv.IMul %2, %3 : vector<4xi32>\n\n    ```",
    "inputs": [
      { "name": "operand1", "type": "SPIRV_ScalarOrVectorOrCoopMatrixOf" },
      { "name": "operand2", "type": "SPIRV_ScalarOrVectorOrCoopMatrixOf" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_ScalarOrVectorOrCoopMatrixOf" }
    ],
    "assemblyFormat": "operands attr-dict `:` type($result)"
  },
  {
    "name": "spirv.INotEqual",
    "summary": "Integer comparison for inequality.",
    "description": "Result Type must be a scalar or vector of Boolean type.\n\n    The type of Operand 1 and Operand 2  must be a scalar or vector of\n    integer type.  They must have the same component width, and they must\n    have the same number of components as Result Type.\n\n    Results are computed per component.\n\n    #### Example:\n\n    ```mlir\n    %4 = spirv.INotEqual %0, %1 : i32\n    %5 = spirv.INotEqual %2, %3 : vector<4xi32>\n\n    ```",
    "inputs": [
      { "name": "operand1", "type": "SPIRV_ScalarOrVectorOf" },
      { "name": "operand2", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "assemblyFormat": "$operand1 `,` $operand2 `:` type($operand1) attr-dict"
  },
  {
    "name": "spirv.IsFinite",
    "summary": "Result is true if x is an IEEE Finite, otherwise result is false",
    "description": "Result Type must be a scalar or vector of Boolean type.\n\n    x must be a scalar or vector of floating-point type.  It must have the\n    same number of components as Result Type.\n\n    Results are computed per component.\n\n    #### Example:\n\n    ```mlir\n    %2 = spirv.IsFinite %0: f32\n    %3 = spirv.IsFinite %1: vector<4xf32>\n    ```",
    "inputs": [
      { "name": "operand", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "assemblyFormat": "$operand `:` type($operand) attr-dict"
  },
  {
    "name": "spirv.IsInf",
    "summary": "Result is true if x is an IEEE Inf, otherwise result is false",
    "description": "Result Type must be a scalar or vector of Boolean type.\n\n    x must be a scalar or vector of floating-point type.  It must have the\n    same number of components as Result Type.\n\n    Results are computed per component.\n\n    #### Example:\n\n    ```mlir\n    %2 = spirv.IsInf %0: f32\n    %3 = spirv.IsInf %1: vector<4xf32>\n    ```",
    "inputs": [
      { "name": "operand", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "assemblyFormat": "$operand `:` type($operand) attr-dict"
  },
  {
    "name": "spirv.IsNan",
    "summary": "Result is true if x is an IEEE NaN, otherwise result is false.",
    "description": "Result Type must be a scalar or vector of Boolean type.\n\n    x must be a scalar or vector of floating-point type.  It must have the\n    same number of components as Result Type.\n\n    Results are computed per component.\n\n    #### Example:\n\n    ```mlir\n    %2 = spirv.IsNan %0: f32\n    %3 = spirv.IsNan %1: vector<4xf32>\n    ```",
    "inputs": [
      { "name": "operand", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "assemblyFormat": "$operand `:` type($operand) attr-dict"
  },
  {
    "name": "spirv.ISub",
    "summary": "Integer subtraction of Operand 2 from Operand 1.",
    "description": "Result Type must be a scalar or vector of integer type.\n\n    The type of Operand 1 and Operand 2  must be a scalar or vector of\n    integer type.  They must have the same number of components as Result\n    Type. They must have the same component width as Result Type.\n\n    The resulting value will equal the low-order N bits of the correct\n    result R, where N is the component width and R is computed with enough\n    precision to avoid overflow and underflow.\n\n    Results are computed per component.\n\n    #### Example:\n\n    ```mlir\n    %4 = spirv.ISub %0, %1 : i32\n    %5 = spirv.ISub %2, %3 : vector<4xi32>\n\n    ```",
    "inputs": [
      { "name": "operand1", "type": "SPIRV_ScalarOrVectorOrCoopMatrixOf" },
      { "name": "operand2", "type": "SPIRV_ScalarOrVectorOrCoopMatrixOf" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_ScalarOrVectorOrCoopMatrixOf" }
    ],
    "assemblyFormat": "operands attr-dict `:` type($result)"
  },
  {
    "name": "spirv.ISubBorrow",
    "summary": "Result is the unsigned integer subtraction of Operand 2 from Operand 1,\n    and what it needed to borrow.",
    "description": "Result Type must be from OpTypeStruct.  The struct must have two\n    members, and the two members must be the same type.  The member type\n    must be a scalar or vector of integer type, whose Signedness operand is\n    0.\n\n    Operand 1 and Operand 2 must have the same type as the members of Result\n    Type. These are consumed as unsigned integers.\n\n     Results are computed per component.\n\n    Member 0 of the result gets the low-order bits (full component width) of\n    the subtraction. That is, if Operand 1 is larger than Operand 2, member\n    0 gets the full value of the subtraction;  if Operand 2 is larger than\n    Operand 1, member 0 gets 2w + Operand 1 - Operand 2, where w is the\n    component width.\n\n    Member 1 of the result gets 0 if Operand 1 ≥ Operand 2, and gets 1\n    otherwise.\n\n    <!-- End of AutoGen section -->\n\n    #### Example:\n\n    ```mlir\n    %2 = spirv.ISubBorrow %0, %1 : !spirv.struct<(i32, i32)>\n    %2 = spirv.ISubBorrow %0, %1 : !spirv.struct<(vector<2xi32>, vector<2xi32>)>\n    ```",
    "inputs": [
      { "name": "operand1", "type": "SPIRV_ScalarOrVectorOf" },
      { "name": "operand2", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_AnyStruct" }
    ]
  },
  {
    "name": "spirv.Kill",
    "summary": "Deprecated (use OpTerminateInvocation or OpDemoteToHelperInvocation).",
    "description": "Fragment-shader discard.\n\n    Ceases all further processing in any invocation that executes it: Only\n    instructions these invocations executed before OpKill have observable\n    side effects. If this instruction is executed in non-uniform control\n    flow, all subsequent control flow is non-uniform (for invocations that\n    continue to execute).\n\n    This instruction must be the last instruction in a block.\n\n    This instruction is only valid in the Fragment Execution Model.\n\n    <!-- End of AutoGen section -->\n\n    #### Example:\n\n    ```mlir\n    spirv.Kill\n    ```",
    "assemblyFormat": "attr-dict"
  },
  {
    "name": "spirv.LogicalAnd",
    "summary": "Result is true if both Operand 1 and Operand 2 are true. Result is false\n    if either Operand 1 or Operand 2 are false.",
    "description": "Result Type must be a scalar or vector of Boolean type.\n\n    The type of Operand 1 must be the same as Result Type.\n\n    The type of Operand 2 must be the same as Result Type.\n\n    Results are computed per component.\n\n    #### Example:\n\n    ```mlir\n    %2 = spirv.LogicalAnd %0, %1 : i1\n    %2 = spirv.LogicalAnd %0, %1 : vector<4xi1>\n    ```",
    "inputs": [
      { "name": "operand1", "type": "SPIRV_ScalarOrVectorOf" },
      { "name": "operand2", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "assemblyFormat": "$operand1 `,` $operand2 `:` type($operand1) attr-dict"
  },
  {
    "name": "spirv.LogicalEqual",
    "summary": "Result is true if Operand 1 and Operand 2 have the same value. Result is\n    false if Operand 1 and Operand 2 have different values.",
    "description": "Result Type must be a scalar or vector of Boolean type.\n\n    The type of Operand 1 must be the same as Result Type.\n\n    The type of Operand 2 must be the same as Result Type.\n\n    Results are computed per component.\n\n    #### Example:\n\n    ```mlir\n    %2 = spirv.LogicalEqual %0, %1 : i1\n    %2 = spirv.LogicalEqual %0, %1 : vector<4xi1>\n    ```",
    "inputs": [
      { "name": "operand1", "type": "SPIRV_ScalarOrVectorOf" },
      { "name": "operand2", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "assemblyFormat": "$operand1 `,` $operand2 `:` type($operand1) attr-dict"
  },
  {
    "name": "spirv.LogicalNot",
    "summary": "Result is true if Operand is false.  Result is false if Operand is true.",
    "description": "Result Type must be a scalar or vector of Boolean type.\n\n    The type of Operand must be the same as Result Type.\n\n    Results are computed per component.\n\n    #### Example:\n\n    ```mlir\n    %2 = spirv.LogicalNot %0 : i1\n    %2 = spirv.LogicalNot %0 : vector<4xi1>\n    ```",
    "inputs": [
      { "name": "operand", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "assemblyFormat": "$operand `:` type($operand) attr-dict"
  },
  {
    "name": "spirv.LogicalNotEqual",
    "summary": "Result is true if Operand 1 and Operand 2 have different values. Result\n    is false if Operand 1 and Operand 2 have the same value.",
    "description": "Result Type must be a scalar or vector of Boolean type.\n\n    The type of Operand 1 must be the same as Result Type.\n\n    The type of Operand 2 must be the same as Result Type.\n\n    Results are computed per component.\n\n    #### Example:\n\n    ```mlir\n    %2 = spirv.LogicalNotEqual %0, %1 : i1\n    %2 = spirv.LogicalNotEqual %0, %1 : vector<4xi1>\n    ```",
    "inputs": [
      { "name": "operand1", "type": "SPIRV_ScalarOrVectorOf" },
      { "name": "operand2", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "assemblyFormat": "$operand1 `,` $operand2 `:` type($operand1) attr-dict"
  },
  {
    "name": "spirv.LogicalOr",
    "summary": "Result is true if either Operand 1 or Operand 2 is true. Result is false\n    if both Operand 1 and Operand 2 are false.",
    "description": "Result Type must be a scalar or vector of Boolean type.\n\n    The type of Operand 1 must be the same as Result Type.\n\n    The type of Operand 2 must be the same as Result Type.\n\n    Results are computed per component.\n\n    #### Example:\n\n    ```mlir\n    %2 = spirv.LogicalOr %0, %1 : i1\n    %2 = spirv.LogicalOr %0, %1 : vector<4xi1>\n    ```",
    "inputs": [
      { "name": "operand1", "type": "SPIRV_ScalarOrVectorOf" },
      { "name": "operand2", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "assemblyFormat": "$operand1 `,` $operand2 `:` type($operand1) attr-dict"
  },
  {
    "name": "spirv.MemoryBarrier",
    "summary": "Control the order that memory accesses are observed.",
    "description": "Ensures that memory accesses issued before this instruction will be\n    observed before memory accesses issued after this instruction. This\n    control is ensured only for memory accesses issued by this invocation\n    and observed by another invocation executing within Memory scope. If the\n    Vulkan memory model is declared, this ordering only applies to memory\n    accesses that use the NonPrivatePointer memory operand or\n    NonPrivateTexel image operand.\n\n    Semantics declares what kind of memory is being controlled and what kind\n    of control to apply.\n\n    To execute both a memory barrier and a control barrier, see\n    OpControlBarrier.\n\n    #### Example:\n\n    ```mlir\n    spirv.MemoryBarrier \"Device\", \"Acquire|UniformMemory\"\n    ```",
    "attributes": [
      { "name": "memory_scope", "type": "SPIRV_ScopeAttr" },
      { "name": "memory_semantics", "type": "SPIRV_MemorySemanticsAttr" }
    ],
    "assemblyFormat": "$memory_scope `,` $memory_semantics attr-dict"
  },
  {
    "name": "spirv.mlir.addressof",
    "summary": "Get the address of a global variable.",
    "description": "Variables in module scope are defined using symbol names. This op generates\n    an SSA value that can be used to refer to the symbol within function scope\n    for use in ops that expect an SSA value. This operation has no corresponding\n    SPIR-V instruction; it's merely used for modelling purpose in the SPIR-V\n    dialect. Since variables in module scope in SPIR-V dialect are of pointer\n    type, this op returns a pointer type as well, and the type is the same as\n    the variable referenced.\n\n    #### Example:\n\n    ```mlir\n    %0 = spirv.mlir.addressof @global_var : !spirv.ptr<f32, Input>\n    ```",
    "outputs": [
      { "name": "pointer", "type": "SPIRV_AnyPtr" }
    ],
    "attributes": [
      { "name": "variable", "type": "FlatSymbolRefAttr" }
    ],
    "assemblyFormat": "$variable attr-dict `:` type($pointer)"
  },
  {
    "name": "spirv.mlir.loop",
    "summary": "Define a structured loop.",
    "description": "SPIR-V can explicitly declare structured control-flow constructs using merge\n    instructions. These explicitly declare a header block before the control\n    flow diverges and a merge block where control flow subsequently converges.\n    These blocks delimit constructs that must nest, and can only be entered\n    and exited in structured ways. See \"2.11. Structured Control Flow\" of the\n    SPIR-V spec for more details.\n\n    Instead of having a `spirv.LoopMerge` op to directly model loop merge\n    instruction for indicating the merge and continue target, we use regions\n    to delimit the boundary of the loop: the merge target is the next op\n    following the `spirv.mlir.loop` op and the continue target is the block that\n    has a back-edge pointing to the entry block inside the `spirv.mlir.loop`'s region.\n    This way it's easier to discover all blocks belonging to a construct and\n    it plays nicer with the MLIR system.\n\n    The `spirv.mlir.loop` region should contain at least four blocks: one entry block,\n    one loop header block, one loop continue block, one loop merge block.\n    The entry block should be the first block and it should jump to the loop\n    header block, which is the second block. The loop merge block should be the\n    last block. The merge block should only contain a `spirv.mlir.merge` op.\n    The continue block should be the second to last block and it should have a\n    branch to the loop header block. The loop continue block should be the only\n    block, except the entry block, branching to the header block.\n\n    Values defined inside the loop regions cannot be directly used\n    outside of them; however, the loop region can yield values. These values are\n    yielded using a `spirv.mlir.merge` op and returned as a result of the loop op.",
    "outputs": [
      { "name": "results", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "loop_control", "type": "SPIRV_LoopControlAttr" }
    ]
  },
  {
    "name": "spirv.mlir.merge",
    "summary": "A special terminator for merging a structured selection/loop.",
    "description": "We use `spirv.mlir.selection`/`spirv.mlir.loop` for modelling structured selection/loop.\n    This op is a terminator used inside their regions to mean jumping to the\n    merge point, which is the next op following the `spirv.mlir.selection` or\n    `spirv.mlir.loop` op. This op does not have a corresponding instruction in the\n    SPIR-V binary format; it's solely for structural purpose.\n\n    The instruction is also used to yield values from inside the selection/loop region\n    to the outside, as values that were sunk into the region cannot otherwise escape it.",
    "inputs": [
      { "name": "operands", "type": "Variadic" }
    ],
    "assemblyFormat": "attr-dict ($operands^ `:` type($operands))?"
  },
  {
    "name": "spirv.mlir.referenceof",
    "summary": "Reference a specialization constant.",
    "description": "Specialization constants in module scope are defined using symbol names.\n    This op generates an SSA value that can be used to refer to the symbol\n    within function scope for use in ops that expect an SSA value.\n    This operation has no corresponding SPIR-V instruction; it's merely used\n    for modelling purpose in the SPIR-V dialect. This op's return type is\n    the same as the specialization constant.\n\n    #### Example:\n\n    ```mlir\n    %0 = spirv.mlir.referenceof @spec_const : f32\n    ```\n\n    TODO Add support for composite specialization constants.",
    "outputs": [
      { "name": "reference", "type": "SPIRV_Type" }
    ],
    "attributes": [
      { "name": "spec_const", "type": "FlatSymbolRefAttr" }
    ],
    "assemblyFormat": "$spec_const attr-dict `:` type($reference)"
  },
  {
    "name": "spirv.mlir.selection",
    "summary": "Define a structured selection.",
    "description": "SPIR-V can explicitly declare structured control-flow constructs using merge\n    instructions. These explicitly declare a header block before the control\n    flow diverges and a merge block where control flow subsequently converges.\n    These blocks delimit constructs that must nest, and can only be entered\n    and exited in structured ways. See \"2.11. Structured Control Flow\" of the\n    SPIR-V spec for more details.\n\n    Instead of having a `spirv.SelectionMerge` op to directly model selection\n    merge instruction for indicating the merge target, we use regions to delimit\n    the boundary of the selection: the merge target is the next op following the\n    `spirv.mlir.selection` op. This way it's easier to discover all blocks belonging to\n    the selection and it plays nicer with the MLIR system.\n\n    The `spirv.mlir.selection` region should contain at least two blocks: one selection\n    header block, and one selection merge. The selection header block should be\n    the first block. The selection merge block should be the last block.\n    The merge block should only contain a `spirv.mlir.merge` op.\n\n    Values defined inside the selection regions cannot be directly used\n    outside of them; however, the selection region can yield values. These values are\n    yielded using a `spirv.mlir.merge` op and returned as a result of the selection op.",
    "outputs": [
      { "name": "results", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "selection_control", "type": "SPIRV_SelectionControlAttr" }
    ]
  },
  {
    "name": "spirv.mlir.yield",
    "summary": "Yields the result computed in `spirv.SpecConstantOperation`'s\n    region back to the parent op.",
    "description": "This op is a special terminator whose only purpose is to terminate\n    an `spirv.SpecConstantOperation`'s enclosed region. It accepts a\n    single operand produced by the preceeding (and only other) instruction\n    in its parent block (see SPIRV_SpecConstantOperation for further\n    details). This op has no corresponding SPIR-V instruction.\n\n    #### Example:\n\n    ```mlir\n    %0 = ... (some op supported by SPIR-V OpSpecConstantOp)\n    spirv.mlir.yield %0\n    ```",
    "inputs": [
      { "name": "operand", "type": "AnyType" }
    ],
    "assemblyFormat": "attr-dict $operand `:` type($operand)"
  },
  {
    "name": "spirv.module",
    "summary": "The top-level op that defines a SPIR-V module",
    "description": "This op defines a SPIR-V module using a MLIR region. The region contains\n    one block. Module-level operations, including functions definitions,\n    are all placed in this block.\n\n    Using an op with a region to define a SPIR-V module enables \"embedding\"\n    SPIR-V modules in other dialects in a clean manner: this op guarantees\n    the validity and serializability of a SPIR-V module and thus serves as\n    a clear-cut boundary.\n\n    This op takes no operands and generates no results. This op should not\n    implicitly capture values from the enclosing environment.\n\n    This op has only one region, which only contains one block. The block\n    has no terminator.\n\n    <!-- End of AutoGen section -->\n\n    ```\n    addressing-model ::= `Logical` | `Physical32` | `Physical64` | ...\n    memory-model ::= `Simple` | `GLSL450` | `OpenCL` | `Vulkan` | ...\n    spv-module-op ::= `spirv.module` addressing-model memory-model\n                      (requires  spirv-vce-attribute)?\n                      (`attributes` attribute-dict)?\n                      region\n    ```\n\n    #### Example:\n\n    ```mlir\n    spirv.module Logical GLSL450  {}\n\n    spirv.module Logical Vulkan\n        requires #spirv.vce<v1.0, [Shader], [SPV_KHR_vulkan_memory_model]>\n        attributes { some_additional_attr = ... } {\n      spirv.func @do_nothing() -> () {\n        spirv.Return\n      }\n    }\n    ```",
    "attributes": [
      { "name": "addressing_model", "type": "SPIRV_AddressingModelAttr" },
      { "name": "memory_model", "type": "SPIRV_MemoryModelAttr" },
      { "name": "vce_triple", "type": "OptionalAttr" },
      { "name": "sym_name", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "spirv.Not",
    "summary": "Complement the bits of Operand.",
    "description": "Results are computed per component, and within each component, per bit.\n\n    Result Type must be a scalar or vector of integer type.\n\n    Operand's type  must be a scalar or vector of integer type.  It must\n    have the same number of components as Result Type.  The component width\n    must equal the component width in Result Type.\n\n    #### Example:\n\n    ```mlir\n    %2 = spirv.Not %0 : i32\n    %3 = spirv.Not %1 : vector<4xi32>\n    ```",
    "inputs": [
      { "name": "operand", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "assemblyFormat": "$operand `:` type($operand) attr-dict"
  },
  {
    "name": "spirv.Ordered",
    "summary": "Result is true if both x == x and y == y are true, where IEEE comparison\n    is used, otherwise result is false.",
    "description": "Result Type must be a scalar or vector of Boolean type.\n\n    x must be a scalar or vector of floating-point type.  It must have the\n    same number of components as Result Type.\n\n    y must have the same type as x.\n\n    Results are computed per component.\n\n    #### Example:\n\n    ```mlir\n    %4 = spirv.Ordered %0, %1 : f32\n    %5 = spirv.Ordered %2, %3 : vector<4xf32>\n    ```",
    "inputs": [
      { "name": "operand1", "type": "SPIRV_ScalarOrVectorOf" },
      { "name": "operand2", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "assemblyFormat": "$operand1 `,` $operand2 `:` type($operand1) attr-dict"
  },
  {
    "name": "spirv.PtrCastToGeneric",
    "summary": "Convert a pointer’s Storage Class to Generic.",
    "description": "Result Type must be an OpTypePointer. Its Storage Class must be Generic.\n\n    Pointer must point to the Workgroup, CrossWorkgroup, or Function Storage\n    Class.\n\n    Result Type and Pointer must point to the same type.\n\n    <!-- End of AutoGen section -->\n\n    #### Example:\n\n    ```mlir\n    %1 = spirv.PtrCastToGenericOp %0 : !spirv.ptr<f32, CrossWorkGroup> to\n         !spirv.ptr<f32, Generic>\n    ```",
    "inputs": [
      { "name": "pointer", "type": "SPIRV_AnyPtr" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_AnyPtr" }
    ],
    "assemblyFormat": "$pointer attr-dict `:` type($pointer) `to` type($result)"
  },
  {
    "name": "spirv.Return",
    "summary": "Return with no value from a function with void return type.",
    "description": "This instruction must be the last instruction in a block.\n\n    #### Example:\n\n    ```mlir\n    spirv.Return\n    ```",
    "assemblyFormat": "attr-dict"
  },
  {
    "name": "spirv.ReturnValue",
    "summary": "Return a value from a function.",
    "description": "Value is the value returned, by copy, and must match the Return Type\n    operand of the OpTypeFunction type of the OpFunction body this return\n    instruction is in.\n\n    This instruction must be the last instruction in a block.\n\n    #### Example:\n\n    ```mlir\n    spirv.ReturnValue %0 : f32\n    ```",
    "inputs": [
      { "name": "value", "type": "SPIRV_Type" }
    ],
    "assemblyFormat": "$value attr-dict `:` type($value)"
  },
  {
    "name": "spirv.SConvert",
    "summary": "Convert signed width.  This is either a truncate or a sign extend.",
    "description": "Result Type must be a scalar or vector of integer type.\n\n    Signed Value must be a scalar or vector of integer type.  It must have\n    the same number of components as Result Type.  The component width\n    cannot equal the component width in Result Type.\n\n    Results are computed per component.\n\n    #### Example:\n\n    ```mlir\n    %1 = spirv.SConvertOp %0 : i32 to i64\n    %3 = spirv.SConvertOp %2 : vector<3xi32> to vector<3xi64>\n    ```",
    "inputs": [
      { "name": "operand", "type": "SPIRV_ScalarOrVectorOrCoopMatrixOf" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_ScalarOrVectorOrCoopMatrixOf" }
    ],
    "assemblyFormat": "$operand attr-dict `:` type($operand) `to` type($result)"
  },
  {
    "name": "spirv.SDiv",
    "summary": "Signed-integer division of Operand 1 divided by Operand 2.",
    "description": "Result Type must be a scalar or vector of integer type.\n\n    The type of Operand 1 and Operand 2  must be a scalar or vector of\n    integer type.  They must have the same number of components as Result\n    Type. They must have the same component width as Result Type.\n\n    Results are computed per component.  The resulting value is undefined\n    if Operand 2 is 0.\n\n    #### Example:\n\n    ```mlir\n    %4 = spirv.SDiv %0, %1 : i32\n    %5 = spirv.SDiv %2, %3 : vector<4xi32>\n\n    ```",
    "inputs": [
      { "name": "operand1", "type": "SPIRV_ScalarOrVectorOrCoopMatrixOf" },
      { "name": "operand2", "type": "SPIRV_ScalarOrVectorOrCoopMatrixOf" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_ScalarOrVectorOrCoopMatrixOf" }
    ],
    "assemblyFormat": "operands attr-dict `:` type($result)"
  },
  {
    "name": "spirv.Select",
    "summary": "Select between two objects. Before version 1.4, results are only\n    computed per component.",
    "description": "Before version 1.4, Result Type must be a pointer, scalar, or vector.\n\n    The types of Object 1 and Object 2 must be the same as Result Type.\n\n    Condition must be a scalar or vector of Boolean type.\n\n    If Condition is a scalar and true, the result is Object 1. If Condition\n    is a scalar and false, the result is Object 2.\n\n    If Condition is a vector, Result Type must be a vector with the same\n    number of components as Condition and the result is a mix of Object 1\n    and Object 2: When a component of Condition is true, the corresponding\n    component in the result is taken from Object 1, otherwise it is taken\n    from Object 2.\n\n    #### Example:\n\n    ```mlir\n    %3 = spirv.Select %0, %1, %2 : i1, f32\n    %3 = spirv.Select %0, %1, %2 : i1, vector<3xi32>\n    %3 = spirv.Select %0, %1, %2 : vector<3xi1>, vector<3xf32>\n    ```",
    "inputs": [
      { "name": "condition", "type": "SPIRV_ScalarOrVectorOf" },
      { "name": "true_value", "type": "SPIRV_SelectType" },
      { "name": "false_value", "type": "SPIRV_SelectType" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_SelectType" }
    ],
    "assemblyFormat": "operands attr-dict `:` type($condition) `,` type($result)"
  },
  {
    "name": "spirv.SGreaterThan",
    "summary": "Signed-integer comparison if Operand 1 is greater than  Operand 2.",
    "description": "Result Type must be a scalar or vector of Boolean type.\n\n    The type of Operand 1 and Operand 2  must be a scalar or vector of\n    integer type.  They must have the same component width, and they must\n    have the same number of components as Result Type.\n\n    Results are computed per component.\n\n    #### Example:\n\n    ```mlir\n    %4 = spirv.SGreaterThan %0, %1 : i32\n    %5 = spirv.SGreaterThan %2, %3 : vector<4xi32>\n\n    ```",
    "inputs": [
      { "name": "operand1", "type": "SPIRV_ScalarOrVectorOf" },
      { "name": "operand2", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "assemblyFormat": "$operand1 `,` $operand2 `:` type($operand1) attr-dict"
  },
  {
    "name": "spirv.SGreaterThanEqual",
    "summary": "Signed-integer comparison if Operand 1 is greater than or equal to\n    Operand 2.",
    "description": "Result Type must be a scalar or vector of Boolean type.\n\n    The type of Operand 1 and Operand 2  must be a scalar or vector of\n    integer type.  They must have the same component width, and they must\n    have the same number of components as Result Type.\n\n    Results are computed per component.\n\n    #### Example:\n\n    ```mlir\n    %4 = spirv.SGreaterThanEqual %0, %1 : i32\n    %5 = spirv.SGreaterThanEqual %2, %3 : vector<4xi32>\n    ```",
    "inputs": [
      { "name": "operand1", "type": "SPIRV_ScalarOrVectorOf" },
      { "name": "operand2", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "assemblyFormat": "$operand1 `,` $operand2 `:` type($operand1) attr-dict"
  },
  {
    "name": "spirv.ShiftLeftLogical",
    "summary": "Shift the bits in Base left by the number of bits specified in Shift.\n    The least-significant bits are zero filled.",
    "description": "Result Type must be a scalar or vector of integer type.\n\n    The type of each Base and Shift must be a scalar or vector of integer\n    type. Base and Shift must have the same number of components.  The\n    number of components and bit width of the type of Base must be the same\n    as in Result Type.\n\n    Shift is treated as unsigned. The result is undefined if Shift is\n    greater than or equal to the bit width of the components of Base.\n\n    The number of components and bit width of Result Type must match those\n    Base type. All types must be integer types.\n\n    Results are computed per component.\n\n    #### Example:\n\n    ```mlir\n    %2 = spirv.ShiftLeftLogical %0, %1 : i32, i16\n    %5 = spirv.ShiftLeftLogical %3, %4 : vector<3xi32>, vector<3xi16>\n    ```",
    "inputs": [
      { "name": "operand1", "type": "SPIRV_ScalarOrVectorOf" },
      { "name": "operand2", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "assemblyFormat": "operands attr-dict `:` type($operand1) `,` type($operand2)"
  },
  {
    "name": "spirv.ShiftRightArithmetic",
    "summary": "Shift the bits in Base right by the number of bits specified in Shift.\n    The most-significant bits are filled with the sign bit from Base.",
    "description": "Result Type must be a scalar or vector of integer type.\n\n    The type of each Base and Shift must be a scalar or vector of integer\n    type. Base and Shift must have the same number of components.  The\n    number of components and bit width of the type of Base must be the same\n    as in Result Type.\n\n    Shift is treated as unsigned. The result is undefined if Shift is\n    greater than or equal to the bit width of the components of Base.\n\n    Results are computed per component.\n\n    #### Example:\n\n    ```mlir\n    %2 = spirv.ShiftRightArithmetic %0, %1 : i32, i16\n    %5 = spirv.ShiftRightArithmetic %3, %4 : vector<3xi32>, vector<3xi16>\n    ```",
    "inputs": [
      { "name": "operand1", "type": "SPIRV_ScalarOrVectorOf" },
      { "name": "operand2", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "assemblyFormat": "operands attr-dict `:` type($operand1) `,` type($operand2)"
  },
  {
    "name": "spirv.ShiftRightLogical",
    "summary": "Shift the bits in Base right by the number of bits specified in Shift.\n    The most-significant bits are zero filled.",
    "description": "Result Type must be a scalar or vector of integer type.\n\n    The type of each Base and Shift must be a scalar or vector of integer\n    type. Base and Shift must have the same number of components.  The\n    number of components and bit width of the type of Base must be the same\n    as in Result Type.\n\n    Shift is consumed as an unsigned integer. The result is undefined if\n    Shift is greater than or equal to the bit width of the components of\n    Base.\n\n    Results are computed per component.\n\n    #### Example:\n\n    ```mlir\n    %2 = spirv.ShiftRightLogical %0, %1 : i32, i16\n    %5 = spirv.ShiftRightLogical %3, %4 : vector<3xi32>, vector<3xi16>\n    ```",
    "inputs": [
      { "name": "operand1", "type": "SPIRV_ScalarOrVectorOf" },
      { "name": "operand2", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "assemblyFormat": "operands attr-dict `:` type($operand1) `,` type($operand2)"
  },
  {
    "name": "spirv.SLessThan",
    "summary": "Signed-integer comparison if Operand 1 is less than Operand 2.",
    "description": "Result Type must be a scalar or vector of Boolean type.\n\n    The type of Operand 1 and Operand 2  must be a scalar or vector of\n    integer type.  They must have the same component width, and they must\n    have the same number of components as Result Type.\n\n    Results are computed per component.\n\n    #### Example:\n\n    ```mlir\n    %4 = spirv.SLessThan %0, %1 : i32\n    %5 = spirv.SLessThan %2, %3 : vector<4xi32>\n\n    ```",
    "inputs": [
      { "name": "operand1", "type": "SPIRV_ScalarOrVectorOf" },
      { "name": "operand2", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "assemblyFormat": "$operand1 `,` $operand2 `:` type($operand1) attr-dict"
  },
  {
    "name": "spirv.SLessThanEqual",
    "summary": "Signed-integer comparison if Operand 1 is less than or equal to Operand\n    2.",
    "description": "Result Type must be a scalar or vector of Boolean type.\n\n    The type of Operand 1 and Operand 2  must be a scalar or vector of\n    integer type.  They must have the same component width, and they must\n    have the same number of components as Result Type.\n\n    Results are computed per component.\n\n    #### Example:\n\n    ```mlir\n    %4 = spirv.SLessThanEqual %0, %1 : i32\n    %5 = spirv.SLessThanEqual %2, %3 : vector<4xi32>\n    ```",
    "inputs": [
      { "name": "operand1", "type": "SPIRV_ScalarOrVectorOf" },
      { "name": "operand2", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "assemblyFormat": "$operand1 `,` $operand2 `:` type($operand1) attr-dict"
  },
  {
    "name": "spirv.SMod",
    "summary": "Signed remainder operation for the remainder whose sign matches the sign\n    of Operand 2.",
    "description": "Result Type must be a scalar or vector of integer type.\n\n    The type of Operand 1 and Operand 2  must be a scalar or vector of\n    integer type.  They must have the same number of components as Result\n    Type. They must have the same component width as Result Type.\n\n    Results are computed per component.  The resulting value is undefined\n    if Operand 2 is 0.  Otherwise, the result is the remainder r of Operand\n    1 divided by Operand 2 where if r ≠ 0, the sign of r is the same as the\n    sign of Operand 2.\n\n    #### Example:\n\n    ```mlir\n    %4 = spirv.SMod %0, %1 : i32\n    %5 = spirv.SMod %2, %3 : vector<4xi32>\n\n    ```",
    "inputs": [
      { "name": "operand1", "type": "SPIRV_ScalarOrVectorOf" },
      { "name": "operand2", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "assemblyFormat": "operands attr-dict `:` type($result)"
  },
  {
    "name": "spirv.SMulExtended",
    "summary": "Result is the full value of the signed integer multiplication of Operand\n    1 and Operand 2.",
    "description": "Result Type must be from OpTypeStruct.  The struct must have two\n    members, and the two members must be the same type.  The member type\n    must be a scalar or vector of integer type.\n\n    Operand 1 and Operand 2 must have the same type as the members of Result\n    Type. These are consumed as signed integers.\n\n    Results are computed per component.\n\n    Member 0 of the result gets the low-order bits of the multiplication.\n\n    Member 1 of the result gets the high-order bits of the multiplication.\n\n    <!-- End of AutoGen section -->\n\n    #### Example:\n\n    ```mlir\n    %2 = spirv.SMulExtended %0, %1 : !spirv.struct<(i32, i32)>\n    %2 = spirv.SMulExtended %0, %1 : !spirv.struct<(vector<2xi32>, vector<2xi32>)>\n    ```",
    "inputs": [
      { "name": "operand1", "type": "SPIRV_ScalarOrVectorOf" },
      { "name": "operand2", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_AnyStruct" }
    ]
  },
  {
    "name": "spirv.SNegate",
    "summary": "Signed-integer subtract of Operand from zero.",
    "description": "Result Type must be a scalar or vector of integer type.\n\n    Operand's type  must be a scalar or vector of integer type.  It must\n    have the same number of components as Result Type.  The component width\n    must equal the component width in Result Type.\n\n     Results are computed per component.\n\n    <!-- End of AutoGen section -->\n\n    #### Example:\n\n    ```mlir\n    %1 = spirv.SNegate %0 : i32\n    %3 = spirv.SNegate %2 : vector<4xi32>\n    ```",
    "inputs": [
      { "name": "operand", "type": "SPIRV_ScalarOrVectorOrCoopMatrixOf" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_ScalarOrVectorOrCoopMatrixOf" }
    ],
    "assemblyFormat": "operands attr-dict `:` type($result)"
  },
  {
    "name": "spirv.SpecConstant",
    "summary": "Declare a new integer-type or floating-point-type scalar specialization\n    constant.",
    "description": "This op declares a SPIR-V scalar specialization constant. SPIR-V has\n    multiple constant instructions covering different scalar types:\n\n    * `OpSpecConstantTrue` and `OpSpecConstantFalse` for boolean constants\n    * `OpSpecConstant` for scalar constants\n\n    Similar as `spirv.Constant`, this op represents all of the above cases.\n    `OpSpecConstantComposite` and `OpSpecConstantOp` are modelled with\n    separate ops.\n\n    <!-- End of AutoGen section -->\n\n    ```\n    spv-spec-constant-op ::= `spirv.SpecConstant` symbol-ref-id\n                             `spec_id(` integer `)`\n                             `=` attribute-value (`:` spirv-type)?\n    ```\n\n    where `spec_id` specifies the SPIR-V SpecId decoration associated with\n    the op.\n\n    #### Example:\n\n    ```mlir\n    spirv.SpecConstant @spec_const1 = true\n    spirv.SpecConstant @spec_const2 spec_id(5) = 42 : i32\n    ```",
    "attributes": [
      { "name": "sym_name", "type": "StrAttr" },
      { "name": "default_value", "type": "TypedAttrInterface" }
    ]
  },
  {
    "name": "spirv.SpecConstantComposite",
    "summary": "Declare a new composite specialization constant.",
    "description": "This op declares a SPIR-V composite specialization constant. This covers\n    the `OpSpecConstantComposite` SPIR-V instruction. Scalar constants are\n    covered by `spirv.SpecConstant`.\n\n    A constituent of a spec constant composite can be:\n    - A symbol referring of another spec constant.\n    - The SSA ID of a non-specialization constant (i.e. defined through\n      `spirv.SpecConstant`).\n    - The SSA ID of a `spirv.Undef`.\n\n    ```\n    spv-spec-constant-composite-op ::= `spirv.SpecConstantComposite` symbol-ref-id ` (`\n                                       symbol-ref-id (`, ` symbol-ref-id)*\n                                       `) :` composite-type\n    ```\n\n     where `composite-type` is some non-scalar type that can be represented in the `spv`\n     dialect: `spirv.struct`, `spirv.array`, or `vector`.\n\n     #### Example:\n\n     ```mlir\n     spirv.SpecConstant @sc1 = 1   : i32\n     spirv.SpecConstant @sc2 = 2.5 : f32\n     spirv.SpecConstant @sc3 = 3.5 : f32\n     spirv.SpecConstantComposite @scc (@sc1, @sc2, @sc3) : !spirv.struct<i32, f32, f32>\n     ```\n\n    TODO Add support for constituents that are:\n    - regular constants.\n    - undef.\n    - spec constant composite.",
    "attributes": [
      { "name": "type", "type": "TypeAttr" },
      { "name": "sym_name", "type": "StrAttr" },
      { "name": "constituents", "type": "SymbolRefArrayAttr" }
    ]
  },
  {
    "name": "spirv.SpecConstantOperation",
    "summary": "Declare a new specialization constant that results from doing an operation.",
    "description": "This op declares a SPIR-V specialization constant that results from\n    doing an operation on other constants (specialization or otherwise).\n\n    In the `spv` dialect, this op is modelled as follows:\n\n    ```\n    spv-spec-constant-operation-op ::= `spirv.SpecConstantOperation` `wraps`\n                                         generic-spirv-op `:` function-type\n    ```\n\n    In particular, an `spirv.SpecConstantOperation` contains exactly one\n    region. In turn, that region, contains exactly 2 instructions:\n    - One of SPIR-V's instructions that are allowed within an\n    OpSpecConstantOp.\n    - An `spirv.mlir.yield` instruction as the terminator.\n\n    The following SPIR-V instructions are valid:\n    - OpSConvert,\n    - OpUConvert,\n    - OpFConvert,\n    - OpSNegate,\n    - OpNot,\n    - OpIAdd,\n    - OpISub,\n    - OpIMul,\n    - OpUDiv,\n    - OpSDiv,\n    - OpUMod,\n    - OpSRem,\n    - OpSMod\n    - OpShiftRightLogical,\n    - OpShiftRightArithmetic,\n    - OpShiftLeftLogical\n    - OpBitwiseOr,\n    - OpBitwiseXor,\n    - OpBitwiseAnd\n    - OpVectorShuffle,\n    - OpCompositeExtract,\n    - OpCompositeInsert\n    - OpLogicalOr,\n    - OpLogicalAnd,\n    - OpLogicalNot,\n    - OpLogicalEqual,\n    - OpLogicalNotEqual\n    - OpSelect\n    - OpIEqual,\n    - OpINotEqual\n    - OpULessThan,\n    - OpSLessThan\n    - OpUGreaterThan,\n    - OpSGreaterThan\n    - OpULessThanEqual,\n    - OpSLessThanEqual\n    - OpUGreaterThanEqual,\n    - OpSGreaterThanEqual\n\n    TODO Add capability-specific ops when supported.\n\n    #### Example:\n    ```mlir\n    %0 = spirv.Constant 1: i32\n    %1 = spirv.Constant 1: i32\n\n    %2 = spirv.SpecConstantOperation wraps \"spirv.IAdd\"(%0, %1) : (i32, i32) -> i32\n    ```",
    "outputs": [
      { "name": "result", "type": "AnyType" }
    ]
  },
  {
    "name": "spirv.SRem",
    "summary": "Signed remainder operation for the remainder whose sign matches the sign\n    of Operand 1.",
    "description": "Result Type must be a scalar or vector of integer type.\n\n    The type of Operand 1 and Operand 2  must be a scalar or vector of\n    integer type.  They must have the same number of components as Result\n    Type. They must have the same component width as Result Type.\n\n    Results are computed per component.  The resulting value is undefined\n    if Operand 2 is 0.  Otherwise, the result is the remainder r of Operand\n    1 divided by Operand 2 where if r ≠ 0, the sign of r is the same as the\n    sign of Operand 1.\n\n    #### Example:\n\n    ```mlir\n    %4 = spirv.SRem %0, %1 : i32\n    %5 = spirv.SRem %2, %3 : vector<4xi32>\n\n    ```",
    "inputs": [
      { "name": "operand1", "type": "SPIRV_ScalarOrVectorOf" },
      { "name": "operand2", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "assemblyFormat": "operands attr-dict `:` type($result)"
  },
  {
    "name": "spirv.UConvert",
    "summary": "Convert unsigned width. This is either a truncate or a zero extend.",
    "description": "Result Type must be a scalar or vector of integer type, whose Signedness\n    operand is 0.\n\n    Unsigned Value must be a scalar or vector of integer type.  It must have\n    the same number of components as Result Type.  The component width\n    cannot equal the component width in Result Type.\n\n    Results are computed per component.\n\n    #### Example:\n\n    ```mlir\n    %1 = spirv.UConvertOp %0 : i32 to i64\n    %3 = spirv.UConvertOp %2 : vector<3xi32> to vector<3xi64>\n    ```",
    "inputs": [
      { "name": "operand", "type": "SPIRV_ScalarOrVectorOrCoopMatrixOf" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_ScalarOrVectorOrCoopMatrixOf" }
    ],
    "assemblyFormat": "$operand attr-dict `:` type($operand) `to` type($result)"
  },
  {
    "name": "spirv.UDiv",
    "summary": "Unsigned-integer division of Operand 1 divided by Operand 2.",
    "description": "Result Type must be a scalar or vector of integer type, whose Signedness\n    operand is 0.\n\n    The types of Operand 1 and Operand 2 both must be the same as Result\n    Type.\n\n    Results are computed per component.  The resulting value is undefined\n    if Operand 2 is 0.\n\n    #### Example:\n\n    ```mlir\n    %4 = spirv.UDiv %0, %1 : i32\n    %5 = spirv.UDiv %2, %3 : vector<4xi32>\n    ```",
    "inputs": [
      { "name": "operand1", "type": "SPIRV_ScalarOrVectorOrCoopMatrixOf" },
      { "name": "operand2", "type": "SPIRV_ScalarOrVectorOrCoopMatrixOf" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_ScalarOrVectorOrCoopMatrixOf" }
    ],
    "assemblyFormat": "operands attr-dict `:` type($result)"
  },
  {
    "name": "spirv.UGreaterThan",
    "summary": "Unsigned-integer comparison if Operand 1 is greater than  Operand 2.",
    "description": "Result Type must be a scalar or vector of Boolean type.\n\n    The type of Operand 1 and Operand 2  must be a scalar or vector of\n    integer type.  They must have the same component width, and they must\n    have the same number of components as Result Type.\n\n    Results are computed per component.\n\n    #### Example:\n\n    ```mlir\n    %4 = spirv.UGreaterThan %0, %1 : i32\n    %5 = spirv.UGreaterThan %2, %3 : vector<4xi32>\n    ```",
    "inputs": [
      { "name": "operand1", "type": "SPIRV_ScalarOrVectorOf" },
      { "name": "operand2", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "assemblyFormat": "$operand1 `,` $operand2 `:` type($operand1) attr-dict"
  },
  {
    "name": "spirv.UGreaterThanEqual",
    "summary": "Unsigned-integer comparison if Operand 1 is greater than or equal to\n    Operand 2.",
    "description": "Result Type must be a scalar or vector of Boolean type.\n\n    The type of Operand 1 and Operand 2  must be a scalar or vector of\n    integer type.  They must have the same component width, and they must\n    have the same number of components as Result Type.\n\n    Results are computed per component.\n\n    #### Example:\n\n    ```mlir\n    %4 = spirv.UGreaterThanEqual %0, %1 : i32\n    %5 = spirv.UGreaterThanEqual %2, %3 : vector<4xi32>\n    ```",
    "inputs": [
      { "name": "operand1", "type": "SPIRV_ScalarOrVectorOf" },
      { "name": "operand2", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "assemblyFormat": "$operand1 `,` $operand2 `:` type($operand1) attr-dict"
  },
  {
    "name": "spirv.ULessThan",
    "summary": "Unsigned-integer comparison if Operand 1 is less than Operand 2.",
    "description": "Result Type must be a scalar or vector of Boolean type.\n\n    The type of Operand 1 and Operand 2  must be a scalar or vector of\n    integer type.  They must have the same component width, and they must\n    have the same number of components as Result Type.\n\n    Results are computed per component.\n\n    #### Example:\n\n    ```mlir\n    %4 = spirv.ULessThan %0, %1 : i32\n    %5 = spirv.ULessThan %2, %3 : vector<4xi32>\n    ```",
    "inputs": [
      { "name": "operand1", "type": "SPIRV_ScalarOrVectorOf" },
      { "name": "operand2", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "assemblyFormat": "$operand1 `,` $operand2 `:` type($operand1) attr-dict"
  },
  {
    "name": "spirv.ULessThanEqual",
    "summary": "Unsigned-integer comparison if Operand 1 is less than or equal to\n    Operand 2.",
    "description": "Result Type must be a scalar or vector of Boolean type.\n\n    The type of Operand 1 and Operand 2  must be a scalar or vector of\n    integer type.  They must have the same component width, and they must\n    have the same number of components as Result Type.\n\n    Results are computed per component.\n\n    #### Example:\n\n    ```mlir\n    %4 = spirv.ULessThanEqual %0, %1 : i32\n    %5 = spirv.ULessThanEqual %2, %3 : vector<4xi32>\n    ```",
    "inputs": [
      { "name": "operand1", "type": "SPIRV_ScalarOrVectorOf" },
      { "name": "operand2", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "assemblyFormat": "$operand1 `,` $operand2 `:` type($operand1) attr-dict"
  },
  {
    "name": "spirv.UMod",
    "summary": "Unsigned modulo operation of Operand 1 modulo Operand 2.",
    "description": "Result Type must be a scalar or vector of integer type, whose Signedness\n    operand is 0.\n\n    The types of Operand 1 and Operand 2 both must be the same as Result\n    Type.\n\n    Results are computed per component.  The resulting value is undefined\n    if Operand 2 is 0.\n\n    #### Example:\n\n    ```mlir\n    %4 = spirv.UMod %0, %1 : i32\n    %5 = spirv.UMod %2, %3 : vector<4xi32>\n    ```",
    "inputs": [
      { "name": "operand1", "type": "SPIRV_ScalarOrVectorOf" },
      { "name": "operand2", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "assemblyFormat": "operands attr-dict `:` type($result)"
  },
  {
    "name": "spirv.UMulExtended",
    "summary": "Result is the full value of the unsigned integer multiplication of\n    Operand 1 and Operand 2.",
    "description": "Result Type must be from OpTypeStruct.  The struct must have two\n    members, and the two members must be the same type.  The member type\n    must be a scalar or vector of integer type, whose Signedness operand is\n    0.\n\n    Operand 1 and Operand 2 must have the same type as the members of Result\n    Type. These are consumed as unsigned integers.\n\n    Results are computed per component.\n\n    Member 0 of the result gets the low-order bits of the multiplication.\n\n    Member 1 of the result gets the high-order bits of the multiplication.\n\n    <!-- End of AutoGen section -->\n\n    #### Example:\n\n    ```mlir\n    %2 = spirv.UMulExtended %0, %1 : !spirv.struct<(i32, i32)>\n    %2 = spirv.UMulExtended %0, %1 : !spirv.struct<(vector<2xi32>, vector<2xi32>)>\n    ```",
    "inputs": [
      { "name": "operand1", "type": "SPIRV_ScalarOrVectorOf" },
      { "name": "operand2", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_AnyStruct" }
    ]
  },
  {
    "name": "spirv.Unordered",
    "summary": "Result is true if either x or y is an IEEE NaN, otherwise result is\n    false.",
    "description": "Result Type must be a scalar or vector of Boolean type.\n\n    x must be a scalar or vector of floating-point type.  It must have the\n    same number of components as Result Type.\n\n    y must have the same type as x.\n\n    Results are computed per component.\n\n    #### Example:\n\n    ```mlir\n    %4 = spirv.Unordered %0, %1 : f32\n    %5 = spirv.Unordered %2, %3 : vector<4xf32>\n    ```",
    "inputs": [
      { "name": "operand1", "type": "SPIRV_ScalarOrVectorOf" },
      { "name": "operand2", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "assemblyFormat": "$operand1 `,` $operand2 `:` type($operand1) attr-dict"
  },
  {
    "name": "spirv.Unreachable",
    "summary": "Behavior is undefined if this instruction is executed.",
    "description": "This instruction must be the last instruction in a block.",
    "assemblyFormat": "attr-dict"
  },
  {
    "name": "spirv.VectorExtractDynamic",
    "summary": "Extract a single, dynamically selected, component of a vector.",
    "description": "Result Type must be a scalar type.\n\n    Vector must have a type OpTypeVector whose Component Type is Result\n    Type.\n\n    Index must be a scalar integer. It is interpreted as a 0-based index of\n    which component of Vector to extract.\n\n    Behavior is undefined if Index's value is less than zero or greater than\n    or equal to the number of components in Vector.\n\n    <!-- End of AutoGen section -->\n\n    #### Example:\n\n    ```\n    %2 = spirv.VectorExtractDynamic %0[%1] : vector<8xf32>, i32\n    ```",
    "inputs": [
      { "name": "vector", "type": "SPIRV_Vector" },
      { "name": "index", "type": "SPIRV_Integer" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_Scalar" }
    ],
    "assemblyFormat": "$vector `[` $index `]` attr-dict `:` type($vector) `,` type($index)"
  },
  {
    "name": "spirv.VectorInsertDynamic",
    "summary": "Make a copy of a vector, with a single, variably selected, component\n    modified.",
    "description": "Result Type must be an OpTypeVector.\n\n    Vector must have the same type as Result Type and is the vector that the\n    non-written components are copied from.\n\n    Component is the value supplied for the component selected by Index. It\n    must have the same type as the type of components in Result Type.\n\n    Index must be a scalar integer. It is interpreted as a 0-based index of\n    which component to modify.\n\n    Behavior is undefined if Index's value is less than zero or greater than\n    or equal to the number of components in Vector.\n\n    #### Example:\n\n    ```mlir\n    %scalar = ... : f32\n    %2 = spirv.VectorInsertDynamic %scalar %0[%1] : f32, vector<8xf32>, i32\n    ```",
    "inputs": [
      { "name": "vector", "type": "SPIRV_Vector" },
      { "name": "component", "type": "SPIRV_Scalar" },
      { "name": "index", "type": "SPIRV_Integer" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_Vector" }
    ],
    "assemblyFormat": "$component `,` $vector `[` $index `]` attr-dict `:` type($vector) `,` type($index)"
  },
  {
    "name": "spirv.VectorShuffle",
    "summary": "Select arbitrary components from two vectors to make a new vector.",
    "description": "Result Type must be an OpTypeVector. The number of components in Result\n    Type must be the same as the number of Component operands.\n\n    Vector 1 and Vector 2 must both have vector types, with the same\n    Component Type as Result Type. They do not have to have the same number\n    of components as Result Type or with each other. They are logically\n    concatenated, forming a single vector with Vector 1's components\n    appearing before Vector 2's. The components of this logical vector are\n    logically numbered with a single consecutive set of numbers from 0 to N\n    - 1, where N is the total number of components.\n\n    Components are these logical numbers (see above), selecting which of the\n    logically numbered components form the result. Each component is an\n    unsigned 32-bit integer.  They can select the components in any order\n    and can repeat components. The first component of the result is selected\n    by the first Component operand,  the second component of the result is\n    selected by the second Component operand, etc. A Component literal may\n    also be FFFFFFFF, which means the corresponding result component has no\n    source and is undefined. All Component literals must either be FFFFFFFF\n    or in [0, N - 1] (inclusive).\n\n    Note: A vector “swizzle” can be done by using the vector for both Vector\n    operands, or using an OpUndef for one of the Vector operands.\n\n    <!-- End of AutoGen section -->\n\n    #### Example:\n\n    ```mlir\n    %0 = spirv.VectorShuffle [1: i32, 3: i32, 5: i32] %vector1, %vector2 :\n      vector<4xf32>, vector<2xf32> -> vector<3xf32>\n    ```",
    "inputs": [
      { "name": "vector1", "type": "SPIRV_Vector" },
      { "name": "vector2", "type": "SPIRV_Vector" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_Vector" }
    ],
    "attributes": [
      { "name": "components", "type": "I32ArrayAttr" }
    ],
    "assemblyFormat": "attr-dict $components $vector1 `,` $vector2 `:`\n      type($vector1) `,` type($vector2) `->` type($result)"
  },
  {
    "name": "spirv.VectorTimesScalar",
    "summary": "Scale a floating-point vector.",
    "description": "Result Type must be a vector of floating-point type.\n\n     The type of Vector must be the same as Result Type. Each component of\n    Vector is multiplied by Scalar.\n\n    Scalar must have the same type as the Component Type in Result Type.\n\n    <!-- End of AutoGen section -->\n\n    #### Example:\n\n    ```mlir\n    %0 = spirv.VectorTimesScalar %vector, %scalar : vector<4xf32>\n    ```",
    "inputs": [
      { "name": "vector", "type": "VectorOfLengthAndType" },
      { "name": "scalar", "type": "SPIRV_Float" }
    ],
    "outputs": [
      { "name": "result", "type": "VectorOfLengthAndType" }
    ],
    "assemblyFormat": "operands attr-dict `:` `(` type(operands) `)` `->` type($result)"
  },
  {
    "name": "spirv.vendorName#.#mnemonic",
    "summary": "See extension SPV_KHR_shader_ballot",
    "description": "Computes a bitfield value combining the Predicate value from all invocations\n    in the current Subgroup that execute the same dynamic instance of this\n    instruction. The bit is set to one if the corresponding invocation is active\n    and the predicate is evaluated to true; otherwise, it is set to zero.\n\n    Predicate must be a Boolean type.\n\n    Result Type must be a 4 component vector of 32 bit integer types.\n\n    Result is a set of bitfields where the first invocation is represented in bit\n    0 of the first vector component and the last (up to SubgroupSize) is the\n    higher bit number of the last bitmask needed to represent all bits of the\n    subgroup invocations.\n\n    <!-- End of AutoGen section -->\n\n    ```\n    subgroup-ballot-op ::= ssa-id `=` `spirv.KHR.SubgroupBallot`\n                                ssa-use `:` `vector` `<` 4 `x` `i32` `>`\n    ```\n\n    #### Example:\n\n    ```mlir\n    %0 = spirv.KHR.SubgroupBallot %predicate : vector<4xi32>\n    ```",
    "inputs": [
      { "name": "predicate", "type": "SPIRV_Bool" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_Int32Vec4" }
    ],
    "assemblyFormat": "$predicate attr-dict `:` type($result)"
  },
  {
    "name": "stablehlo.abs",
    "summary": "Abs operation",
    "description": "Performs element-wise abs operation on `operand` tensor and produces a\n    `result` tensor.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#abs\n\n    Example:\n    ```mlir\n    %result = stablehlo.abs %operand : tensor<3xi32>\n    ```",
    "inputs": [
      { "name": "operand", "type": "OperandType" }
    ],
    "outputs": [
      { "name": "result", "type": "ResultType" }
    ],
    "assemblyFormat": "$operand attr-dict `:` custom<SameOperandsAndResultType>(type($operand), type($result))"
  },
  {
    "name": "stablehlo.add",
    "summary": "Add operation",
    "description": "Performs element-wise addition of two tensors `lhs` and `rhs` and produces a\n    `result` tensor.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#add\n\n    Example:\n    ```mlir\n    %result = stablehlo.add %lhs, %rhs : tensor<2x2xi32>\n    ```",
    "inputs": [
      { "name": "lhs", "type": "OperandType" },
      { "name": "rhs", "type": "OperandType" }
    ],
    "outputs": [
      { "name": "result", "type": "ResultType" }
    ],
    "assemblyFormat": "$lhs `,` $rhs attr-dict\n      `:` custom<SameOperandsAndResultType>(type($lhs), type($rhs), type($result))"
  },
  {
    "name": "stablehlo.after_all",
    "summary": "AfterAll operation",
    "description": "Ensures that the operations producing the `inputs` are executed before any\n    operations that depend on `result`.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#after_all\n\n    Example:\n    ```mlir\n    %result = stablehlo.after_all %input0, %input1 : !stablehlo.token\n    ```",
    "inputs": [
      { "name": "inputs", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "result", "type": "HLO_Token" }
    ],
    "assemblyFormat": "$inputs attr-dict\n      `:` custom<VariadicSameOperandsAndResultType>(ref($inputs), type($inputs), type($result))"
  },
  {
    "name": "stablehlo.all_gather",
    "summary": "AllGather operation",
    "description": "Within each process group in the process grid, concatenates the values of the\n    `operand` tensor from each process along `all_gather_dim` and produces a\n    `result` tensor.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#all_gather\n\n    Example:\n    ```mlir\n    %result:2 = \"stablehlo.all_gather\"(%operand0, %operand1) {\n      all_gather_dim = 1 : i64,\n      replica_groups = dense<[[0, 1]]> : tensor<1x2xi64>,\n      channel_handle = #stablehlo.channel_handle<handle = 0, type = 0>\n    } : (tensor<2x2xi64>, tensor<2x2xi64>) -> (tensor<2x4xi64>, tensor<2x4xi64>)\n    ```",
    "inputs": [
      { "name": "operands", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "all_gather_dim", "type": "ConfinedAttr" },
      { "name": "replica_groups", "type": "I64ElementsAttr" },
      { "name": "channel_handle", "type": "OptionalAttr" },
      { "name": "use_global_device_ids", "type": "UnitAttr" }
    ]
  },
  {
    "name": "stablehlo.all_reduce",
    "summary": "AllReduce operation",
    "description": "Within each process group in the process grid, applies a reduction function\n    `computation` to the values of the `operand` tensor from each process and\n    produces a `result` tensor.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#all_reduce\n\n    Example:\n    ```mlir\n    %result:2 = \"stablehlo.all_reduce\"(%operand0, %operand0) ({\n      ^bb0(%arg0: tensor<i64>, %arg1: tensor<i64>):\n      %0 = \"stablehlo.add\"(%arg0, %arg1) : (tensor<i64>, tensor<i64>) -> tensor<i64>\n      \"stablehlo.return\"(%0) : (tensor<i64>) -> ()\n    }) {\n      replica_groups = dense<[[0, 1]]> : tensor<1x2xi64>,\n      channel_handle = #stablehlo.channel_handle<handle = 0, type = 0>\n    } : (tensor<4xi64>, tensor<4xi64>) -> (tensor<4xi64>, tensor<4xi64>)\n    ```",
    "inputs": [
      { "name": "operands", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "replica_groups", "type": "I64ElementsAttr" },
      { "name": "channel_handle", "type": "OptionalAttr" },
      { "name": "use_global_device_ids", "type": "UnitAttr" }
    ]
  },
  {
    "name": "stablehlo.all_to_all",
    "summary": "AllToAll operation",
    "description": "Within each process group in the process grid, splits the values of the\n    `operand` tensor along `split_dimension` into parts, scatters the split parts\n    between the processes, concatenates the scattered parts along `concat_dimension`\n    and produces a `result` tensor.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#all_to_all\n\n    Example:\n    ```mlir\n    %result:2 = \"stablehlo.all_to_all\"(%operand1, %operand2) {\n      split_dimension = 1 : i64,\n      concat_dimension = 0 : i64,\n      split_count = 2 : i64,\n      replica_groups = dense<[[0, 1]]> : tensor<1x2xi64>\n    } : (tensor<2x4xi64>, tensor<2x4xi64>) -> (tensor<4x2xi64>, tensor<4x2xi64>)\n    ```",
    "inputs": [
      { "name": "operands", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "split_dimension", "type": "ConfinedAttr" },
      { "name": "concat_dimension", "type": "ConfinedAttr" },
      { "name": "split_count", "type": "ConfinedAttr" },
      { "name": "replica_groups", "type": "I64ElementsAttr" },
      { "name": "channel_handle", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "stablehlo.and",
    "summary": "And operation",
    "description": "Performs element-wise AND of two tensors `lhs` and `rhs` and produces a\n    `result` tensor\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#and\n\n    Example:\n    ```mlir\n    %result = stablehlo.and %lhs, %rhs : tensor<2x2xi32>\n    ```",
    "inputs": [
      { "name": "lhs", "type": "HLO_PredOrIntTensor" },
      { "name": "rhs", "type": "HLO_PredOrIntTensor" }
    ],
    "outputs": [
      { "name": "result", "type": "ResultType" }
    ],
    "assemblyFormat": "$lhs `,` $rhs attr-dict\n      `:` custom<SameOperandsAndResultType>(type($lhs), type($rhs), type($result))"
  },
  {
    "name": "stablehlo.atan2",
    "summary": "Atan2 operation",
    "description": "Performs element-wise atan2 operation on `lhs` and `rhs` tensor and produces\n    a `result` tensor.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#atan2\n\n    Example:\n    ```mlir\n    %result = stablehlo.atan2 %lhs, %rhs : tensor<3xf64>\n    ```",
    "inputs": [
      { "name": "lhs", "type": "OperandType" },
      { "name": "rhs", "type": "OperandType" }
    ],
    "outputs": [
      { "name": "result", "type": "ResultType" }
    ],
    "assemblyFormat": "$lhs `,` $rhs attr-dict\n      `:` custom<SameOperandsAndResultType>(type($lhs), type($rhs), type($result))"
  },
  {
    "name": "stablehlo.batch_norm_grad",
    "summary": "BatchNormGrad operation",
    "description": "Computes gradients of several inputs of BatchNormTrainingOp backpropagating\n    from `grad_output`, and produces `grad_operand`, `grad_scale` and\n    `grad_offset` tensors.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#batch_norm_grad\n\n    Example:\n    ```mlir\n    %grad_operand, %grad_scale, %grad_offset =\n    \"stablehlo.batch_norm_grad\"(%operand, %scale, %mean, %variance, %grad_output) {\n      epsilon = 0.0 : f32,\n      feature_index = 2 : i64\n    } : (tensor<2x2x2xf64>, tensor<2xf64>, tensor<2xf64>, tensor<2xf64>,\n         tensor<2x2x2xf64>) -> (tensor<2x2x2xf64>, tensor<2xf64>, tensor<2xf64>)\n    ```",
    "inputs": [
      { "name": "operand", "type": "RankedTensorOf" },
      { "name": "scale", "type": "DTensorOf" },
      { "name": "mean", "type": "DTensorOf" },
      { "name": "variance", "type": "DTensorOf" },
      { "name": "grad_output", "type": "RankedTensorOf" }
    ],
    "outputs": [
      { "name": "grad_operand", "type": "RankedTensorOf" },
      { "name": "grad_scale", "type": "DTensorOf" },
      { "name": "grad_offset", "type": "DTensorOf" }
    ],
    "attributes": [
      { "name": "epsilon", "type": "F32Attr" },
      { "name": "feature_index", "type": "ConfinedAttr" }
    ]
  },
  {
    "name": "stablehlo.batch_norm_inference",
    "summary": "BatchNormInference operation",
    "description": "Normalizes the `operand` tensor across all dimensions except for the\n    `feature_index` dimension and produces a `result` tensor.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#batch_norm_inference\n\n    Example:\n    ```mlir\n    %result = \"stablehlo.batch_norm_inference\"(%operand, %scale, %offset, %mean, %variance) {\n      epsilon = 0.0 : f32,\n      feature_index = 2 : i64\n    } : (tensor<2x2x2xf64>, tensor<2xf64>, tensor<2xf64>, tensor<2xf64>, tensor<2xf64>) -> tensor<2x2x2xf64>\n    ```",
    "inputs": [
      { "name": "operand", "type": "RankedTensorOf" },
      { "name": "scale", "type": "DTensorOf" },
      { "name": "offset", "type": "DTensorOf" },
      { "name": "mean", "type": "DTensorOf" },
      { "name": "variance", "type": "DTensorOf" }
    ],
    "outputs": [
      { "name": "result", "type": "RankedTensorOf" }
    ],
    "attributes": [
      { "name": "epsilon", "type": "F32Attr" },
      { "name": "feature_index", "type": "ConfinedAttr" }
    ]
  },
  {
    "name": "stablehlo.batch_norm_training",
    "summary": "BatchNormTraining operation",
    "description": "Computes mean and variance across batch and spatial dimensions and\n    normalizes the `operand` tensor, for each feature in the `feature_index`\n    dimension and produces `output`, `batch_mean` and `batch_var` tensors.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#batch_norm_training\n\n    Example:\n    ```mlir\n    %output, %batch_mean, %batch_var = \"stablehlo.batch_norm_training\"(%operand, %scale, %offset) {\n      epsilon = 0.0 : f32,\n      feature_index = 2 : i64\n    } : (tensor<2x2x2xf64>, tensor<2xf64>, tensor<2xf64>) ->\n        (tensor<2x2x2xf64>, tensor<2xf64>, tensor<2xf64>)\n    ```",
    "inputs": [
      { "name": "operand", "type": "RankedTensorOf" },
      { "name": "scale", "type": "DTensorOf" },
      { "name": "offset", "type": "DTensorOf" }
    ],
    "outputs": [
      { "name": "output", "type": "RankedTensorOf" },
      { "name": "batch_mean", "type": "DTensorOf" },
      { "name": "batch_var", "type": "DTensorOf" }
    ],
    "attributes": [
      { "name": "epsilon", "type": "F32Attr" },
      { "name": "feature_index", "type": "ConfinedAttr" }
    ]
  },
  {
    "name": "stablehlo.bitcast_convert",
    "summary": "BitcastConvert operation",
    "description": "Performs a bitcast operation on `operand` tensor and produces a `result`\n    tensor where the bits of the entire `operand` tensor are reinterpreted using\n    the type of the `result` tensor.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#bitcast_convert\n\n    Example:\n    ```mlir\n    %result = stablehlo.bitcast_convert %operand : (tensor<f64>) -> tensor<4xf16>\n    ```",
    "inputs": [
      { "name": "operand", "type": "HLO_TensorOrPerAxisQuantizedTensor" }
    ],
    "assemblyFormat": "operands attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "stablehlo.broadcast",
    "summary": "Broadcast operation",
    "description": "This operation is on its way out of StableHLO, so it is not included in\n    the StableHLO specification: https://github.com/openxla/stablehlo/issues/3.\n\n    Informally, this operation does the same thing as XLA's Broadcast:\n    https://www.tensorflow.org/xla/operation_semantics#broadcast\n\n    Example:\n    ```mlir\n    %result = stablehlo.broadcast %operand, sizes = [1, 2] : (tensor<3xi32>) -> tensor<1x2x3xi32>\n    ```",
    "inputs": [
      { "name": "operand", "type": "HLO_Tensor" }
    ],
    "attributes": [
      { "name": "broadcast_sizes", "type": "DenseI64ArrayAttr" }
    ],
    "assemblyFormat": "$operand `,` `sizes` `=` $broadcast_sizes\n      attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "stablehlo.broadcast_in_dim",
    "summary": "BroadcastInDim operation",
    "description": "Expands the dimensions and/or rank of an input tensor by duplicating the\n    data in the `operand` tensor and produces a `result` tensor.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#broadcast_in_dim\n\n    Example:\n    ```mlir\n    %result = stablehlo.broadcast_in_dim %operand, dims = [2, 1] : (tensor<1x3xi32>) -> tensor<2x3x2xi32>\n    ```",
    "inputs": [
      { "name": "operand", "type": "HLO_TensorOrPerAxisQuantizedTensor" }
    ],
    "attributes": [
      { "name": "broadcast_dimensions", "type": "DenseI64ArrayAttr" }
    ],
    "assemblyFormat": "$operand `,` `dims` `=` $broadcast_dimensions\n      attr-dict `:` functional-type(operands, results)",
    "category": "Shape"
  },
  {
    "name": "stablehlo.case",
    "summary": "Case operation",
    "description": "Produces the output from executing exactly one `function` from `branches`\n    depending on the value of `index`.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#case\n\n    Example:\n    ```mlir\n    %result0, %result1 = \"stablehlo.case\"(%index) ({\n      stablehlo.return %result_branch0, %result_branch0 : tensor<2xi64>, tensor<2xi64>\n    }, {\n      stablehlo.return %result_branch1, %result_branch1 : tensor<2xi64>, tensor<2xi64>\n    }) : (tensor<i32>) -> (tensor<2xi64>, tensor<2xi64>)\n    ```",
    "inputs": [
      { "name": "index", "type": "I32RankedTensor" }
    ]
  },
  {
    "name": "stablehlo.cbrt",
    "summary": "Cbrt operation",
    "description": "Performs element-wise cubic root operation on `operand` tensor and produces\n    a `result` tensor.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#cbrt\n\n    Example:\n    ```mlir\n    %result = stablehlo.cbrt %operand : tensor<4xf64>\n    ```",
    "inputs": [
      { "name": "operand", "type": "HLO_FpComplexOrQuantizedIntTensor" }
    ],
    "outputs": [
      { "name": "result", "type": "HLO_FpComplexOrQuantizedIntTensor" }
    ],
    "attributes": [
      { "name": "result_accuracy", "type": "DefaultValuedOptionalAttr" }
    ],
    "assemblyFormat": "$operand attr-dict `:` custom<SameOperandsAndResultType>(type($operand), type($result))"
  },
  {
    "name": "stablehlo.ceil",
    "summary": "Ceil operation",
    "description": "Performs element-wise ceil of `operand` tensor and produces a `result` tensor.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#ceil\n\n    Example:\n    ```mlir\n    %result = stablehlo.ceil %operand : tensor<5xf32>\n    ```",
    "inputs": [
      { "name": "operand", "type": "OperandType" }
    ],
    "outputs": [
      { "name": "result", "type": "ResultType" }
    ],
    "assemblyFormat": "$operand attr-dict `:` custom<SameOperandsAndResultType>(type($operand), type($result))"
  },
  {
    "name": "stablehlo.cholesky",
    "summary": "Cholesky operation",
    "description": "Computes the Cholesky decomposition of a batch of matrices.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#cholesky\n\n    Example:\n    ```mlir\n    %result = stablehlo.cholesky %a, lower = true : tensor<3x3xf64>\n    ```",
    "inputs": [
      { "name": "a", "type": "HLO_FpComplexOrQuantizedIntTensor" }
    ],
    "outputs": [
      { "name": "result", "type": "HLO_FpComplexOrQuantizedIntTensor" }
    ],
    "attributes": [
      { "name": "lower", "type": "DefaultValuedOptionalAttr" }
    ],
    "assemblyFormat": "$a (`,` `lower` `=` $lower^)? attr-dict `:` custom<SameOperandsAndResultType>(type($a), type($result))"
  },
  {
    "name": "stablehlo.clamp",
    "summary": "Clamp operation",
    "description": "Clamps every element of the `operand` tensor between a minimum and maximum\n    value and produces a `result` tensor.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#clamp\n\n    Example:\n    ```mlir\n    %result = stablehlo.clamp %min, %operand, %max : tensor<3xi32>\n    ```",
    "inputs": [
      { "name": "min", "type": "HLO_Tensor" },
      { "name": "operand", "type": "HLO_Tensor" },
      { "name": "max", "type": "HLO_Tensor" }
    ],
    "outputs": [
      { "name": "result", "type": "HLO_Tensor" }
    ],
    "assemblyFormat": "$min `,` $operand `,` $max attr-dict\n      `:` custom<SameOperandsAndResultType>(type($min), type($operand), type($max), type($result))"
  },
  {
    "name": "stablehlo.collective_broadcast",
    "summary": "CollectiveBroadcast operation",
    "description": "Within each process group in the process grid, send the value of the\n    `operand` tensor from the source process to the target processes and produce a\n    `result` tensor.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#collective_broadcast\n\n    Example:\n    ```mlir\n    %result = \"stablehlo.collective_broadcast\"(%operand) {\n      replica_groups = dense<[[0, 1]]> : tensor<1x2xi64>,\n      channel_handle = #stablehlo.channel_handle<handle = 0, type = 0>\n    } : (tensor<1x2xi64>) -> tensor<1x2xi64>\n    ```",
    "inputs": [
      { "name": "operand", "type": "HLO_Tensor" }
    ],
    "attributes": [
      { "name": "replica_groups", "type": "I64ElementsAttr" },
      { "name": "channel_handle", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "stablehlo.collective_permute",
    "summary": "CollectivePermute operation",
    "description": "Within each process group in the process grid, sends the value of the\n    `operand` tensor from the source process to the target process and produces\n    a `result` tensor.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#collective_permute\n\n    Example:\n    ```mlir\n    %result = \"stablehlo.collective_permute\"(%operand) {\n      source_target_pairs = dense<[[0, 1], [1, 2]]> : tensor<2x2xi64>,\n      channel_handle = #stablehlo.channel_handle<handle = 0, type = 0>\n    } : (tensor<2x2xi64>) -> tensor<2x2xi64>\n    ```",
    "inputs": [
      { "name": "operand", "type": "HLO_Tensor" }
    ],
    "attributes": [
      { "name": "source_target_pairs", "type": "I64ElementsAttr" },
      { "name": "channel_handle", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "stablehlo.compare",
    "summary": "Compare operation",
    "description": "Performs element-wise comparison of `lhs` and `rhs` tensors according to\n    `comparison_direction` and `compare_type`, and produces a `result` tensor.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#compare\n\n    Example:\n    ```mlir\n    %result = stablehlo.compare LT, %lhs, %rhs, FLOAT : (tensor<2xf32>, tensor<2xf32>) -> tensor<2xi1>\n    ```",
    "inputs": [
      { "name": "lhs", "type": "HLO_Tensor" },
      { "name": "rhs", "type": "HLO_Tensor" }
    ],
    "attributes": [
      { "name": "comparison_direction", "type": "StableHLO_ComparisonDirectionAttr" },
      { "name": "compare_type", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "$comparison_direction `,` $lhs `,` $rhs (`,` $compare_type^)?\n      attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "stablehlo.complex",
    "summary": "Complex operation",
    "description": "Performs element-wise conversion to a complex value from a pair of real and\n    imaginary values, `lhs` and `rhs`, and produces a `result` tensor.\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#complex\n    Example:\n    ```mlir\n    %result = stablehlo.complex %lhs, %rhs : tensor<2xcomplex<f64>>\n    ```",
    "inputs": [
      { "name": "lhs", "type": "HLO_Fp32Or64Tensor" },
      { "name": "rhs", "type": "HLO_Fp32Or64Tensor" }
    ],
    "outputs": [
      { "name": "result", "type": "HLO_ComplexTensor" }
    ],
    "assemblyFormat": "operands attr-dict\n      `:` custom<ComplexOpType>(type($lhs), type($rhs), type($result))"
  },
  {
    "name": "stablehlo.composite",
    "summary": "Composite operation",
    "description": "Encapsulates an operation made up (composed) of other StableHLO operations,\n    taking `inputs` and `composite_attributes` and producing `results`. The\n    semantics of the op are implemented by the `decomposition` attribute. The\n    `composite` op can be replaced with its decomposition without changing program\n    semantics. In cases where inlining the decomposition does not provide the same\n    op semantics, prefer using `custom_call`.\n\n    The `version` field (defaults to `0`) is used to denote when a composite's\n    semantics change.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#composite\n\n    Example:\n    ```mlir\n    %results = stablehlo.composite \"my.op\" %input0, %input1 {\n      composite_attributes = {\n        my_attribute = \"my_value\"\n      },\n      decomposition = @my_op,\n      version = 1 : i32\n    } : (tensor<f32>, tensor<f32>) -> tensor<f32>\n    ```",
    "inputs": [
      { "name": "inputs", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "name", "type": "StrAttr" },
      { "name": "composite_attributes", "type": "DefaultValuedOptionalAttr" },
      { "name": "decomposition", "type": "FlatSymbolRefAttr" },
      { "name": "version", "type": "DefaultValuedOptionalAttr" }
    ],
    "assemblyFormat": "$name $inputs attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "stablehlo.concatenate",
    "summary": "Concatenate operation",
    "description": "Concatenates a variadic number of tensors in `inputs` along `dimension`\n    dimension in the same order as the given arguments and produces a `result`\n    tensor.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#concatenate\n\n    Example:\n    ```mlir\n    %result = stablehlo.concatenate %input0, %input1, dim = 0 : (tensor<3x2xi64>, tensor<1x2xi64>) -> tensor<4x2xi64>\n    ```",
    "inputs": [
      { "name": "inputs", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "dimension", "type": "ConfinedAttr" }
    ],
    "assemblyFormat": "custom<VariadicOperandWithAttribute>($inputs) `dim` `=` $dimension attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "stablehlo.constant",
    "summary": "Constant operation",
    "description": "Produces an `output` tensor from a constant `value`.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#constant\n\n    Example:\n    ```mlir\n    %output = stablehlo.constant dense<[[0.0, 1.0], [2.0, 3.0]]> : tensor<2x2xf32>\n    ```",
    "outputs": [
      { "name": "output", "type": "HLO_StaticShapeTensorOrPerAxisQuantizedTensor" }
    ],
    "attributes": [
      { "name": "value", "type": "ElementsAttr" }
    ]
  },
  {
    "name": "stablehlo.convert",
    "summary": "Convert operation",
    "description": "Performs an element-wise conversion from one element type to another on\n    `operand` tensor and produces a `result` tensor.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#convert\n\n    Example:\n    ```mlir\n    %result = stablehlo.convert %operand : (tensor<3xi64>) -> tensor<3xcomplex<f64>>\n    ```",
    "inputs": [
      { "name": "operand", "type": "OperandType" }
    ],
    "outputs": [
      { "name": "result", "type": "ResultType" }
    ],
    "assemblyFormat": "$operand attr-dict `:` custom<SameOperandsAndResultType>(type($operand), type($result))"
  },
  {
    "name": "stablehlo.convolution",
    "summary": "Convolution operation",
    "description": "Computes dot products between windows of `lhs` and slices of `rhs` and\n    produces `result`.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#convolution\n\n    Example:\n    ```mlir\n    %result = stablehlo.convolution(%lhs, %rhs)\n      dim_numbers = [b, 0, 1, f]x[0, 1, i, o]->[b, 0, 1, f],\n      window = {\n        stride = [4, 4],\n        pad = [[0, 0], [0, 0]],\n        lhs_dilate = [2, 2],\n        rhs_dilate = [1, 1],\n        reverse = [0, 0]\n      } {\n        feature_group_count = 1 : i64,\n        batch_group_count = 1 : i64,\n        precision_config = [#stablehlo<precision DEFAULT>, #stablehlo<precision DEFAULT>]\n      } :\n    (tensor<1x4x4x1xi64>, tensor<3x3x1x1xi64>) -> tensor<1x2x2x1xi64>\n    ```",
    "inputs": [
      { "name": "lhs", "type": "HLO_Tensor" },
      { "name": "rhs", "type": "HLO_TensorOrPerAxisQuantizedTensor" },
      { "name": "dimension_numbers", "type": "StableHLO_ConvDimensionNumbers" }
    ],
    "attributes": [
      { "name": "window_strides", "type": "OptionalAttr" },
      { "name": "padding", "type": "OptionalAttr" },
      { "name": "lhs_dilation", "type": "OptionalAttr" },
      { "name": "rhs_dilation", "type": "OptionalAttr" },
      { "name": "window_reversal", "type": "OptionalAttr" },
      { "name": "feature_group_count", "type": "ConfinedAttr" },
      { "name": "batch_group_count", "type": "ConfinedAttr" },
      { "name": "precision_config", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "`(`operands`)`\n       `dim_numbers` `=` custom<ConvolutionDimensions>($dimension_numbers) `,`\n       `window` `=` `{` custom<WindowAttributes>($window_strides, $padding,\n                                                 $lhs_dilation, $rhs_dilation,\n                                                 $window_reversal) `}`\n       attr-dict `:` functional-type(operands, results)",
    "category": "Layer"
  },
  {
    "name": "stablehlo.cosine",
    "summary": "Cosine operation",
    "description": "Performs element-wise cosine operation on `operand` tensor and produces a\n    `result` tensor.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#cosine\n\n    Example:\n    ```mlir\n    %result = stablehlo.cosine %operand : tensor<2xf32>\n    ```",
    "inputs": [
      { "name": "operand", "type": "HLO_FpComplexOrQuantizedIntTensor" }
    ],
    "outputs": [
      { "name": "result", "type": "HLO_FpComplexOrQuantizedIntTensor" }
    ],
    "attributes": [
      { "name": "result_accuracy", "type": "DefaultValuedOptionalAttr" }
    ],
    "assemblyFormat": "$operand attr-dict `:` custom<SameOperandsAndResultType>(type($operand), type($result))"
  },
  {
    "name": "stablehlo.count_leading_zeros",
    "summary": "Clz operation",
    "description": "Performs element-wise count of the number of leading zero bits in the\n    `operand` tensor and produces a `result` tensor.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#count_leading_zeros\n\n    Example:\n    ```mlir\n    %result = stablehlo.count_leading_zeros %operand : tensor<2x2xi64>\n    ```",
    "inputs": [
      { "name": "operand", "type": "OperandType" }
    ],
    "outputs": [
      { "name": "result", "type": "ResultType" }
    ],
    "assemblyFormat": "$operand attr-dict `:` custom<SameOperandsAndResultType>(type($operand), type($result))"
  },
  {
    "name": "stablehlo.create_token",
    "summary": "CreateToken operation",
    "description": "This operation is on its way out of StableHLO, so it is not included in\n    the StableHLO specification: https://github.com/openxla/stablehlo/issues/3.\n\n    Informally, this operation does the same thing as AfterAllOp with 0 inputs:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#after_all\n\n    Example:\n    ```mlir\n    %output = stablehlo.create_token : !stablehlo.token\n    ```",
    "outputs": [
      { "name": "output", "type": "HLO_Token" }
    ],
    "assemblyFormat": "attr-dict `:` type(results)"
  },
  {
    "name": "stablehlo.cross-replica-sum",
    "summary": "CrossReplicaSum operation",
    "description": "This operation is on its way out of StableHLO, so it is not included in\n    the StableHLO specification: https://github.com/openxla/stablehlo/issues/3.\n\n    Informally, this operation does the same thing as AllReduceOp with\n    `channel_id = 0`, `use_global_device_ids = false` and `computation`\n    implementing addition:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#all_reduce\n\n    Example:\n    ```mlir\n    %result = \"stablehlo.cross-replica-sum\"(%operand) {\n      replica_groups = dense<[[0, 1]]> : tensor<1x2xi64>\n    } : (tensor<4xf32>) -> tensor<4xf32>\n    ```",
    "inputs": [
      { "name": "operand", "type": "HLO_Tensor" }
    ],
    "attributes": [
      { "name": "replica_groups", "type": "I64ElementsAttr" }
    ]
  },
  {
    "name": "stablehlo.custom_call",
    "summary": "CustomCall operation",
    "description": "Encapsulates an implementation-defined operation `call_target_name` that\n    takes `inputs` and `called_computations` and produces `results`.\n\n    Depending on the API version there are two ways to pass extra bits of static\n    information to the external function:\n    1. Use `API_VERSION_TYPED_FFI` which allows passing a dictionary attribute.\n    2. Use a previous API version with a StringAttr to encode backend config.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#custom_call\n\n    Example:\n    ```mlir\n    %results = stablehlo.custom_call @foo(%input0) {\n      backend_config = {bar = 42 : i32},\n      api_version = 4 : i32,\n      called_computations = [@foo]\n    } : (tensor<f64>) -> tensor<f64>\n    ```",
    "inputs": [
      { "name": "inputs", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "call_target_name", "type": "StrAttr" },
      { "name": "has_side_effect", "type": "DefaultValuedOptionalAttr" },
      { "name": "backend_config", "type": "OptionalAttr" },
      { "name": "api_version", "type": "DefaultValuedOptionalAttr" },
      { "name": "called_computations", "type": "DefaultValuedOptionalAttr" },
      { "name": "operand_layouts", "type": "OptionalAttr" },
      { "name": "result_layouts", "type": "OptionalAttr" },
      { "name": "output_operand_aliases", "type": "DefaultValuedOptionalAttr" }
    ],
    "assemblyFormat": "custom<CustomCallTarget>($call_target_name) `(` $inputs `)`\n      attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "stablehlo.divide",
    "summary": "Div operation",
    "description": "Performs element-wise division of dividend `lhs` and divisor `rhs` tensors\n    and produces a `result` tensor.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#divide\n\n    Example:\n    ```mlir\n    %result = stablehlo.divide %lhs, %rhs : tensor<4xf32>\n    ```",
    "inputs": [
      { "name": "lhs", "type": "OperandType" },
      { "name": "rhs", "type": "OperandType" }
    ],
    "outputs": [
      { "name": "result", "type": "ResultType" }
    ],
    "assemblyFormat": "$lhs `,` $rhs attr-dict\n      `:` custom<SameOperandsAndResultType>(type($lhs), type($rhs), type($result))"
  },
  {
    "name": "stablehlo.dot",
    "summary": "Dot operation",
    "description": "This operation is on its way out of StableHLO, so it is not included in\n    the StableHLO specification: https://github.com/openxla/stablehlo/issues/3.\n\n    Informally, this operation does the same thing as XLA's Dot:\n    https://www.tensorflow.org/xla/operation_semantics#dot\n\n    Example:\n    ```mlir\n    %0 = stablehlo.dot %arg0, %arg1 : (tensor<1x2xi32>, tensor<2x1xi32>) -> tensor<1x1xi32>\n    ```",
    "inputs": [
      { "name": "lhs", "type": "HLO_Tensor" },
      { "name": "rhs", "type": "HLO_TensorOrPerAxisQuantizedTensor" }
    ],
    "attributes": [
      { "name": "precision_config", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "$lhs `,` $rhs `` custom<PrecisionConfig>($precision_config) attr-dict\n      `:` functional-type(operands, results)"
  },
  {
    "name": "stablehlo.dot_general",
    "summary": "DotGeneral operation",
    "description": "Computes dot products between slices of `lhs` and slices of `rhs` and\n    produces a `result` tensor.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#dot_general\n\n    Example:\n    ```mlir\n    %result = stablehlo.dot_general %lhs, %rhs,\n      batching_dims = [0] x [0],\n      contracting_dims = [2] x [1],\n      precision = [DEFAULT, DEFAULT],\n      algorithm = <lhs_precision_type = tf32, rhs_precision_type = tf32, accumulation_type = f32, lhs_component_count = 1, rhs_component_count = 1, num_primitive_operations = 1, allow_imprecise_accumulation = false>\n      : (tensor<2x2x2xi64>, tensor<2x2x2xi64>) -> tensor<2x2x2xi64>\n    ```",
    "inputs": [
      { "name": "lhs", "type": "HLO_Tensor" },
      { "name": "rhs", "type": "HLO_TensorOrPerAxisQuantizedTensor" },
      { "name": "dot_dimension_numbers", "type": "StableHLO_DotDimensionNumbers" }
    ],
    "attributes": [
      { "name": "precision_config", "type": "OptionalAttr" },
      { "name": "algorithm", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "$lhs `,` $rhs `,` custom<DotDimensionNumbers>($dot_dimension_numbers) ``\n    custom<PrecisionConfigAndAlgorithm>($precision_config, $algorithm) attr-dict\n      `:` functional-type(operands, results)"
  },
  {
    "name": "stablehlo.dynamic_broadcast_in_dim",
    "summary": "DynamicBroadcastInDim operation",
    "description": "This operation is functionally identical to\n    [broadcast_in_dim](https://github.com/openxla/stablehlo/blob/main/docs/spec.md#broadcast_in_dim)\n    op, but the result shape is specified dynamically via `output_dimensions`.\n\n    It also accepts optional attributes to express static knowledge about the\n    expanding behavior of dimensions. If not specified, all dimensions are\n    assumed to be possibly expanding. The sets of dimensions that are known to\n    be expanding and the set of dimensions that are known to be non-expanding\n    must be disjoint and they must be a subset of the operand's dimensions.\n\n    See: https://github.com/openxla/stablehlo/blob/main/docs/spec.md#dynamic_broadcast_in_dim\n\n    Example:\n    ```mlir\n    %operand = stablehlo.constant dense<[[1, 2, 3]]> : tensor<1x3xi64>\n    %output_dimensions = stablehlo.constant dense<[2, 3, 2]> : tensor<3xi64>\n    %result = \"stablehlo.dynamic_broadcast_in_dim\"(%operand, %output_dimensions) {\n      broadcast_dimensions = array<i64: 2, 1>,\n      known_expanding_dimensions = array<i64: 0>,\n      known_nonexpanding_dimensions = array<i64: 1>\n    } : (tensor<1x3xi64>, tensor<3xi64>) -> tensor<2x3x2xi64>\n    ```",
    "inputs": [
      { "name": "operand", "type": "HLO_TensorOrPerAxisQuantizedTensor" },
      { "name": "output_dimensions", "type": "HLO_StaticDimensionTensor" }
    ],
    "attributes": [
      { "name": "broadcast_dimensions", "type": "DenseI64ArrayAttr" },
      { "name": "known_expanding_dimensions", "type": "OptionalAttr" },
      { "name": "known_nonexpanding_dimensions", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "$operand `,` $output_dimensions `,` `dims` `=` $broadcast_dimensions\n      attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "stablehlo.dynamic_conv",
    "summary": "DynamicConv operation",
    "description": "This operation is functionally identical to\n    [convolution](https://github.com/openxla/stablehlo/blob/main/docs/spec.md#convolution)\n    op, but the padding is specified dynamically via `padding`.\n\n    Example:\n    ```mlir\n    %padding = stablehlo.constant dense<2> : tensor<2x2xi64>\n    %result = \"stablehlo.dynamic_conv\"(%lhs, %rhs, %padding) {\n      window_strides = array<i64: 4, 4>,\n      lhs_dilation = array<i64: 2, 2>,\n      rhs_dilation = array<i64: 1, 1>,\n      window_reversal = array<i1: false, false>,\n      dimension_numbers = #stablehlo.conv<[b, 0, 1, f]x[0, 1, i, o]->[b, 0, 1, f]>,\n      batch_group_count = 1 : i64,\n      feature_group_count = 1 : i64,\n      precision_config = [#stablehlo<precision DEFAULT>, #stablehlo<precision DEFAULT>]\n    } : (tensor<1x4x4x1xi64>, tensor<3x3x1x1xi64>, tensor<2x2xi64>) -> tensor<1x2x2x1xi64>\n    ```",
    "inputs": [
      { "name": "lhs", "type": "HLO_Tensor" },
      { "name": "rhs", "type": "HLO_Tensor" },
      { "name": "padding", "type": "HLO_Static2DIntTensor" },
      { "name": "dimension_numbers", "type": "StableHLO_ConvDimensionNumbers" }
    ],
    "attributes": [
      { "name": "window_strides", "type": "OptionalAttr" },
      { "name": "lhs_dilation", "type": "OptionalAttr" },
      { "name": "rhs_dilation", "type": "OptionalAttr" },
      { "name": "window_reversal", "type": "OptionalAttr" },
      { "name": "feature_group_count", "type": "ConfinedAttr" },
      { "name": "batch_group_count", "type": "ConfinedAttr" },
      { "name": "precision_config", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "stablehlo.dynamic_gather",
    "summary": "DynamicGather operation",
    "description": "This operation is functionally identical to\n    [gather](https://github.com/openxla/stablehlo/blob/main/docs/spec.md#gather)\n    op, with the `slice_sizes` specified dynamically as an operand.\n\n    Example:\n    ```mlir\n    %slice_sizes = stablehlo.constant dense<[1, 2, 2]> : tensor<3xi64>\n    %result = \"stablehlo.dynamic_gather\"(%operand, %start_indices, %slice_sizes) {\n      dimension_numbers = #stablehlo.gather<\n        offset_dims = [2, 3],\n        collapsed_slice_dims = [0],\n        start_index_map = [0, 2],\n        index_vector_dim = 2>,\n      indices_are_sorted = false\n    } : (tensor<3x4x2xi64>, tensor<2x3x2xi64>, tensor<3xi64>) -> tensor<2x3x2x2xi64>\n    ```",
    "inputs": [
      { "name": "operand", "type": "HLO_Tensor" },
      { "name": "start_indices", "type": "HLO_IntTensor" },
      { "name": "slice_sizes", "type": "HLO_Static1DIntTensor" },
      { "name": "dimension_numbers", "type": "StableHLO_GatherDimensionNumbers" }
    ],
    "attributes": [
      { "name": "indices_are_sorted", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "stablehlo.dynamic_iota",
    "summary": "DynamicIota operation",
    "description": "This operation is functionally identical to\n    [iota](https://github.com/openxla/stablehlo/blob/main/docs/spec.md#iota)\n    op, but the result shape is specified dynamically via `output_shape`.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#dynamic_iota\n\n    Example:\n    ```mlir\n    %output_shape = stablehlo.constant dense<[4, 5]> : tensor<2xi64>\n    %0 = stablehlo.dynamic_iota %output_shape, dim = 0 : (tensor<2xi64>) -> tensor<4x5xi64>\n    ```",
    "inputs": [
      { "name": "output_shape", "type": "HLO_StaticDimensionTensor" }
    ],
    "outputs": [
      { "name": "result", "type": "HLO_Tensor" }
    ],
    "attributes": [
      { "name": "iota_dimension", "type": "ConfinedAttr" }
    ],
    "assemblyFormat": "$output_shape `,` `dim` `=` $iota_dimension attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "stablehlo.dynamic_pad",
    "summary": "DynamicPad operation",
    "description": "This operation is functionally identical to\n    [pad](https://github.com/openxla/stablehlo/blob/main/docs/spec.md#pad)\n    https://github.com/openxla/stablehlo/pull/2306#discussion_r1595669709\n    op, but with `edge_padding_low`, `edge_padding_high` and `interior_padding`\n    specified dynamically as values.\n\n    See: https://github.com/openxla/stablehlo/blob/main/docs/spec.md#dynamic_pad\n\n    Example:\n    ```mlir\n    %edge_padding_low = stablehlo.constant dense<[0, 1]> : tensor<2xi32>\n    %edge_padding_high = stablehlo.constant dense<[2, 1]> : tensor<2xi32>\n    %interior_padding = stablehlo.constant dense<[1, 2]> : tensor<2xi32>\n    %result = stablehlo.dynamic_pad %operand, %padding_value,\n                %edge_padding_low, %edge_padding_high, %interior_padding\n                : (tensor<2x3xi64>, tensor<i64>, tensor<2xi64>, tensor<2xi64>, tensor<2xi64>) -> tensor<5x9xi64>\n    ```",
    "inputs": [
      { "name": "operand", "type": "HLO_Tensor" },
      { "name": "padding_value", "type": "HLO_ScalarTensor" },
      { "name": "edge_padding_low", "type": "HLO_StaticDimensionTensor" },
      { "name": "edge_padding_high", "type": "HLO_StaticDimensionTensor" },
      { "name": "interior_padding", "type": "HLO_StaticDimensionTensor" }
    ],
    "outputs": [
      { "name": "result", "type": "HLO_Tensor" }
    ],
    "assemblyFormat": "operands attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "stablehlo.dynamic_reshape",
    "summary": "DynamicReshape operation",
    "description": "This operation is functionally identical to\n    [reshape](https://github.com/openxla/stablehlo/blob/main/docs/spec.md#reshape)\n    op, but the result shape is specified dynamically via `output_shape`.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#dynamic_reshape\n\n    Example:\n    ```mlir\n    %output_shape = stablehlo.constant dense<[3, 2]> : tensor<2xi64>\n    %result = stablehlo.dynamic_reshape %operand, %output_shape : (tensor<2x3xi64>, tensor<2xi64>) -> tensor<3x2xi64>\n    ```",
    "inputs": [
      { "name": "operand", "type": "HLO_TensorOrPerAxisQuantizedTensor" },
      { "name": "output_shape", "type": "HLO_StaticDimensionTensor" }
    ],
    "outputs": [
      { "name": "result", "type": "HLO_TensorOrPerAxisQuantizedTensor" }
    ],
    "assemblyFormat": "operands attr-dict `:` functional-type(operands, results)",
    "category": "Shape"
  },
  {
    "name": "stablehlo.dynamic_slice",
    "summary": "DynamicSlice operation",
    "description": "Extracts a slice from the `operand` using dynamically-computed starting\n    indices and produces a `result` tensor.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#dynamic_slice\n\n    Example:\n    ```mlir\n    %result = stablehlo.dynamic_slice %operand, %start_indices0, %start_indices1, sizes = [2, 2]\n      : (tensor<4x4xi32>, tensor<i64>, tensor<i64>) -> tensor<2x2xi32>\n    ```",
    "inputs": [
      { "name": "operand", "type": "HLO_Tensor" },
      { "name": "start_indices", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "result", "type": "HLO_Tensor" }
    ],
    "attributes": [
      { "name": "slice_sizes", "type": "DenseI64ArrayAttr" }
    ],
    "assemblyFormat": "$operand `,` custom<VariadicOperandWithAttribute>($start_indices)\n      `sizes` `=` $slice_sizes attr-dict `:` functional-type(operands, results)",
    "category": "Tensor"
  },
  {
    "name": "stablehlo.dynamic_update_slice",
    "summary": "DynamicUpdateSlice operation",
    "description": "Produces a `result` tensor which is equal to the `operand` tensor except\n    that the slice starting at `start_indices` is updated with the values in\n    `update`.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#dynamic_update_slice\n\n    Example:\n    ```mlir\n    %result = stablehlo.dynamic_update_slice %operand, %update, %start_indices0, %start_indices1\n      : (tensor<4x4xi32>, tensor<2x2xi32>, tensor<i64>, tensor<i64>) -> tensor<4x4xi32>\n    ```",
    "inputs": [
      { "name": "operand", "type": "HLO_Tensor" },
      { "name": "update", "type": "HLO_Tensor" },
      { "name": "start_indices", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "result", "type": "HLO_Tensor" }
    ],
    "assemblyFormat": "operands attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "stablehlo.einsum",
    "summary": "Einsum operation",
    "description": "This operation is on its way out of StableHLO, so it is not included in\n    the StableHLO specification: https://github.com/openxla/stablehlo/issues/3.\n\n    Informally, this operation does the same thing as TF's einsum:\n    https://www.tensorflow.org/api_docs/python/tf/einsum\n\n    Example:\n    ```mlir\n    %result = \"stablehlo.einsum\"(%lhs, %rhs) {\n      einsum_config = \"ab,bc->ac\"\n    } : (tensor<4x16xf32>, tensor<16x4xf32>) -> tensor<4x4xf32>\n    ```",
    "inputs": [
      { "name": "lhs", "type": "HLO_Tensor" },
      { "name": "rhs", "type": "HLO_Tensor" }
    ],
    "attributes": [
      { "name": "einsum_config", "type": "StrAttr" }
    ],
    "assemblyFormat": "$lhs `,` $rhs `,` `config` `=` $einsum_config attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "stablehlo.exponential",
    "summary": "Exp operation",
    "description": "Performs element-wise exponential operation on `operand` tensor and produces\n    a `result` tensor.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#exponential\n\n    Example:\n    ```mlir\n    %result = stablehlo.exponential %operand : tensor<2x2xf64>\n    ```",
    "inputs": [
      { "name": "operand", "type": "HLO_FpComplexOrQuantizedIntTensor" }
    ],
    "outputs": [
      { "name": "result", "type": "HLO_FpComplexOrQuantizedIntTensor" }
    ],
    "attributes": [
      { "name": "result_accuracy", "type": "DefaultValuedOptionalAttr" }
    ],
    "assemblyFormat": "$operand attr-dict `:` custom<SameOperandsAndResultType>(type($operand), type($result))"
  },
  {
    "name": "stablehlo.exponential_minus_one",
    "summary": "Expm1 operation",
    "description": "Performs element-wise exponential minus one operation on `operand` tensor\n    and produces a `result` tensor.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#exponential_minus_one\n\n    Example:\n    ```mlir\n    %result = stablehlo.exponential_minus_one %operand : tensor<2xf64>\n    ```",
    "inputs": [
      { "name": "operand", "type": "HLO_FpComplexOrQuantizedIntTensor" }
    ],
    "outputs": [
      { "name": "result", "type": "HLO_FpComplexOrQuantizedIntTensor" }
    ],
    "attributes": [
      { "name": "result_accuracy", "type": "DefaultValuedOptionalAttr" }
    ],
    "assemblyFormat": "$operand attr-dict `:` custom<SameOperandsAndResultType>(type($operand), type($result))"
  },
  {
    "name": "stablehlo.fft",
    "summary": "Fft operation",
    "description": "Performs the forward and inverse Fourier transforms for real and complex\n    inputs/outputs.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#fft\n\n    Example:\n    ```mlir\n    %result = stablehlo.fft %operand, type = FFT, length = [4] : (tensor<4xcomplex<f32>>) -> tensor<4xcomplex<f32>>\n    ```",
    "inputs": [
      { "name": "operand", "type": "HLO_FpOrComplexTensor" }
    ],
    "attributes": [
      { "name": "fft_type", "type": "StableHLO_FftTypeAttr" },
      { "name": "fft_length", "type": "DenseI64ArrayAttr" }
    ],
    "assemblyFormat": "$operand `,` `type` `=` $fft_type `,` `length` `=` $fft_length\n      attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "stablehlo.floor",
    "summary": "Floor operation",
    "description": "Performs element-wise floor of `operand` tensor and produces a `result`\n    tensor.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#floor\n\n    Example:\n    ```mlir\n    %result = stablehlo.floor %operand : tensor<2xf32>\n    ```",
    "inputs": [
      { "name": "operand", "type": "OperandType" }
    ],
    "outputs": [
      { "name": "result", "type": "ResultType" }
    ],
    "assemblyFormat": "$operand attr-dict `:` custom<SameOperandsAndResultType>(type($operand), type($result))"
  },
  {
    "name": "stablehlo.gather",
    "summary": "Gather operation",
    "description": "Gathers slices from `operand` tensor from offsets specified in\n    `start_indices` and produces a `result` tensor.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#gather\n\n    Example:\n    ```mlir\n    %result = \"stablehlo.gather\"(%operand, %start_indices) {\n      dimension_numbers = #stablehlo.gather<\n        offset_dims = [3, 4],\n        collapsed_slice_dims = [1],\n        operand_batching_dims = [0],\n        start_indices_batching_dims = [1],\n        start_index_map = [2, 1],\n        index_vector_dim = 3>,\n      slice_sizes = array<i64: 1, 1, 2, 2>,\n      indices_are_sorted = false\n    } : (tensor<2x3x4x2xi64>, tensor<2x2x3x2xi64>) -> tensor<2x2x3x2x2xi64>\n    ```",
    "inputs": [
      { "name": "operand", "type": "HLO_Tensor" },
      { "name": "start_indices", "type": "HLO_IntTensor" },
      { "name": "dimension_numbers", "type": "StableHLO_GatherDimensionNumbers" }
    ],
    "outputs": [
      { "name": "result", "type": "HLO_Tensor" }
    ],
    "attributes": [
      { "name": "slice_sizes", "type": "DenseI64ArrayAttr" },
      { "name": "indices_are_sorted", "type": "DefaultValuedOptionalAttr" }
    ],
    "category": "Tensor"
  },
  {
    "name": "stablehlo.get_dimension_size",
    "summary": "GetDimensionSize operation",
    "description": "Produces the size of the given `dimension` of the `operand`.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#get_dimension_size\n\n    Example:\n    ```mlir\n    %result = stablehlo.get_dimension_size %operand, dim = 1 : (tensor<2x3xi64>) -> tensor<i32>\n    ```",
    "inputs": [
      { "name": "operand", "type": "HLO_TensorOrPerAxisQuantizedTensor" }
    ],
    "attributes": [
      { "name": "dimension", "type": "ConfinedAttr" }
    ],
    "assemblyFormat": "$operand `,` `dim` `=` $dimension attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "stablehlo.get_tuple_element",
    "summary": "GetTupleElement operation",
    "description": "Extracts element at `index` position of the `operand` tuple and produces a\n    `result`.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#get_tuple_element\n\n    Example:\n    ```mlir\n    %result = stablehlo.get_tuple_element %operand[0] : (tuple<tensor<2xf64>, tuple<tensor<i64>>>) -> tensor<2xf64>\n    ```",
    "inputs": [
      { "name": "operand", "type": "HLO_Tuple" }
    ],
    "attributes": [
      { "name": "index", "type": "ConfinedAttr" }
    ],
    "assemblyFormat": "$operand `[` $index `]` attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "stablehlo.if",
    "summary": "If operation",
    "description": "Produces the output from executing exactly one branch from `true_branch` or\n    `false_branch` depending on the value of `pred`.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#if\n\n    Example:\n    %result = \"stablehlo.if\"(%pred) ({\n      \"stablehlo.return\"(%result_true_branch) : (tensor<i32>) -> ()\n    }, {\n      \"stablehlo.return\"(%result_false_branch) : (tensor<i32>) -> ()\n    }) : (tensor<i1>) -> tensor<i32>",
    "inputs": [
      { "name": "pred", "type": "HLO_PredTensor" }
    ]
  },
  {
    "name": "stablehlo.imag",
    "summary": "Imag operation",
    "description": "Extracts the imaginary part, element-wise, from the `operand` and produces a\n    `result` tensor.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#imag\n\n    Example:\n    ```mlir\n    %result = stablehlo.imag %operand : (tensor<2xcomplex<f32>>) -> tensor<2xf32>\n    ```",
    "inputs": [
      { "name": "operand", "type": "OperandType" }
    ],
    "outputs": [
      { "name": "result", "type": "ResultType" }
    ],
    "assemblyFormat": "$operand attr-dict `:` custom<SameOperandsAndResultType>(type($operand), type($result))"
  },
  {
    "name": "stablehlo.infeed",
    "summary": "Infeed operation",
    "description": "Reads data from the infeed and produces `results`.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#infeed\n\n    Example:\n    ```mlir\n    %results0:2 = \"stablehlo.infeed\"(%token) :\n        (!stablehlo.token) -> (tensor<2x2xi64>, !stablehlo.token)\n    ```",
    "inputs": [
      { "name": "token", "type": "HLO_Token" }
    ],
    "attributes": [
      { "name": "infeed_config", "type": "DefaultValuedStrAttr" },
      { "name": "layout", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "stablehlo.iota",
    "summary": "Iota operation",
    "description": "Fills an `output` tensor with values in increasing order starting from zero\n    along the `iota_dimension` dimension.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#iota\n\n    Example:\n    ```mlir\n    %output = stablehlo.iota dim = 0 : tensor<4x5xi32>\n    ```",
    "outputs": [
      { "name": "output", "type": "HLO_StaticShapeIntFpComplexOrQuantizedTensor" }
    ],
    "attributes": [
      { "name": "iota_dimension", "type": "ConfinedAttr" }
    ],
    "assemblyFormat": "`dim` `=` $iota_dimension attr-dict `:` type($output)"
  },
  {
    "name": "stablehlo.is_finite",
    "summary": "IsFinite operation",
    "description": "Performs element-wise check whether the value in `x` is finite (i.e. is\n    neither +Inf, -Inf, nor NaN) and produces a `y` tensor.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#is_finite\n\n    Example:\n    ```mlir\n    %y = stablehlo.is_finite %x : (tensor<7xf64>) -> tensor<7xi1>\n    ```",
    "inputs": [
      { "name": "x", "type": "HLO_FpOrQuantizedIntTensor" }
    ],
    "outputs": [
      { "name": "y", "type": "HLO_PredTensor" }
    ],
    "assemblyFormat": "operands attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "stablehlo.log",
    "summary": "Log operation",
    "description": "Performs element-wise logarithm operation on `operand` tensor and produces a\n    `result` tensor.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#log\n\n    Example:\n    ```mlir\n    %result = stablehlo.log %operand : tensor<2x2xf64>\n    ```",
    "inputs": [
      { "name": "operand", "type": "HLO_FpComplexOrQuantizedIntTensor" }
    ],
    "outputs": [
      { "name": "result", "type": "HLO_FpComplexOrQuantizedIntTensor" }
    ],
    "attributes": [
      { "name": "result_accuracy", "type": "DefaultValuedOptionalAttr" }
    ],
    "assemblyFormat": "$operand attr-dict `:` custom<SameOperandsAndResultType>(type($operand), type($result))"
  },
  {
    "name": "stablehlo.log_plus_one",
    "summary": "Log1p operation",
    "description": "Performs element-wise logarithm plus one operation on `operand` tensor and\n    produces a `result` tensor.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#log_plus_one\n\n    Example:\n    ```mlir\n    %result = stablehlo.log_plus_one %operand : tensor<5xf64>\n    ```",
    "inputs": [
      { "name": "operand", "type": "HLO_FpComplexOrQuantizedIntTensor" }
    ],
    "outputs": [
      { "name": "result", "type": "HLO_FpComplexOrQuantizedIntTensor" }
    ],
    "attributes": [
      { "name": "result_accuracy", "type": "DefaultValuedOptionalAttr" }
    ],
    "assemblyFormat": "$operand attr-dict `:` custom<SameOperandsAndResultType>(type($operand), type($result))"
  },
  {
    "name": "stablehlo.logistic",
    "summary": "Logistic operation",
    "description": "Performs element-wise logistic operation on `operand` tensor and produces a\n    `result` tensor.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#logistic\n\n    Example:\n    ```mlir\n    %result = stablehlo.logistic %operand : tensor<2x2xf64>\n    ```",
    "inputs": [
      { "name": "operand", "type": "HLO_FpComplexOrQuantizedIntTensor" }
    ],
    "outputs": [
      { "name": "result", "type": "HLO_FpComplexOrQuantizedIntTensor" }
    ],
    "attributes": [
      { "name": "result_accuracy", "type": "DefaultValuedOptionalAttr" }
    ],
    "assemblyFormat": "$operand attr-dict `:` custom<SameOperandsAndResultType>(type($operand), type($result))"
  },
  {
    "name": "stablehlo.map",
    "summary": "Map operation",
    "description": "Applies a map function `computation` to `inputs` along the `dimensions` and\n    produces a `result` tensor.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#map\n\n    Example:\n    ```mlir\n    %result = \"stablehlo.map\"(%input0, %input1) ({\n      ^bb0(%arg0: tensor<i64>, %arg1: tensor<i64>):\n        %0 = stablehlo.multiply %arg0, %arg1 : tensor<i64>\n        stablehlo.return %0 : tensor<i64>\n    }) {\n      dimensions = array<i64: 0, 1>\n    } : (tensor<2x2xi64>, tensor<2x2xi64>) -> tensor<2x2xi64>\n    ```",
    "inputs": [
      { "name": "inputs", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "dimensions", "type": "DenseI64ArrayAttr" }
    ]
  },
  {
    "name": "stablehlo.maximum",
    "summary": "Max operation",
    "description": "Performs element-wise max operation on tensors `lhs` and `rhs` and produces\n    a `result` tensor.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#maximum\n\n    Example:\n    ```mlir\n    %result = stablehlo.maximum %lhs, %rhs : tensor<4xf32>\n    ```",
    "inputs": [
      { "name": "lhs", "type": "OperandType" },
      { "name": "rhs", "type": "OperandType" }
    ],
    "outputs": [
      { "name": "result", "type": "ResultType" }
    ],
    "assemblyFormat": "$lhs `,` $rhs attr-dict\n      `:` custom<SameOperandsAndResultType>(type($lhs), type($rhs), type($result))"
  },
  {
    "name": "stablehlo.minimum",
    "summary": "Min operation",
    "description": "Performs element-wise min operation on tensors `lhs` and `rhs` and produces a\n    `result` tensor.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#minimum\n\n    Example:\n    ```mlir\n    %result = stablehlo.minimum %lhs, %rhs : tensor<4xf32>\n    ```",
    "inputs": [
      { "name": "lhs", "type": "OperandType" },
      { "name": "rhs", "type": "OperandType" }
    ],
    "outputs": [
      { "name": "result", "type": "ResultType" }
    ],
    "assemblyFormat": "$lhs `,` $rhs attr-dict\n      `:` custom<SameOperandsAndResultType>(type($lhs), type($rhs), type($result))"
  },
  {
    "name": "stablehlo.multiply",
    "summary": "Mul operation",
    "description": "Performs element-wise product of two tensors `lhs` and `rhs` and produces a\n    `result` tensor.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#multiply\n\n    Example:\n    ```mlir\n    %result = stablehlo.multiply %lhs, %rhs : tensor<2xi32>\n    ```",
    "inputs": [
      { "name": "lhs", "type": "OperandType" },
      { "name": "rhs", "type": "OperandType" }
    ],
    "outputs": [
      { "name": "result", "type": "ResultType" }
    ],
    "assemblyFormat": "$lhs `,` $rhs attr-dict\n      `:` custom<SameOperandsAndResultType>(type($lhs), type($rhs), type($result))"
  },
  {
    "name": "stablehlo.negate",
    "summary": "Neg operation",
    "description": "Performs element-wise negation of `operand` tensor and produces a `result`\n    tensor.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#negate\n\n    Example:\n    ```mlir\n    %result = stablehlo.negate %operand : tensor<2x3xi32>\n    ```",
    "inputs": [
      { "name": "operand", "type": "OperandType" }
    ],
    "outputs": [
      { "name": "result", "type": "ResultType" }
    ],
    "assemblyFormat": "$operand attr-dict `:` custom<SameOperandsAndResultType>(type($operand), type($result))"
  },
  {
    "name": "stablehlo.not",
    "summary": "Not operation",
    "description": "Performs element-wise NOT of tensor `operand` of type integer and produces\n    a `result` tensor.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#not\n\n    Example:\n    ```mlir\n    %result = stablehlo.not %operand : tensor<5x3x1xi1>\n    ```",
    "inputs": [
      { "name": "operand", "type": "OperandType" }
    ],
    "outputs": [
      { "name": "result", "type": "ResultType" }
    ],
    "assemblyFormat": "$operand attr-dict `:` custom<SameOperandsAndResultType>(type($operand), type($result))"
  },
  {
    "name": "stablehlo.optimization_barrier",
    "summary": "OptimizationBarrier operation",
    "description": "Ensures that the operations that produce the `operand` are executed before any\n    operations that depend on the `result` and prevents compiler transformations\n    from moving operations across the barrier. Other than that, the operation is\n    an identity, i.e. `result` = `operand`.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#optimization_barrier\n\n    Example:\n    ```mlir\n    %result0, %result1 = stablehlo.optimization_barrier %operand0, %operand1 : tensor<f32>, tensor<f32>\n    ```",
    "inputs": [
      { "name": "operand", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "result", "type": "Variadic" }
    ],
    "assemblyFormat": "attr-dict ($operand^ `:` custom<PairwiseOpType>(type($operand), type($result))):(`(` `)`)?"
  },
  {
    "name": "stablehlo.or",
    "summary": "Or operation",
    "description": "Performs element-wise OR of two tensors `lhs` and `rhs` and produces a\n    `result` tensor.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#or\n\n    Example:\n    ```mlir\n    %result = stablehlo.or %lhs, %rhs : tensor<2xi1>\n    ```",
    "inputs": [
      { "name": "lhs", "type": "HLO_PredOrIntTensor" },
      { "name": "rhs", "type": "HLO_PredOrIntTensor" }
    ],
    "outputs": [
      { "name": "result", "type": "ResultType" }
    ],
    "assemblyFormat": "$lhs `,` $rhs attr-dict\n      `:` custom<SameOperandsAndResultType>(type($lhs), type($rhs), type($result))"
  },
  {
    "name": "stablehlo.outfeed",
    "summary": "Outfeed operation",
    "description": "Writes `inputs` to the outfeed and produces a `result` token.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#outfeed\n\n    Example:\n    ```mlir\n    %result = \"stablehlo.outfeed\"(%input0, %token) :\n        (tensor<2x2x2xi64>, !stablehlo.token) -> !stablehlo.token\n    ```",
    "inputs": [
      { "name": "inputs", "type": "Variadic" },
      { "name": "token", "type": "HLO_Token" }
    ],
    "attributes": [
      { "name": "outfeed_config", "type": "DefaultValuedStrAttr" }
    ]
  },
  {
    "name": "stablehlo.pad",
    "summary": "Pad operation",
    "description": "Expands `operand` by padding around the tensor as well as between the\n    elements of the tensor with the given `padding_value`.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#pad\n\n    Example:\n    ```mlir\n    %0 = stablehlo.pad %arg0, %arg1, low = [0, 1], high = [2, 1], interior = [1, 2]\n      : (tensor<2x3xi32>, tensor<i32>) -> tensor<5x9xi32>\n    ```",
    "inputs": [
      { "name": "operand", "type": "HLO_Tensor" },
      { "name": "padding_value", "type": "HLO_ScalarTensor" }
    ],
    "attributes": [
      { "name": "edge_padding_low", "type": "DenseI64ArrayAttr" },
      { "name": "edge_padding_high", "type": "DenseI64ArrayAttr" },
      { "name": "interior_padding", "type": "DenseI64ArrayAttr" }
    ],
    "assemblyFormat": "$operand `,` $padding_value `,`\n      `low` `=` $edge_padding_low `,`\n      `high` `=` $edge_padding_high `,`\n      `interior` `=` $interior_padding\n      attr-dict `:` functional-type(operands, results)",
    "category": "Transform"
  },
  {
    "name": "stablehlo.partition_id",
    "summary": "PartitionId operation",
    "description": "Produces `partition_id` of the current process.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#partition_id\n\n    Example:\n    ```mlir\n    %result = stablehlo.partition_id : tensor<ui32>\n    ```",
    "assemblyFormat": "attr-dict `:` type(results)"
  },
  {
    "name": "stablehlo.popcnt",
    "summary": "PopulationCount operation",
    "description": "Performs element-wise count of the number of bits set in the `operand`\n    tensor and produces a `result` tensor.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#popcnt\n\n    Example:\n    ```mlir\n    %result = stablehlo.popcnt %operand : tensor<4xi64>\n    ```",
    "inputs": [
      { "name": "operand", "type": "OperandType" }
    ],
    "outputs": [
      { "name": "result", "type": "ResultType" }
    ],
    "assemblyFormat": "$operand attr-dict `:` custom<SameOperandsAndResultType>(type($operand), type($result))"
  },
  {
    "name": "stablehlo.power",
    "summary": "Power operation",
    "description": "Performs element-wise exponentiation of `lhs` tensor by `rhs` tensor and\n    produces a `result` tensor.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#power\n\n    Example:\n    ```mlir\n    %result = stablehlo.power %lhs, %rhs : tensor<6xf64>\n    ```",
    "inputs": [
      { "name": "lhs", "type": "OperandType" },
      { "name": "rhs", "type": "OperandType" }
    ],
    "outputs": [
      { "name": "result", "type": "ResultType" }
    ],
    "assemblyFormat": "$lhs `,` $rhs attr-dict\n      `:` custom<SameOperandsAndResultType>(type($lhs), type($rhs), type($result))"
  },
  {
    "name": "stablehlo.real",
    "summary": "Real operation",
    "description": "Extracts the real part, element-wise, from the `operand` and produces a\n    `result` tensor.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#real\n\n    Example:\n    ```mlir\n    %result = stablehlo.real %operand : (tensor<2xcomplex<f32>>) -> tensor<2xf32>\n    ```",
    "inputs": [
      { "name": "operand", "type": "OperandType" }
    ],
    "outputs": [
      { "name": "result", "type": "ResultType" }
    ],
    "assemblyFormat": "$operand attr-dict `:` custom<SameOperandsAndResultType>(type($operand), type($result))"
  },
  {
    "name": "stablehlo.real_dynamic_slice",
    "summary": "RealDynamicSlice operation",
    "description": "This operation is a work in progress, so it is not yet included in\n    the StableHLO specification: https://github.com/openxla/stablehlo/issues/8.\n\n    Informally, this operation does the same thing as SliceOp except\n    that `start_indices`, `limit_indices` and `strides` are specified dynamically:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#slice\n\n    Example:\n    ```mlir\n    %result = stablehlo.real_dynamic_slice %operand,\n                %start_indices, %limit_indices, %strides\n           : (tensor<256x?xf32>, tensor<2xindex>, tensor<2xindex>, tensor<2xindex>) -> tensor<256x?xf32>\n    ```",
    "inputs": [
      { "name": "operand", "type": "HLO_Tensor" },
      { "name": "start_indices", "type": "HLO_DimensionTensor" },
      { "name": "limit_indices", "type": "HLO_DimensionTensor" },
      { "name": "strides", "type": "HLO_DimensionTensor" }
    ],
    "outputs": [
      { "name": "result", "type": "HLO_Tensor" }
    ],
    "assemblyFormat": "operands attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "stablehlo.recv",
    "summary": "Recv operation",
    "description": "Receives data from a channel with `channel_id` and produces `results`.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#recv\n\n    Example:\n    ```mlir\n    %results:2 = \"stablehlo.recv\"(%token) {\n      channel_handle = #stablehlo.channel_handle<handle = 0, type = 1>,\n      is_host_transfer = false,\n      source_target_pairs = dense<[[0, 1], [1, 2]]> : tensor<2x2xi64>\n    } : (!stablehlo.token) -> (tensor<2x2xi64>, !stablehlo.token)\n    ```",
    "inputs": [
      { "name": "token", "type": "HLO_Token" },
      { "name": "channel_handle", "type": "StableHLO_ChannelHandle" }
    ],
    "attributes": [
      { "name": "is_host_transfer", "type": "DefaultValuedOptionalAttr" },
      { "name": "source_target_pairs", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "stablehlo.reduce",
    "summary": "Reduce operation",
    "description": "Applies a reduction function `body` to `inputs` and `init_values` along the\n    `dimensions` and produces a `result` tensor.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#reduce\n\n    Example:\n    ```mlir\n    %result = \"stablehlo.reduce\"(%input, %init_value) ({\n      ^bb0(%arg0: tensor<i64>, %arg1: tensor<i64>):\n        %0 = stablehlo.add %arg0, %arg1 : tensor<i64>\n        stablehlo.return %0 : tensor<i64>\n    }) {\n      dimensions = array<i64: 1>\n    } : (tensor<1x6xi64>, tensor<i64>) -> tensor<1xi64>\n    ```",
    "inputs": [
      { "name": "inputs", "type": "Variadic" },
      { "name": "init_values", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "dimensions", "type": "DenseI64ArrayAttr" }
    ]
  },
  {
    "name": "stablehlo.reduce_precision",
    "summary": "ReducePrecision operation",
    "description": "Performs element-wise conversion of `operand` to another floating-point type\n    that uses `exponent_bits` and `mantissa_bits` and back to the original\n    floating-point type and produces an `output` tensor.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#reduce_precision\n\n    Example:\n    ```mlir\n    %output = stablehlo.reduce_precision %operand, format = e5m10 : tensor<6xf64>\n    ```",
    "inputs": [
      { "name": "operand", "type": "HLO_FpOrQuantizedIntTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "HLO_FpOrQuantizedIntTensor" }
    ],
    "attributes": [
      { "name": "exponent_bits", "type": "ConfinedAttr" },
      { "name": "mantissa_bits", "type": "ConfinedAttr" }
    ],
    "assemblyFormat": "$operand `,` `format` `=` custom<ExponentMantissa>($exponent_bits, $mantissa_bits)\n      attr-dict `:` custom<SameOperandsAndResultType>(type($operand), type($output))"
  },
  {
    "name": "stablehlo.reduce_scatter",
    "summary": "ReduceScatter operation",
    "description": "Within each process group in the process grid, performs reduction, using\n     `computations`, over the values of the `operand` tensor from each process,\n     splits the reduction result along `scatter_dimension` into parts, and\n     scatters the split parts between the processes to produce the `result`.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#reduce_scatter\n\n    Example:\n    ```mlir\n    %result = \"stablehlo.reduce_scatter\"(%operand) ({\n      ^bb0(%arg0: tensor<i64>, %arg1: tensor<i64>):\n      %0 = stablehlo.add %arg0, %arg1 : tensor<i64>\n      stablehlo.return %0 : tensor<i64>\n    }) {\n      scatter_dimension = 1 : i64,\n      replica_groups = dense<[[0, 1]]> : tensor<1x2xi64>,\n      channel_handle = #stablehlo.channel_handle<handle = 0, type = 0>\n    } : (tensor<2x4xi64>) -> tensor<2x2xi64>\n    ```",
    "inputs": [
      { "name": "operand", "type": "HLO_Tensor" }
    ],
    "attributes": [
      { "name": "scatter_dimension", "type": "ConfinedAttr" },
      { "name": "replica_groups", "type": "I64ElementsAttr" },
      { "name": "channel_handle", "type": "OptionalAttr" },
      { "name": "use_global_device_ids", "type": "UnitAttr" }
    ]
  },
  {
    "name": "stablehlo.reduce_window",
    "summary": "ReduceWindow operation",
    "description": "Applies a reduction function `body` to windows of `inputs` and `init_values`\n    and produces `results`.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#reduce_window\n\n    Example:\n    ```mlir\n    %result = \"stablehlo.reduce_window\"(%input, %init_value) ({\n      ^bb0(%arg0: tensor<i64>, %arg1: tensor<i64>):\n        %0 = stablehlo.add %arg0, %arg1 : tensor<i64>\n        stablehlo.return %0 : tensor<i64>\n    }) {\n      window_dimensions = array<i64: 2, 1>,\n      window_strides = array<i64: 4, 1>,\n      base_dilations = array<i64: 2, 1>,\n      window_dilations = array<i64: 3, 1>,\n      padding = dense<[[2, 1], [0, 0]]> : tensor<2x2xi64>\n    } : (tensor<3x2xi64>, tensor<i64>) -> tensor<2x2xi64>\n    ```",
    "inputs": [
      { "name": "inputs", "type": "Variadic" },
      { "name": "init_values", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "window_dimensions", "type": "DenseI64ArrayAttr" },
      { "name": "window_strides", "type": "OptionalAttr" },
      { "name": "base_dilations", "type": "OptionalAttr" },
      { "name": "window_dilations", "type": "OptionalAttr" },
      { "name": "padding", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "stablehlo.remainder",
    "summary": "Rem operation",
    "description": "Performs element-wise remainder of dividend `lhs` and divisor `rhs` tensors\n    and produces a `result` tensor.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#remainder\n\n    Example:\n    ```mlir\n    %result = stablehlo.remainder %lhs, %rhs : tensor<4xi64>\n    ```",
    "inputs": [
      { "name": "lhs", "type": "OperandType" },
      { "name": "rhs", "type": "OperandType" }
    ],
    "outputs": [
      { "name": "result", "type": "ResultType" }
    ],
    "assemblyFormat": "$lhs `,` $rhs attr-dict\n      `:` custom<SameOperandsAndResultType>(type($lhs), type($rhs), type($result))"
  },
  {
    "name": "stablehlo.replica_id",
    "summary": "ReplicaId operation",
    "description": "Produces `replica_id` of the current process.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#replica_id\n\n    Example:\n    ```mlir\n    %result = stablehlo.replica_id : tensor<ui32>\n    ```",
    "assemblyFormat": "attr-dict `:` type(results)"
  },
  {
    "name": "stablehlo.reshape",
    "summary": "Reshape operation",
    "description": "Performs reshape of `operand` tensor to a `result` tensor.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#reshape\n\n    Example:\n    ```mlir\n    %result = stablehlo.reshape %operand : (tensor<2xf32>) -> tensor<1x2xf32>\n    ```",
    "inputs": [
      { "name": "operand", "type": "HLO_TensorOrPerAxisQuantizedTensor" }
    ],
    "assemblyFormat": "operands attr-dict `:` functional-type(operands, results)",
    "category": "Shape"
  },
  {
    "name": "stablehlo.return",
    "summary": "This operation is a work in progress, so it is not yet included in\n    the StableHLO specification: https://github.com/openxla/stablehlo/issues/425.\n\n    Informally, this operation serves as a terminator for regions defined by\n    the StableHLO ops. Non-StableHLO ops, e.g. `func.func`, have their own\n    terminators, e.g. `func.return`.\n\n    Example:\n    ```mlir\n    %result = \"stablehlo.reduce\"(%input, %init_value) ({\n      ^bb0(%arg0: tensor<i32>, %arg1: tensor<i32>):\n        %0 = \"stablehlo.add\"(%arg0, %arg1) : (tensor<i32>, tensor<i32>) -> tensor<i32>\n        \"stablehlo.return\"(%0) : (tensor<i32>) -> ()\n    }) {\n      dimensions = array<i64: 1>\n    } : (tensor<1x6xi32>, tensor<i32>) -> tensor<1xi32>\n    ```",
    "inputs": [
      { "name": "results", "type": "Variadic" }
    ],
    "assemblyFormat": "$results attr-dict (`:` type($results)^)?"
  },
  {
    "name": "stablehlo.reverse",
    "summary": "Reverse operation",
    "description": "Reverses the order of elements in the `operand` along the specified\n    `dimensions` and produces a `result` tensor.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#reverse\n\n    Example:\n    ```mlir\n    %result = stablehlo.reverse %operand, dims = [1] : tensor<3x2xi32>\n    ```",
    "inputs": [
      { "name": "operand", "type": "HLO_Tensor" }
    ],
    "outputs": [
      { "name": "result", "type": "HLO_Tensor" }
    ],
    "attributes": [
      { "name": "dimensions", "type": "DenseI64ArrayAttr" }
    ],
    "assemblyFormat": "$operand `,` `dims` `=` $dimensions\n      attr-dict `:` custom<SameOperandsAndResultType>(type($operand), type($result))",
    "category": "Transform"
  },
  {
    "name": "stablehlo.rng",
    "summary": "Rng operation",
    "description": "Generates random numbers using the `rng_distribution` algorithm and produces\n    a `result` tensor of a given shape `shape`.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#rng\n\n    Example:\n    ```mlir\n    %result = stablehlo.rng %a, %b, %shape, distribution = NORMAL : (tensor<i32>, tensor<i32>, tensor<2xi64>) -> tensor<3x3xi32>\n    ```",
    "inputs": [
      { "name": "a", "type": "DTensorOf" },
      { "name": "b", "type": "DTensorOf" },
      { "name": "shape", "type": "HLO_StaticDimensionTensor" }
    ],
    "outputs": [
      { "name": "result", "type": "HLO_PredIntOrFpTensor" }
    ],
    "attributes": [
      { "name": "rng_distribution", "type": "StableHLO_RngDistributionAttr" }
    ],
    "assemblyFormat": "$a `,` $b `,` $shape `,` `distribution` `=` $rng_distribution\n      attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "stablehlo.rng_bit_generator",
    "summary": "RngBitGenerator operation",
    "description": "Returns an `output` filled with uniform random data and an updated output\n    state `output_state` given an initial state `initial_state` using the\n    pseudorandom number generator algorithm `rng_algorithm`.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#rng_bit_generator\n\n    Example:\n    ```mlir\n    %output_state, %output = stablehlo.rng_bit_generator %initial_state, algorithm = THREE_FRY : (tensor<2xui64>) -> (tensor<2xui64>, tensor<2x2xui64>)\n    ```",
    "inputs": [
      { "name": "initial_state", "type": "HLO_IntOrFpTensor" }
    ],
    "outputs": [
      { "name": "output_state", "type": "HLO_IntOrFpTensor" },
      { "name": "output", "type": "HLO_StaticShapeIntOrFpTensor" }
    ],
    "attributes": [
      { "name": "rng_algorithm", "type": "StableHLO_RngAlgorithmAttr" }
    ],
    "assemblyFormat": "$initial_state `,` `algorithm` `=` $rng_algorithm attr-dict\n      `:` functional-type(operands, results)"
  },
  {
    "name": "stablehlo.round_nearest_afz",
    "summary": "Round operation",
    "description": "Performs element-wise rounding towards the nearest integer, breaking ties\n    away from zero, on the `operand` tensor and produces a `result` tensor.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#round_nearest_afz\n\n    Example:\n    ```mlir\n    %result = stablehlo.round_nearest_afz %operand : tensor<5xf64>\n    ```",
    "inputs": [
      { "name": "operand", "type": "OperandType" }
    ],
    "outputs": [
      { "name": "result", "type": "ResultType" }
    ],
    "assemblyFormat": "$operand attr-dict `:` custom<SameOperandsAndResultType>(type($operand), type($result))"
  },
  {
    "name": "stablehlo.round_nearest_even",
    "summary": "RoundNearestEven operation",
    "description": "Performs element-wise rounding towards the nearest integer, breaking ties\n    towards the even integer, on the `operand` tensor and produces a `result`\n    tensor.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#round_nearest_even\n\n    Example:\n    ```mlir\n    %result = stablehlo.round_nearest_even %operand : tensor<5xf64>\n    ```",
    "inputs": [
      { "name": "operand", "type": "OperandType" }
    ],
    "outputs": [
      { "name": "result", "type": "ResultType" }
    ],
    "assemblyFormat": "$operand attr-dict `:` custom<SameOperandsAndResultType>(type($operand), type($result))"
  },
  {
    "name": "stablehlo.rsqrt",
    "summary": "Rsqrt operation",
    "description": "Performs element-wise reciprocal square root operation on `operand` tensor\n    and produces a `result` tensor, implementing the `rSqrt` operation from the\n    IEEE-754 specification.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#rsqrt\n\n    Example:\n    ```mlir\n    %result = stablehlo.rsqrt %operand : tensor<2x2xf32>\n    ```",
    "inputs": [
      { "name": "operand", "type": "HLO_FpComplexOrQuantizedIntTensor" }
    ],
    "outputs": [
      { "name": "result", "type": "HLO_FpComplexOrQuantizedIntTensor" }
    ],
    "attributes": [
      { "name": "result_accuracy", "type": "DefaultValuedOptionalAttr" }
    ],
    "assemblyFormat": "$operand attr-dict `:` custom<SameOperandsAndResultType>(type($operand), type($result))"
  },
  {
    "name": "stablehlo.scatter",
    "summary": "Scatter operation",
    "description": "Produces `results` tensors which are equal to `inputs` tensors except that\n    several slices specified by `scatter_indices` are updated with the values\n    `updates` using `update_computation`.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#scatter\n\n   Example:\n   ```mlir\n   %result = \"stablehlo.scatter\"(%input, %scatter_indices, %update) ({\n     ^bb0(%arg0: tensor<i64>, %arg1: tensor<i64>):\n       %0 = stablehlo.add %arg0, %arg1 : tensor<i64>\n       stablehlo.return %0 : tensor<i64>\n   }) {\n     scatter_dimension_numbers = #stablehlo.scatter<\n       update_window_dims = [3, 4],\n       inserted_window_dims = [1],\n       input_batching_dims = [0],\n       scatter_indices_batching_dims = [1],\n       scatter_dims_to_operand_dims = [2, 1],\n       index_vector_dim = 3>,\n     indices_are_sorted = false,\n     unique_indices = false\n   } : (tensor<2x3x4x2xi64>, tensor<2x2x3x2xi64>, tensor<2x2x3x2x2xi64>) -> tensor<2x3x4x2xi64>\n   ```",
    "inputs": [
      { "name": "inputs", "type": "Variadic" },
      { "name": "scatter_indices", "type": "RankedTensorOf" },
      { "name": "updates", "type": "Variadic" },
      { "name": "scatter_dimension_numbers", "type": "StableHLO_ScatterDimensionNumbers" }
    ],
    "attributes": [
      { "name": "indices_are_sorted", "type": "DefaultValuedOptionalAttr" },
      { "name": "unique_indices", "type": "DefaultValuedOptionalAttr" }
    ],
    "category": "Tensor"
  },
  {
    "name": "stablehlo.select",
    "summary": "Select operation",
    "description": "Produces a `result` tensor where each element is selected from `on_true` or\n    `on_false` tensor based on the value of the corresponding element of `pred`.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#select\n\n    Example:\n    ```mlir\n    %result = stablehlo.select %pred, %on_true, %on_false : tensor<2x2xi1>, tensor<2x2xi32>\n    ```",
    "inputs": [
      { "name": "pred", "type": "HLO_PredTensor" },
      { "name": "on_true", "type": "HLO_Tensor" },
      { "name": "on_false", "type": "HLO_Tensor" }
    ],
    "outputs": [
      { "name": "result", "type": "HLO_Tensor" }
    ],
    "assemblyFormat": "operands attr-dict `:`\n      custom<SelectOpType>(type($pred), type($on_true), type($on_false), type($result))"
  },
  {
    "name": "stablehlo.select_and_scatter",
    "summary": "SelectAndScatter operation",
    "description": "Scatters the values from the `source` tensor using `scatter` based on the\n    outcome of `reduce_window` of the `input` tensor using `select` and produces\n    a `result` tensor.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#select_and_scatter\n\n    Example:\n    ```mlir\n    %result = \"stablehlo.select_and_scatter\"(%operand, %source, %init_value) ({\n      ^bb0(%arg0: tensor<i64>, %arg1: tensor<i64>):\n        %0 = stablehlo.compare GE, %arg0, %arg1 : (tensor<i64>, tensor<i64>) -> tensor<i1>\n        stablehlo.return %0 : tensor<i1>\n    }, {\n      ^bb0(%arg0: tensor<i64>, %arg1: tensor<i64>):\n        %0 = stablehlo.add %arg0, %arg1 : tensor<i64>\n        stablehlo.return %0 : tensor<i64>\n    }) {\n      window_dimensions = array<i64: [3, 1]>,\n      window_strides = array<i64: [2, 1]>,\n      padding = dense<[[0, 1], [0, 0]]> : tensor<2x2xi64>\n    } : (tensor<4x2xi64>, tensor<2x2xi64>, tensor<i64>) -> tensor<4x2xi64>\n    ```",
    "inputs": [
      { "name": "operand", "type": "HLO_Tensor" },
      { "name": "source", "type": "HLO_Tensor" },
      { "name": "init_value", "type": "HLO_Tensor" }
    ],
    "attributes": [
      { "name": "window_dimensions", "type": "OptionalAttr" },
      { "name": "window_strides", "type": "OptionalAttr" },
      { "name": "padding", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "stablehlo.send",
    "summary": "Send operation",
    "description": "Sends `inputs` to a channel `channel_id` and produces a `result` token.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#send\n\n    Example:\n    ```mlir\n    %result = \"stablehlo.send\"(%operand, %token) {\n      channel_handle = #stablehlo.channel_handle<handle = 0, type = 1>,\n      is_host_transfer = false,\n      source_target_pairs = dense<[[0, 1], [1, 2]]> : tensor<2x2xi64>\n    } : (tensor<2x2xi64>, !stablehlo.token) -> !stablehlo.token\n    ```",
    "inputs": [
      { "name": "inputs", "type": "Variadic" },
      { "name": "token", "type": "HLO_Token" },
      { "name": "channel_handle", "type": "StableHLO_ChannelHandle" }
    ],
    "attributes": [
      { "name": "is_host_transfer", "type": "DefaultValuedOptionalAttr" },
      { "name": "source_target_pairs", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "stablehlo.set_dimension_size",
    "summary": "SetDimensionSize operation",
    "description": "This operation is a work in progress, so it is not yet included in\n    the StableHLO specification: https://github.com/openxla/stablehlo/issues/8.\n\n    Informally, this operation does the same thing as XLA's SetDimensionSize:\n    https://www.tensorflow.org/xla/operation_semantics#setdimensionsize\n\n    Example:\n    ```mlir\n    %0 = stablehlo.set_dimension_size %arg0, %arg1, dim = 1 : (tensor<4x2xf32>, tensor<i32>) -> tensor<4x2xf32>\n    ```",
    "inputs": [
      { "name": "operand", "type": "HLO_Tensor" },
      { "name": "size", "type": "I32RankedTensor" }
    ],
    "attributes": [
      { "name": "dimension", "type": "ConfinedAttr" }
    ],
    "assemblyFormat": "$operand `,` $size  `,` `dim` `=` $dimension attr-dict\n      `:` functional-type(operands, results)"
  },
  {
    "name": "stablehlo.shift_left",
    "summary": "ShiftLeft operation",
    "description": "Performs element-wise left-shift operation on the `lhs` tensor by `rhs`\n    number of bits and produces a `result` tensor.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#shift_left\n\n    Example:\n    ```mlir\n    %result = stablehlo.shift_left %lhs, %rhs : tensor<3xi64>\n    ```",
    "inputs": [
      { "name": "lhs", "type": "OperandType" },
      { "name": "rhs", "type": "OperandType" }
    ],
    "outputs": [
      { "name": "result", "type": "ResultType" }
    ],
    "assemblyFormat": "$lhs `,` $rhs attr-dict\n      `:` custom<SameOperandsAndResultType>(type($lhs), type($rhs), type($result))"
  },
  {
    "name": "stablehlo.shift_right_arithmetic",
    "summary": "ShiftRightArithmetic operation",
    "description": "Performs element-wise arithmetic right-shift operation on the `lhs` tensor\n    by `rhs` number of bits and produces a `result` tensor.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#shift_right_arithmetic\n\n    Example:\n    ```mlir\n    %result = stablehlo.shift_right_arithmetic %lhs, %rhs : tensor<3xi64>\n    ```",
    "inputs": [
      { "name": "lhs", "type": "OperandType" },
      { "name": "rhs", "type": "OperandType" }
    ],
    "outputs": [
      { "name": "result", "type": "ResultType" }
    ],
    "assemblyFormat": "$lhs `,` $rhs attr-dict\n      `:` custom<SameOperandsAndResultType>(type($lhs), type($rhs), type($result))"
  },
  {
    "name": "stablehlo.shift_right_logical",
    "summary": "ShiftRightLogical operation",
    "description": "Performs element-wise logical right-shift operation on the `lhs` tensor by\n    `rhs` number of bits and produces a `result` tensor.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#shift_right_logical\n\n    Example:\n    ```mlir\n    %result = stablehlo.shift_right_logical %lhs, %rhs : tensor<3xi64>\n    ```",
    "inputs": [
      { "name": "lhs", "type": "OperandType" },
      { "name": "rhs", "type": "OperandType" }
    ],
    "outputs": [
      { "name": "result", "type": "ResultType" }
    ],
    "assemblyFormat": "$lhs `,` $rhs attr-dict\n      `:` custom<SameOperandsAndResultType>(type($lhs), type($rhs), type($result))"
  },
  {
    "name": "stablehlo.sign",
    "summary": "Sign operation",
    "description": "Returns the sign of the `operand` element-wise and produces a `result`\n    tensor.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#sign\n\n    Example:\n    ```mlir\n    %result = stablehlo.sign %operand : tensor<5xf64>\n    ```",
    "inputs": [
      { "name": "operand", "type": "OperandType" }
    ],
    "outputs": [
      { "name": "result", "type": "ResultType" }
    ],
    "assemblyFormat": "$operand attr-dict `:` custom<SameOperandsAndResultType>(type($operand), type($result))"
  },
  {
    "name": "stablehlo.sine",
    "summary": "Sine operation",
    "description": "Performs element-wise sine operation on `operand` tensor and produces a\n    `result` tensor.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#sine\n\n    Example:\n    ```mlir\n    %result = stablehlo.sine %operand : tensor<2xf32>\n    ```",
    "inputs": [
      { "name": "operand", "type": "HLO_FpComplexOrQuantizedIntTensor" }
    ],
    "outputs": [
      { "name": "result", "type": "HLO_FpComplexOrQuantizedIntTensor" }
    ],
    "attributes": [
      { "name": "result_accuracy", "type": "DefaultValuedOptionalAttr" }
    ],
    "assemblyFormat": "$operand attr-dict `:` custom<SameOperandsAndResultType>(type($operand), type($result))"
  },
  {
    "name": "stablehlo.slice",
    "summary": "Slice operation",
    "description": "Extracts a slice from the `operand` using statically-computed starting\n    indices and produces a `result` tensor.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#slice\n\n    Example:\n    ```mlir\n    %result = stablehlo.slice %operand [1:3, 4:8:2]\n       : (tensor<3x8xi64>) -> tensor<2x2xi64>\n\n    // Same in generic form: the `1:3` above is mapped to the first entry in\n    // `start_indices` and `limit_indices`, while `strides` is implicitly 1.\n    // The `4:8:2` above is parsed into the second entry of `start_indices`,\n    // `limit_indices` and `strides` respectively.\n    %result = \"stablehlo.slice\" (%operand) {\n      start_indices = array<i64: 1, 4>,\n      limit_indices = array<i64: 3, 8>,\n      strides = array<i64: 1, 2>\n    } : (tensor<3x8xi64>) -> tensor<2x2xi64>\n    ```",
    "inputs": [
      { "name": "operand", "type": "HLO_Tensor" }
    ],
    "attributes": [
      { "name": "start_indices", "type": "DenseI64ArrayAttr" },
      { "name": "limit_indices", "type": "DenseI64ArrayAttr" },
      { "name": "strides", "type": "DenseI64ArrayAttr" }
    ],
    "assemblyFormat": "$operand custom<SliceRanges>($start_indices, $limit_indices, $strides)\n      attr-dict `:` functional-type(operands, results)",
    "category": "Tensor"
  },
  {
    "name": "stablehlo.sort",
    "summary": "Sort operation",
    "description": "Sorts a variadic number of tensors in `inputs` together, according to a\n    custom `comparator`, along the given `dimension` and produces a variadic\n    number of tensors as `results`.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#sort\n\n    Example:\n    ```mlir\n    %result0, %result1 = \"stablehlo.sort\"(%input0, %input1) ({\n      ^bb0(%arg0: tensor<i64>, %arg1: tensor<i64>, %arg2: tensor<i64>, %arg3: tensor<i64>):\n        %predicate = stablehlo.compare GT, %arg0, %arg1 : (tensor<i64>, tensor<i64>) -> tensor<i1>\n        stablehlo.return %predicate : tensor<i1>\n    }) {\n      dimension = 0 : i64,\n      is_stable = true\n    } : (tensor<2x3xi64>, tensor<2x3xi64>) -> (tensor<2x3xi64>, tensor<2x3xi64>)",
    "inputs": [
      { "name": "inputs", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "dimension", "type": "DefaultValuedOptionalAttr" },
      { "name": "is_stable", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "stablehlo.sqrt",
    "summary": "Sqrt operation",
    "description": "Performs element-wise square root operation on `operand` tensor and produces\n    a `result` tensor.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#sqrt\n\n    Example:\n    ```mlir\n    %result = stablehlo.sqrt %operand : tensor<2x2xf32>\n    ```",
    "inputs": [
      { "name": "operand", "type": "HLO_FpComplexOrQuantizedIntTensor" }
    ],
    "outputs": [
      { "name": "result", "type": "HLO_FpComplexOrQuantizedIntTensor" }
    ],
    "attributes": [
      { "name": "result_accuracy", "type": "DefaultValuedOptionalAttr" }
    ],
    "assemblyFormat": "$operand attr-dict `:` custom<SameOperandsAndResultType>(type($operand), type($result))"
  },
  {
    "name": "stablehlo.subtract",
    "summary": "Subtract operation",
    "description": "Performs element-wise subtraction of two tensors `lhs` and `rhs` and\n    produces a `result` tensor.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#subtract\n\n    Example:\n    ```mlir\n    %result = stablehlo.subtract %lhs, %rhs : tensor<2xi32>\n    ```",
    "inputs": [
      { "name": "lhs", "type": "OperandType" },
      { "name": "rhs", "type": "OperandType" }
    ],
    "outputs": [
      { "name": "result", "type": "ResultType" }
    ],
    "assemblyFormat": "$lhs `,` $rhs attr-dict\n      `:` custom<SameOperandsAndResultType>(type($lhs), type($rhs), type($result))"
  },
  {
    "name": "stablehlo.tan",
    "summary": "Tan operation",
    "description": "Performs element-wise tangent operation on `operand` tensor and\n    produces a `result` tensor.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#tan\n\n    Example:\n    ```mlir\n    %result = stablehlo.tan %operand : tensor<2x2xf64>\n    ```",
    "inputs": [
      { "name": "operand", "type": "HLO_FpComplexOrQuantizedIntTensor" }
    ],
    "outputs": [
      { "name": "result", "type": "HLO_FpComplexOrQuantizedIntTensor" }
    ],
    "attributes": [
      { "name": "result_accuracy", "type": "DefaultValuedOptionalAttr" }
    ],
    "assemblyFormat": "$operand attr-dict `:` custom<SameOperandsAndResultType>(type($operand), type($result))"
  },
  {
    "name": "stablehlo.tanh",
    "summary": "Tanh operation",
    "description": "Performs element-wise hyperbolic tangent operation on `operand` tensor and\n    produces a `result` tensor.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#tanh\n\n    Example:\n    ```mlir\n    %result = stablehlo.tanh %operand : tensor<2xf32>\n    ```",
    "inputs": [
      { "name": "operand", "type": "HLO_FpComplexOrQuantizedIntTensor" }
    ],
    "outputs": [
      { "name": "result", "type": "HLO_FpComplexOrQuantizedIntTensor" }
    ],
    "attributes": [
      { "name": "result_accuracy", "type": "DefaultValuedOptionalAttr" }
    ],
    "assemblyFormat": "$operand attr-dict `:` custom<SameOperandsAndResultType>(type($operand), type($result))",
    "category": "Activation"
  },
  {
    "name": "stablehlo.torch_index_select",
    "summary": "TorchIndexSelect operation",
    "description": "This operation is on its way out of StableHLO, so it is not included in\n    the StableHLO specification: https://github.com/openxla/stablehlo/issues/3.\n\n    Informally, this operation does the same thing as PyTorch's index_select,\n    augmented with support for batch dimensions:\n    https://pytorch.org/docs/stable/generated/torch.index_select.html.\n\n    The `batch_dims` attribute specifies the number of major batch dimensions\n    (0 or more) that act like a multidimensional loop over both the operand and\n    the index.\n\n    Example:\n    ```mlir\n    %result = \"stablehlo.torch_index_select\"(%operand, %index) {\n      dim = 2 : i64,\n      batch_dims = 1 : i64\n    } : (tensor<8x128x3072x64xf32>, tensor<8x16x1024xi32>) -> tensor<8x128x16x1024x64xf32>\n    ```",
    "inputs": [
      { "name": "operand", "type": "HLO_Tensor" },
      { "name": "index", "type": "HLO_Tensor" }
    ],
    "attributes": [
      { "name": "dim", "type": "I64Attr" },
      { "name": "batch_dims", "type": "I64Attr" }
    ]
  },
  {
    "name": "stablehlo.transpose",
    "summary": "Transpose operation",
    "description": "Permutes the dimensions of `operand` tensor using `permutation` and produces\n    a `result` tensor.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#transpose\n\n    Example:\n    ```mlir\n    %0 = stablehlo.transpose %arg0, dims = [2, 1, 0] : (tensor<1x2x3xi32>) -> tensor<3x2x1xi32>\n    ```",
    "inputs": [
      { "name": "operand", "type": "HLO_TensorOrPerAxisQuantizedTensor" }
    ],
    "outputs": [
      { "name": "result", "type": "HLO_TensorOrPerAxisQuantizedTensor" }
    ],
    "attributes": [
      { "name": "permutation", "type": "DenseI64ArrayAttr" }
    ],
    "assemblyFormat": "$operand `,` `dims` `=` $permutation\n      attr-dict `:` functional-type(operands, results)",
    "category": "Transform"
  },
  {
    "name": "stablehlo.triangular_solve",
    "summary": "TriangularSolve operation",
    "description": "Solves batches of systems of linear equations with lower or upper triangular\n    coefficient matrices.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#triangular_solve\n\n    Example:\n    ```mlir\n    %result = \"stablehlo.triangular_solve\"(%a, %b) {\n      left_side = true,\n      lower = true,\n      unit_diagonal = false,\n      transpose_a = #stablehlo<transpose NO_TRANSPOSE>\n    } : (tensor<3x3xf32>, tensor<3x3xf32>) -> tensor<3x3xf32>\n    ```",
    "inputs": [
      { "name": "a", "type": "HLO_FpOrComplexTensor" },
      { "name": "b", "type": "HLO_FpOrComplexTensor" }
    ],
    "attributes": [
      { "name": "left_side", "type": "BoolAttr" },
      { "name": "lower", "type": "BoolAttr" },
      { "name": "unit_diagonal", "type": "BoolAttr" },
      { "name": "transpose_a", "type": "StableHLO_TransposeAttr" }
    ]
  },
  {
    "name": "stablehlo.tuple",
    "summary": "Tuple operation",
    "description": "Produces a `result` tuple from values `val`.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#tuple\n\n    Example:\n    ```mlir\n    %result = stablehlo.tuple %val0, %val1 : tuple<tensor<2xf64>, tuple<tensor<i64>>>\n    ```",
    "inputs": [
      { "name": "val", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "result", "type": "HLO_Tuple" }
    ],
    "assemblyFormat": "$val attr-dict `:` custom<TupleOpType>(type($val), type($result))"
  },
  {
    "name": "stablehlo.unary_einsum",
    "summary": "UnaryEinsum operation",
    "description": "This operation is on its way out of StableHLO, so it is not included in\n    the StableHLO specification: https://github.com/openxla/stablehlo/issues/3.\n\n    Informally, this operation does the same thing as TF's einsum:\n    https://www.tensorflow.org/api_docs/python/tf/einsum\n\n    Example:\n    ```mlir\n    %result = \"stablehlo.unary_einsum\"(%operand) {\n      einsum_config = \"ab->a\"\n    } : (tensor<4x16xf32>) -> tensor<4xf32>\n    ```",
    "inputs": [
      { "name": "operand", "type": "HLO_Tensor" }
    ],
    "attributes": [
      { "name": "einsum_config", "type": "StrAttr" }
    ],
    "assemblyFormat": "$operand `,` `config` `=` $einsum_config attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "stablehlo.uniform_dequantize",
    "summary": "UniformDequantize operation",
    "description": "Performs element-wise conversion of quantized tensor `operand` to a\n    floating-point tensor `result` according to the quantization parameters\n    defined by the `operand` type.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#uniform_dequantize\n\n    Example:\n    ```mlir\n    %result = stablehlo.uniform_dequantize %operand : (tensor<2x!quant.uniform<i8:f32:0, {0.1:-30,0.5:-20}>>) -> tensor<2xf32>\n    ```",
    "inputs": [
      { "name": "operand", "type": "OperandType" }
    ],
    "outputs": [
      { "name": "result", "type": "ResultType" }
    ],
    "assemblyFormat": "$operand attr-dict `:` custom<SameOperandsAndResultType>(type($operand), type($result))"
  },
  {
    "name": "stablehlo.uniform_quantize",
    "summary": "UniformQuantize operation",
    "description": "Performs element-wise conversion of floating-point tensor or quantized\n    tensor `operand` to a quantized tensor `result` according to the\n    quantization parameters defined by the `result` type.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#uniform_quantize\n\n    Example:\n    ```mlir\n    %result = stablehlo.uniform_quantize %operand : (tensor<2xf32>) -> tensor<2x!quant.uniform<i8:f32:0, {0.1:-30,0.5:-20}>>\n    ```",
    "inputs": [
      { "name": "operand", "type": "OperandType" }
    ],
    "outputs": [
      { "name": "result", "type": "ResultType" }
    ],
    "assemblyFormat": "$operand attr-dict `:` custom<SameOperandsAndResultType>(type($operand), type($result))"
  },
  {
    "name": "stablehlo.while",
    "summary": "While operation",
    "description": "Produces the output from executing `body` function 0 or more times while the\n    `cond` function outputs `true`.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#while\n\n    Example:\n    ```mlir\n    %results0, %results1 = stablehlo.while(%arg0 = %init_i, %arg1 = %init_sum) : tensor<i64>, tensor<i64>\n    cond {\n      %cond = stablehlo.compare LT, %arg0, %ten : (tensor<i64>, tensor<i64>) -> tensor<i1>\n      stablehlo.return %cond : tensor<i1>\n    } do {\n      %new_sum = stablehlo.add %arg1, %one : tensor<i64>\n      %new_i = stablehlo.add %arg0, %one : tensor<i64>\n      stablehlo.return %new_i, %new_sum : tensor<i64>, tensor<i64>\n    }\n    ```",
    "inputs": [
      { "name": "operand", "type": "Variadic" }
    ]
  },
  {
    "name": "stablehlo.xor",
    "summary": "Xor operation",
    "description": "Performs element-wise XOR of two tensors `lhs` and `rhs` and produces a\n    `result` tensor.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#xor\n\n    Example:\n    ```mlir\n    %result = stablehlo.xor %lhs, %rhs : tensor<2xi32>\n    ```",
    "inputs": [
      { "name": "lhs", "type": "HLO_PredOrIntTensor" },
      { "name": "rhs", "type": "HLO_PredOrIntTensor" }
    ],
    "outputs": [
      { "name": "result", "type": "ResultType" }
    ],
    "assemblyFormat": "$lhs `,` $rhs attr-dict\n      `:` custom<SameOperandsAndResultType>(type($lhs), type($rhs), type($result))"
  },
  {
    "name": "tensor.bitcast",
    "summary": "tensor bitcast operation",
    "description": "Bitcast a tensor from one type to another type of equivalent element width.\n    If both are ranked, then the rank should be the same and static dimensions\n    should match.\n\n    Example:\n\n    ```mlir\n    // Bitcast from unsigned to signed or signless integer.\n    %2 = tensor.bitcast %1 : tensor<4xui32> to tensor<4xi32>\n    ```",
    "inputs": [
      { "name": "source", "type": "TensorOf" }
    ],
    "outputs": [
      { "name": "dest", "type": "TensorOf" }
    ],
    "assemblyFormat": "$source attr-dict `:` type($source) `to` type($dest)"
  },
  {
    "name": "tensor.cast",
    "summary": "tensor cast operation",
    "description": "Convert a tensor from one type to an equivalent type without changing any\n    data elements. The source and destination types must both be tensor types\n    with the same element type. If both are ranked, then the rank should be the\n    same and static dimensions should match. The operation is invalid if\n    converting to a mismatching constant dimension.\n\n    Example:\n\n    ```mlir\n    // Convert from unknown rank to rank 2 with unknown dimension sizes.\n    %2 = tensor.cast %1 : tensor<*xf32> to tensor<?x?xf32>\n\n    // Convert to a type with more known dimensions.\n    %3 = tensor.cast %2 : tensor<?x?xf32> to tensor<4x?xf32>\n\n    // Discard static dimension and rank information.\n    %4 = tensor.cast %3 : tensor<4x?xf32> to tensor<?x?xf32>\n    %5 = tensor.cast %4 : tensor<?x?xf32> to tensor<*xf32>\n    ```",
    "inputs": [
      { "name": "source", "type": "AnyTensor" }
    ],
    "outputs": [
      { "name": "dest", "type": "AnyTensor" }
    ],
    "assemblyFormat": "$source attr-dict `:` type($source) `to` type($dest)"
  },
  {
    "name": "tensor.collapse_shape",
    "summary": "operation to produce a tensor with a smaller rank",
    "description": "The `tensor.collapse_shape` op produces a new tensor of lower (or equal)\n    rank whose dimension sizes are a reassociation of the original `src` dimensions.\n\n    A reassociation is defined as a continuous grouping of dimensions and is\n    represented by an array of DenseI64ArrayAttr attribute. The reassociation\n    maps are applied to the operand shape to obtain the result shape.\n\n\n    Example:\n\n    ```mlir\n    // Dimension collapse (i, j) -> i' and k -> k'\n    %b = tensor.collapse_shape %a [[0, 1], [2]]\n        : tensor<?x?x?xf32> into tensor<?x?xf32>\n    ```",
    "inputs": [
      { "name": "src", "type": "AnyTensor" }
    ],
    "attributes": [
      { "name": "reassociation", "type": "IndexListArrayAttr" }
    ],
    "assemblyFormat": "$src $reassociation attr-dict `:` type($src) `into` type($result)"
  },
  {
    "name": "tensor.concat",
    "summary": "tensor concatenation operation",
    "description": "The \"concat\" operation constructs a tensor out of a variadic list of input\n    tensors, concatenated along a static dimension number. All inputs and the\n    result type must share the same rank.\n\n    `dim` specifies the dimension along which to concatenate. The size of the\n    concatenated dimension in the result must be equal to the sum of the sizes\n    of the inputs along that dimension. All other dimensions in both the inputs\n    and result must be the same size.\n\n    Example:\n\n    ```mlir\n    %0 = tensor.concat dim(0) %0, %1, %2 :\n        (tensor<3x6xf32>, tensor<3x6xf32>, tensor<1x6xf32) -> tensor<7x6xf32>\n\n    // Dynamic + dynamic -> static\n    %0 = tensor.concat dim(1) %0, %1, %2 :\n        (tensor<3x?xf32>, tensor<3x2xf32>, tensor<3x?xf32) -> tensor<3x10xf32>\n    ```",
    "inputs": [
      { "name": "inputs", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyRankedTensor" }
    ],
    "attributes": [
      { "name": "dim", "type": "I64Attr" }
    ],
    "assemblyFormat": "`dim` `(` $dim `)` $inputs attr-dict\n    `:` functional-type(operands, results)"
  },
  {
    "name": "tensor.dim",
    "summary": "dimension index operation",
    "description": "The `tensor.dim` operation takes a tensor and a dimension operand of type\n    `index`. It returns the size of the requested dimension of the given\n    tensor. If the dimension index is out of bounds, the behavior is undefined.\n\n    The specified tensor type is that of the first operand.\n\n    Example:\n\n    ```mlir\n    // Always returns 4, can be constant folded:\n    %c0 = arith.constant 0 : index\n    %x = tensor.dim %A, %c0 : tensor<4x?xf32>\n\n    // Return the dynamic dimension of %A.\n    %c1 = arith.constant 1 : index\n    %y = tensor.dim %A, %c1 : tensor<4x?xf32>\n\n    // Equivalent generic form:\n    %x = \"tensor.dim\"(%A, %c0) : (tensor<4x?xf32>, index) -> index\n    %y = \"tensor.dim\"(%A, %c1) : (tensor<4x?xf32>, index) -> index\n    ```",
    "inputs": [
      { "name": "source", "type": "AnyNon0RankedOrUnrankedTensor" },
      { "name": "index", "type": "Index" }
    ],
    "outputs": [
      { "name": "result", "type": "Index" }
    ],
    "assemblyFormat": "attr-dict $source `,` $index `:` type($source)"
  },
  {
    "name": "tensor.empty",
    "summary": "empty tensor operation",
    "description": "`tensor.empty` is an operation that defines a tensor of a particular shape.\n    The shape could be dynamic or static. The contents of the tensor are\n    unspecified and the only purpose of the op result is to materialize the\n    specified shape in IR and make it available to other transformations.\n\n    `tensor.empty` is useful in transformations that expect destination style\n    ops. I.e., ops that implement `DestinationStyleOpInterface`. Ops that are\n    not in destination style can be made compatible with such transformations\n    with a `tensor.empty` destination.\n\n    Note: This op can be lowered to a `bufferization.alloc_tensor`, at which\n    point it turns into an explicit buffer allocation.",
    "inputs": [
      { "name": "dynamicSizes", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyRankedTensor" }
    ],
    "assemblyFormat": "`(`$dynamicSizes`)` attr-dict `:` type($result)"
  },
  {
    "name": "tensor.expand_shape",
    "summary": "operation to produce a tensor with a higher rank",
    "description": "The `tensor.expand_shape` op produces a tensor of higher (or equal)\n    rank than the operand `src` whose dimension sizes are a reassociation of\n    `src`.\n\n    A reassociation is defined as a continuous grouping of dimensions and is\n    represented with an array of DenseI64ArrayAttr attribute.  The reassociation\n    maps applied to the result tensor with the higher rank must result in the\n    operand tensor with the smaller rank.\n\n    The representation for the output shape supports a partially-static\n    specification via attributes specified through the `static_output_shape`\n    argument.  A special sentinel value `ShapedType::kDynamic` encodes that the\n    corresponding entry has a dynamic value.  There must be exactly as many SSA\n    inputs in `output_shape` as there are `ShapedType::kDynamic` entries in\n    `static_output_shape`.\n\n    Example:\n\n    ```mlir\n    // Dimension expansion i -> (i', j') and (k) -> (k')\n    %b = tensor.expand_shape %a [[0, 1], [2]] output_shape [%sz0, %sz1, 32]\n        : tensor<?x32xf32> into tensor<?x?x32xf32>\n    ```",
    "inputs": [
      { "name": "src", "type": "AnyTensor" },
      { "name": "output_shape", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "reassociation", "type": "IndexListArrayAttr" },
      { "name": "static_output_shape", "type": "DenseI64ArrayAttr" }
    ],
    "assemblyFormat": "$src $reassociation `output_shape`\n    custom<DynamicIndexList>($output_shape, $static_output_shape) attr-dict `:`\n    type($src) `into` type($result)"
  },
  {
    "name": "tensor.extract",
    "summary": "element extraction operation",
    "description": "The `tensor.extract` op reads a ranked tensor and returns one element as\n    specified by the given indices. The result of the op is a value with the\n    same type as the elements of the tensor. The arity of indices must match\n    the rank of the accessed value. All indices should all be of `index` type.\n\n    Example:\n\n    ```mlir\n    %4 = tensor.extract %t[%1, %2] : tensor<4x4xi32>\n    %5 = tensor.extract %rt[%1, %2] : tensor<?x?xi32>\n    ```",
    "inputs": [
      { "name": "tensor", "type": "AnyRankedTensor" },
      { "name": "indices", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyType" }
    ],
    "assemblyFormat": "$tensor `[` $indices `]` attr-dict `:` type($tensor)"
  },
  {
    "name": "tensor.extract_slice",
    "summary": "extract slice operation",
    "description": "The \"extract_slice\" operation extract a tensor from another tensor as\n    specified by the operation's offsets, sizes and strides arguments.\n\n    The extract_slice operation supports the following arguments:\n\n    * source: the \"base\" tensor from which to extract a slice.\n    * offsets: tensor-rank number of offsets into the \"base\" tensor from which\n               to extract the slice.\n    * sizes: tensor-rank number of sizes which specify the sizes of the result\n             tensor type.\n    * strides: tensor-rank number of strides specifying subsampling in each\n               dimension.\n\n    The representation based on offsets, sizes and strides support a\n    partially-static specification via attributes specified through the\n    `static_offsets`, `static_sizes` and `static_strides` arguments. A special\n    sentinel value ShapedType::kDynamic encodes that the corresponding entry has\n    a dynamic value.\n\n    After buffer allocation, the \"extract_slice\" op is expected to lower into a\n    memref.subview op.\n\n    An extract_slice operation may additionally reduce the rank of the resulting\n    tensor by removing dimensions that are statically known to be of size 1.\n    This rank-reduction behavior is not required by the op semantics: this\n    flexibility allows to progressively drop unit dimensions while lowering\n    between different flavors of ops on that operate on tensors.\n\n    #### Verification vs Inference in the rank-reduced case\n\n    Note that there may be multiple ways to infer a resulting rank-reduced type.\n      e.g. 1x6x1 could potentially rank-reduce to either 1x6 or 6x1 2-D shapes.\n\n    To disambiguate, the inference helpers `inferCanonicalRankReducedResultType`\n    only drop the first unit dimensions, in order:\n      e.g. 1x6x1 rank-reduced to 2-D will infer the 6x1 2-D shape, but not 1x6.\n\n    Verification however has access to result type and does not need to infer.\n    The verifier calls `isRankReducedType(getSource(), getResult())` to\n    determine whether the result type is rank-reduced from the source type.\n    This computes a so-called rank-reduction mask, consisting of dropped unit\n    dims, to map the rank-reduced type to the source type by dropping ones:\n      e.g. 1x6 is a rank-reduced version of 1x6x1 by mask {2}\n           6x1 is a rank-reduced version of 1x6x1 by mask {0}\n           1x2x1x4 is a rank-reduced version of 1x1x2x1x1x4x1 by mask {1, 4, 6}\n             (remaining common 1 dimensions are matched eagerly)\n\n    Example:\n\n    ```mlir\n    // Rank-reducing extract_slice.\n    %1 = tensor.extract_slice %0[0, 0, 0][1, 16, 4][1, 1, 1] :\n      tensor<8x16x4xf32> to tensor<16x4xf32>\n    %3 = tensor.extract_slice %2[%o0, 4, %o2][1, %sz1, 1][1, %st1, 1] :\n      tensor<8x16x4xf32> to tensor<1x?xf32>\n    ```",
    "inputs": [
      { "name": "source", "type": "AnyRankedTensor" },
      { "name": "offsets", "type": "Variadic" },
      { "name": "sizes", "type": "Variadic" },
      { "name": "strides", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyRankedTensor" }
    ],
    "attributes": [
      { "name": "static_offsets", "type": "DenseI64ArrayAttr" },
      { "name": "static_sizes", "type": "DenseI64ArrayAttr" },
      { "name": "static_strides", "type": "DenseI64ArrayAttr" }
    ],
    "assemblyFormat": "$source ``\n    custom<DynamicIndexList>($offsets, $static_offsets)\n    custom<DynamicIndexList>($sizes, $static_sizes)\n    custom<DynamicIndexList>($strides, $static_strides)\n    attr-dict `:` type($source) `to` type($result)"
  },
  {
    "name": "tensor.from_elements",
    "summary": "tensor from elements operation.",
    "description": "Create a N-D tensor from a range of same-type arguments. The number of\n    provided `elements` should equal to the number of the elements in the\n    result type. The `elements` correspond to a flattened tensor.\n\n    Example:\n\n    ```mlir\n    tensor.from_elements %a, %b, %c, %d, %e, %f :  tensor<2x3xindex>\n    ```\n\n    will result in a tensor\n\n    [[%a, %b, %c]\n     [%d, %e, %f]]",
    "inputs": [
      { "name": "elements", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyStaticShapeTensor" }
    ],
    "assemblyFormat": "$elements attr-dict `:` type($result)"
  },
  {
    "name": "tensor.gather",
    "summary": "gather a subset of a tensor at specified indices",
    "description": "The `gather` operation extracts a subset of the elements from a `source`\n    tensor at the given indices.\n\n    In its most general form, the tensor of indices specifies all the coordinates\n    of every element to extract (i.e. COO format, without the payload).\n    The indices are expected to be confined to coordinate values that fit the\n    range of the `source` tensor, otherwise the behavior is undefined.\n\n    The leading dimensions of the index tensor give the result tensor its leading\n    dimensions. The trailing dimensions of the result tensor are obtained from\n    the source tensor by omitting the dimensions specified in `gather_dims`\n    (rank-reducing semantics) or setting them to `1` (rank-preserving semantics)\n    (see examples).\n    The trailing dimension of the index tensor contains the coordinates and is\n    expected to have its size equal to the number of dimensions being gathered.\n    This convention allows an idiomatic specification and lowering of \"gathering\n    multiple N-D slices from the source tensor\".\n\n    Note: in the examples below, we separate out the indexing part of the tensor\n    type by a whitespace for readability purposes.\n\n    Example:\n\n    ```mlir\n        // For each 1x2 triple of coordinates in %indices, extract the\n        // element (i.e. 0-D subset) at the coordinates triple in %source.\n        //\n        %out = tensor.gather %source[%indices] gather_dims([0, 1, 2]) :\n          (tensor<4x4x4xf32>, tensor<1x2x 3xindex>) -> tensor<1x2x 1x1x1xf32>\n\n        // Note: result type may be further rank-reduced to tensor<1x2x f32>.\n    ```\n\n    A slice variant is provided to allow specifying whole slices of the source\n    tensor.\n\n    Example:\n\n    ```mlir\n        // For each 5x6 singleton of coordinates in %indices, extract the 2-D\n        // slice %source[*, %indices[...]:%indices[...] + 1, *] with the indices\n        // corresponding to the `gather_dims` attribute specified by %indices.\n        //\n        %out = tensor.gather %source[%indices] gather_dims([1]) :\n          (tensor<3x4x5xf32>, tensor<6x7x 1xindex>) -> tensor<6x7x 3x1x5xf32>\n\n        // Note: result type may be further rank-reduced to tensor<6x7x 3x5xf32>.\n    ```\n\n    The dimensions specified in the gather_dims attribute are ones for which the\n    result tensor has size `1`.\n    I.e. if the source type is `axbxcxd` and the coordinates are [1, 3], then\n    the shape suffix is `ax1xcx1`.\n    Gather also allows rank-reducing semantics where the shape `ax1xcx1` can be\n    further simplified to `axc`.\n\n    The elemental type of the indices tensor can be any integer type.\n    In the absence of target-specific or problem specific information the default\n    type one should use is `index`.\n\n    This operation does not support unranked tensors.\n\n    An optional `unique` unit attribute may be specified to indicate that the\n    coordinates in `indices` are statically guaranteed to be unique at runtime.\n    Incorrectly setting the `unique` attribute when the coordinates are not truly\n    unique is undefined behavior.\n\n    Only full slices are meant to be supported by this op, if one desires\n    partial slices (e.g. strided windows) one should compose this op with other\n    tensor ops (e.g. tensor.extract_slice). This is to avoid a slippery slope of\n    complexity that would make the op unusable in practice.\n\n    At the tensor-level, the index tensor is specified in an AoS form (i.e.\n    coordinate tuple is the most minor). It is the responsibility of further\n    lowerings and bufferization to implement various concrete layouts.\n\n    Note: As currently specified, the operation must lower to an abstraction that\n    performs copies to the output tensor. This is because the buffer type system\n    is currently not rich enough to allow multiple non-contiguous views in the\n    same type. This is visible more clearly in a notional buffer version of the\n    op:\n\n    ```mlir\n        // memref<?x4x1xf32> is a contiguous buffer of ?x4x1 elements.\n        // gather from random source slices must copy to the contiguous output.\n        %out = memref.gather %source[%indices] gather_dims([1]) :\n          (memref<4x4xf32>, memref<?x 1xindex>) -> memref<?x 4x1xf32>\n\n        // Nested buffer support would allow gather to directly index into the\n        // source buffer (i.e. represent a jagged view into the source).\n        %out = memref.gather %source[%indices] gather_dims([1]) :\n          (memref<4x4xf32>, memref<?x 1xindex>) -> memref<? x memref<4x1xf32>>\n    ```",
    "inputs": [
      { "name": "source", "type": "AnyRankedTensor" },
      { "name": "indices", "type": "RankedTensorOf" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyRankedTensor" }
    ],
    "attributes": [
      { "name": "gather_dims", "type": "DenseI64ArrayAttr" },
      { "name": "unique", "type": "UnitAttr" }
    ],
    "assemblyFormat": "$source `[` $indices `]`\n      `gather_dims` `(` $gather_dims `)`\n      (`unique` $unique^)?\n      attr-dict\n    `:` functional-type(operands, results)"
  },
  {
    "name": "tensor.generate",
    "summary": "Creates a dynamically sized tensor from elements",
    "description": "This operation creates a dynamically sized tensor with elements of any type.\n    It expects one index operand per dynamic extent of the result tensor.\n\n    The body region defines the tensor's elements. It takes index operands as\n    its region arguments that span the index space. The element at the given\n    position is yielded with the `yield` operation (see `YieldOp`). There is\n    no defined ordering to the invocations of the body. It is conceptually\n    a \"parallel map\" operation.\n\n    Example:\n\n    ```mlir\n      %tnsr = tensor.generate %m, %n {\n      ^bb0(%i : index, %j : index, %k : index):\n        ...\n        yield %elem : f32\n      } : tensor<?x3x?f32>\n    ```",
    "inputs": [
      { "name": "dynamicExtents", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyRankedTensor" }
    ],
    "assemblyFormat": "$dynamicExtents $body attr-dict `:` type($result)"
  },
  {
    "name": "tensor.insert",
    "summary": "element insertion operation",
    "description": "The `tensor.insert` op inserts a scalar into a ranked tensor `dest` as\n    specified by the operation's indices.\n\n    It returns a copy of `dest` with the indexed position updated to the value\n    of `scalar`.\n\n    The arity of `indices `must match the rank of the tensor `dest`. All\n    indices should be of `index` type.\n\n    Example:\n\n    ```mlir\n    %4 = tensor.insert %t into %dest[%1, %2] : tensor<4x4xi32>\n    %5 = tensor.insert %rt into %dest[%1, %2] : tensor<?x?xi32>\n    ```",
    "inputs": [
      { "name": "scalar", "type": "AnyType" },
      { "name": "dest", "type": "AnyRankedTensor" },
      { "name": "indices", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyRankedTensor" }
    ],
    "assemblyFormat": "$scalar `into` $dest `[` $indices `]` attr-dict `:` type($dest)"
  },
  {
    "name": "tensor.insert_slice",
    "summary": "insert_slice operation",
    "description": "The \"insert_slice\" operation insert a tensor `source` into another\n    tensor `dest` as specified by the operation's offsets, sizes and strides\n    arguments.\n\n    It returns a copy of `dest` with the proper slice updated with the value\n    of `source`.\n\n    The insert_slice operation supports the following arguments:\n\n    * source: the tensor that is inserted.\n    * dest: the tensor into which the source tensor is inserted.\n    * offsets: tensor-rank number of offsets into the `dest` tensor into which\n               the slice is inserted.\n    * sizes: tensor-rank number of sizes which specify the sizes of the source\n             tensor type.\n    * strides: tensor-rank number of strides that specify subsampling in each\n               dimension.\n\n    The representation based on offsets, sizes and strides support a\n    partially-static specification via attributes specified through the\n    `static_offsets`, `static_sizes` and `static_strides` arguments. A special\n    sentinel value ShapedType::kDynamic encodes that the corresponding entry has\n    a dynamic value.\n\n    After buffer allocation, the \"insert_slice\" op is expected to lower into a\n    memref.subview op.\n\n    An insert_slice operation may additionally specify insertion into a tensor\n    of higher rank than the source tensor, along dimensions that are statically\n    known to be of size 1.\n    This rank-altering behavior is not required by the op semantics: this\n    flexibility allows to progressively drop unit dimensions while lowering\n    between different flavors of ops on that operate on tensors.\n    The rank-altering behavior of tensor.insert_slice matches the rank-reducing\n    behavior of tensor.extract_slice.\n\n    #### Verification in the rank-reduced case\n\n    The same verification discussion and mechanisms apply as for ExtractSliceOp.\n    Unlike ExtractSliceOp however, there is no need for a specific inference.\n\n    Example:\n\n    ```mlir\n    // Rank-altering insert_slice.\n    %1 = tensor.insert_slice %t into %0[0, 0, 0][1, 16, 4][1, 1, 1] :\n      tensor<16x4xf32> into tensor<8x16x4xf32>\n    %3 = tensor.insert_slice %tt into %2[%o0, 4, %o2][1, %sz1, 1][1, %st1, 1] :\n      tensor<1x?xf32> into tensor<8x16x4xf32>\n    ```",
    "inputs": [
      { "name": "source", "type": "AnyRankedTensor" },
      { "name": "dest", "type": "AnyRankedTensor" },
      { "name": "offsets", "type": "Variadic" },
      { "name": "sizes", "type": "Variadic" },
      { "name": "strides", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyRankedTensor" }
    ],
    "attributes": [
      { "name": "static_offsets", "type": "DenseI64ArrayAttr" },
      { "name": "static_sizes", "type": "DenseI64ArrayAttr" },
      { "name": "static_strides", "type": "DenseI64ArrayAttr" }
    ],
    "assemblyFormat": "$source `into` $dest ``\n    custom<DynamicIndexList>($offsets, $static_offsets)\n    custom<DynamicIndexList>($sizes, $static_sizes)\n    custom<DynamicIndexList>($strides, $static_strides)\n    attr-dict `:` type($source) `into` type($dest)"
  },
  {
    "name": "tensor.pad",
    "summary": "tensor pad operation",
    "description": "`tensor.pad` is an operation that pads the `source` tensor\n    with given `low` and `high` padding config.\n\n    The PadOp operation supports the following arguments:\n\n    * source: the \"base\" tensor on which to pad.\n    * low: A list contains the padding along the start of each\n           dimension, i.e., how many padded values are prepended\n           to the beginning of the tensor in each dimension.\n    * high: A list contains the padding along the end of each\n            dimension, i.e., how many padded values are appended\n            to the end of the tensor in each dimension.\n    * nofold: indicates that the operation should not be folded when source and\n              result types are equal.\n\n    The result tensor dimensions are `low[i]` + `dim[i]` + `high[i]` for each\n    dimension `i`. The number of elements of `low` and `high` must match the\n    rank of the input tensor. They can be either a constant or a dynamic value.\n\n    The region of the `tensor.pad` operation returns the value to use\n    for the padding. The arguments of the region represent the index\n    of the source being accessed. There should be as many arguments as\n    the rank of the `source` tensor. The value `yield`-ed by the\n    region is used as the value of the view at the given position.\n\n    If `nofold` is set, the padding operation will not be folded away even\n    if the source type and the padded type have the same static shape. This can\n    be used, e.g., for packing or promotion to faster memory.\n\n    Example 1: add 3 zeros to the beginning and 5 zeros to the end of a 1D\n    tensor.\n\n    ```mlir\n      %arg0 = ... : tensor<10xi32>\n      %c0_i32 = arith.constant 0 : i32\n      %padded = tensor.pad %arg0 low[3] high[5] {\n      ^bb0(%arg1: index):\n        tensor.yield %c0_i32 : i32\n      } : tensor<10xi32> to tensor<18xi32>\n    ```\n\n    Example 2: add 1 value to the beginning of dimension 0, 2 values to the end\n    of dimension 0, 2 values to the start of dimension 1, and 3 values to the\n    end of dimension 1.\n\n    ```mlir\n      %pad_value = ... : f32\n      %0 = tensor.pad %0 low[1, 2] high[2, 3] {\n      ^bb0(%arg0 : index, %arg1 : index):\n        tensor.yield %pad_value : f32\n      } : tensor<?x?xf32> to tensor<?x?xf32>\n    ```\n\n    Example 3:\n\n    ```mlir\n      %pad_value = ... : f32\n      %0 = tensor.pad %arg0 low[2, %arg1, 3, 3] high[3, 3, %arg1, 2] {\n      ^bb0(%arg2: index, %arg3: index, %arg4: index, %arg5: index):\n          tensor.yield %pad_value : f32\n      } : tensor<1x2x2x?xf32> to tensor<6x?x?x?xf32>\n    ```\n\n    Example 4:\n\n    ```mlir\n      %pad_value = ... : f32\n      %0 = tensor.pad %arg0 low[0, 0] high[%ub0, %ub1] {\n      ^bb0(%arg1: index, %arg2: index):\n        tensor.yield %pad_value : f32\n      } : tensor<2x3xf32> to tensor<?x?xf32>\n    ```\n\n    Example 5: Force a padded value to be always exist with `nofold`, even\n    though the padding config specifies that no new elements will be added to\n    the tensor.\n\n    ```mlir\n      %pad_value = ... : f32\n      %0 = tensor.pad %arg0 nofold low[0, 0] high[0, 0] {\n      ^bb0(%arg1: index, %arg2: index):\n        tensor.yield %pad_value : f32\n      } : tensor<2x3xf32> to tensor<2x3xf32>\n    ```",
    "inputs": [
      { "name": "source", "type": "AnyRankedTensor" },
      { "name": "low", "type": "Variadic" },
      { "name": "high", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyRankedTensor" }
    ],
    "attributes": [
      { "name": "static_low", "type": "DenseI64ArrayAttr" },
      { "name": "static_high", "type": "DenseI64ArrayAttr" },
      { "name": "nofold", "type": "UnitAttr" }
    ],
    "assemblyFormat": "$source\n    (`nofold` $nofold^)?\n    `low` `` custom<DynamicIndexList>($low, $static_low)\n    `high` `` custom<DynamicIndexList>($high, $static_high)\n    $region attr-dict `:` type($source) `to` type($result)"
  },
  {
    "name": "tensor.parallel_insert_slice",
    "summary": "Specify the tensor slice update of a single thread of a parent\n    InParallelOpInterface op.",
    "description": "The `parallel_insert_slice` yields a subset tensor value to its parent\n    InParallelOpInterface. These subset tensor values are aggregated to\n    in some unspecified order into a full tensor value returned by the parent\n    parallel iterating op.\n    The `parallel_insert_slice` is one such op allowed in the\n    InParallelOpInterface op.\n\n    Conflicting writes result in undefined semantics, in that the indices written\n    to by multiple parallel updates might contain data from any of the updates,\n    or even a malformed bit pattern.\n\n    If an index is updated exactly once, the value contained at that index\n    in the resulting tensor will be equal to the value at a corresponding index\n    of a slice that was used for the updated. If an index is not updated at all,\n    its value will be equal to the one in the original tensor.\n\n    This op does not create a new value, which allows maintaining a clean\n    separation between the subset and full tensor.\n\n    Note that we cannot mark this operation as pure (Pures), even\n    though it has no side effects, because it will get DCEd during\n    canonicalization.\n\n    The parallel_insert_slice operation supports the following arguments:\n\n    * source: the tensor that is inserted.\n    * dest: the tensor into which the source tensor is inserted.\n    * offsets: tensor-rank number of offsets into the `dest` tensor into which\n               the slice is inserted.\n    * sizes: tensor-rank number of sizes which specify the sizes of the source\n             tensor type.\n    * strides: tensor-rank number of strides that specify subsampling in each\n               dimension.\n\n    The representation based on offsets, sizes and strides support a\n    partially-static specification via attributes specified through the\n    `static_offsets`, `static_sizes` and `static_strides` arguments. A special\n    sentinel value ShapedType::kDynamic encodes that the corresponding entry has\n    a dynamic value.\n\n    After buffer allocation, the \"parallel_insert_slice\" op is expected to lower\n    into a memref.subview op.\n\n    A parallel_insert_slice operation may additionally specify insertion into a\n    tensor of higher rank than the source tensor, along dimensions that are\n    statically known to be of size 1.\n    This rank-altering behavior is not required by the op semantics: this\n    flexibility allows to progressively drop unit dimensions while lowering\n    between different flavors of ops on that operate on tensors.\n    The rank-altering behavior of tensor.parallel_insert_slice matches the\n    rank-reducing behavior of tensor.insert_slice and tensor.extract_slice.\n\n    #### Verification in the rank-reduced case\n\n    The same verification discussion and mechanisms apply as for ExtractSliceOp.\n    Unlike ExtractSliceOp however, there is no need for a specific inference.",
    "inputs": [
      { "name": "source", "type": "AnyRankedTensor" },
      { "name": "dest", "type": "AnyRankedTensor" },
      { "name": "offsets", "type": "Variadic" },
      { "name": "sizes", "type": "Variadic" },
      { "name": "strides", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "static_offsets", "type": "DenseI64ArrayAttr" },
      { "name": "static_sizes", "type": "DenseI64ArrayAttr" },
      { "name": "static_strides", "type": "DenseI64ArrayAttr" }
    ],
    "assemblyFormat": "$source `into` $dest ``\n    custom<DynamicIndexList>($offsets, $static_offsets)\n    custom<DynamicIndexList>($sizes, $static_sizes)\n    custom<DynamicIndexList>($strides, $static_strides)\n    attr-dict `:` type($source) `into` type($dest)"
  },
  {
    "name": "tensor.rank",
    "summary": "rank operation",
    "description": "The `tensor.rank` operation takes a tensor operand and returns its rank.\n\n    Example:\n\n    ```mlir\n    %0 = tensor.rank %arg0 : tensor<*xf32>\n    %1 = tensor.rank %arg1 : tensor<?x?xf32>\n    ```",
    "inputs": [
      { "name": "tensor", "type": "AnyTensor" }
    ],
    "assemblyFormat": "$tensor attr-dict `:` type($tensor)"
  },
  {
    "name": "tensor.reshape",
    "summary": "tensor reshape operation",
    "description": "The `reshape` operation converts a tensor from one type to an equivalent\n    type with a provided shape. The source and destination types are compatible\n    if both have the same element type, same number of elements. The following\n    combinations are possible:\n\n    a. Source type is ranked or unranked. Shape argument has static size.\n    Result type is ranked.\n\n    ```mlir\n    // Reshape statically-shaped tensor.\n    %dst = tensor.reshape %src(%shape)\n             : (tensor<4x1xf32>, tensor<1xi32>) -> tensor<4xf32>\n    %dst0 = tensor.reshape %src(%shape0)\n             : (tensor<4x1xf32>, tensor<2xi32>) -> tensor<2x2xf32>\n    // Flatten unranked tensor.\n    %dst = tensor.reshape %src(%shape)\n             : (tensor<*xf32>, tensor<1xi32>) -> tensor<?xf32>\n    ```\n\n    b. Source type is ranked or unranked. Shape argument has dynamic size.\n    Result type is unranked.\n\n    ```mlir\n    // Reshape dynamically-shaped 1D tensor.\n    %dst = tensor.reshape %src(%shape)\n             : (tensor<?xf32>, tensor<?xi32>) -> tensor<*xf32>\n    // Reshape unranked tensor.\n    %dst = tensor.reshape %src(%shape)\n             : (tensor<*xf32>, tensor<?xi32>) -> tensor<*xf32>\n    ```",
    "inputs": [
      { "name": "source", "type": "AnyTensor" },
      { "name": "shape", "type": "TensorRankOf" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTensor" }
    ],
    "assemblyFormat": "$source `(` $shape `)` attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "tensor.scatter",
    "summary": "scatter a tensor into a destination tensor at specified indices",
    "description": "The `scatter` operation inserts a `source` tensor into a `dest` tensor at\n    the given indices.\n\n    In its most general form, the tensor of indices specifies all the coordinates\n    of every element to insert (i.e. COO format, without the payload).\n    The indices are expected to be confined to coordinate values that fit the\n    range of the `dest` tensor, otherwise the behavior is undefined.\n\n    The leading dimensions of the index tensor must match that of the dest\n    tensor. The trailing dimensions of the dest tensor must match those of the\n    source tensor by omitting the dimensions specified in scatter_dims\n    (rank-reducing semantics) or setting them to `1` (rank-preserving semantics)\n    (see examples).\n    This convention allows an idiomatic specification and lowering of\n    \"scattering multiple N-D slices into the dest tensor\".\n    The result type must match the type of the dest tensor.\n\n    Note: in the examples below, we separate out the indexing part of the tensor\n    type by a whitespace for readability purposes.\n\n    Example:\n\n    ```mlir\n        // For each 1x2 triple of coordinates in %indices, insert the\n        // element (i.e. 0-D subset) at the coordinates triple in %dest.\n        //\n        %out = tensor.scatter %source into %dest[%indices]\n            scatter_dims([0, 1, 2]) unique :\n          (tensor<1x2x 1x1x1xf32>, tensor<4x4x4xf32>, tensor<1x2x 3xindex>)\n            -> tensor<4x4x4xf32>\n\n        // Note: source type may be further rank-reduced to tensor<1x2x f32>.\n    ```\n\n    A slice variant is provided to allow specifying insertion of whole tensor\n    slices into the `dest` tensor.\n\n    Example:\n\n    ```mlir\n        // For each 3 singleton of coordinates in %indices, insert the 2-D\n        // slice into %dest[*, %indices[...]:%indices[...] + 1, *] with the\n        // indices corresponding to the scatter_dims attribute specified by\n        // %indices.\n        //\n        %out = tensor.scatter %source into %dest[%indices] scatter_dims([1]) unique :\n          (tensor<3x 4x1x6xf32>, tensor<4x5x6xf32>, tensor<3x 1xindex>)\n            -> tensor<4x5x6xf32>\n    ```\n\n    The dimensions specified in the scatter_dims attribute are ones for which the\n    source tensor has size `1`.\n    I.e. if the dest type is `axbxcxd` and the coordinates are [1, 3], then\n    the source type suffix is `ax1xcx1`.\n    Scatter also allows rank-reducing semantics where the shape `ax1xcx1` can be\n    further simplified to `axc`.\n\n    The elemental type of the indices tensor can be any integer type.\n    In the absence of target-specific or problem specific information the default\n    type one should use is `index`.\n\n    This operation does not support unranked tensors.\n\n    A `unique` unit attribute must be be specified to indicate that the\n    coordinates are statically guaranteed to be unique at runtime. If coordinates\n    are not truly unique at runtime, the behavior is undefined.\n\n    Only full slices are meant to be supported by this op, if one desires\n    partial slices (e.g. strided windows) one should compose this op with other\n    tensor ops (e.g. tensor.insert_slice). This is to avoid a slippery slope of\n    complexity that would make the op unusable in practice.\n\n    At the tensor-level, the index tensor is specified in an AoS form (i.e.\n    coordinate tuple is the most minor). It is the responsibility of further\n    lowerings and bufferization to implement various concrete layouts.\n\n    Note: As currently specified, the operation must lower to an abstraction that\n    performs copies to the output tensor. This is because the buffer type system\n    is currently not rich enough to allow multiple non-contiguous views in the\n    same type. This is visible more clearly in a notional buffer version of the\n    op:\n\n    ```mlir\n        // memref<?x 4xf32> is a contiguous buffer of ?x4 elements, scatter into\n        // random dest slices must copy to the contiguous dest.\n        //\n        some_side_effecting_op_writing_into %source, ...: memref<3x 4xf32>\n        memref.scatter %source into %dest[%indices] scatter_dims([1]) unique :\n          (memref<3x 4xf32>, memref<?x 4xf32>, memref<?x 1xindex>)\n\n        // Nested buffer support in the producing op would allow writing directly\n        // into the dest buffer.\n        %v = some_nested_buffer_view_op %dest[%indices] scatter_dims([1]) unique :\n          memref<? x memref<4xf32>>\n        some_side_effecting_op_writing_into %v, ...: memref<? x memref<4xf32>>\n    ```",
    "inputs": [
      { "name": "source", "type": "AnyRankedTensor" },
      { "name": "dest", "type": "AnyRankedTensor" },
      { "name": "indices", "type": "RankedTensorOf" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyRankedTensor" }
    ],
    "attributes": [
      { "name": "scatter_dims", "type": "DenseI64ArrayAttr" },
      { "name": "unique", "type": "UnitAttr" }
    ],
    "assemblyFormat": "$source `into` $dest `[` $indices `]`\n      `scatter_dims` `(` $scatter_dims `)`\n      (`unique` $unique^)?\n      attr-dict\n    `:` functional-type(operands, results)"
  },
  {
    "name": "tensor.splat",
    "summary": "tensor splat or broadcast operation",
    "description": "Broadcast the operand to all elements of the result tensor.\n\n    An additional argument of type `index` must be provided for each dynamic\n    dimension present in the result type.\n\n    Example for a statically shaped tensor:\n\n    ```mlir\n    %s = arith.constant 1.0 : f32\n    %t = tensor.splat %s : tensor<8x16xf32>\n    ```\n\n    Example for a tensor containing dynamic dimensions:\n\n    ```mlir\n    // Broadcasts %s to a 3D dynamically shaped tensor, with %m and %n binding\n    // to dimensions 0 and 2 of the resulting tensor, respectively.\n    %m = arith.constant 10 : index\n    %n = arith.constant 30 : index\n    %t = tensor.splat %s[%m, %n] : tensor<?x20x?xf32>\n    ```",
    "inputs": [
      { "name": "input", "type": "AnyType" },
      { "name": "dynamicSizes", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "aggregate", "type": "AnyRankedTensor" }
    ],
    "assemblyFormat": "$input (`[` $dynamicSizes^ `]`)? attr-dict `:` type($aggregate)"
  },
  {
    "name": "tensor.yield",
    "summary": "Yield a value from a region",
    "description": "This operation is used to yield a single value from a within a region. It\n     is used to create dynamically sized tensors\n     (see `tensor.generate` and `tensor.pad` ops).",
    "inputs": [
      { "name": "value", "type": "AnyType" }
    ],
    "assemblyFormat": "$value attr-dict `:` type($value)"
  },
  {
    "name": "tf._ArrayToList",
    "summary": "Converts an array of tensors to a list of tensors.",
    "inputs": [
      { "name": "input", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "output", "type": "Variadic" }
    ]
  },
  {
    "name": "tf._EagerConst",
    "inputs": [
      { "name": "input", "type": "TF_Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "TF_Tensor" }
    ]
  },
  {
    "name": "tf._FusedBatchNormEx",
    "summary": "Internal FusedBatchNorm operation: reserved for internal use.",
    "description": "Do not invoke this operator directly in Python. A fusion optimization is\nexpected to create these operators.",
    "inputs": [
      { "name": "x", "type": "TensorOf" },
      { "name": "scale", "type": "TF_Float32Tensor" },
      { "name": "offset", "type": "TF_Float32Tensor" },
      { "name": "mean", "type": "TF_Float32Tensor" },
      { "name": "variance", "type": "TF_Float32Tensor" },
      { "name": "side_input", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "y", "type": "TensorOf" },
      { "name": "batch_mean", "type": "TF_Float32Tensor" },
      { "name": "batch_variance", "type": "TF_Float32Tensor" },
      { "name": "reserve_space_1", "type": "TF_Float32Tensor" },
      { "name": "reserve_space_2", "type": "TF_Float32Tensor" },
      { "name": "reserve_space_3", "type": "TF_Float32Tensor" }
    ],
    "attributes": [
      { "name": "epsilon", "type": "DefaultValuedOptionalAttr" },
      { "name": "exponential_avg_factor", "type": "DefaultValuedOptionalAttr" },
      { "name": "activation_mode", "type": "DefaultValuedOptionalAttr" },
      { "name": "data_format", "type": "DefaultValuedOptionalAttr" },
      { "name": "is_training", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf._FusedConv2D",
    "summary": "Performs a convolution followed by a specified series of operations.",
    "description": "The inputs to the convolution are `input` and `filter`. The series of operations\nthat follows is specified by the `fused_ops` attribute, which is a list of TF op\nnames specified as strings (e.g. \"Relu\"). They are performed in order, where the\n(first) input to each op is the output of the preceding op. The first input and\nthe output of each fused_op must be of type T.\n\nCurrently supported fused_op combinations are: [X] and [X,A], where X is one of\n{\"BiasAdd\",\"FusedBatchNorm\"} and A is one of {\"Elu\",\"Relu\",\"Relu6\"}.\n\n* The first input to op X is the Conv2D result, and the additional input(s) to X\nare specified by `args`.\n* If there is an op A specified, the output of op X is the input to op A, and op\nA produces the _FusedConv2D output. Otherwise, op X produces the _FusedConv2D\noutput.\n\n*NOTE*: Do not invoke this operator directly in Python. Grappler is expected to\ncreate these operators.",
    "inputs": [
      { "name": "input", "type": "TensorOf" },
      { "name": "filter", "type": "TensorOf" },
      { "name": "args", "type": "Variadic" },
      { "name": "host_args", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "output", "type": "TensorOf" }
    ],
    "attributes": [
      { "name": "num_args", "type": "ConfinedAttr" },
      { "name": "strides", "type": "I64ArrayAttr" },
      { "name": "padding", "type": "TF_AnyStrAttrOf" },
      { "name": "explicit_paddings", "type": "DefaultValuedOptionalAttr" },
      { "name": "data_format", "type": "DefaultValuedOptionalAttr" },
      { "name": "filter_format", "type": "DefaultValuedOptionalAttr" },
      { "name": "dilations", "type": "DefaultValuedOptionalAttr" },
      { "name": "use_cudnn_on_gpu", "type": "DefaultValuedOptionalAttr" },
      { "name": "fused_ops", "type": "DefaultValuedOptionalAttr" },
      { "name": "epsilon", "type": "DefaultValuedOptionalAttr" },
      { "name": "leakyrelu_alpha", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf._FusedMatMul",
    "summary": "Performs a MatMul followed by a specified series of operations.",
    "description": "The inputs to the MatMul are specified by `a` and `b`. The series of operations\nthat follows is specified by the `fused_ops` attribute, which is a list of TF op\nnames specified as strings (e.g. \"Relu\"). They are performed in order, where the\n(first) input to each op is the output of the preceding op. The first input and\nthe output of each fused_op must be of type T.\n\nCurrently supported fused_op combinations are: [\"BiasAdd\"] and [\"BiasAdd\",A],\nwhere A is one of {\"Elu\",\"Relu\",\"Relu6\"}.\n\n* The first input to BiasAdd is the MatMul result, and the additional BiasAdd\ninput is specified by `args`.\n* If there is an op A specified, the output of the BiasAdd is the input to op A,\nand op A produces the _FusedConv2D output. Otherwise, the BiasAdd produces the\n_FusedConv2D output.\n\n*NOTE*: Do not invoke this operator directly in Python. Grappler is\nexpected to create these operators.",
    "inputs": [
      { "name": "a", "type": "TensorOf" },
      { "name": "b", "type": "TensorOf" },
      { "name": "args", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "product", "type": "TensorOf" }
    ],
    "attributes": [
      { "name": "transpose_a", "type": "DefaultValuedOptionalAttr" },
      { "name": "transpose_b", "type": "DefaultValuedOptionalAttr" },
      { "name": "fused_ops", "type": "DefaultValuedOptionalAttr" },
      { "name": "epsilon", "type": "DefaultValuedOptionalAttr" },
      { "name": "leakyrelu_alpha", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf._HostRecv",
    "summary": "Receives the named tensor from send_device on recv_device.",
    "description": "_HostRecv produces its output on host memory whereas _Recv produces its\noutput on device memory.",
    "outputs": [
      { "name": "tensor", "type": "Res" }
    ],
    "attributes": [
      { "name": "tensor_name", "type": "StrAttr" },
      { "name": "send_device", "type": "StrAttr" },
      { "name": "send_device_incarnation", "type": "I64Attr" },
      { "name": "recv_device", "type": "StrAttr" },
      { "name": "client_terminated", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf._HostSend",
    "summary": "Sends the named tensor from send_device to recv_device.",
    "description": "_HostSend requires its input on host memory whereas _Send requires its\ninput on device memory.",
    "inputs": [
      { "name": "tensor", "type": "Arg" }
    ],
    "attributes": [
      { "name": "tensor_name", "type": "StrAttr" },
      { "name": "send_device", "type": "StrAttr" },
      { "name": "send_device_incarnation", "type": "I64Attr" },
      { "name": "recv_device", "type": "StrAttr" },
      { "name": "client_terminated", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf._InternalTestMustExecuteTrait_",
    "summary": "Internal op for testing only"
  },
  {
    "name": "tf._InternalTestNonResourceValueSideEffects_",
    "summary": "Internal op for testing only",
    "inputs": [
      { "name": "key", "type": "Arg" }
    ]
  },
  {
    "name": "tf._ListToArray",
    "summary": "Converts a list of tensors to an array of tensors.",
    "inputs": [
      { "name": "input", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "output", "type": "Variadic" }
    ]
  },
  {
    "name": "tf._Recv",
    "summary": "Receives the named tensor from send_device on recv_device.",
    "outputs": [
      { "name": "tensor", "type": "Res" }
    ],
    "attributes": [
      { "name": "tensor_name", "type": "StrAttr" },
      { "name": "send_device", "type": "StrAttr" },
      { "name": "send_device_incarnation", "type": "I64Attr" },
      { "name": "recv_device", "type": "StrAttr" },
      { "name": "client_terminated", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf._Send",
    "summary": "Sends the named tensor from send_device to recv_device.",
    "inputs": [
      { "name": "tensor", "type": "Arg" }
    ],
    "attributes": [
      { "name": "tensor_name", "type": "StrAttr" },
      { "name": "send_device", "type": "StrAttr" },
      { "name": "send_device_incarnation", "type": "I64Attr" },
      { "name": "recv_device", "type": "StrAttr" },
      { "name": "client_terminated", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf._TPUCompileMlir",
    "summary": "Compiles a computations for execution on one or more TPU devices.",
    "description": "For the internal use of the distributed TPU compiler.\n\n'mlir_module' is a serialized MLIR module with a `main` function that contains\ntarget computation.\n'dynamic_shapes' contains dynamic shapes of arguments whose shapes were not\nknown statically at TPUReplication rewrite time.\n'metadata' is a serialized TPUCompileMetadataProto describing the shapes and\ntypes of the inputs to the computation, as well as a mapping onto the TPU pod\ntopology.\n'program' output is a string key that is passed to the TPUExecute op and used to\nlook up the program in the compilation cache.",
    "inputs": [
      { "name": "dynamic_shapes", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "compilation_status", "type": "TF_StrTensor" },
      { "name": "program", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "mlir_module", "type": "DefaultValuedOptionalAttr" },
      { "name": "metadata", "type": "StrAttr" }
    ]
  },
  {
    "name": "tf._TPUDeviceOrdinalPlaceholder",
    "summary": "Placeholder for a device ordinal that depends on its tf_device.replicate ancestor.",
    "description": "This op must have a tf_device.replicate ancestor. The ancestor replica_id and\nlogical_core attribute correspond to a TPU core. This op maps the TPU core to a\ndevice_ordinal, where the device ordinal is the index of the core relative to\nits host.\n\nThe replicate_to_island pass removes and flattens tf_device.replicate, so it\nconverts this op to the constant index of the core relative to its host.",
    "outputs": [
      { "name": "device_ordinal", "type": "TF_Int64Tensor" }
    ],
    "attributes": [
      { "name": "logical_core", "type": "I64Attr" }
    ]
  },
  {
    "name": "tf._UnaryOpsComposition",
    "summary": "*NOTE*: Do not invoke this operator directly in Python. Graph rewrite pass is",
    "description": "expected to create these operators.",
    "inputs": [
      { "name": "x", "type": "TensorOf" }
    ],
    "outputs": [
      { "name": "y", "type": "TensorOf" }
    ],
    "attributes": [
      { "name": "op_names", "type": "StrArrayAttr" }
    ]
  },
  {
    "name": "tf._XlaCompile",
    "summary": "XLA Compile Op. For use by the XLA JIT only.",
    "description": "Compiles a TensorFlow function into an XLA LocalExecutable and returns a key\nthat _XlaRun can use to look up the LocalExecutable and execute it.",
    "inputs": [
      { "name": "constants", "type": "Variadic" },
      { "name": "args", "type": "Variadic" },
      { "name": "resources", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "key", "type": "Res" },
      { "name": "compilation_successful", "type": "Res" }
    ],
    "attributes": [
      { "name": "must_compile", "type": "BoolAttr" },
      { "name": "function", "type": "SymbolRefAttr" }
    ]
  },
  {
    "name": "tf._XlaCompileMlirPlaceholderProgramKey",
    "summary": "Placeholder program key (compilation cache key) of a XLA `program`.",
    "description": "This op can be used when certain rewrite passes materialize ops that require a\nprogram key but the _TPUCompileMlir or _XlaCompile op has not been added yet.\nSubsequent rewrite passes must replace this op with `program` output.",
    "outputs": [
      { "name": "program", "type": "TF_StrTensor" }
    ]
  },
  {
    "name": "tf._XlaHostComputeMlir",
    "summary": "A pseudo-op to represent host-side computation in an XLA program.",
    "inputs": [
      { "name": "inputs", "type": "Arg" }
    ],
    "outputs": [
      { "name": "outputs", "type": "Res" }
    ],
    "attributes": [
      { "name": "send_key", "type": "StrAttr" },
      { "name": "recv_key", "type": "StrAttr" },
      { "name": "host_mlir_module", "type": "DefaultValuedOptionalAttr" },
      { "name": "manual_sharding", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf._XlaRecvAtHost",
    "summary": "A placeholder op to receive values from a running XLA computation.",
    "inputs": [
      { "name": "dynamic_key", "type": "Arg" }
    ],
    "outputs": [
      { "name": "outputs", "type": "Res" }
    ],
    "attributes": [
      { "name": "key", "type": "StrAttr" },
      { "name": "device_ordinal", "type": "I64Attr" },
      { "name": "device_type", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf._XlaRecvAtHostV2",
    "summary": "A placeholder op to receive values from a running XLA computation with support for a runtime device ordinal.",
    "inputs": [
      { "name": "dynamic_key", "type": "Arg" },
      { "name": "device_ordinal", "type": "Arg" }
    ],
    "outputs": [
      { "name": "outputs", "type": "Res" }
    ],
    "attributes": [
      { "name": "key", "type": "StrAttr" },
      { "name": "device_type", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf._XlaRun",
    "summary": "XLA Run Op. For use by the XLA JIT only.",
    "description": "Executes a TensorFlow function previously compiled into a LocalExecutable by an\n_XlaCompile op.",
    "inputs": [
      { "name": "args", "type": "Variadic" },
      { "name": "key", "type": "TF_StrTensor" }
    ],
    "outputs": [
      { "name": "results", "type": "Variadic" }
    ]
  },
  {
    "name": "tf._XlaSendFromHost",
    "summary": "A placeholder op to send values to a running XLA computation.",
    "inputs": [
      { "name": "inputs", "type": "Arg" },
      { "name": "dynamic_key", "type": "Arg" }
    ],
    "attributes": [
      { "name": "key", "type": "StrAttr" },
      { "name": "device_ordinal", "type": "I64Attr" },
      { "name": "device_type", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf._XlaSendFromHostV2",
    "summary": "A placeholder op to send values to a running XLA computation with support for a runtime device ordinal.",
    "inputs": [
      { "name": "inputs", "type": "Arg" },
      { "name": "dynamic_key", "type": "Arg" },
      { "name": "device_ordinal", "type": "Arg" }
    ],
    "attributes": [
      { "name": "key", "type": "StrAttr" },
      { "name": "device_type", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.Abs",
    "summary": "Computes the absolute value of a tensor.",
    "description": "Given a tensor `x`, this operation returns a tensor containing the absolute\nvalue of each element in `x`. For example, if x is an input element and y is\nan output element, this operation computes \\\\(y = |x|\\\\).",
    "inputs": [
      { "name": "x", "type": "TensorOf" }
    ],
    "outputs": [
      { "name": "y", "type": "TensorOf" }
    ]
  },
  {
    "name": "tf.Acos",
    "summary": "Computes acos of x element-wise.",
    "description": "Provided an input tensor, the `tf.math.acos` operation returns the inverse cosine of each element of the tensor. If `y = tf.math.cos(x)` then, `x = tf.math.acos(y)`.\n\n  Input range is `[-1, 1]` and the output has a range of `[0, pi]`.",
    "inputs": [
      { "name": "x", "type": "TF_FpOrComplexTensor" }
    ],
    "outputs": [
      { "name": "y", "type": "TF_FpOrComplexTensor" }
    ]
  },
  {
    "name": "tf.Acosh",
    "summary": "Computes inverse hyperbolic cosine of x element-wise.",
    "description": "Given an input tensor, the function computes inverse hyperbolic cosine of every element.\nInput range is `[1, inf]`. It returns `nan` if the input lies outside the range.\n\n```python\nx = tf.constant([-2, -0.5, 1, 1.2, 200, 10000, float(\"inf\")])\ntf.math.acosh(x) ==> [nan nan 0. 0.62236255 5.9914584 9.903487 inf]\n```",
    "inputs": [
      { "name": "x", "type": "TF_FpOrComplexTensor" }
    ],
    "outputs": [
      { "name": "y", "type": "TF_FpOrComplexTensor" }
    ]
  },
  {
    "name": "tf.Add",
    "summary": "Returns x + y element-wise.",
    "description": "*NOTE*: `Add` supports broadcasting. `AddN` does not. More about broadcasting\n[here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n\nGiven two input tensors, the `tf.add` operation computes the sum for every element in the tensor.\n\nBoth input and output have a range `(-inf, inf)`.",
    "inputs": [
      { "name": "x", "type": "TF_NumberNotQuantizedOrStrTensor" },
      { "name": "y", "type": "TF_NumberNotQuantizedOrStrTensor" }
    ],
    "outputs": [
      { "name": "z", "type": "TF_NumberNotQuantizedOrStrTensor" }
    ]
  },
  {
    "name": "tf.AddN",
    "summary": "Add all input tensors element wise.",
    "description": "Inputs must be of same size and shape.\n\n  ```python\n  x = [9, 7, 10]\n  tf.math.add_n(x) ==> 26\n  ```",
    "inputs": [
      { "name": "inputs", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "sum", "type": "TensorOf" }
    ]
  },
  {
    "name": "tf.AddV2",
    "summary": "Returns x + y element-wise.",
    "description": "*NOTE*: `Add` supports broadcasting. `AddN` does not. More about broadcasting\n[here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)",
    "inputs": [
      { "name": "x", "type": "TensorOf" },
      { "name": "y", "type": "TensorOf" }
    ],
    "outputs": [
      { "name": "z", "type": "TensorOf" }
    ]
  },
  {
    "name": "tf.AdjustContrastv2",
    "summary": "Adjust the contrast of one or more images.",
    "description": "`images` is a tensor of at least 3 dimensions.  The last 3 dimensions are\ninterpreted as `[height, width, channels]`.  The other dimensions only\nrepresent a collection of images, such as `[batch, height, width, channels].`\n\nContrast is adjusted independently for each channel of each image.\n\nFor each channel, the Op first computes the mean of the image pixels in the\nchannel and then adjusts each component of each pixel to\n`(x - mean) * contrast_factor + mean`.",
    "inputs": [
      { "name": "images", "type": "Arg" },
      { "name": "contrast_factor", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ]
  },
  {
    "name": "tf.AdjustHue",
    "summary": "Adjust the hue of one or more images.",
    "description": "`images` is a tensor of at least 3 dimensions.  The last dimension is\ninterpreted as channels, and must be three.\n\nThe input image is considered in the RGB colorspace. Conceptually, the RGB\ncolors are first mapped into HSV. A delta is then applied all the hue values,\nand then remapped back to RGB colorspace.",
    "inputs": [
      { "name": "images", "type": "Arg" },
      { "name": "delta", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ]
  },
  {
    "name": "tf.AdjustSaturation",
    "summary": "Adjust the saturation of one or more images.",
    "description": "`images` is a tensor of at least 3 dimensions.  The last dimension is\ninterpreted as channels, and must be three.\n\nThe input image is considered in the RGB colorspace. Conceptually, the RGB\ncolors are first mapped into HSV. A scale is then applied all the saturation\nvalues, and then remapped back to RGB colorspace.",
    "inputs": [
      { "name": "images", "type": "Arg" },
      { "name": "scale", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ]
  },
  {
    "name": "tf.All",
    "summary": "Computes the \"logical and\" of elements across dimensions of a tensor.",
    "description": "Reduces `input` along the dimensions given in `axis`. Unless\n`keep_dims` is true, the rank of the tensor is reduced by 1 for each entry in\n`axis`. If `keep_dims` is true, the reduced dimensions are\nretained with length 1.",
    "inputs": [
      { "name": "input", "type": "Arg" },
      { "name": "reduction_indices", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "keep_dims", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.AllToAll",
    "summary": "An Op to exchange data across TPU replicas.",
    "description": "On each replica, the input is split into `split_count` blocks along\n`split_dimension` and send to the other replicas given group_assignment. After\nreceiving `split_count` - 1 blocks from other replicas, we concatenate the\nblocks along `concat_dimension` as the output.\n\nFor example, suppose there are 2 TPU replicas:\nreplica 0 receives input: `[[A, B]]`\nreplica 1 receives input: `[[C, D]]`\n\ngroup_assignment=`[[0, 1]]`\nconcat_dimension=0\nsplit_dimension=1\nsplit_count=2\n\nreplica 0's output: `[[A], [C]]`\nreplica 1's output: `[[B], [D]]`",
    "inputs": [
      { "name": "input", "type": "Arg" },
      { "name": "group_assignment", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "concat_dimension", "type": "I64Attr" },
      { "name": "split_dimension", "type": "I64Attr" },
      { "name": "split_count", "type": "I64Attr" }
    ]
  },
  {
    "name": "tf.Angle",
    "summary": "Returns the argument of a complex number.",
    "description": "Given a tensor `input` of complex numbers, this operation returns a tensor of\ntype `float` that is the argument of each element in `input`. All elements in\n`input` must be complex numbers of the form \\\\(a + bj\\\\), where *a*\nis the real part and *b* is the imaginary part.\n\nThe argument returned by this operation is of the form \\\\(atan2(b, a)\\\\).\n\nFor example:\n\n```\n# tensor 'input' is [-2.25 + 4.75j, 3.25 + 5.75j]\ntf.math.angle(input) ==> [2.0132, 1.056]\n```\n\n@compatibility(numpy)\nEquivalent to np.angle.\n@end_compatibility",
    "inputs": [
      { "name": "input", "type": "TensorOf" }
    ],
    "outputs": [
      { "name": "output", "type": "TF_F32OrF64Tensor" }
    ]
  },
  {
    "name": "tf.AnonymousIterator",
    "summary": "A container for an iterator resource.",
    "outputs": [
      { "name": "handle", "type": "Res" }
    ],
    "attributes": [
      { "name": "output_types", "type": "ConfinedAttr" },
      { "name": "output_shapes", "type": "ConfinedAttr" }
    ]
  },
  {
    "name": "tf.AnonymousIteratorV2",
    "summary": "A container for an iterator resource.",
    "outputs": [
      { "name": "handle", "type": "Res" },
      { "name": "deleter", "type": "Res" }
    ],
    "attributes": [
      { "name": "output_types", "type": "ConfinedAttr" },
      { "name": "output_shapes", "type": "ConfinedAttr" }
    ]
  },
  {
    "name": "tf.AnonymousIteratorV3",
    "summary": "A container for an iterator resource.",
    "outputs": [
      { "name": "handle", "type": "Res" }
    ],
    "attributes": [
      { "name": "output_types", "type": "ConfinedAttr" },
      { "name": "output_shapes", "type": "ConfinedAttr" }
    ]
  },
  {
    "name": "tf.AnonymousMemoryCache",
    "outputs": [
      { "name": "handle", "type": "Res" },
      { "name": "deleter", "type": "TF_VariantTensor" }
    ]
  },
  {
    "name": "tf.AnonymousMultiDeviceIterator",
    "summary": "A container for a multi device iterator resource.",
    "outputs": [
      { "name": "handle", "type": "Res" },
      { "name": "deleter", "type": "Res" }
    ],
    "attributes": [
      { "name": "devices", "type": "ConfinedAttr" },
      { "name": "output_types", "type": "ConfinedAttr" },
      { "name": "output_shapes", "type": "ConfinedAttr" }
    ]
  },
  {
    "name": "tf.AnonymousMultiDeviceIteratorV3",
    "summary": "A container for a multi device iterator resource.",
    "outputs": [
      { "name": "handle", "type": "Res" }
    ],
    "attributes": [
      { "name": "devices", "type": "ConfinedAttr" },
      { "name": "output_types", "type": "ConfinedAttr" },
      { "name": "output_shapes", "type": "ConfinedAttr" }
    ]
  },
  {
    "name": "tf.AnonymousRandomSeedGenerator",
    "inputs": [
      { "name": "seed", "type": "TF_Int64Tensor" },
      { "name": "seed2", "type": "TF_Int64Tensor" }
    ],
    "outputs": [
      { "name": "handle", "type": "Res" },
      { "name": "deleter", "type": "TF_VariantTensor" }
    ]
  },
  {
    "name": "tf.AnonymousSeedGenerator",
    "inputs": [
      { "name": "seed", "type": "TF_Int64Tensor" },
      { "name": "seed2", "type": "TF_Int64Tensor" },
      { "name": "reshuffle", "type": "TF_BoolTensor" }
    ],
    "outputs": [
      { "name": "handle", "type": "Res" },
      { "name": "deleter", "type": "TF_VariantTensor" }
    ]
  },
  {
    "name": "tf.Any",
    "summary": "Computes the \"logical or\" of elements across dimensions of a tensor.",
    "description": "Reduces `input` along the dimensions given in `axis`. Unless\n`keep_dims` is true, the rank of the tensor is reduced by 1 for each entry in\n`axis`. If `keep_dims` is true, the reduced dimensions are\nretained with length 1.",
    "inputs": [
      { "name": "input", "type": "Arg" },
      { "name": "reduction_indices", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "keep_dims", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.ApproximateEqual",
    "summary": "Returns the truth value of abs(x-y) < tolerance element-wise.",
    "inputs": [
      { "name": "x", "type": "TF_NumberTensor" },
      { "name": "y", "type": "TF_NumberTensor" }
    ],
    "outputs": [
      { "name": "z", "type": "TF_BoolTensor" }
    ],
    "attributes": [
      { "name": "tolerance", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.ApproxTopK",
    "summary": "Returns min/max k values and their indices of the input operand in an approximate manner.",
    "description": "See https://arxiv.org/abs/2206.14286 for the algorithm details.\nThis op is only optimized on TPU currently.",
    "inputs": [
      { "name": "input", "type": "Arg" }
    ],
    "outputs": [
      { "name": "values", "type": "Res" },
      { "name": "indices", "type": "Res" }
    ],
    "attributes": [
      { "name": "k", "type": "ConfinedAttr" },
      { "name": "reduction_dimension", "type": "DefaultValuedOptionalAttr" },
      { "name": "recall_target", "type": "DefaultValuedOptionalAttr" },
      { "name": "is_max_k", "type": "DefaultValuedOptionalAttr" },
      { "name": "reduction_input_size_override", "type": "DefaultValuedOptionalAttr" },
      { "name": "aggregate_to_topk", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.ArgMax",
    "summary": "Returns the index with the largest value across dimensions of a tensor.",
    "description": "Note that in case of ties the identity of the return value is not guaranteed.\n\nUsage:\n  ```python\n  import tensorflow as tf\n  a = [1, 10, 26.9, 2.8, 166.32, 62.3]\n  b = tf.math.argmax(input = a)\n  c = tf.keras.backend.eval(b)\n  # c = 4\n  # here a[4] = 166.32 which is the largest element of a across axis 0\n  ```",
    "inputs": [
      { "name": "input", "type": "TensorOf" },
      { "name": "dimension", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "TensorOf" }
    ]
  },
  {
    "name": "tf.ArgMin",
    "summary": "Returns the index with the smallest value across dimensions of a tensor.",
    "description": "Note that in case of ties the identity of the return value is not guaranteed.\n\nUsage:\n  ```python\n  import tensorflow as tf\n  a = [1, 10, 26.9, 2.8, 166.32, 62.3]\n  b = tf.math.argmin(input = a)\n  c = tf.keras.backend.eval(b)\n  # c = 0\n  # here a[0] = 1 which is the smallest element of a across axis 0\n  ```",
    "inputs": [
      { "name": "input", "type": "TensorOf" },
      { "name": "dimension", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "TF_I32OrI64Tensor" }
    ]
  },
  {
    "name": "tf.Asin",
    "summary": "Computes the trignometric inverse sine of x element-wise.",
    "description": "The `tf.math.asin` operation returns the inverse of `tf.math.sin`, such that\nif `y = tf.math.sin(x)` then, `x = tf.math.asin(y)`.\n\n**Note**: The output of `tf.math.asin` will lie within the invertible range\nof sine, i.e [-pi/2, pi/2].\n\nFor example:\n\n```python\n# Note: [1.047, 0.785] ~= [(pi/3), (pi/4)]\nx = tf.constant([1.047, 0.785])\ny = tf.math.sin(x) # [0.8659266, 0.7068252]\n\ntf.math.asin(y) # [1.047, 0.785] = x\n```",
    "inputs": [
      { "name": "x", "type": "TF_FpOrComplexTensor" }
    ],
    "outputs": [
      { "name": "y", "type": "TF_FpOrComplexTensor" }
    ]
  },
  {
    "name": "tf.Asinh",
    "summary": "Computes inverse hyperbolic sine of x element-wise.",
    "description": "Given an input tensor, this function computes inverse hyperbolic sine\n  for every element in the tensor. Both input and output has a range of\n  `[-inf, inf]`.\n\n  ```python\n  x = tf.constant([-float(\"inf\"), -2, -0.5, 1, 1.2, 200, 10000, float(\"inf\")])\n  tf.math.asinh(x) ==> [-inf -1.4436355 -0.4812118 0.8813736 1.0159732 5.991471 9.903487 inf]\n  ```",
    "inputs": [
      { "name": "x", "type": "TF_FpOrComplexTensor" }
    ],
    "outputs": [
      { "name": "y", "type": "TF_FpOrComplexTensor" }
    ]
  },
  {
    "name": "tf.Assert",
    "summary": "Asserts that the given condition is true.",
    "description": "If `condition` evaluates to false, print the list of tensors in `data`.\n`summarize` determines how many entries of the tensors to print.",
    "inputs": [
      { "name": "condition", "type": "Arg" },
      { "name": "data", "type": "Arg" }
    ],
    "attributes": [
      { "name": "summarize", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.Assign",
    "summary": "Update 'ref' by assigning 'value' to it.",
    "description": "This operation outputs \"ref\" after the assignment is done.\nThis makes it easier to chain operations that need to use the reset value.",
    "inputs": [
      { "name": "ref", "type": "Arg" },
      { "name": "value", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output_ref", "type": "Res" }
    ],
    "attributes": [
      { "name": "validate_shape", "type": "DefaultValuedOptionalAttr" },
      { "name": "use_locking", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.AssignAddVariableOp",
    "summary": "Adds a value to the current value of a variable.",
    "description": "Any ReadVariableOp with a control dependency on this op is guaranteed to\nsee the incremented value or a subsequent newer one.",
    "inputs": [
      { "name": "resource", "type": "Arg" },
      { "name": "value", "type": "Arg" }
    ]
  },
  {
    "name": "tf.AssignSubVariableOp",
    "summary": "Subtracts a value from the current value of a variable.",
    "description": "Any ReadVariableOp with a control dependency on this op is guaranteed to\nsee the decremented value or a subsequent newer one.",
    "inputs": [
      { "name": "resource", "type": "Arg" },
      { "name": "value", "type": "Arg" }
    ]
  },
  {
    "name": "tf.AssignVariableOp",
    "summary": "Assigns a new value to a variable.",
    "description": "Any ReadVariableOp with a control dependency on this op is guaranteed to return\nthis value or a subsequent newer value of the variable.",
    "inputs": [
      { "name": "resource", "type": "Arg" },
      { "name": "value", "type": "Arg" }
    ],
    "attributes": [
      { "name": "validate_shape", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.AsString",
    "summary": "Converts each entry in the given tensor to strings.",
    "description": "Supports many numeric types and boolean.\n\nFor Unicode, see the\n[https://www.tensorflow.org/text/guide/unicode](Working with Unicode text)\ntutorial.\n\nExamples:\n\n>>> tf.strings.as_string([3, 2])\n<tf.Tensor: shape=(2,), dtype=string, numpy=array([b'3', b'2'], dtype=object)>\n>>> tf.strings.as_string([3.1415926, 2.71828], precision=2).numpy()\narray([b'3.14', b'2.72'], dtype=object)",
    "inputs": [
      { "name": "input", "type": "TensorOf" }
    ],
    "outputs": [
      { "name": "output", "type": "TF_StrTensor" }
    ],
    "attributes": [
      { "name": "precision", "type": "DefaultValuedOptionalAttr" },
      { "name": "scientific", "type": "DefaultValuedOptionalAttr" },
      { "name": "shortest", "type": "DefaultValuedOptionalAttr" },
      { "name": "width", "type": "DefaultValuedOptionalAttr" },
      { "name": "fill", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.Atan",
    "summary": "Computes the trignometric inverse tangent of x element-wise.",
    "description": "The `tf.math.atan` operation returns the inverse of `tf.math.tan`, such that\nif `y = tf.math.tan(x)` then, `x = tf.math.atan(y)`.\n\n**Note**: The output of `tf.math.atan` will lie within the invertible range\nof tan, i.e (-pi/2, pi/2).\n\nFor example:\n\n```python\n# Note: [1.047, 0.785] ~= [(pi/3), (pi/4)]\nx = tf.constant([1.047, 0.785])\ny = tf.math.tan(x) # [1.731261, 0.99920404]\n\ntf.math.atan(y) # [1.047, 0.785] = x\n```",
    "inputs": [
      { "name": "x", "type": "TF_FpOrComplexTensor" }
    ],
    "outputs": [
      { "name": "y", "type": "TF_FpOrComplexTensor" }
    ]
  },
  {
    "name": "tf.Atan2",
    "summary": "Computes arctangent of `y/x` element-wise, respecting signs of the arguments.",
    "description": "This is the angle \\\\( \\theta \\in [-\\pi, \\pi] \\\\) such that\n\\\\[ x = r \\cos(\\theta) \\\\]\nand\n\\\\[ y = r \\sin(\\theta) \\\\]\nwhere \\\\(r = \\sqrt{x^2 + y^2} \\\\).\n\nFor example:\n\n>>> x = [1., 1.]\n>>> y = [1., -1.]\n>>> print((tf.math.atan2(y,x) * (180 / np.pi)).numpy())\n[ 45. -45.]",
    "inputs": [
      { "name": "y", "type": "TF_FloatTensor" },
      { "name": "x", "type": "TF_FloatTensor" }
    ],
    "outputs": [
      { "name": "z", "type": "TF_FloatTensor" }
    ]
  },
  {
    "name": "tf.Atanh",
    "summary": "Computes inverse hyperbolic tangent of x element-wise.",
    "description": "Given an input tensor, this function computes inverse hyperbolic tangent\n  for every element in the tensor. Input range is `[-1,1]` and output range is\n  `[-inf, inf]`. If input is `-1`, output will be `-inf` and if the\n  input is `1`, output will be `inf`. Values outside the range will have\n  `nan` as output.\n\n  ```python\n  x = tf.constant([-float(\"inf\"), -1, -0.5, 1, 0, 0.5, 10, float(\"inf\")])\n  tf.math.atanh(x) ==> [nan -inf -0.54930615 inf  0. 0.54930615 nan nan]\n  ```",
    "inputs": [
      { "name": "x", "type": "TF_FpOrComplexTensor" }
    ],
    "outputs": [
      { "name": "y", "type": "TF_FpOrComplexTensor" }
    ]
  },
  {
    "name": "tf.AvgPool",
    "summary": "Performs average pooling on the input.",
    "description": "Each entry in `output` is the mean of the corresponding size `ksize`\nwindow in `value`.",
    "inputs": [
      { "name": "value", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "ksize", "type": "ConfinedAttr" },
      { "name": "strides", "type": "ConfinedAttr" },
      { "name": "padding", "type": "TF_AnyStrAttrOf" },
      { "name": "data_format", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.AvgPool3D",
    "summary": "Performs 3D average pooling on the input.",
    "description": "Each entry in `output` is the mean of the corresponding size `ksize` window in\n`value`.",
    "inputs": [
      { "name": "input", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "ksize", "type": "ConfinedAttr" },
      { "name": "strides", "type": "ConfinedAttr" },
      { "name": "padding", "type": "TF_AnyStrAttrOf" },
      { "name": "data_format", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.AvgPool3DGrad",
    "summary": "Computes gradients of average pooling function.",
    "inputs": [
      { "name": "orig_input_shape", "type": "Arg" },
      { "name": "grad", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "ksize", "type": "ConfinedAttr" },
      { "name": "strides", "type": "ConfinedAttr" },
      { "name": "padding", "type": "TF_AnyStrAttrOf" },
      { "name": "data_format", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.AvgPoolGrad",
    "summary": "Computes gradients of the average pooling function.",
    "inputs": [
      { "name": "orig_input_shape", "type": "Arg" },
      { "name": "grad", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "ksize", "type": "ConfinedAttr" },
      { "name": "strides", "type": "ConfinedAttr" },
      { "name": "padding", "type": "TF_AnyStrAttrOf" },
      { "name": "data_format", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.BatchDatasetV2",
    "summary": "Creates a dataset that batches `batch_size` elements from `input_dataset`.",
    "inputs": [
      { "name": "input_dataset", "type": "TF_VariantTensor" },
      { "name": "batch_size", "type": "Arg" },
      { "name": "drop_remainder", "type": "Arg" }
    ],
    "outputs": [
      { "name": "handle", "type": "TF_VariantTensor" }
    ],
    "attributes": [
      { "name": "parallel_copy", "type": "DefaultValuedOptionalAttr" },
      { "name": "output_types", "type": "ConfinedAttr" },
      { "name": "output_shapes", "type": "ConfinedAttr" },
      { "name": "metadata", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.BatchFunction",
    "summary": "Batches all the inputs tensors to the computation done by the function.",
    "description": "So, for example, in the following code\n\n  ```python\n\n  # This input will be captured.\n  y = tf.placeholder_with_default(1.0, shape=[])\n\n  @tf.Defun(tf.float32)\n  def computation(a):\n    return tf.matmul(a, a) + y\n\n  b = gen_batch_ops.batch_function(\n          f=computation\n          in_tensors=[a],\n          captured_tensors=computation.captured_inputs,\n          Tout=[o.type for o in computation.definition.signature.output_arg],\n          num_batch_threads=1,\n          max_batch_size=10,\n          batch_timeout_micros=100000,  # 100ms\n          allowed_batch_sizes=[3, 10],\n          batching_queue=\"\")\n  ```\n\nIf more than one session.run call is simultaneously trying to compute `b`\nthe values of `a` will be gathered, non-deterministically concatenated\nalong the first axis, and only one thread will run the computation.\n\nAssumes that all arguments of the function are Tensors which will be batched\nalong their first dimension.\n\nArguments that are captured, are not batched. The session.run call which does\nthe concatenation, will use the values of the captured tensors available to it.\nTherefore, typical uses of captured tensors should involve values which remain\nunchanged across session.run calls. Inference is a good example of this.\n\nSparseTensor is not supported. The return value of the decorated function\nmust be a Tensor or a list/tuple of Tensors.",
    "inputs": [
      { "name": "in_tensors", "type": "Arg" },
      { "name": "captured_tensors", "type": "Arg" }
    ],
    "outputs": [
      { "name": "out_tensors", "type": "Res" }
    ],
    "attributes": [
      { "name": "f", "type": "SymbolRefAttr" },
      { "name": "num_batch_threads", "type": "I64Attr" },
      { "name": "max_batch_size", "type": "I64Attr" },
      { "name": "batch_timeout_micros", "type": "I64Attr" },
      { "name": "max_enqueued_batches", "type": "DefaultValuedOptionalAttr" },
      { "name": "allowed_batch_sizes", "type": "DefaultValuedOptionalAttr" },
      { "name": "container", "type": "DefaultValuedOptionalAttr" },
      { "name": "shared_name", "type": "DefaultValuedOptionalAttr" },
      { "name": "batching_queue", "type": "DefaultValuedOptionalAttr" },
      { "name": "low_priority_max_batch_size", "type": "DefaultValuedOptionalAttr" },
      { "name": "low_priority_batch_timeout_micros", "type": "DefaultValuedOptionalAttr" },
      { "name": "low_priority_allowed_batch_sizes", "type": "DefaultValuedOptionalAttr" },
      { "name": "low_priority_max_enqueued_batches", "type": "DefaultValuedOptionalAttr" },
      { "name": "mixed_priority_policy", "type": "DefaultValuedOptionalAttr" },
      { "name": "batch_padding_policy", "type": "DefaultValuedOptionalAttr" },
      { "name": "enable_large_batch_splitting", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.BatchMatMul",
    "summary": "Multiplies slices of two tensors in batches.",
    "description": "Multiplies all slices of `Tensor` `x` and `y` (each slice can be\nviewed as an element of a batch), and arranges the individual results\nin a single output tensor of the same batch size. Each of the\nindividual slices can optionally be adjointed (to adjoint a matrix\nmeans to transpose and conjugate it) before multiplication by setting\nthe `adj_x` or `adj_y` flag to `True`, which are by default `False`.\n\nThe input tensors `x` and `y` are 2-D or higher with shape `[..., r_x, c_x]`\nand `[..., r_y, c_y]`.\n\nThe output tensor is 2-D or higher with shape `[..., r_o, c_o]`, where:\n\n    r_o = c_x if adj_x else r_x\n    c_o = r_y if adj_y else c_y\n\nIt is computed as:\n\n    output[..., :, :] = matrix(x[..., :, :]) * matrix(y[..., :, :])",
    "inputs": [
      { "name": "x", "type": "Arg" },
      { "name": "y", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "adj_x", "type": "DefaultValuedOptionalAttr" },
      { "name": "adj_y", "type": "DefaultValuedOptionalAttr" },
      { "name": "grad_x", "type": "DefaultValuedOptionalAttr" },
      { "name": "grad_y", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.BatchMatMulV2",
    "summary": "Multiplies slices of two tensors in batches.",
    "description": "Multiplies all slices of `Tensor` `x` and `y` (each slice can be\nviewed as an element of a batch), and arranges the individual results\nin a single output tensor of the same batch size. Each of the\nindividual slices can optionally be adjointed (to adjoint a matrix\nmeans to transpose and conjugate it) before multiplication by setting\nthe `adj_x` or `adj_y` flag to `True`, which are by default `False`.\n\nThe input tensors `x` and `y` are 2-D or higher with shape `[..., r_x, c_x]`\nand `[..., r_y, c_y]`.\n\nThe output tensor is 2-D or higher with shape `[..., r_o, c_o]`, where:\n\n    r_o = c_x if adj_x else r_x\n    c_o = r_y if adj_y else c_y\n\nIt is computed as:\n\n    output[..., :, :] = matrix(x[..., :, :]) * matrix(y[..., :, :])\n\n*NOTE*: `BatchMatMulV2` supports broadcasting in the batch dimensions. More\nabout broadcasting\n[here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html).",
    "inputs": [
      { "name": "x", "type": "Arg" },
      { "name": "y", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "adj_x", "type": "DefaultValuedOptionalAttr" },
      { "name": "adj_y", "type": "DefaultValuedOptionalAttr" },
      { "name": "grad_x", "type": "DefaultValuedOptionalAttr" },
      { "name": "grad_y", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.BatchMatMulV3",
    "summary": "Multiplies slices of two tensors in batches.",
    "description": "Multiplies all slices of `Tensor` `x` and `y` (each slice can be\nviewed as an element of a batch), and arranges the individual results\nin a single output tensor of the same batch size. Each of the\nindividual slices can optionally be adjointed (to adjoint a matrix\nmeans to transpose and conjugate it) before multiplication by setting\nthe `adj_x` or `adj_y` flag to `True`, which are by default `False`.\n\nThe input tensors `x` and `y` are 2-D or higher with shape `[..., r_x, c_x]`\nand `[..., r_y, c_y]`.\n\nThe output tensor is 2-D or higher with shape `[..., r_o, c_o]`, where:\n\n    r_o = c_x if adj_x else r_x\n    c_o = r_y if adj_y else c_y\n\nIt is computed as:\n\n    output[..., :, :] = matrix(x[..., :, :]) * matrix(y[..., :, :])\n\n*NOTE*: `BatchMatMulV3` supports broadcasting in the batch dimensions. More\nabout broadcasting\n[here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html).",
    "inputs": [
      { "name": "x", "type": "Arg" },
      { "name": "y", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "adj_x", "type": "DefaultValuedOptionalAttr" },
      { "name": "adj_y", "type": "DefaultValuedOptionalAttr" },
      { "name": "grad_x", "type": "DefaultValuedOptionalAttr" },
      { "name": "grad_y", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.BatchNormWithGlobalNormalization",
    "summary": "Batch normalization.",
    "description": "This op is deprecated. Prefer `tf.nn.batch_normalization`.",
    "inputs": [
      { "name": "x", "type": "Arg" },
      { "name": "m", "type": "Arg" },
      { "name": "v", "type": "Arg" },
      { "name": "beta", "type": "Arg" },
      { "name": "gamma", "type": "Arg" }
    ],
    "outputs": [
      { "name": "result", "type": "TensorOf" }
    ],
    "attributes": [
      { "name": "variance_epsilon", "type": "F32Attr" },
      { "name": "scale_after_normalization", "type": "BoolAttr" }
    ]
  },
  {
    "name": "tf.BatchToSpace",
    "summary": "BatchToSpace for 4-D tensors of type T.",
    "description": "This is a legacy version of the more general BatchToSpaceND.\n\nRearranges (permutes) data from batch into blocks of spatial data, followed by\ncropping. This is the reverse transformation of SpaceToBatch. More specifically,\nthis op outputs a copy of the input tensor where values from the `batch`\ndimension are moved in spatial blocks to the `height` and `width` dimensions,\nfollowed by cropping along the `height` and `width` dimensions.",
    "inputs": [
      { "name": "input", "type": "Arg" },
      { "name": "crops", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "block_size", "type": "ConfinedAttr" }
    ]
  },
  {
    "name": "tf.BatchToSpaceND",
    "summary": "BatchToSpace for N-D tensors of type T.",
    "description": "This operation reshapes the \"batch\" dimension 0 into `M + 1` dimensions of shape\n`block_shape + [batch]`, interleaves these blocks back into the grid defined by\nthe spatial dimensions `[1, ..., M]`, to obtain a result with the same rank as\nthe input.  The spatial dimensions of this intermediate result are then\noptionally cropped according to `crops` to produce the output.  This is the\nreverse of SpaceToBatch.  See below for a precise description.",
    "inputs": [
      { "name": "input", "type": "Arg" },
      { "name": "block_shape", "type": "Arg" },
      { "name": "crops", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "TF_Tensor" }
    ]
  },
  {
    "name": "tf.BesselI0e",
    "summary": "Computes the Bessel i0e function of `x` element-wise.",
    "description": "Exponentially scaled modified Bessel function of order 0 defined as\n`bessel_i0e(x) = exp(-abs(x)) bessel_i0(x)`.\n\nThis function is faster and numerically stabler than `bessel_i0(x)`.",
    "inputs": [
      { "name": "x", "type": "TF_FloatTensor" }
    ],
    "outputs": [
      { "name": "y", "type": "TF_FloatTensor" }
    ]
  },
  {
    "name": "tf.BesselI1e",
    "summary": "Computes the Bessel i1e function of `x` element-wise.",
    "description": "Exponentially scaled modified Bessel function of order 0 defined as\n`bessel_i1e(x) = exp(-abs(x)) bessel_i1(x)`.\n\nThis function is faster and numerically stabler than `bessel_i1(x)`.",
    "inputs": [
      { "name": "x", "type": "TF_FloatTensor" }
    ],
    "outputs": [
      { "name": "y", "type": "TF_FloatTensor" }
    ]
  },
  {
    "name": "tf.Betainc",
    "summary": "Compute the regularized incomplete beta integral \\\\(I_x(a, b)\\\\).",
    "description": "The regularized incomplete beta integral is defined as:\n\n\n\\\\(I_x(a, b) = \\frac{B(x; a, b)}{B(a, b)}\\\\)\n\nwhere\n\n\n\\\\(B(x; a, b) = \\int_0^x t^{a-1} (1 - t)^{b-1} dt\\\\)\n\n\nis the incomplete beta function and \\\\(B(a, b)\\\\) is the *complete*\nbeta function.",
    "inputs": [
      { "name": "a", "type": "TF_F32OrF64Tensor" },
      { "name": "b", "type": "TF_F32OrF64Tensor" },
      { "name": "x", "type": "TF_F32OrF64Tensor" }
    ],
    "outputs": [
      { "name": "z", "type": "TF_F32OrF64Tensor" }
    ]
  },
  {
    "name": "tf.BiasAdd",
    "summary": "Adds `bias` to `value`.",
    "description": "This is a special case of `tf.add` where `bias` is restricted to be 1-D.\nBroadcasting is supported, so `value` may have any number of dimensions.",
    "inputs": [
      { "name": "value", "type": "Arg" },
      { "name": "bias", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "data_format", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.BiasAddGrad",
    "summary": "The backward operation for \"BiasAdd\" on the \"bias\" tensor.",
    "description": "It accumulates all the values from out_backprop into the feature dimension.\nFor NHWC data format, the feature dimension is the last. For NCHW data format,\nthe feature dimension is the third-to-last.",
    "inputs": [
      { "name": "out_backprop", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "data_format", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.BiasAddV1",
    "summary": "Adds `bias` to `value`.",
    "description": "This is a deprecated version of BiasAdd and will be soon removed.\n\nThis is a special case of `tf.add` where `bias` is restricted to be 1-D.\nBroadcasting is supported, so `value` may have any number of dimensions.",
    "inputs": [
      { "name": "value", "type": "Arg" },
      { "name": "bias", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ]
  },
  {
    "name": "tf.Bincount",
    "summary": "Counts the number of occurrences of each value in an integer array.",
    "description": "Outputs a vector with length `size` and the same dtype as `weights`. If\n`weights` are empty, then index `i` stores the number of times the value `i` is\ncounted in `arr`. If `weights` are non-empty, then index `i` stores the sum of\nthe value in `weights` at each index where the corresponding value in `arr` is\n`i`.\n\nValues in `arr` outside of the range [0, size) are ignored.",
    "inputs": [
      { "name": "arr", "type": "Arg" },
      { "name": "size", "type": "Arg" },
      { "name": "weights", "type": "Arg" }
    ],
    "outputs": [
      { "name": "bins", "type": "Res" }
    ]
  },
  {
    "name": "tf.Bitcast",
    "summary": "Bitcasts a tensor from one type to another without copying data.",
    "description": "Given a tensor `input`, this operation returns a tensor that has the same buffer\ndata as `input` with datatype `type`.\n\nIf the input datatype `T` is larger than the output datatype `type` then the\nshape changes from [...] to [..., sizeof(`T`)/sizeof(`type`)].\n\nIf `T` is smaller than `type`, the operator requires that the rightmost\ndimension be equal to sizeof(`type`)/sizeof(`T`). The shape then goes from\n[..., sizeof(`type`)/sizeof(`T`)] to [...].\n\ntf.bitcast() and tf.cast() work differently when real dtype is casted as a complex dtype\n(e.g. tf.complex64 or tf.complex128) as tf.cast() make imaginary part 0 while tf.bitcast()\ngives module error.\nFor example,\n\nExample 1:\n\n>>> a = [1., 2., 3.]\n>>> equality_bitcast = tf.bitcast(a, tf.complex128)\nTraceback (most recent call last):\n...\nInvalidArgumentError: Cannot bitcast from 1 to 18 [Op:Bitcast]\n>>> equality_cast = tf.cast(a, tf.complex128)\n>>> print(equality_cast)\ntf.Tensor([1.+0.j 2.+0.j 3.+0.j], shape=(3,), dtype=complex128)\n\nExample 2:\n\n>>> tf.bitcast(tf.constant(0xffffffff, dtype=tf.uint32), tf.uint8)\n<tf.Tensor: shape=(4,), dtype=uint8, numpy=array([255, 255, 255, 255], dtype=uint8)>\n\nExample 3:\n\n>>> x = [1., 2., 3.]\n>>> y = [0., 2., 3.]\n>>> equality= tf.equal(x,y)\n>>> equality_cast = tf.cast(equality,tf.float32)\n>>> equality_bitcast = tf.bitcast(equality_cast,tf.uint8)\n>>> print(equality)\ntf.Tensor([False True True], shape=(3,), dtype=bool)\n>>> print(equality_cast)\ntf.Tensor([0. 1. 1.], shape=(3,), dtype=float32)\n>>> print(equality_bitcast)\ntf.Tensor(\n    [[  0   0   0   0]\n     [  0   0 128  63]\n     [  0   0 128  63]], shape=(3, 4), dtype=uint8)\n\n*NOTE*: Bitcast is implemented as a low-level cast, so machines with different\nendian orderings will give different results. A copy from input buffer to output\nbuffer is made on BE machines when types are of different sizes in order to get\nthe same casting results as on LE machines.",
    "inputs": [
      { "name": "input", "type": "TF_NumberTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "TF_NumberTensor" }
    ]
  },
  {
    "name": "tf.BitwiseAnd",
    "summary": "Elementwise computes the bitwise AND of `x` and `y`.",
    "description": "The result will have those bits set, that are set in both `x` and `y`. The\ncomputation is performed on the underlying representations of `x` and `y`.\n\nFor example:\n\n```python\nimport tensorflow as tf\nfrom tensorflow.python.ops import bitwise_ops\ndtype_list = [tf.int8, tf.int16, tf.int32, tf.int64,\n              tf.uint8, tf.uint16, tf.uint32, tf.uint64]\n\nfor dtype in dtype_list:\n  lhs = tf.constant([0, 5, 3, 14], dtype=dtype)\n  rhs = tf.constant([5, 0, 7, 11], dtype=dtype)\n  exp = tf.constant([0, 0, 3, 10], dtype=tf.float32)\n\n  res = bitwise_ops.bitwise_and(lhs, rhs)\n  tf.assert_equal(tf.cast(res, tf.float32), exp) # TRUE\n```",
    "inputs": [
      { "name": "x", "type": "TF_IntTensor" },
      { "name": "y", "type": "TF_IntTensor" }
    ],
    "outputs": [
      { "name": "z", "type": "TF_IntTensor" }
    ]
  },
  {
    "name": "tf.BitwiseOr",
    "summary": "Elementwise computes the bitwise OR of `x` and `y`.",
    "description": "The result will have those bits set, that are set in `x`, `y` or both. The\ncomputation is performed on the underlying representations of `x` and `y`.\n\nFor example:\n\n```python\nimport tensorflow as tf\nfrom tensorflow.python.ops import bitwise_ops\ndtype_list = [tf.int8, tf.int16, tf.int32, tf.int64,\n              tf.uint8, tf.uint16, tf.uint32, tf.uint64]\n\nfor dtype in dtype_list:\n  lhs = tf.constant([0, 5, 3, 14], dtype=dtype)\n  rhs = tf.constant([5, 0, 7, 11], dtype=dtype)\n  exp = tf.constant([5, 5, 7, 15], dtype=tf.float32)\n\n  res = bitwise_ops.bitwise_or(lhs, rhs)\n  tf.assert_equal(tf.cast(res,  tf.float32), exp)  # TRUE\n```",
    "inputs": [
      { "name": "x", "type": "TF_IntTensor" },
      { "name": "y", "type": "TF_IntTensor" }
    ],
    "outputs": [
      { "name": "z", "type": "TF_IntTensor" }
    ]
  },
  {
    "name": "tf.BitwiseXor",
    "summary": "Elementwise computes the bitwise XOR of `x` and `y`.",
    "description": "The result will have those bits set, that are different in `x` and `y`. The\ncomputation is performed on the underlying representations of `x` and `y`.\n\nFor example:\n\n```python\nimport tensorflow as tf\nfrom tensorflow.python.ops import bitwise_ops\ndtype_list = [tf.int8, tf.int16, tf.int32, tf.int64,\n              tf.uint8, tf.uint16, tf.uint32, tf.uint64]\n\nfor dtype in dtype_list:\n  lhs = tf.constant([0, 5, 3, 14], dtype=dtype)\n  rhs = tf.constant([5, 0, 7, 11], dtype=dtype)\n  exp = tf.constant([5, 5, 4, 5],  dtype=tf.float32)\n\n  res = bitwise_ops.bitwise_xor(lhs, rhs)\n  tf.assert_equal(tf.cast(res, tf.float32), exp) # TRUE\n```",
    "inputs": [
      { "name": "x", "type": "TF_IntTensor" },
      { "name": "y", "type": "TF_IntTensor" }
    ],
    "outputs": [
      { "name": "z", "type": "TF_IntTensor" }
    ]
  },
  {
    "name": "tf.BoostedTreesBucketize",
    "summary": "Bucketize each feature based on bucket boundaries.",
    "description": "An op that returns a list of float tensors, where each tensor represents the\nbucketized values for a single feature.",
    "inputs": [
      { "name": "float_values", "type": "Arg" },
      { "name": "bucket_boundaries", "type": "Arg" }
    ],
    "outputs": [
      { "name": "buckets", "type": "Res" }
    ]
  },
  {
    "name": "tf.BroadcastArgs",
    "summary": "Return the shape of s0 op s1 with broadcast.",
    "description": "Given `s0` and `s1`, tensors that represent shapes, compute `r0`, the\nbroadcasted shape. `s0`, `s1` and `r0` are all integer vectors.",
    "inputs": [
      { "name": "s0", "type": "TF_I32OrI64Tensor" },
      { "name": "s1", "type": "TF_I32OrI64Tensor" }
    ],
    "outputs": [
      { "name": "r0", "type": "TF_I32OrI64Tensor" }
    ]
  },
  {
    "name": "tf.BroadcastGradientArgs",
    "summary": "Return the reduction indices for computing gradients of s0 op s1 with broadcast.",
    "description": "This is typically used by gradient computations for a broadcasting operation.",
    "inputs": [
      { "name": "s0", "type": "TF_I32OrI64Tensor" },
      { "name": "s1", "type": "TF_I32OrI64Tensor" }
    ],
    "outputs": [
      { "name": "r0", "type": "TF_I32OrI64Tensor" },
      { "name": "r1", "type": "TF_I32OrI64Tensor" }
    ]
  },
  {
    "name": "tf.BroadcastTo",
    "summary": "Broadcast an array for a compatible shape.",
    "description": "Broadcasting is the process of making arrays to have compatible shapes\nfor arithmetic operations. Two shapes are compatible if for each\ndimension pair they are either equal or one of them is one.\n\nFor example:\n\n>>> x = tf.constant([[1, 2, 3]])   # Shape (1, 3,)\n>>> y = tf.broadcast_to(x, [2, 3])\n>>> print(y)\ntf.Tensor(\n    [[1 2 3]\n     [1 2 3]], shape=(2, 3), dtype=int32)\n\nIn the above example, the input Tensor with the shape of `[1, 3]`\nis broadcasted to output Tensor with shape of `[2, 3]`.\n\nWhen broadcasting, if a tensor has fewer axes than necessary its shape is\npadded on the left with ones. So this gives the same result as the previous\nexample:\n\n>>> x = tf.constant([1, 2, 3])   # Shape (3,)\n>>> y = tf.broadcast_to(x, [2, 3])\n\n\nWhen doing broadcasted operations such as multiplying a tensor\nby a scalar, broadcasting (usually) confers some time or space\nbenefit, as the broadcasted tensor is never materialized.\n\nHowever, `broadcast_to` does not carry with it any such benefits.\nThe newly-created tensor takes the full memory of the broadcasted\nshape. (In a graph context, `broadcast_to` might be fused to\nsubsequent operation and then be optimized away, however.)",
    "inputs": [
      { "name": "input", "type": "Arg" },
      { "name": "shape", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ]
  },
  {
    "name": "tf.Bucketize",
    "summary": "Bucketizes 'input' based on 'boundaries'.",
    "description": "For example, if the inputs are\n    boundaries = [0, 10, 100]\n    input = [[-5, 10000]\n             [150,   10]\n             [5,    100]]\n\nthen the output will be\n    output = [[0, 3]\n              [3, 2]\n              [1, 3]]",
    "inputs": [
      { "name": "input", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "boundaries", "type": "F32ArrayAttr" }
    ]
  },
  {
    "name": "tf.CacheDatasetV2",
    "inputs": [
      { "name": "input_dataset", "type": "TF_VariantTensor" },
      { "name": "filename", "type": "TF_StrTensor" },
      { "name": "cache", "type": "Arg" }
    ],
    "outputs": [
      { "name": "handle", "type": "TF_VariantTensor" }
    ],
    "attributes": [
      { "name": "output_types", "type": "ConfinedAttr" },
      { "name": "output_shapes", "type": "ConfinedAttr" },
      { "name": "metadata", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.Case",
    "summary": "An n-way switch statement which calls a single branch function.",
    "description": "An n-way switch statement, implementing the following:\n    ```\n    switch (branch_index) {\n      case 0:\n        output = branches[0](input);\n        break;\n      case 1:\n        output = branches[1](input);\n        break;\n      ...\n      case [[nbranches-1]]:\n      default:\n        output = branches[nbranches-1](input);\n        break;\n    }\n    ```",
    "inputs": [
      { "name": "branch_index", "type": "I32Tensor" },
      { "name": "input", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "output", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "branches", "type": "ConfinedAttr" },
      { "name": "is_stateless", "type": "BoolAttr" }
    ]
  },
  {
    "name": "tf.CaseRegion",
    "summary": "An n-way switch statement which calls a single branch function.",
    "description": "An n-way switch statement, implementing the following:\n    ```\n    switch (branch_index) {\n      case 0:\n        output = branches[0](input);\n        break;\n      case 1:\n        output = branches[1](input);\n        break;\n      ...\n      case [[nbranches-1]]:\n      default:\n        output = branches[nbranches-1](input);\n        break;\n    }\n    ```",
    "inputs": [
      { "name": "branch_index", "type": "I32Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "is_stateless", "type": "BoolAttr" }
    ]
  },
  {
    "name": "tf.Cast",
    "summary": "Cast x of type SrcT to y of DstT.",
    "inputs": [
      { "name": "x", "type": "TF_Tensor" }
    ],
    "outputs": [
      { "name": "y", "type": "TF_Tensor" }
    ],
    "attributes": [
      { "name": "Truncate", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.Ceil",
    "summary": "Returns element-wise smallest integer not less than x.",
    "inputs": [
      { "name": "x", "type": "TF_FloatTensor" }
    ],
    "outputs": [
      { "name": "y", "type": "TF_FloatTensor" }
    ]
  },
  {
    "name": "tf.CheckNumerics",
    "summary": "Checks a tensor for NaN and Inf values.",
    "description": "When run, reports an `InvalidArgument` error if `tensor` has any values\nthat are not a number (NaN) or infinity (Inf). Otherwise, returns the input\ntensor.\n\nExample usage:\n\n``` python\na = tf.Variable(1.0)\ntf.debugging.check_numerics(a, message='')\n\nb = tf.Variable(np.nan)\ntry:\n  tf.debugging.check_numerics(b, message='Checking b')\nexcept Exception as e:\n  assert \"Checking b : Tensor had NaN values\" in e.message\n\nc = tf.Variable(np.inf)\ntry:\n  tf.debugging.check_numerics(c, message='Checking c')\nexcept Exception as e:\n  assert \"Checking c : Tensor had Inf values\" in e.message\n```",
    "inputs": [
      { "name": "tensor", "type": "TF_FloatTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "TF_FloatTensor" }
    ],
    "attributes": [
      { "name": "message", "type": "StrAttr" }
    ]
  },
  {
    "name": "tf.Cholesky",
    "summary": "Computes the Cholesky decomposition of one or more square matrices.",
    "description": "The input is a tensor of shape `[..., M, M]` whose inner-most 2 dimensions\nform square matrices.\n\nThe input has to be symmetric and positive definite. Only the lower-triangular\npart of the input will be used for this operation. The upper-triangular part\nwill not be read.\n\nThe output is a tensor of the same shape as the input\ncontaining the Cholesky decompositions for all input submatrices `[..., :, :]`.\n\n**Note**: The gradient computation on GPU is faster for large matrices but\nnot for large batch dimensions when the submatrices are small. In this\ncase it might be faster to use the CPU.",
    "inputs": [
      { "name": "input", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ]
  },
  {
    "name": "tf.ClipByValue",
    "summary": "Clips tensor values to a specified min and max.",
    "description": "Given a tensor `x`, this operation returns a tensor of the same type and\nshape as `x` with its values clipped to `clip_value_min` and `clip_value_max`.\nAny values less than `clip_value_min` are set to `clip_value_min`. Any values\ngreater than `clip_value_max` are set to `clip_value_max`.",
    "inputs": [
      { "name": "x", "type": "Arg" },
      { "name": "clip_value_min", "type": "Arg" },
      { "name": "clip_value_max", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ]
  },
  {
    "name": "tf.CloseSummaryWriter",
    "summary": "Flushes and closes the summary writer.",
    "description": "Also removes it from the resource manager. To reopen, use another\nCreateSummaryFileWriter op.\n\nwriter: A handle to the summary writer resource.",
    "inputs": [
      { "name": "writer", "type": "Arg" }
    ]
  },
  {
    "name": "tf.CollateTPUEmbeddingMemory",
    "summary": "An op that merges the string-encoded memory config protos from all hosts.",
    "inputs": [
      { "name": "memory_configs", "type": "Arg" }
    ],
    "outputs": [
      { "name": "merged_memory_config", "type": "TF_StrTensor" }
    ]
  },
  {
    "name": "tf.CollectiveAllToAllV2",
    "summary": "Mutually exchanges multiple tensors of identical type and shape.",
    "description": "`is_stateless` means each op does not need control dependencies to other\ncollective ops. In this case, keys that are unique at runtime\n(e.g. `instance_key`) should be used to distinguish collective groups.",
    "inputs": [
      { "name": "input", "type": "TF_FpOrI32OrI64Tensor" },
      { "name": "group_size", "type": "TF_Int32Tensor" },
      { "name": "group_key", "type": "TF_Int32Tensor" },
      { "name": "instance_key", "type": "TF_Int32Tensor" },
      { "name": "ordering_token", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "data", "type": "TF_FpOrI32OrI64Tensor" }
    ],
    "attributes": [
      { "name": "communication_hint", "type": "DefaultValuedOptionalAttr" },
      { "name": "timeout_seconds", "type": "DefaultValuedOptionalAttr" },
      { "name": "is_stateless", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.CollectiveAssignGroupV2",
    "summary": "Assign group keys based on group assignment.",
    "inputs": [
      { "name": "group_assignment", "type": "TF_Int32Tensor" },
      { "name": "device_index", "type": "TF_Int32Tensor" },
      { "name": "base_key", "type": "TF_Int32Tensor" }
    ],
    "outputs": [
      { "name": "group_size", "type": "TF_Int32Tensor" },
      { "name": "group_key", "type": "TF_Int32Tensor" }
    ]
  },
  {
    "name": "tf.CollectiveBcastRecv",
    "summary": "Receives a tensor value broadcast from another device.",
    "outputs": [
      { "name": "data", "type": "TensorOf" }
    ],
    "attributes": [
      { "name": "group_size", "type": "I64Attr" },
      { "name": "group_key", "type": "I64Attr" },
      { "name": "instance_key", "type": "I64Attr" },
      { "name": "shape", "type": "TF_ShapeAttr" },
      { "name": "communication_hint", "type": "DefaultValuedOptionalAttr" },
      { "name": "timeout_seconds", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.CollectiveBcastSend",
    "summary": "Broadcasts a tensor value to one or more other devices.",
    "inputs": [
      { "name": "input", "type": "TensorOf" }
    ],
    "outputs": [
      { "name": "data", "type": "TensorOf" }
    ],
    "attributes": [
      { "name": "group_size", "type": "I64Attr" },
      { "name": "group_key", "type": "I64Attr" },
      { "name": "instance_key", "type": "I64Attr" },
      { "name": "shape", "type": "TF_ShapeAttr" },
      { "name": "communication_hint", "type": "DefaultValuedOptionalAttr" },
      { "name": "timeout_seconds", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.CollectiveGather",
    "summary": "Mutually accumulates multiple tensors of identical type and shape.",
    "inputs": [
      { "name": "input", "type": "TensorOf" }
    ],
    "outputs": [
      { "name": "data", "type": "TensorOf" }
    ],
    "attributes": [
      { "name": "group_size", "type": "I64Attr" },
      { "name": "group_key", "type": "I64Attr" },
      { "name": "instance_key", "type": "I64Attr" },
      { "name": "shape", "type": "TF_ShapeAttr" },
      { "name": "communication_hint", "type": "DefaultValuedOptionalAttr" },
      { "name": "timeout_seconds", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.CollectiveGatherV2",
    "summary": "Mutually accumulates multiple tensors of identical type and shape.",
    "description": "`is_stateless` means each op does not need control dependencies to other\ncollective ops. In this case, keys that are unique at runtime\n(e.g. `instance_key`) should be used to distinguish collective groups.",
    "inputs": [
      { "name": "input", "type": "TensorOf" },
      { "name": "group_size", "type": "TF_Int32Tensor" },
      { "name": "group_key", "type": "TF_Int32Tensor" },
      { "name": "instance_key", "type": "TF_Int32Tensor" },
      { "name": "ordering_token", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "data", "type": "TensorOf" }
    ],
    "attributes": [
      { "name": "communication_hint", "type": "DefaultValuedOptionalAttr" },
      { "name": "timeout_seconds", "type": "DefaultValuedOptionalAttr" },
      { "name": "is_stateless", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.CollectivePermute",
    "summary": "An Op to permute tensors across replicated TPU instances.",
    "description": "Each instance supplies its own input.\n\nFor example, suppose there are 4 TPU instances: `[A, B, C, D]`. Passing\nsource_target_pairs=`[[0,1],[1,2],[2,3],[3,0]]` gets the outputs:\n`[D, A, B, C]`.",
    "inputs": [
      { "name": "input", "type": "Arg" },
      { "name": "source_target_pairs", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ]
  },
  {
    "name": "tf.CollectiveReduce",
    "summary": "Mutually reduces multiple tensors of identical type and shape.",
    "inputs": [
      { "name": "input", "type": "TF_FpOrI32OrI64Tensor" }
    ],
    "outputs": [
      { "name": "data", "type": "TF_FpOrI32OrI64Tensor" }
    ],
    "attributes": [
      { "name": "group_size", "type": "I64Attr" },
      { "name": "group_key", "type": "I64Attr" },
      { "name": "instance_key", "type": "I64Attr" },
      { "name": "merge_op", "type": "TF_AnyStrAttrOf" },
      { "name": "final_op", "type": "TF_AnyStrAttrOf" },
      { "name": "subdiv_offsets", "type": "I64ArrayAttr" },
      { "name": "wait_for", "type": "DefaultValuedOptionalAttr" },
      { "name": "communication_hint", "type": "DefaultValuedOptionalAttr" },
      { "name": "timeout_seconds", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.CollectiveReduceScatterV2",
    "summary": "Mutually reduces multiple tensors of identical type and shape and scatters the result.",
    "description": "`is_stateless` means each op does not need control dependencies to other\ncollective ops. In this case, keys that are unique at runtime\n(e.g. `instance_key`) should be used to distinguish collective groups.",
    "inputs": [
      { "name": "input", "type": "TF_FpOrI32OrI64Tensor" },
      { "name": "group_size", "type": "TF_Int32Tensor" },
      { "name": "group_key", "type": "TF_Int32Tensor" },
      { "name": "instance_key", "type": "TF_Int32Tensor" },
      { "name": "ordering_token", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "data", "type": "TF_FpOrI32OrI64Tensor" }
    ],
    "attributes": [
      { "name": "merge_op", "type": "TF_AnyStrAttrOf" },
      { "name": "final_op", "type": "TF_AnyStrAttrOf" },
      { "name": "communication_hint", "type": "DefaultValuedOptionalAttr" },
      { "name": "timeout_seconds", "type": "DefaultValuedOptionalAttr" },
      { "name": "is_stateless", "type": "DefaultValuedOptionalAttr" },
      { "name": "max_subdivs_per_device", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.CollectiveReduceV2",
    "summary": "Mutually reduces multiple tensors of identical type and shape.",
    "description": "`is_stateless` means each op does not need control dependencies to other\ncollective ops. In this case, keys that are unique at runtime\n(e.g. `instance_key`) should be used to distinguish collective groups.",
    "inputs": [
      { "name": "input", "type": "TF_FpOrI32OrI64Tensor" },
      { "name": "group_size", "type": "TF_Int32Tensor" },
      { "name": "group_key", "type": "TF_Int32Tensor" },
      { "name": "instance_key", "type": "TF_Int32Tensor" },
      { "name": "ordering_token", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "data", "type": "TF_FpOrI32OrI64Tensor" }
    ],
    "attributes": [
      { "name": "merge_op", "type": "TF_AnyStrAttrOf" },
      { "name": "final_op", "type": "TF_AnyStrAttrOf" },
      { "name": "communication_hint", "type": "DefaultValuedOptionalAttr" },
      { "name": "timeout_seconds", "type": "DefaultValuedOptionalAttr" },
      { "name": "is_stateless", "type": "DefaultValuedOptionalAttr" },
      { "name": "max_subdivs_per_device", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.Complex",
    "summary": "Converts two real numbers to a complex number.",
    "description": "Given a tensor `real` representing the real part of a complex number, and a\ntensor `imag` representing the imaginary part of a complex number, this\noperation returns complex numbers elementwise of the form \\\\(a + bj\\\\), where\n*a* represents the `real` part and *b* represents the `imag` part.\n\nThe input tensors `real` and `imag` must have the same shape.\n\nFor example:\n\n```\n# tensor 'real' is [2.25, 3.25]\n# tensor `imag` is [4.75, 5.75]\ntf.complex(real, imag) ==> [[2.25 + 4.75j], [3.25 + 5.75j]]\n```",
    "inputs": [
      { "name": "real", "type": "TF_F32OrF64Tensor" },
      { "name": "imag", "type": "TF_F32OrF64Tensor" }
    ],
    "outputs": [
      { "name": "out", "type": "TensorOf" }
    ]
  },
  {
    "name": "tf.ComplexAbs",
    "summary": "Computes the complex absolute value of a tensor.",
    "description": "Given a tensor `x` of complex numbers, this operation returns a tensor of type\n`float` or `double` that is the absolute value of each element in `x`. All\nelements in `x` must be complex numbers of the form \\\\(a + bj\\\\). The absolute\nvalue is computed as \\\\( \\sqrt{a^2 + b^2}\\\\).\n\nFor example:\n\n>>> x = tf.complex(3.0, 4.0)\n>>> print((tf.raw_ops.ComplexAbs(x=x, Tout=tf.dtypes.float32, name=None)).numpy())\n5.0",
    "inputs": [
      { "name": "x", "type": "TensorOf" }
    ],
    "outputs": [
      { "name": "y", "type": "TF_F32OrF64Tensor" }
    ]
  },
  {
    "name": "tf.Concat",
    "summary": "Concatenates tensors along one dimension.",
    "inputs": [
      { "name": "concat_dim", "type": "Arg" },
      { "name": "values", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ]
  },
  {
    "name": "tf.ConcatOffset",
    "summary": "Computes offsets of concat inputs within its output.",
    "description": "For example:\n\n>>> x = [2, 2, 7]\n>>> y = [2, 3, 7]\n>>> z = [2, 9, 7]\n>>> offsets = concat_offset(1, [x, y, z])\n>>> [[a.item() for a in list(off.numpy())] for off in offsets]\n[[0, 0, 0], [0, 2, 0], [0, 5, 0]]\n\nThis is typically used by gradient computations for a concat operation.",
    "inputs": [
      { "name": "concat_dim", "type": "Arg" },
      { "name": "shape", "type": "Arg" }
    ],
    "outputs": [
      { "name": "offset", "type": "Res" }
    ]
  },
  {
    "name": "tf.ConcatV2",
    "summary": "Concatenates tensors along one dimension.",
    "inputs": [
      { "name": "values", "type": "Arg" },
      { "name": "axis", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ]
  },
  {
    "name": "tf.ConfigureAndInitializeGlobalTPU",
    "summary": "An op that initialize the TPU system in a multi-client set up.",
    "description": "Initializes global TPU system for mutli-client execution.\n\nThis op does the work of both ConfigureDistributedTpuOp and\nInitializeHostForDistributedTpuOp, and outputs the latter's result.",
    "outputs": [
      { "name": "output", "type": "Res" }
    ]
  },
  {
    "name": "tf.ConfigureDistributedTPU",
    "summary": "Sets up the centralized structures for a distributed TPU system.",
    "outputs": [
      { "name": "topology", "type": "Res" }
    ],
    "attributes": [
      { "name": "embedding_config", "type": "DefaultValuedOptionalAttr" },
      { "name": "tpu_embedding_config", "type": "DefaultValuedOptionalAttr" },
      { "name": "is_global_init", "type": "DefaultValuedOptionalAttr" },
      { "name": "enable_whole_mesh_compilations", "type": "DefaultValuedOptionalAttr" },
      { "name": "compilation_failure_closes_chips", "type": "DefaultValuedOptionalAttr" },
      { "name": "tpu_cancellation_closes_chips", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.ConfigureTPUEmbedding",
    "summary": "Sets up TPUEmbedding in a distributed TPU system.",
    "attributes": [
      { "name": "config", "type": "StrAttr" }
    ]
  },
  {
    "name": "tf.ConfigureTPUEmbeddingHost",
    "summary": "An op that configures the TPUEmbedding software on a host.",
    "inputs": [
      { "name": "common_config", "type": "Arg" },
      { "name": "memory_config", "type": "Arg" }
    ],
    "outputs": [
      { "name": "network_config", "type": "Res" }
    ],
    "attributes": [
      { "name": "config", "type": "StrAttr" }
    ]
  },
  {
    "name": "tf.ConfigureTPUEmbeddingMemory",
    "summary": "An op that configures the TPUEmbedding software on a host.",
    "inputs": [
      { "name": "common_config", "type": "Arg" }
    ],
    "outputs": [
      { "name": "memory_config", "type": "Res" }
    ]
  },
  {
    "name": "tf.Conj",
    "summary": "Returns the complex conjugate of a complex number.",
    "description": "Given a tensor `input` of complex numbers, this operation returns a tensor of\ncomplex numbers that are the complex conjugate of each element in `input`. The\ncomplex numbers in `input` must be of the form \\\\(a + bj\\\\), where *a* is the\nreal part and *b* is the imaginary part.\n\nThe complex conjugate returned by this operation is of the form \\\\(a - bj\\\\).\n\nFor example:\n\n```\n# tensor 'input' is [-2.25 + 4.75j, 3.25 + 5.75j]\ntf.conj(input) ==> [-2.25 - 4.75j, 3.25 - 5.75j]\n```",
    "inputs": [
      { "name": "input", "type": "TensorOf" }
    ],
    "outputs": [
      { "name": "output", "type": "TensorOf" }
    ]
  },
  {
    "name": "tf.ConjugateTranspose",
    "summary": "Shuffle dimensions of x according to a permutation and conjugate the result.",
    "description": "The output `y` has the same rank as `x`. The shapes of `x` and `y` satisfy:\n  `y.shape[i] == x.shape[perm[i]] for i in [0, 1, ..., rank(x) - 1]`\n  `y[i,j,k,...,s,t,u] == conj(x[perm[i], perm[j], perm[k],...,perm[s], perm[t], perm[u]])`",
    "inputs": [
      { "name": "x", "type": "TF_Tensor" },
      { "name": "perm", "type": "TF_I32OrI64Tensor" }
    ],
    "outputs": [
      { "name": "y", "type": "TF_Tensor" }
    ]
  },
  {
    "name": "tf.ConnectTPUEmbeddingHosts",
    "summary": "An op that sets up communication between TPUEmbedding host software instances",
    "description": "after ConfigureTPUEmbeddingHost has been called on each host.",
    "inputs": [
      { "name": "network_configs", "type": "Arg" }
    ]
  },
  {
    "name": "tf.Const",
    "summary": "Constant tensor op",
    "outputs": [
      { "name": "output", "type": "TF_Tensor" }
    ],
    "attributes": [
      { "name": "value", "type": "ElementsAttr" }
    ]
  },
  {
    "name": "tf.Conv",
    "summary": "Computes a N-D convolution given (N+1+batch_dims)-D `input` and (N+2)-D `filter` tensors.",
    "description": "General function for computing a N-D convolution. It is required that\n`1 <= N <= 3`.",
    "inputs": [
      { "name": "input", "type": "Arg" },
      { "name": "filter", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "strides", "type": "I64ArrayAttr" },
      { "name": "padding", "type": "TF_AnyStrAttrOf" },
      { "name": "explicit_paddings", "type": "DefaultValuedOptionalAttr" },
      { "name": "data_format", "type": "DefaultValuedOptionalAttr" },
      { "name": "dilations", "type": "DefaultValuedOptionalAttr" },
      { "name": "batch_dims", "type": "DefaultValuedOptionalAttr" },
      { "name": "groups", "type": "DefaultValuedOptionalAttr" }
    ],
    "category": "Layer"
  },
  {
    "name": "tf.Conv2D",
    "summary": "Computes a 2-D convolution given 4-D `input` and `filter` tensors.",
    "description": "Given an input tensor of shape `[batch, in_height, in_width, in_channels]`\nand a filter / kernel tensor of shape\n`[filter_height, filter_width, in_channels, out_channels]`, this op\nperforms the following:\n\n1. Flattens the filter to a 2-D matrix with shape\n   `[filter_height * filter_width * in_channels, output_channels]`.\n2. Extracts image patches from the input tensor to form a *virtual*\n   tensor of shape `[batch, out_height, out_width,\n   filter_height * filter_width * in_channels]`.\n3. For each patch, right-multiplies the filter matrix and the image patch\n   vector.\n\nIn detail, with the default NHWC format,\n\n    output[b, i, j, k] =\n        sum_{di, dj, q} input[b, strides[1] * i + di, strides[2] * j + dj, q] *\n                        filter[di, dj, q, k]\n\nMust have `strides[0] = strides[3] = 1`.  For the most common case of the same\nhorizontal and vertices strides, `strides = [1, stride, stride, 1]`.",
    "inputs": [
      { "name": "input", "type": "Arg" },
      { "name": "filter", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "strides", "type": "I64ArrayAttr" },
      { "name": "use_cudnn_on_gpu", "type": "DefaultValuedOptionalAttr" },
      { "name": "padding", "type": "TF_AnyStrAttrOf" },
      { "name": "explicit_paddings", "type": "DefaultValuedOptionalAttr" },
      { "name": "data_format", "type": "DefaultValuedOptionalAttr" },
      { "name": "dilations", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.Conv2DBackpropFilter",
    "summary": "Computes the gradients of convolution with respect to the filter.",
    "inputs": [
      { "name": "input", "type": "Arg" },
      { "name": "filter_sizes", "type": "Arg" },
      { "name": "out_backprop", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "strides", "type": "I64ArrayAttr" },
      { "name": "use_cudnn_on_gpu", "type": "DefaultValuedOptionalAttr" },
      { "name": "padding", "type": "TF_AnyStrAttrOf" },
      { "name": "explicit_paddings", "type": "DefaultValuedOptionalAttr" },
      { "name": "data_format", "type": "DefaultValuedOptionalAttr" },
      { "name": "dilations", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.Conv2DBackpropFilterV2",
    "summary": "Computes the gradients of convolution with respect to the filter.",
    "inputs": [
      { "name": "input", "type": "Arg" },
      { "name": "filter", "type": "Arg" },
      { "name": "out_backprop", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "strides", "type": "I64ArrayAttr" },
      { "name": "use_cudnn_on_gpu", "type": "DefaultValuedOptionalAttr" },
      { "name": "padding", "type": "TF_AnyStrAttrOf" },
      { "name": "explicit_paddings", "type": "DefaultValuedOptionalAttr" },
      { "name": "data_format", "type": "DefaultValuedOptionalAttr" },
      { "name": "dilations", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.Conv2DBackpropInput",
    "summary": "Computes the gradients of convolution with respect to the input.",
    "inputs": [
      { "name": "input_sizes", "type": "Arg" },
      { "name": "filter", "type": "Arg" },
      { "name": "out_backprop", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "strides", "type": "I64ArrayAttr" },
      { "name": "use_cudnn_on_gpu", "type": "DefaultValuedOptionalAttr" },
      { "name": "padding", "type": "TF_AnyStrAttrOf" },
      { "name": "explicit_paddings", "type": "DefaultValuedOptionalAttr" },
      { "name": "data_format", "type": "DefaultValuedOptionalAttr" },
      { "name": "dilations", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.Conv2DBackpropInputV2",
    "summary": "Computes the gradients of convolution with respect to the input.",
    "inputs": [
      { "name": "input", "type": "Arg" },
      { "name": "filter", "type": "Arg" },
      { "name": "out_backprop", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "strides", "type": "I64ArrayAttr" },
      { "name": "use_cudnn_on_gpu", "type": "DefaultValuedOptionalAttr" },
      { "name": "padding", "type": "TF_AnyStrAttrOf" },
      { "name": "explicit_paddings", "type": "DefaultValuedOptionalAttr" },
      { "name": "data_format", "type": "DefaultValuedOptionalAttr" },
      { "name": "dilations", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.Conv3D",
    "summary": "Computes a 3-D convolution given 5-D `input` and `filter` tensors.",
    "description": "In signal processing, cross-correlation is a measure of similarity of\ntwo waveforms as a function of a time-lag applied to one of them. This\nis also known as a sliding dot product or sliding inner-product.\n\nOur Conv3D implements a form of cross-correlation.",
    "inputs": [
      { "name": "input", "type": "Arg" },
      { "name": "filter", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "TF_FloatTensor" }
    ],
    "attributes": [
      { "name": "strides", "type": "ConfinedAttr" },
      { "name": "padding", "type": "TF_AnyStrAttrOf" },
      { "name": "data_format", "type": "DefaultValuedOptionalAttr" },
      { "name": "dilations", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.Conv3DBackpropFilter",
    "summary": "Computes the gradients of 3-D convolution with respect to the filter.",
    "inputs": [
      { "name": "input", "type": "Arg" },
      { "name": "filter", "type": "Arg" },
      { "name": "out_backprop", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "TensorOf" }
    ],
    "attributes": [
      { "name": "strides", "type": "ConfinedAttr" },
      { "name": "padding", "type": "TF_AnyStrAttrOf" },
      { "name": "dilations", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.Conv3DBackpropFilterV2",
    "summary": "Computes the gradients of 3-D convolution with respect to the filter.",
    "inputs": [
      { "name": "input", "type": "Arg" },
      { "name": "filter_sizes", "type": "Arg" },
      { "name": "out_backprop", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "TF_FloatTensor" }
    ],
    "attributes": [
      { "name": "strides", "type": "ConfinedAttr" },
      { "name": "padding", "type": "TF_AnyStrAttrOf" },
      { "name": "data_format", "type": "DefaultValuedOptionalAttr" },
      { "name": "dilations", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.Conv3DBackpropInput",
    "summary": "Computes the gradients of 3-D convolution with respect to the input.",
    "inputs": [
      { "name": "input", "type": "Arg" },
      { "name": "filter", "type": "Arg" },
      { "name": "out_backprop", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "TensorOf" }
    ],
    "attributes": [
      { "name": "strides", "type": "ConfinedAttr" },
      { "name": "padding", "type": "TF_AnyStrAttrOf" },
      { "name": "dilations", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.Conv3DBackpropInputV2",
    "summary": "Computes the gradients of 3-D convolution with respect to the input.",
    "inputs": [
      { "name": "input_sizes", "type": "Arg" },
      { "name": "filter", "type": "Arg" },
      { "name": "out_backprop", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "TF_FloatTensor" }
    ],
    "attributes": [
      { "name": "strides", "type": "ConfinedAttr" },
      { "name": "padding", "type": "TF_AnyStrAttrOf" },
      { "name": "data_format", "type": "DefaultValuedOptionalAttr" },
      { "name": "dilations", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.ConvertToCooTensor",
    "summary": "Op that converts tensors into coo format.",
    "description": "This op coverts the dense, sparse and ragged tensor into standard coo tensor\nformat which contains three 1D tensors.",
    "inputs": [
      { "name": "indices_or_row_splits", "type": "TF_Int32Tensor" },
      { "name": "values", "type": "TF_Int32Tensor" },
      { "name": "weights", "type": "TF_Float32Tensor" }
    ],
    "outputs": [
      { "name": "row_ids", "type": "TF_Int32Tensor" },
      { "name": "col_ids", "type": "TF_Int32Tensor" },
      { "name": "gains", "type": "TF_Float32Tensor" }
    ],
    "attributes": [
      { "name": "sample_count", "type": "ConfinedAttr" },
      { "name": "combiner", "type": "StrAttr" }
    ]
  },
  {
    "name": "tf.ConvertToListOfSparseCoreCooTensors",
    "summary": "An op which converts the sparse/ragged/dense tensor into a list of COO tensor for each SparseCore.",
    "inputs": [
      { "name": "indices_or_row_splits", "type": "TF_Int32Tensor" },
      { "name": "values", "type": "TF_Int32Tensor" },
      { "name": "weights", "type": "TF_Float32Tensor" }
    ],
    "outputs": [
      { "name": "row_ids_list", "type": "Variadic" },
      { "name": "col_ids_list", "type": "Variadic" },
      { "name": "gains_list", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "sample_count", "type": "ConfinedAttr" },
      { "name": "row_offset", "type": "ConfinedAttr" },
      { "name": "col_offset", "type": "ConfinedAttr" },
      { "name": "col_shift", "type": "ConfinedAttr" },
      { "name": "num_sc_shards", "type": "ConfinedAttr" },
      { "name": "stacked_table_sample_count", "type": "ConfinedAttr" },
      { "name": "combiner", "type": "StrAttr" }
    ]
  },
  {
    "name": "tf.ConvertToSparseCoreCsrWrappedCooTensorOp",
    "summary": "An op which converts the sorted coo tensor into sparse core CSR wrapped COO format.",
    "inputs": [
      { "name": "sorted_row_ids_list", "type": "Variadic" },
      { "name": "sorted_col_ids_list", "type": "Variadic" },
      { "name": "sorted_gains_list", "type": "Variadic" },
      { "name": "id_counts_list", "type": "Variadic" },
      { "name": "splits", "type": "TF_Int64Tensor" }
    ],
    "outputs": [
      { "name": "row_pointers", "type": "TF_Int32Tensor" },
      { "name": "sorted_sample_ids", "type": "TF_Int32Tensor" },
      { "name": "sorted_token_ids", "type": "TF_Int32Tensor" },
      { "name": "sorted_gains", "type": "TF_Float32Tensor" },
      { "name": "row_pointers_unpadded_size", "type": "TF_Int32Tensor" },
      { "name": "ids_unpadded_size", "type": "TF_Int32Tensor" },
      { "name": "num_minibatches_per_sc", "type": "TF_Int32Tensor" }
    ],
    "attributes": [
      { "name": "sample_count_per_sc", "type": "ConfinedAttr" },
      { "name": "num_replica", "type": "ConfinedAttr" },
      { "name": "max_minibatches_per_sc", "type": "ConfinedAttr" },
      { "name": "max_ids_per_chip_per_sample", "type": "ConfinedAttr" },
      { "name": "table_vocab_size", "type": "ConfinedAttr" },
      { "name": "feature_width", "type": "ConfinedAttr" },
      { "name": "table_name", "type": "StrAttr" },
      { "name": "allow_id_dropping", "type": "BoolAttr" }
    ]
  },
  {
    "name": "tf.Cos",
    "summary": "Computes cos of x element-wise.",
    "description": "Given an input tensor, this function computes cosine of every\n  element in the tensor. Input range is `(-inf, inf)` and\n  output range is `[-1,1]`. If input lies outside the boundary, `nan`\n  is returned.\n\n  ```python\n  x = tf.constant([-float(\"inf\"), -9, -0.5, 1, 1.2, 200, 10000, float(\"inf\")])\n  tf.math.cos(x) ==> [nan -0.91113025 0.87758255 0.5403023 0.36235774 0.48718765 -0.95215535 nan]\n  ```",
    "inputs": [
      { "name": "x", "type": "TF_FpOrComplexTensor" }
    ],
    "outputs": [
      { "name": "y", "type": "TF_FpOrComplexTensor" }
    ]
  },
  {
    "name": "tf.Cosh",
    "summary": "Computes hyperbolic cosine of x element-wise.",
    "description": "Given an input tensor, this function computes hyperbolic cosine of every\n  element in the tensor. Input range is `[-inf, inf]` and output range\n  is `[1, inf]`.\n\n  ```python\n  x = tf.constant([-float(\"inf\"), -9, -0.5, 1, 1.2, 2, 10, float(\"inf\")])\n  tf.math.cosh(x) ==> [inf 4.0515420e+03 1.1276259e+00 1.5430807e+00 1.8106556e+00 3.7621956e+00 1.1013233e+04 inf]\n  ```",
    "inputs": [
      { "name": "x", "type": "TF_FpOrComplexTensor" }
    ],
    "outputs": [
      { "name": "y", "type": "TF_FpOrComplexTensor" }
    ]
  },
  {
    "name": "tf.CreateSummaryDbWriter",
    "summary": "Creates summary database writer accessible by given resource handle.",
    "description": "This can be used to write tensors from the execution graph directly\nto a database. Only SQLite is supported right now. This function\nwill create the schema if it doesn't exist. Entries in the Users,\nExperiments, and Runs tables will be created automatically if they\ndon't already exist.\n\nwriter: Handle to SummaryWriter resource to overwrite.\ndb_uri: For example \"file:/tmp/foo.sqlite\".\nexperiment_name: Can't contain ASCII control characters or <>. Case\n  sensitive. If empty, then the Run will not be associated with any\n  Experiment.\nrun_name: Can't contain ASCII control characters or <>. Case sensitive.\n  If empty, then each Tag will not be associated with any Run.\nuser_name: Must be valid as both a DNS label and Linux username. If\n  empty, then the Experiment will not be associated with any User.",
    "inputs": [
      { "name": "writer", "type": "Arg" },
      { "name": "db_uri", "type": "TF_StrTensor" },
      { "name": "experiment_name", "type": "TF_StrTensor" },
      { "name": "run_name", "type": "TF_StrTensor" },
      { "name": "user_name", "type": "TF_StrTensor" }
    ]
  },
  {
    "name": "tf.CreateSummaryFileWriter",
    "summary": "Creates a summary file writer accessible by the given resource handle.",
    "description": "writer: A handle to the summary writer resource\nlogdir: Directory where the event file will be written.\nmax_queue: Size of the queue of pending events and summaries.\nflush_millis: How often, in milliseconds, to flush the pending events and\n  summaries to disk.\nfilename_suffix: Every event file's name is suffixed with this suffix.",
    "inputs": [
      { "name": "writer", "type": "Arg" },
      { "name": "logdir", "type": "TF_StrTensor" },
      { "name": "max_queue", "type": "TF_Int32Tensor" },
      { "name": "flush_millis", "type": "TF_Int32Tensor" },
      { "name": "filename_suffix", "type": "TF_StrTensor" }
    ]
  },
  {
    "name": "tf.Cross",
    "summary": "Compute the pairwise cross product.",
    "description": "`a` and `b` must be the same shape; they can either be simple 3-element vectors,\nor any shape where the innermost dimension is 3. In the latter case, each pair\nof corresponding 3-element vectors is cross-multiplied independently.",
    "inputs": [
      { "name": "a", "type": "Arg" },
      { "name": "b", "type": "Arg" }
    ],
    "outputs": [
      { "name": "product", "type": "Res" }
    ]
  },
  {
    "name": "tf.CrossReplicaSum",
    "summary": "An Op to sum inputs across replicated TPU instances.",
    "description": "Each instance supplies its own input.\n\nFor example, suppose there are 8 TPU instances: `[A, B, C, D, E, F, G, H]`.\nPassing group_assignment=`[[0,2,4,6],[1,3,5,7]]` sets `A, C, E, G` as group 0,\nand `B, D, F, H` as group 1. Thus we get the outputs:\n`[A+C+E+G, B+D+F+H, A+C+E+G, B+D+F+H, A+C+E+G, B+D+F+H, A+C+E+G, B+D+F+H]`.",
    "inputs": [
      { "name": "input", "type": "Arg" },
      { "name": "group_assignment", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ]
  },
  {
    "name": "tf.Cumprod",
    "summary": "Compute the cumulative product of the tensor `x` along `axis`.",
    "description": "By default, this op performs an inclusive cumprod, which means that the first\nelement of the input is identical to the first element of the output:\n\n```python\ntf.cumprod([a, b, c])  # => [a, a * b, a * b * c]\n```\n\nBy setting the `exclusive` kwarg to `True`, an exclusive cumprod is\nperformed instead:\n\n```python\ntf.cumprod([a, b, c], exclusive=True)  # => [1, a, a * b]\n```\n\nBy setting the `reverse` kwarg to `True`, the cumprod is performed in the\nopposite direction:\n\n```python\ntf.cumprod([a, b, c], reverse=True)  # => [a * b * c, b * c, c]\n```\n\nThis is more efficient than using separate `tf.reverse` ops.\n\nThe `reverse` and `exclusive` kwargs can also be combined:\n\n```python\ntf.cumprod([a, b, c], exclusive=True, reverse=True)  # => [b * c, c, 1]\n```",
    "inputs": [
      { "name": "x", "type": "Arg" },
      { "name": "axis", "type": "Arg" }
    ],
    "outputs": [
      { "name": "out", "type": "TF_NumberTensor" }
    ],
    "attributes": [
      { "name": "exclusive", "type": "DefaultValuedOptionalAttr" },
      { "name": "reverse", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.Cumsum",
    "summary": "Compute the cumulative sum of the tensor `x` along `axis`.",
    "description": "By default, this op performs an inclusive cumsum, which means that the first\nelement of the input is identical to the first element of the output:\n\n```python\ntf.cumsum([a, b, c])  # => [a, a + b, a + b + c]\n```\n\nBy setting the `exclusive` kwarg to `True`, an exclusive cumsum is\nperformed instead:\n\n```python\ntf.cumsum([a, b, c], exclusive=True)  # => [0, a, a + b]\n```\n\nBy setting the `reverse` kwarg to `True`, the cumsum is performed in the\nopposite direction:\n\n```python\ntf.cumsum([a, b, c], reverse=True)  # => [a + b + c, b + c, c]\n```\n\nThis is more efficient than using separate `tf.reverse` ops.\n\nThe `reverse` and `exclusive` kwargs can also be combined:\n\n```python\ntf.cumsum([a, b, c], exclusive=True, reverse=True)  # => [b + c, c, 0]\n```",
    "inputs": [
      { "name": "x", "type": "Arg" },
      { "name": "axis", "type": "Arg" }
    ],
    "outputs": [
      { "name": "out", "type": "TF_NumberTensor" }
    ],
    "attributes": [
      { "name": "exclusive", "type": "DefaultValuedOptionalAttr" },
      { "name": "reverse", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.CumulativeLogsumexp",
    "summary": "Compute the cumulative product of the tensor `x` along `axis`.",
    "description": "By default, this op performs an inclusive cumulative log-sum-exp,\nwhich means that the first\nelement of the input is identical to the first element of the output:\n```python\ntf.math.cumulative_logsumexp([a, b, c])  # => [a, log(exp(a) + exp(b)), log(exp(a) + exp(b) + exp(c))]\n```\n\nBy setting the `exclusive` kwarg to `True`, an exclusive cumulative log-sum-exp is\nperformed instead:\n```python\ntf.cumulative_logsumexp([a, b, c], exclusive=True)  # => [-inf, a, log(exp(a) * exp(b))]\n```\nNote that the neutral element of the log-sum-exp operation is `-inf`,\nhowever, for performance reasons, the minimal value representable by the\nfloating point type is used instead.\n\nBy setting the `reverse` kwarg to `True`, the cumulative log-sum-exp is performed in the\nopposite direction.",
    "inputs": [
      { "name": "x", "type": "Arg" },
      { "name": "axis", "type": "Arg" }
    ],
    "outputs": [
      { "name": "out", "type": "TF_FloatTensor" }
    ],
    "attributes": [
      { "name": "exclusive", "type": "DefaultValuedOptionalAttr" },
      { "name": "reverse", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.DataFormatDimMap",
    "summary": "Returns the dimension index in the destination data format given the one in",
    "description": "the source data format.",
    "inputs": [
      { "name": "x", "type": "Arg" }
    ],
    "outputs": [
      { "name": "y", "type": "Res" }
    ],
    "attributes": [
      { "name": "src_format", "type": "DefaultValuedOptionalAttr" },
      { "name": "dst_format", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.DataFormatVecPermute",
    "summary": "Permute input tensor from `src_format` to `dst_format`.",
    "description": "Given source and destination format strings of length n=4 or 5, the input\ntensor must be a vector of size n or n-2, or a 2D tensor of shape\n(n, 2) or (n-2, 2).\n\nIf the first dimension of the input tensor is n-2, it is assumed that\nnon-spatial dimensions are omitted (i.e `N`, `C`).\n\nFor example, with `src_format` of `NHWC`, `dst_format` of `NCHW`, and input:\n```\n[1, 2, 3, 4]\n```\n, the output will be:\n```\n[1, 4, 2, 3]\n```\nWith `src_format` of `NDHWC`, `dst_format` of `NCDHW`, and input:\n```\n[[1, 6], [2, 7], [3, 8], [4, 9], [5, 10]]\n```\n, the output will be:\n```\n[[1, 6], [5, 10], [2, 7], [3, 8], [4, 9]]\n```\nWith `src_format` of `NHWC`, `dst_format` of `NCHW`, and input:\n```\n[1, 2]\n```\n, the output will be:\n```\n[1, 2]\n```",
    "inputs": [
      { "name": "x", "type": "Arg" }
    ],
    "outputs": [
      { "name": "y", "type": "Res" }
    ],
    "attributes": [
      { "name": "src_format", "type": "DefaultValuedOptionalAttr" },
      { "name": "dst_format", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.DebugIdentity",
    "summary": "Provides an identity mapping of the non-Ref type input tensor for debugging.",
    "description": "Provides an identity mapping of the non-Ref type input tensor for debugging.",
    "inputs": [
      { "name": "input", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "TF_Tensor" }
    ],
    "attributes": [
      { "name": "device_name", "type": "DefaultValuedOptionalAttr" },
      { "name": "tensor_name", "type": "DefaultValuedOptionalAttr" },
      { "name": "debug_urls", "type": "DefaultValuedOptionalAttr" },
      { "name": "gated_grpc", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.DebugIdentityV2",
    "summary": "Debug Identity V2 Op.",
    "description": "Provides an identity mapping from input to output, while writing the content of\nthe input tensor by calling DebugEventsWriter.\n\nThe semantics of the input tensor depends on tensor_debug_mode. In typical\nusage, the input tensor comes directly from the user computation only when\ngraph_debug_mode is FULL_TENSOR (see protobuf/debug_event.proto for a\nlist of all the possible values of graph_debug_mode). For the other debug modes,\nthe input tensor should be produced by an additional op or subgraph that\ncomputes summary information about one or more tensors.",
    "inputs": [
      { "name": "input", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "TF_Tensor" }
    ],
    "attributes": [
      { "name": "tfdbg_context_id", "type": "DefaultValuedOptionalAttr" },
      { "name": "op_name", "type": "DefaultValuedOptionalAttr" },
      { "name": "output_slot", "type": "DefaultValuedOptionalAttr" },
      { "name": "tensor_debug_mode", "type": "DefaultValuedOptionalAttr" },
      { "name": "debug_urls", "type": "DefaultValuedOptionalAttr" },
      { "name": "circular_buffer_size", "type": "DefaultValuedOptionalAttr" },
      { "name": "tfdbg_run_id", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.DecodeAndCropJpeg",
    "summary": "Decode and Crop a JPEG-encoded image to a uint8 tensor.",
    "description": "The attr `channels` indicates the desired number of color channels for the\ndecoded image.\n\nAccepted values are:\n\n*   0: Use the number of channels in the JPEG-encoded image.\n*   1: output a grayscale image.\n*   3: output an RGB image.\n\nIf needed, the JPEG-encoded image is transformed to match the requested number\nof color channels.\n\nThe attr `ratio` allows downscaling the image by an integer factor during\ndecoding.  Allowed values are: 1, 2, 4, and 8.  This is much faster than\ndownscaling the image later.\n\n\nIt is equivalent to a combination of decode and crop, but much faster by only\ndecoding partial jpeg image.",
    "inputs": [
      { "name": "contents", "type": "Arg" },
      { "name": "crop_window", "type": "Arg" }
    ],
    "outputs": [
      { "name": "image", "type": "Res" }
    ],
    "attributes": [
      { "name": "channels", "type": "DefaultValuedOptionalAttr" },
      { "name": "ratio", "type": "DefaultValuedOptionalAttr" },
      { "name": "fancy_upscaling", "type": "DefaultValuedOptionalAttr" },
      { "name": "try_recover_truncated", "type": "DefaultValuedOptionalAttr" },
      { "name": "acceptable_fraction", "type": "DefaultValuedOptionalAttr" },
      { "name": "dct_method", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.DecodeGif",
    "summary": "Decode the frame(s) of a GIF-encoded image to a uint8 tensor.",
    "description": "GIF images with frame or transparency compression are not supported.\nOn Linux and MacOS systems, convert animated GIFs from compressed to\nuncompressed by running:\n\n    convert $src.gif -coalesce $dst.gif\n\nThis op also supports decoding JPEGs and PNGs, though it is cleaner to use\n`tf.io.decode_image`.",
    "inputs": [
      { "name": "contents", "type": "Arg" }
    ],
    "outputs": [
      { "name": "image", "type": "Res" }
    ]
  },
  {
    "name": "tf.DecodeJpeg",
    "summary": "Decode a JPEG-encoded image to a uint8 tensor.",
    "description": "The attr `channels` indicates the desired number of color channels for the\ndecoded image.\n\nAccepted values are:\n\n*   0: Use the number of channels in the JPEG-encoded image.\n*   1: output a grayscale image.\n*   3: output an RGB image.\n\nIf needed, the JPEG-encoded image is transformed to match the requested number\nof color channels.\n\nThe attr `ratio` allows downscaling the image by an integer factor during\ndecoding.  Allowed values are: 1, 2, 4, and 8.  This is much faster than\ndownscaling the image later.\n\n\nThis op also supports decoding PNGs and non-animated GIFs since the interface is\nthe same, though it is cleaner to use `tf.io.decode_image`.",
    "inputs": [
      { "name": "contents", "type": "Arg" }
    ],
    "outputs": [
      { "name": "image", "type": "Res" }
    ],
    "attributes": [
      { "name": "channels", "type": "DefaultValuedOptionalAttr" },
      { "name": "ratio", "type": "DefaultValuedOptionalAttr" },
      { "name": "fancy_upscaling", "type": "DefaultValuedOptionalAttr" },
      { "name": "try_recover_truncated", "type": "DefaultValuedOptionalAttr" },
      { "name": "acceptable_fraction", "type": "DefaultValuedOptionalAttr" },
      { "name": "dct_method", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.DecodePaddedRaw",
    "summary": "Reinterpret the bytes of a string as a vector of numbers.",
    "inputs": [
      { "name": "input_bytes", "type": "Arg" },
      { "name": "fixed_length", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "little_endian", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.DecodePng",
    "summary": "Decode a PNG-encoded image to a uint8 or uint16 tensor.",
    "description": "The attr `channels` indicates the desired number of color channels for the\ndecoded image.\n\nAccepted values are:\n\n*   0: Use the number of channels in the PNG-encoded image.\n*   1: output a grayscale image.\n*   3: output an RGB image.\n*   4: output an RGBA image.\n\nIf needed, the PNG-encoded image is transformed to match the requested number\nof color channels.\n\nThis op also supports decoding JPEGs and non-animated GIFs since the interface\nis the same, though it is cleaner to use `tf.io.decode_image`.",
    "inputs": [
      { "name": "contents", "type": "Arg" }
    ],
    "outputs": [
      { "name": "image", "type": "Res" }
    ],
    "attributes": [
      { "name": "channels", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.DeleteIterator",
    "summary": "A container for an iterator resource.",
    "inputs": [
      { "name": "handle", "type": "Arg" },
      { "name": "deleter", "type": "Arg" }
    ]
  },
  {
    "name": "tf.DeleteMemoryCache",
    "inputs": [
      { "name": "handle", "type": "Arg" },
      { "name": "deleter", "type": "TF_VariantTensor" }
    ]
  },
  {
    "name": "tf.DeleteMultiDeviceIterator",
    "summary": "A container for an iterator resource.",
    "inputs": [
      { "name": "multi_device_iterator", "type": "Arg" },
      { "name": "iterators", "type": "Arg" },
      { "name": "deleter", "type": "Arg" }
    ]
  },
  {
    "name": "tf.DeleteRandomSeedGenerator",
    "inputs": [
      { "name": "handle", "type": "Arg" },
      { "name": "deleter", "type": "TF_VariantTensor" }
    ]
  },
  {
    "name": "tf.DeleteSeedGenerator",
    "inputs": [
      { "name": "handle", "type": "Arg" },
      { "name": "deleter", "type": "TF_VariantTensor" }
    ]
  },
  {
    "name": "tf.DepthToSpace",
    "summary": "DepthToSpace for tensors of type T.",
    "description": "Rearranges data from depth into blocks of spatial data.\nThis is the reverse transformation of SpaceToDepth. More specifically,\nthis op outputs a copy of the input tensor where values from the `depth`\ndimension are moved in spatial blocks to the `height` and `width` dimensions.\nThe attr `block_size` indicates the input block size and how the data is moved.\n\n  * Chunks of data of size `block_size * block_size` from depth are rearranged\n    into non-overlapping blocks of size `block_size x block_size`\n  * The width of the output tensor is `input_depth * block_size`, whereas the\n    height is `input_height * block_size`.\n  * The Y, X coordinates within each block of the output image are determined\n    by the high order component of the input channel index.\n  * The depth of the input tensor must be divisible by\n    `block_size * block_size`.\n\nThe `data_format` attr specifies the layout of the input and output tensors\nwith the following options:\n  \"NHWC\": `[ batch, height, width, channels ]`\n  \"NCHW\": `[ batch, channels, height, width ]`\n  \"NCHW_VECT_C\":\n      `qint8 [ batch, channels / 4, height, width, 4 ]`\n\nIt is useful to consider the operation as transforming a 6-D Tensor.\ne.g. for data_format = NHWC,\n     Each element in the input tensor can be specified via 6 coordinates,\n     ordered by decreasing memory layout significance as:\n     n,iY,iX,bY,bX,oC  (where n=batch index, iX, iY means X or Y coordinates\n                        within the input image, bX, bY means coordinates\n                        within the output block, oC means output channels).\n     The output would be the input transposed to the following layout:\n     n,iY,bY,iX,bX,oC\n\nThis operation is useful for resizing the activations between convolutions\n(but keeping all data), e.g. instead of pooling. It is also useful for training\npurely convolutional models.\n\nFor example, given an input of shape `[1, 1, 1, 4]`, data_format = \"NHWC\" and\nblock_size = 2:\n\n```\nx = [[[[1, 2, 3, 4]]]]\n\n```\n\nThis operation will output a tensor of shape `[1, 2, 2, 1]`:\n\n```\n   [[[[1], [2]],\n     [[3], [4]]]]\n```\n\nHere, the input has a batch of 1 and each batch element has shape `[1, 1, 4]`,\nthe corresponding output will have 2x2 elements and will have a depth of\n1 channel (1 = `4 / (block_size * block_size)`).\nThe output element shape is `[2, 2, 1]`.\n\nFor an input tensor with larger depth, here of shape `[1, 1, 1, 12]`, e.g.\n\n```\nx = [[[[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]]]]\n```\n\nThis operation, for block size of 2, will return the following tensor of shape\n`[1, 2, 2, 3]`\n\n```\n   [[[[1, 2, 3], [4, 5, 6]],\n     [[7, 8, 9], [10, 11, 12]]]]\n\n```\n\nSimilarly, for the following input of shape `[1 2 2 4]`, and a block size of 2:\n\n```\nx =  [[[[1, 2, 3, 4],\n       [5, 6, 7, 8]],\n      [[9, 10, 11, 12],\n       [13, 14, 15, 16]]]]\n```\n\nthe operator will return the following tensor of shape `[1 4 4 1]`:\n\n```\nx = [[[ [1],   [2],  [5],  [6]],\n      [ [3],   [4],  [7],  [8]],\n      [ [9],  [10], [13],  [14]],\n      [ [11], [12], [15],  [16]]]]\n\n```",
    "inputs": [
      { "name": "input", "type": "TF_Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "TF_Tensor" }
    ],
    "attributes": [
      { "name": "block_size", "type": "ConfinedAttr" },
      { "name": "data_format", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.DepthwiseConv2dNative",
    "summary": "Computes a 2-D depthwise convolution given 4-D `input` and `filter` tensors.",
    "description": "Given an input tensor of shape `[batch, in_height, in_width, in_channels]`\nand a filter / kernel tensor of shape\n`[filter_height, filter_width, in_channels, channel_multiplier]`, containing\n`in_channels` convolutional filters of depth 1, `depthwise_conv2d` applies\na different filter to each input channel (expanding from 1 channel to\n`channel_multiplier` channels for each), then concatenates the results\ntogether. Thus, the output has `in_channels * channel_multiplier` channels.\n\n```\nfor k in 0..in_channels-1\n  for q in 0..channel_multiplier-1\n    output[b, i, j, k * channel_multiplier + q] =\n      sum_{di, dj} input[b, strides[1] * i + di, strides[2] * j + dj, k] *\n                        filter[di, dj, k, q]\n```\n\nMust have `strides[0] = strides[3] = 1`.  For the most common case of the same\nhorizontal and vertices strides, `strides = [1, stride, stride, 1]`.",
    "inputs": [
      { "name": "input", "type": "TF_FloatTensor" },
      { "name": "filter", "type": "TF_FloatTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "TF_FloatTensor" }
    ],
    "attributes": [
      { "name": "strides", "type": "I64ArrayAttr" },
      { "name": "padding", "type": "TF_AnyStrAttrOf" },
      { "name": "explicit_paddings", "type": "DefaultValuedOptionalAttr" },
      { "name": "data_format", "type": "DefaultValuedOptionalAttr" },
      { "name": "dilations", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.DepthwiseConv2dNativeBackpropFilter",
    "summary": "Computes the gradients of depthwise convolution with respect to the filter.",
    "inputs": [
      { "name": "input", "type": "Arg" },
      { "name": "filter_sizes", "type": "Arg" },
      { "name": "out_backprop", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "strides", "type": "I64ArrayAttr" },
      { "name": "padding", "type": "TF_AnyStrAttrOf" },
      { "name": "explicit_paddings", "type": "DefaultValuedOptionalAttr" },
      { "name": "data_format", "type": "DefaultValuedOptionalAttr" },
      { "name": "dilations", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.DepthwiseConv2dNativeBackpropInput",
    "summary": "Computes the gradients of depthwise convolution with respect to the input.",
    "inputs": [
      { "name": "input_sizes", "type": "Arg" },
      { "name": "filter", "type": "Arg" },
      { "name": "out_backprop", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "strides", "type": "I64ArrayAttr" },
      { "name": "padding", "type": "TF_AnyStrAttrOf" },
      { "name": "explicit_paddings", "type": "DefaultValuedOptionalAttr" },
      { "name": "data_format", "type": "DefaultValuedOptionalAttr" },
      { "name": "dilations", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.Dequantize",
    "summary": "Dequantize the 'input' tensor into a float or bfloat16 Tensor.",
    "description": "[min_range, max_range] are scalar floats that specify the range for\nthe output. The 'mode' attribute controls exactly which calculations are\nused to convert the float values to their quantized equivalents.\n\nIn 'MIN_COMBINED' mode, each value of the tensor will undergo the following:\n\n```\nif T == qint8: in[i] += (range(T) + 1)/ 2.0\nout[i] = min_range + (in[i]* (max_range - min_range) / range(T))\n```\nhere `range(T) = numeric_limits<T>::max() - numeric_limits<T>::min()`\n\n*MIN_COMBINED Mode Example*\n\nIf the input comes from a QuantizedRelu6, the output type is\nquint8 (range of 0-255) but the possible range of QuantizedRelu6 is\n0-6.  The min_range and max_range values are therefore 0.0 and 6.0.\nDequantize on quint8 will take each value, cast to float, and multiply\nby 6 / 255.\nNote that if quantizedtype is qint8, the operation will additionally add\neach value by 128 prior to casting.\n\nIf the mode is 'MIN_FIRST', then this approach is used:\n\n```c++\nnum_discrete_values = 1 << (# of bits in T)\nrange_adjust = num_discrete_values / (num_discrete_values - 1)\nrange = (range_max - range_min) * range_adjust\nrange_scale = range / num_discrete_values\nconst double offset_input = static_cast<double>(input) - lowest_quantized;\nresult = range_min + ((input - numeric_limits<T>::min()) * range_scale)\n```\n\nIf the mode is `SCALED`, dequantization is performed by multiplying each\ninput value by a scaling_factor. (Thus an input of 0 always maps to 0.0).\n\nThe scaling_factor is determined from `min_range`, `max_range`, and\n`narrow_range` in a way that is compatible with `QuantizeAndDequantize{V2|V3}`\nand `QuantizeV2`, using the following algorithm:\n\n```c++\n\n  const int min_expected_T = std::numeric_limits<T>::min() +\n    (narrow_range ? 1 : 0);\n  const int max_expected_T = std::numeric_limits<T>::max();\n  const float max_expected_T = std::numeric_limits<float>::max();\n\n  const float scale_factor =\n    (std::numeric_limits<T>::min() == 0) ? (max_range / max_expected_T)\n                                         : std::max(min_range / min_expected_T,\n                                                    max_range / max_expected_T);\n```",
    "inputs": [
      { "name": "input", "type": "TensorOf" },
      { "name": "min_range", "type": "Arg" },
      { "name": "max_range", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "TensorOf" }
    ],
    "attributes": [
      { "name": "mode", "type": "DefaultValuedOptionalAttr" },
      { "name": "narrow_range", "type": "DefaultValuedOptionalAttr" },
      { "name": "axis", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.DeserializeIterator",
    "summary": "Converts the given variant tensor to an iterator and stores it in the given resource.",
    "inputs": [
      { "name": "resource_handle", "type": "Arg" },
      { "name": "serialized", "type": "Arg" }
    ]
  },
  {
    "name": "tf.DeserializeSparse",
    "summary": "Deserialize `SparseTensor` objects.",
    "description": "The input `serialized_sparse` must have the shape `[?, ?, ..., ?, 3]` where\nthe last dimension stores serialized `SparseTensor` objects and the other N\ndimensions (N >= 0) correspond to a batch. The ranks of the original\n`SparseTensor` objects must all match. When the final `SparseTensor` is\ncreated, its rank is the rank of the incoming `SparseTensor` objects plus N;\nthe sparse tensors have been concatenated along new dimensions, one for each\nbatch.\n\nThe output `SparseTensor` object's shape values for the original dimensions\nare the max across the input `SparseTensor` objects' shape values for the\ncorresponding dimensions. The new dimensions match the size of the batch.\n\nThe input `SparseTensor` objects' indices are assumed ordered in\nstandard lexicographic order.  If this is not the case, after this\nstep run `SparseReorder` to restore index ordering.\n\nFor example, if the serialized input is a `[2 x 3]` matrix representing two\noriginal `SparseTensor` objects:\n\n    index = [ 0]\n            [10]\n            [20]\n    values = [1, 2, 3]\n    shape = [50]\n\nand\n\n    index = [ 2]\n            [10]\n    values = [4, 5]\n    shape = [30]\n\nthen the final deserialized `SparseTensor` will be:\n\n    index = [0  0]\n            [0 10]\n            [0 20]\n            [1  2]\n            [1 10]\n    values = [1, 2, 3, 4, 5]\n    shape = [2 50]",
    "inputs": [
      { "name": "serialized_sparse", "type": "Arg" }
    ],
    "outputs": [
      { "name": "sparse_indices", "type": "TF_Int64Tensor" },
      { "name": "sparse_values", "type": "TF_Tensor" },
      { "name": "sparse_shape", "type": "TF_Int64Tensor" }
    ]
  },
  {
    "name": "tf.DestroyResourceOp",
    "summary": "Deletes the resource specified by the handle.",
    "description": "All subsequent operations using the resource will result in a NotFound\nerror status.",
    "inputs": [
      { "name": "resource", "type": "Arg" }
    ],
    "attributes": [
      { "name": "ignore_lookup_error", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.DeviceIndex",
    "summary": "Return the index of device the op runs.",
    "description": "Given a list of device names, this operation returns the index of the device\nthis op runs. The length of the list is returned in two cases:\n(1) Device does not exist in the given device list.\n(2) It is in XLA compilation.",
    "outputs": [
      { "name": "index", "type": "TF_Int32Tensor" }
    ],
    "attributes": [
      { "name": "device_names", "type": "StrArrayAttr" }
    ]
  },
  {
    "name": "tf.Diag",
    "summary": "Returns a diagonal tensor with a given diagonal values.",
    "description": "Given a `diagonal`, this operation returns a tensor with the `diagonal` and\neverything else padded with zeros. The diagonal is computed as follows:\n\nAssume `diagonal` has dimensions [D1,..., Dk], then the output is a tensor of\nrank 2k with dimensions [D1,..., Dk, D1,..., Dk] where:\n\n`output[i1,..., ik, i1,..., ik] = diagonal[i1, ..., ik]` and 0 everywhere else.\n\nFor example:\n\n```\n# 'diagonal' is [1, 2, 3, 4]\ntf.diag(diagonal) ==> [[1, 0, 0, 0]\n                       [0, 2, 0, 0]\n                       [0, 0, 3, 0]\n                       [0, 0, 0, 4]]\n```",
    "inputs": [
      { "name": "diagonal", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "TensorOf" }
    ]
  },
  {
    "name": "tf.DiagPart",
    "summary": "Returns the diagonal part of the tensor.",
    "description": "This operation returns a tensor with the `diagonal` part\nof the `input`. The `diagonal` part is computed as follows:\n\nAssume `input` has dimensions `[D1,..., Dk, D1,..., Dk]`, then the output is a\ntensor of rank `k` with dimensions `[D1,..., Dk]` where:\n\n`diagonal[i1,..., ik] = input[i1, ..., ik, i1,..., ik]`.\n\nFor example:\n\n```\n# 'input' is [[1, 0, 0, 0]\n              [0, 2, 0, 0]\n              [0, 0, 3, 0]\n              [0, 0, 0, 4]]\n\ntf.diag_part(input) ==> [1, 2, 3, 4]\n```",
    "inputs": [
      { "name": "input", "type": "Arg" }
    ],
    "outputs": [
      { "name": "diagonal", "type": "Res" }
    ]
  },
  {
    "name": "tf.Digamma",
    "summary": "Computes Psi, the derivative of Lgamma (the log of the absolute value of",
    "description": "`Gamma(x)`), element-wise.",
    "inputs": [
      { "name": "x", "type": "TF_FloatTensor" }
    ],
    "outputs": [
      { "name": "y", "type": "TF_FloatTensor" }
    ]
  },
  {
    "name": "tf.DisableCopyOnRead",
    "summary": "Turns off the copy-on-read mode.",
    "description": "Turns off the copy-on-read mode of a resource variable. If the variable is not in copy-on-read mode, this op has no effect.",
    "inputs": [
      { "name": "resource", "type": "Arg" }
    ]
  },
  {
    "name": "tf.Div",
    "summary": "Returns x / y element-wise.",
    "description": "*NOTE*: `Div` supports broadcasting. More about broadcasting\n[here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)",
    "inputs": [
      { "name": "x", "type": "TensorOf" },
      { "name": "y", "type": "TensorOf" }
    ],
    "outputs": [
      { "name": "z", "type": "TensorOf" }
    ]
  },
  {
    "name": "tf.DivNoNan",
    "summary": "Returns 0 if the denominator is zero.",
    "description": "*NOTE*: `DivNoNan` supports broadcasting. More about broadcasting\n[here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)",
    "inputs": [
      { "name": "x", "type": "TF_FpOrComplexTensor" },
      { "name": "y", "type": "TF_FpOrComplexTensor" }
    ],
    "outputs": [
      { "name": "z", "type": "TF_FpOrComplexTensor" }
    ]
  },
  {
    "name": "tf.DummyMemoryCache",
    "outputs": [
      { "name": "handle", "type": "Res" }
    ]
  },
  {
    "name": "tf.DummySeedGenerator",
    "outputs": [
      { "name": "handle", "type": "Res" }
    ]
  },
  {
    "name": "tf.DynamicEnqueueTPUEmbeddingArbitraryTensorBatch",
    "summary": "Eases the porting of code that uses tf.nn.embedding_lookup_sparse().",
    "description": "embedding_indices[i] and aggregation_weights[i] correspond\nto the ith feature.\n\nThe tensors at corresponding positions in the three input lists (sample_indices,\nembedding_indices and aggregation_weights) must have the same shape, i.e. rank 1\nwith dim_size() equal to the total number of lookups into the table described by\nthe corresponding feature.",
    "inputs": [
      { "name": "sample_indices_or_row_splits", "type": "Arg" },
      { "name": "embedding_indices", "type": "Arg" },
      { "name": "aggregation_weights", "type": "Arg" },
      { "name": "mode_override", "type": "Arg" },
      { "name": "device_ordinal", "type": "Arg" }
    ],
    "attributes": [
      { "name": "combiners", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.DynamicPartition",
    "summary": "Partitions `data` into `num_partitions` tensors using indices from `partitions`.",
    "description": "For each index tuple `js` of size `partitions.ndim`, the slice `data[js, ...]`\nbecomes part of `outputs[partitions[js]]`.  The slices with `partitions[js] = i`\nare placed in `outputs[i]` in lexicographic order of `js`, and the first\ndimension of `outputs[i]` is the number of entries in `partitions` equal to `i`.\nIn detail,\n\n```python\n    outputs[i].shape = [sum(partitions == i)] + data.shape[partitions.ndim:]\n\n    outputs[i] = pack([data[js, ...] for js if partitions[js] == i])\n```\n\n`data.shape` must start with `partitions.shape`.\n\nFor example:\n\n```python\n    # Scalar partitions.\n    partitions = 1\n    num_partitions = 2\n    data = [10, 20]\n    outputs[0] = []  # Empty with shape [0, 2]\n    outputs[1] = [[10, 20]]\n\n    # Vector partitions.\n    partitions = [0, 0, 1, 1, 0]\n    num_partitions = 2\n    data = [10, 20, 30, 40, 50]\n    outputs[0] = [10, 20, 50]\n    outputs[1] = [30, 40]\n```\n\nSee `dynamic_stitch` for an example on how to merge partitions back.\n\n<div style=\"width:70%; margin:auto; margin-bottom:10px; margin-top:20px;\">\n<img style=\"width:100%\" src=\"https://www.tensorflow.org/images/DynamicPartition.png\" alt>\n</div>\n\n\nRaises:\n  * `InvalidArgumentError` in following cases:\n    - If partitions is not in range `[0, num_partiions)`\n    - If `partitions.shape` does not match prefix of `data.shape` argument.",
    "inputs": [
      { "name": "data", "type": "TF_Tensor" },
      { "name": "partitions", "type": "Arg" }
    ],
    "outputs": [
      { "name": "outputs", "type": "Variadic" }
    ]
  },
  {
    "name": "tf.DynamicStitch",
    "summary": "Interleave the values from the `data` tensors into a single tensor.",
    "description": "Builds a merged tensor such that\n\n```python\n    merged[indices[m][i, ..., j], ...] = data[m][i, ..., j, ...]\n```\n\nFor example, if each `indices[m]` is scalar or vector, we have\n\n```python\n    # Scalar indices:\n    merged[indices[m], ...] = data[m][...]\n\n    # Vector indices:\n    merged[indices[m][i], ...] = data[m][i, ...]\n```\n\nEach `data[i].shape` must start with the corresponding `indices[i].shape`,\nand the rest of `data[i].shape` must be constant w.r.t. `i`.  That is, we\nmust have `data[i].shape = indices[i].shape + constant`.  In terms of this\n`constant`, the output shape is\n\n    merged.shape = [max(indices) + 1] + constant\n\nValues are merged in order, so if an index appears in both `indices[m][i]` and\n`indices[n][j]` for `(m,i) < (n,j)` the slice `data[n][j]` will appear in the\nmerged result. If you do not need this guarantee, ParallelDynamicStitch might\nperform better on some devices.\n\nFor example:\n\n```python\n    indices[0] = 6\n    indices[1] = [4, 1]\n    indices[2] = [[5, 2], [0, 3]]\n    data[0] = [61, 62]\n    data[1] = [[41, 42], [11, 12]]\n    data[2] = [[[51, 52], [21, 22]], [[1, 2], [31, 32]]]\n    merged = [[1, 2], [11, 12], [21, 22], [31, 32], [41, 42],\n              [51, 52], [61, 62]]\n```\n\nThis method can be used to merge partitions created by `dynamic_partition`\nas illustrated on the following example:\n\n```python\n    # Apply function (increments x_i) on elements for which a certain condition\n    # apply (x_i != -1 in this example).\n    x=tf.constant([0.1, -1., 5.2, 4.3, -1., 7.4])\n    condition_mask=tf.not_equal(x,tf.constant(-1.))\n    partitioned_data = tf.dynamic_partition(\n        x, tf.cast(condition_mask, tf.int32) , 2)\n    partitioned_data[1] = partitioned_data[1] + 1.0\n    condition_indices = tf.dynamic_partition(\n        tf.range(tf.shape(x)[0]), tf.cast(condition_mask, tf.int32) , 2)\n    x = tf.dynamic_stitch(condition_indices, partitioned_data)\n    # Here x=[1.1, -1., 6.2, 5.3, -1, 8.4], the -1. values remain\n    # unchanged.\n```\n\n<div style=\"width:70%; margin:auto; margin-bottom:10px; margin-top:20px;\">\n<img style=\"width:100%\" src=\"https://www.tensorflow.org/images/DynamicStitch.png\" alt>\n</div>",
    "inputs": [
      { "name": "indices", "type": "Variadic" },
      { "name": "data", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "merged", "type": "TF_Tensor" }
    ]
  },
  {
    "name": "tf.Einsum",
    "summary": "Tensor contraction according to Einstein summation convention.",
    "description": "Implements generalized Tensor contraction and reduction. Each input Tensor must\nhave a corresponding input subscript appearing in the comma-separated left-hand\nside of the equation. The right-hand side of the equation consists of the\noutput subscript. The input subscripts and the output subscript should consist\nof zero or more named axis labels and at most one ellipsis (`...`).\n\nThe named axis labels may be any single character other than those having\nspecial meaning, namely `,.->`. The behavior of this Op is undefined if it\nreceives an ill-formatted equation; since the validation is done at\ngraph-building time, we omit format validation checks at runtime.\n\nNote: This Op is *not* intended to be called by the user; instead users should\ncall `tf.einsum` directly. It is a hidden Op used by `tf.einsum`.\n\nOperations are applied to the input(s) according to the following rules:\n\n (a) Generalized Diagonals: For input dimensions corresponding to axis labels\n     appearing more than once in the same input subscript, we take the\n     generalized (`k`-dimensional) diagonal.\n     For example, in the equation `iii->i` with input shape `[3, 3, 3]`, the\n     generalized diagonal would consist of `3` elements at indices `(0, 0, 0)`,\n     `(1, 1, 1)` and `(2, 2, 2)` to create a Tensor of shape `[3]`.\n\n (b) Reduction: Axes corresponding to labels appearing only in one input\n     subscript but not in the output subscript are summed over prior to Tensor\n     contraction.\n     For example, in the equation `ab,bc->b`, the axis labels `a` and `c` are\n     the reduction axis labels.\n\n (c) Batch Dimensions: Axes corresponding to labels appearing in each of the\n     input subscripts and also in the output subscript make up the batch\n     dimensions in Tensor contraction. Unnamed axis labels corresponding to\n     ellipsis (`...`) also correspond to batch dimensions.\n     For example, for the equation denoting batch matrix multiplication,\n     `bij,bjk->bik`, the axis label `b` corresponds to a batch dimension.\n\n (d) Contraction: In case of binary einsum, axes corresponding to labels\n     appearing in two different inputs (and not in the output) are contracted\n     against each other.\n     Considering the batch matrix multiplication equation again\n     (`bij,bjk->bik`), the contracted axis label is `j`.\n\n (e) Expand Diagonal: If the output subscripts contain repeated (explicit) axis\n     labels, the opposite operation of (a) is applied. For example, in the\n     equation `i->iii`, and input shape `[3]`, the output of shape `[3, 3, 3]`\n     are all zeros, except for the (generalized) diagonal which is populated\n     with values from the input.\n     Note: This operation is not supported by `np.einsum` or `tf.einsum`; it is\n     provided to enable computing the symbolic gradient of `tf.einsum`.\n\nThe output subscripts must contain only labels appearing in at least one of the\ninput subscripts. Furthermore, all dimensions mapping to the same axis label\nmust be equal.\n\nAny of the input and output subscripts may contain at most a single ellipsis\n(`...`). These ellipsis are mapped against dimensions not corresponding to any\nnamed axis label. If two inputs contain ellipsis, then they are broadcasted\naccording to standard NumPy broadcasting\n[rules](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html).\n\nThe broadcasted dimensions are placed in the corresponding location of the\nellipsis in the output subscript. If the broadcasted dimensions are non-empty\nand the output subscripts do not contain ellipsis, then an InvalidArgument error\nis raised.\n\n@compatibility(numpy)\nSimilar to [`numpy.einsum`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.einsum.html).\n\nComparison with `numpy.einsum`:\n\n * This Op only supports unary and binary forms of `numpy.einsum`.\n * This Op does not support implicit form. (i.e. equations without `->`).\n * This Op also supports repeated indices in the output subscript, which is not\n   supported by `numpy.einsum`.\n@end_compatibility",
    "inputs": [
      { "name": "inputs", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "equation", "type": "StrAttr" }
    ]
  },
  {
    "name": "tf.Elu",
    "summary": "Computes the exponential linear function.",
    "description": "The ELU function is defined as:\n\n * $ e ^ x - 1 $ if $ x < 0 $\n * $ x $ if $ x >= 0 $\n\nExamples:\n\n>>> tf.nn.elu(1.0)\n<tf.Tensor: shape=(), dtype=float32, numpy=1.0>\n>>> tf.nn.elu(0.0)\n<tf.Tensor: shape=(), dtype=float32, numpy=0.0>\n>>> tf.nn.elu(-1000.0)\n<tf.Tensor: shape=(), dtype=float32, numpy=-1.0>\n\nSee [Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs)\n](http://arxiv.org/abs/1511.07289)",
    "inputs": [
      { "name": "features", "type": "TF_FloatTensor" }
    ],
    "outputs": [
      { "name": "activations", "type": "TF_FloatTensor" }
    ]
  },
  {
    "name": "tf.EluGrad",
    "summary": "Computes gradients for the exponential linear (Elu) operation.",
    "inputs": [
      { "name": "gradients", "type": "Arg" },
      { "name": "outputs", "type": "Arg" }
    ],
    "outputs": [
      { "name": "backprops", "type": "Res" }
    ]
  },
  {
    "name": "tf.Empty",
    "summary": "Creates a tensor with the given shape.\n\nThis operation creates a tensor of `shape` and `dtype`.",
    "inputs": [
      { "name": "shape", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "init", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.EmptyTensorList",
    "summary": "Creates and returns an empty tensor list.",
    "description": "All list elements must be tensors of dtype element_dtype and shape compatible\nwith element_shape.\n\nhandle: an empty tensor list.\nelement_dtype: the type of elements in the list.\nelement_shape: a shape compatible with that of elements in the list.",
    "inputs": [
      { "name": "element_shape", "type": "TF_I32OrI64Tensor" },
      { "name": "max_num_elements", "type": "TF_Int32Tensor" }
    ],
    "outputs": [
      { "name": "handle", "type": "TF_VariantTensor" }
    ]
  },
  {
    "name": "tf.EncodePng",
    "summary": "PNG-encode an image.",
    "description": "`image` is a 3-D uint8 or uint16 Tensor of shape `[height, width, channels]`\nwhere `channels` is:\n\n*   1: for grayscale.\n*   2: for grayscale + alpha.\n*   3: for RGB.\n*   4: for RGBA.\n\nThe ZLIB compression level, `compression`, can be -1 for the PNG-encoder\ndefault or a value from 0 to 9.  9 is the highest compression level, generating\nthe smallest output, but is slower.",
    "inputs": [
      { "name": "image", "type": "Arg" }
    ],
    "outputs": [
      { "name": "contents", "type": "Res" }
    ],
    "attributes": [
      { "name": "compression", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.EnqueueTPUEmbeddingArbitraryTensorBatch",
    "summary": "Eases the porting of code that uses tf.nn.embedding_lookup_sparse().",
    "description": "embedding_indices[i] and aggregation_weights[i] correspond\nto the ith feature.\n\nThe tensors at corresponding positions in the three input lists (sample_indices,\nembedding_indices and aggregation_weights) must have the same shape, i.e. rank 1\nwith dim_size() equal to the total number of lookups into the table described by\nthe corresponding feature.",
    "inputs": [
      { "name": "sample_indices_or_row_splits", "type": "Arg" },
      { "name": "embedding_indices", "type": "Arg" },
      { "name": "aggregation_weights", "type": "Arg" },
      { "name": "mode_override", "type": "Arg" }
    ],
    "attributes": [
      { "name": "device_ordinal", "type": "DefaultValuedOptionalAttr" },
      { "name": "combiners", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.EnqueueTPUEmbeddingBatch",
    "summary": "An op that enqueues a list of input batch tensors to TPUEmbedding.",
    "description": "An op that enqueues a list of input batch tensors to TPUEmbedding.",
    "inputs": [
      { "name": "batch", "type": "Arg" },
      { "name": "mode_override", "type": "Arg" }
    ],
    "attributes": [
      { "name": "device_ordinal", "type": "DefaultValuedOptionalAttr" },
      { "name": "combiners", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.EnqueueTPUEmbeddingIntegerBatch",
    "summary": "An op that enqueues a list of input batch tensors to TPUEmbedding.",
    "inputs": [
      { "name": "batch", "type": "Arg" },
      { "name": "mode_override", "type": "Arg" }
    ],
    "attributes": [
      { "name": "device_ordinal", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.EnqueueTPUEmbeddingRaggedTensorBatch",
    "summary": "Eases the porting of code that uses tf.nn.embedding_lookup().",
    "description": "sample_splits[i], embedding_indices[i] and aggregation_weights[i] correspond\nto the ith feature. table_ids[i] indicates which embedding table to look up ith\nfeature.\n\nThe tensors at corresponding positions in two of the input lists,\nembedding_indices and aggregation_weights, must have the same shape, i.e. rank 1\nwith dim_size() equal to the total number of lookups into the table described by\nthe corresponding feature.",
    "inputs": [
      { "name": "sample_splits", "type": "Arg" },
      { "name": "embedding_indices", "type": "Arg" },
      { "name": "aggregation_weights", "type": "Arg" },
      { "name": "mode_override", "type": "Arg" }
    ],
    "attributes": [
      { "name": "device_ordinal", "type": "DefaultValuedOptionalAttr" },
      { "name": "combiners", "type": "DefaultValuedOptionalAttr" },
      { "name": "table_ids", "type": "I64ArrayAttr" },
      { "name": "max_sequence_lengths", "type": "DefaultValuedOptionalAttr" },
      { "name": "num_features", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.EnqueueTPUEmbeddingSparseBatch",
    "summary": "An op that enqueues TPUEmbedding input indices from a SparseTensor.",
    "description": "This Op eases the porting of code that uses embedding_lookup_sparse(),\nalthough some Python preprocessing of the SparseTensor arguments to\nembedding_lookup_sparse() is required to produce the arguments to this Op,\nsince only a single EnqueueTPUEmbeddingSparseBatch Op is allowed per training\nstep.\n\nThe tensors at corresponding positions in the three input lists\nmust have the same shape, i.e. rank 1 with dim_size() equal to the total\nnumber of lookups into the table described by the corresponding table_id.",
    "inputs": [
      { "name": "sample_indices", "type": "Arg" },
      { "name": "embedding_indices", "type": "Arg" },
      { "name": "aggregation_weights", "type": "Arg" },
      { "name": "mode_override", "type": "Arg" }
    ],
    "attributes": [
      { "name": "device_ordinal", "type": "DefaultValuedOptionalAttr" },
      { "name": "combiners", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.EnqueueTPUEmbeddingSparseTensorBatch",
    "summary": "Eases the porting of code that uses tf.nn.embedding_lookup_sparse().",
    "description": "sample_indices[i], embedding_indices[i] and aggregation_weights[i] correspond\nto the ith feature. table_ids[i] indicates which embedding table to look up ith\nfeature.\n\nThe tensors at corresponding positions in the three input lists (sample_indices,\nembedding_indices and aggregation_weights) must have the same shape, i.e. rank 1\nwith dim_size() equal to the total number of lookups into the table described by\nthe corresponding feature.",
    "inputs": [
      { "name": "sample_indices", "type": "Arg" },
      { "name": "embedding_indices", "type": "Arg" },
      { "name": "aggregation_weights", "type": "Arg" },
      { "name": "mode_override", "type": "Arg" }
    ],
    "attributes": [
      { "name": "device_ordinal", "type": "DefaultValuedOptionalAttr" },
      { "name": "combiners", "type": "DefaultValuedOptionalAttr" },
      { "name": "table_ids", "type": "I64ArrayAttr" },
      { "name": "max_sequence_lengths", "type": "DefaultValuedOptionalAttr" },
      { "name": "num_features", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.EnsureShape",
    "summary": "Ensures that the tensor's shape matches the expected shape.",
    "description": "Raises an error if the input tensor's shape does not match the specified shape.\nReturns the input tensor otherwise.",
    "inputs": [
      { "name": "input", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "shape", "type": "TF_ShapeAttr" }
    ]
  },
  {
    "name": "tf.Equal",
    "summary": "Returns the truth value of (x == y) element-wise.",
    "description": "*NOTE*: `Equal` supports broadcasting. More about broadcasting\n[here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n\n```python\nx = tf.constant([2, 4])\ny = tf.constant(2)\ntf.math.equal(x, y) ==> array([True, False])\n\nx = tf.constant([2, 4])\ny = tf.constant([2, 4])\ntf.math.equal(x, y) ==> array([True,  True])\n```",
    "inputs": [
      { "name": "x", "type": "TF_Tensor" },
      { "name": "y", "type": "TF_Tensor" }
    ],
    "outputs": [
      { "name": "z", "type": "TF_BoolTensor" }
    ],
    "attributes": [
      { "name": "incompatible_shape_error", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.Erf",
    "summary": "Computes the [Gauss error function](https://en.wikipedia.org/wiki/Error_function) of `x` element-wise. In statistics, for non-negative values of $x$, the error function has the following interpretation: for a random variable $Y$ that is normally distributed with mean 0 and variance $1/\\sqrt{2}$, $erf(x)$ is the probability that $Y$ falls in the range $[−x, x]$.",
    "inputs": [
      { "name": "x", "type": "TF_FloatTensor" }
    ],
    "outputs": [
      { "name": "y", "type": "TF_FloatTensor" }
    ]
  },
  {
    "name": "tf.Erfc",
    "summary": "Computes the complementary error function of `x` element-wise.",
    "inputs": [
      { "name": "x", "type": "TF_FloatTensor" }
    ],
    "outputs": [
      { "name": "y", "type": "TF_FloatTensor" }
    ]
  },
  {
    "name": "tf.Erfinv",
    "inputs": [
      { "name": "x", "type": "TF_FloatTensor" }
    ],
    "outputs": [
      { "name": "y", "type": "TF_FloatTensor" }
    ]
  },
  {
    "name": "tf.ExecuteTPUEmbeddingPartitioner",
    "summary": "An op that executes the TPUEmbedding partitioner on the central configuration",
    "description": "device and computes the HBM size (in bytes) required for TPUEmbedding operation.",
    "outputs": [
      { "name": "common_config", "type": "Res" }
    ],
    "attributes": [
      { "name": "config", "type": "StrAttr" }
    ]
  },
  {
    "name": "tf.Exp",
    "summary": "Computes exponential of x element-wise.  \\\\(y = e^x\\\\).",
    "description": "This function computes the exponential of every element in the input tensor.\n  i.e. `exp(x)` or `e^(x)`, where `x` is the input tensor.\n  `e` denotes Euler's number and is approximately equal to 2.718281.\n  Output is positive for any real input.\n\n  ```python\n  x = tf.constant(2.0)\n  tf.math.exp(x) ==> 7.389056\n\n  x = tf.constant([2.0, 8.0])\n  tf.math.exp(x) ==> array([7.389056, 2980.958], dtype=float32)\n  ```\n\n  For complex numbers, the exponential value is calculated as follows:\n\n  ```\n  e^(x+iy) = e^x * e^iy = e^x * (cos y + i sin y)\n  ```\n\n  Let's consider complex number 1+1j as an example.\n  e^1 * (cos 1 + i sin 1) = 2.7182818284590 * (0.54030230586+0.8414709848j)\n\n  ```python\n  x = tf.constant(1 + 1j)\n  tf.math.exp(x) ==> 1.4686939399158851+2.2873552871788423j\n  ```",
    "inputs": [
      { "name": "x", "type": "TF_FpOrComplexTensor" }
    ],
    "outputs": [
      { "name": "y", "type": "TF_FpOrComplexTensor" }
    ]
  },
  {
    "name": "tf.ExpandDims",
    "summary": "Inserts a dimension of 1 into a tensor's shape.",
    "description": "Given a tensor `input`, this operation inserts a dimension of 1 at the\ndimension index `axis` of `input`'s shape. The dimension index `axis` starts at\nzero; if you specify a negative number for `axis` it is counted backward from\nthe end.\n\nThis operation is useful if you want to add a batch dimension to a single\nelement. For example, if you have a single image of shape `[height, width,\nchannels]`, you can make it a batch of 1 image with `expand_dims(image, 0)`,\nwhich will make the shape `[1, height, width, channels]`.\n\nOther examples:\n\n```\n# 't' is a tensor of shape [2]\nshape(expand_dims(t, 0)) ==> [1, 2]\nshape(expand_dims(t, 1)) ==> [2, 1]\nshape(expand_dims(t, -1)) ==> [2, 1]\n\n# 't2' is a tensor of shape [2, 3, 5]\nshape(expand_dims(t2, 0)) ==> [1, 2, 3, 5]\nshape(expand_dims(t2, 2)) ==> [2, 3, 1, 5]\nshape(expand_dims(t2, 3)) ==> [2, 3, 5, 1]\n```\n\nThis operation requires that:\n\n`-1-input.dims() <= dim <= input.dims()`\n\nThis operation is related to `squeeze()`, which removes dimensions of\nsize 1.",
    "inputs": [
      { "name": "input", "type": "TF_Tensor" },
      { "name": "dim", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ]
  },
  {
    "name": "tf.Expm1",
    "summary": "Computes `exp(x) - 1` element-wise.",
    "description": "i.e. `exp(x) - 1` or `e^(x) - 1`, where `x` is the input tensor.\n  `e` denotes Euler's number and is approximately equal to 2.718281.\n\n  ```python\n  x = tf.constant(2.0)\n  tf.math.expm1(x) ==> 6.389056\n\n  x = tf.constant([2.0, 8.0])\n  tf.math.expm1(x) ==> array([6.389056, 2979.958], dtype=float32)\n\n  x = tf.constant(1 + 1j)\n  tf.math.expm1(x) ==> (0.46869393991588515+2.2873552871788423j)\n  ```",
    "inputs": [
      { "name": "x", "type": "TF_FpOrComplexTensor" }
    ],
    "outputs": [
      { "name": "y", "type": "TF_FpOrComplexTensor" }
    ]
  },
  {
    "name": "tf.ExtractImagePatches",
    "summary": "Extract `patches` from `images` and put them in the \"depth\" output dimension.",
    "inputs": [
      { "name": "images", "type": "Arg" }
    ],
    "outputs": [
      { "name": "patches", "type": "Res" }
    ],
    "attributes": [
      { "name": "ksizes", "type": "ConfinedAttr" },
      { "name": "strides", "type": "ConfinedAttr" },
      { "name": "rates", "type": "ConfinedAttr" },
      { "name": "padding", "type": "TF_AnyStrAttrOf" }
    ]
  },
  {
    "name": "tf.FakeParam",
    "summary": "This op is used as a placeholder in If branch functions. It doesn't provide a\n  valid output when run, so must either be removed (e.g. replaced with a\n  function input) or guaranteed not to be used (e.g. if mirroring an\n  intermediate output needed for the gradient computation of the other branch).",
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "shape", "type": "TF_ShapeAttr" }
    ]
  },
  {
    "name": "tf.FakeQuantWithMinMaxArgs",
    "summary": "Fake-quantize the 'inputs' tensor, type float to 'outputs' tensor of same shape and type.",
    "description": "Quantization is called fake since the output is still in floating point.\n  The API converts inputs into values within the range [min and max] and returns\n  as output.\n\nAttributes\n\n*   `[min; max]` define the clamping range for the `inputs` data.\n*   `inputs` values are quantized into the quantization range (\n`[0; 2^num_bits - 1]` when `narrow_range` is false and `[1; 2^num_bits - 1]`\nwhen it is true) and then de-quantized and output as floats in `[min; max]`\ninterval.\n*   `num_bits` is the bitwidth of the quantization; between 2 and 16, inclusive.\n\nBefore quantization, `min` and `max` values are adjusted with the following\nlogic.\nIt is suggested to have `min <= 0 <= max`. If `0` is not in the range of values,\nthe behavior can be unexpected:\n\n*   If `0 < min < max`: `min_adj = 0` and `max_adj = max - min`.\n*   If `min < max < 0`: `min_adj = min - max` and `max_adj = 0`.\n*   If `min <= 0 <= max`: `scale = (max - min) / (2^num_bits - 1) `,\n`min_adj = scale * round(min / scale)` and `max_adj = max + min_adj - min`.\n\n\nExamples\n\n```python\n\ninp = tf.constant ([10.03, -10.23, 3])\nout = tf.quantization.fake_quant_with_min_max_args(inp, min=-5, max=5,\n                                                   num_bits=16)\nprint(out)\n\n#  Output:\n#  tf.Tensor([ 4.9999237 -5.0000763  3.0000763], shape=(3,), dtype=float32)\n```\n\nRaises:\n  * InvalidArgumentError:\n    - If num_bits are outside of range [2, 16].\n    - If min >= max.\n  * ValueError: If `inputs` are of any other type than float32.",
    "inputs": [
      { "name": "inputs", "type": "TF_Float32Tensor" }
    ],
    "outputs": [
      { "name": "outputs", "type": "TF_Float32Tensor" }
    ],
    "attributes": [
      { "name": "min", "type": "DefaultValuedOptionalAttr" },
      { "name": "max", "type": "DefaultValuedOptionalAttr" },
      { "name": "num_bits", "type": "DefaultValuedOptionalAttr" },
      { "name": "narrow_range", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.FakeQuantWithMinMaxArgsGradient",
    "summary": "Compute gradients for a FakeQuantWithMinMaxArgs operation.",
    "inputs": [
      { "name": "gradients", "type": "Arg" },
      { "name": "inputs", "type": "Arg" }
    ],
    "outputs": [
      { "name": "backprops", "type": "Res" }
    ],
    "attributes": [
      { "name": "min", "type": "DefaultValuedOptionalAttr" },
      { "name": "max", "type": "DefaultValuedOptionalAttr" },
      { "name": "num_bits", "type": "DefaultValuedOptionalAttr" },
      { "name": "narrow_range", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.FakeQuantWithMinMaxVars",
    "summary": "Fake-quantize the 'inputs' tensor of type float via global float scalars",
    "description": "Fake-quantize the `inputs` tensor of type float via global float scalars\n`min` and `max` to `outputs` tensor of same shape as `inputs`.\n\nAttributes\n\n*   `[min; max]` define the clamping range for the `inputs` data.\n*   `inputs` values are quantized into the quantization range (\n`[0; 2^num_bits - 1]` when `narrow_range` is false and `[1; 2^num_bits - 1]`\nwhen it is true) and then de-quantized and output as floats in `[min; max]`\ninterval.\n*   `num_bits` is the bitwidth of the quantization; between 2 and 16, inclusive.\n\nBefore quantization, `min` and `max` values are adjusted with the following\nlogic.\nIt is suggested to have `min <= 0 <= max`. If `0` is not in the range of values,\nthe behavior can be unexpected:\n\n*   If `0 < min < max`: `min_adj = 0` and `max_adj = max - min`.\n*   If `min < max < 0`: `min_adj = min - max` and `max_adj = 0`.\n*   If `min <= 0 <= max`: `scale = (max - min) / (2^num_bits - 1) `,\n`min_adj = scale * round(min / scale)` and `max_adj = max + min_adj - min`.\n\nThis operation has a gradient and thus allows for training `min` and `max`\nvalues.\n\n>>> constant_input = tf.constant([[1.2, -0.3, 0.7], [2.1, 0.5, -1.0]], dtype=tf.float32)\n>>>\n>>> min_val = -0.5\n>>> max_val = 0.8\n>>> num_bits = 8\n>>> narrow_range = False #False:for the quantization range [0; 2^num_bits - 1]\n>>>\n>>> quantized_data = tf.quantization.fake_quant_with_min_max_vars(\n...   inputs=constant_input, min=min_val, max=max_val, num_bits=num_bits, narrow_range=narrow_range\n... )\n>>>\n>>> print(\"Input:\\n\", constant_input.numpy())\nInput:\n[[ 1.2 -0.3  0.7]\n[ 2.1  0.5 -1. ]]\n>>> print(\"Output:\\n\", quantized_data.numpy())\nOutput:\n[[ 0.8003921 -0.3007843  0.6984313]\n[ 0.8003921  0.4996078 -0.4996078]]",
    "inputs": [
      { "name": "inputs", "type": "TF_Float32Tensor" },
      { "name": "min", "type": "TF_Float32Tensor" },
      { "name": "max", "type": "TF_Float32Tensor" }
    ],
    "outputs": [
      { "name": "outputs", "type": "TF_Float32Tensor" }
    ],
    "attributes": [
      { "name": "num_bits", "type": "DefaultValuedOptionalAttr" },
      { "name": "narrow_range", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.FakeQuantWithMinMaxVarsGradient",
    "summary": "Compute gradients for a FakeQuantWithMinMaxVars operation.",
    "inputs": [
      { "name": "gradients", "type": "Arg" },
      { "name": "inputs", "type": "Arg" },
      { "name": "min", "type": "TF_Float32Tensor" },
      { "name": "max", "type": "TF_Float32Tensor" }
    ],
    "outputs": [
      { "name": "backprops_wrt_input", "type": "Res" },
      { "name": "backprop_wrt_min", "type": "Res" },
      { "name": "backprop_wrt_max", "type": "Res" }
    ],
    "attributes": [
      { "name": "num_bits", "type": "DefaultValuedOptionalAttr" },
      { "name": "narrow_range", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.FakeQuantWithMinMaxVarsPerChannel",
    "summary": "Fake-quantize the 'inputs' tensor of type float via per-channel floats",
    "description": "Fake-quantize the `inputs` tensor of type float per-channel and one of the\nshapes: `[d]`, `[b, d]` `[b, h, w, d]` via per-channel floats `min` and `max`\nof shape `[d]` to `outputs` tensor of same shape as `inputs`.\n\nAttributes\n\n*   `[min; max]` define the clamping range for the `inputs` data.\n*   `inputs` values are quantized into the quantization range (\n`[0; 2^num_bits - 1]` when `narrow_range` is false and `[1; 2^num_bits - 1]`\nwhen it is true) and then de-quantized and output as floats in `[min; max]`\ninterval.\n*   `num_bits` is the bitwidth of the quantization; between 2 and 16, inclusive.\n\nBefore quantization, `min` and `max` values are adjusted with the following\nlogic.\nIt is suggested to have `min <= 0 <= max`. If `0` is not in the range of values,\nthe behavior can be unexpected:\n\n*   If `0 < min < max`: `min_adj = 0` and `max_adj = max - min`.\n*   If `min < max < 0`: `min_adj = min - max` and `max_adj = 0`.\n*   If `min <= 0 <= max`: `scale = (max - min) / (2^num_bits - 1) `,\n`min_adj = scale * round(min / scale)` and `max_adj = max + min_adj - min`.\n\nThis operation has a gradient and thus allows for training `min` and `max`\nvalues.",
    "inputs": [
      { "name": "inputs", "type": "TF_Float32Tensor" },
      { "name": "min", "type": "TF_Float32Tensor" },
      { "name": "max", "type": "TF_Float32Tensor" }
    ],
    "outputs": [
      { "name": "outputs", "type": "TF_Float32Tensor" }
    ],
    "attributes": [
      { "name": "num_bits", "type": "DefaultValuedOptionalAttr" },
      { "name": "narrow_range", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.FakeQuantWithMinMaxVarsPerChannelGradient",
    "summary": "Compute gradients for a FakeQuantWithMinMaxVarsPerChannel operation.",
    "inputs": [
      { "name": "gradients", "type": "Arg" },
      { "name": "inputs", "type": "Arg" },
      { "name": "min", "type": "TF_Float32Tensor" },
      { "name": "max", "type": "TF_Float32Tensor" }
    ],
    "outputs": [
      { "name": "backprops_wrt_input", "type": "Res" },
      { "name": "backprop_wrt_min", "type": "Res" },
      { "name": "backprop_wrt_max", "type": "Res" }
    ],
    "attributes": [
      { "name": "num_bits", "type": "DefaultValuedOptionalAttr" },
      { "name": "narrow_range", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.FFT",
    "summary": "Fast Fourier transform.",
    "description": "Computes the 1-dimensional discrete Fourier transform over the inner-most\ndimension of `input`.",
    "inputs": [
      { "name": "input", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ]
  },
  {
    "name": "tf.FFT2D",
    "summary": "2D fast Fourier transform.",
    "description": "Computes the 2-dimensional discrete Fourier transform over the inner-most\n2 dimensions of `input`.",
    "inputs": [
      { "name": "input", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ]
  },
  {
    "name": "tf.FFT3D",
    "summary": "3D fast Fourier transform.",
    "description": "Computes the 3-dimensional discrete Fourier transform over the inner-most 3\ndimensions of `input`.",
    "inputs": [
      { "name": "input", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ]
  },
  {
    "name": "tf.Fill",
    "summary": "Creates a tensor filled with a scalar value.",
    "description": "This operation creates a tensor of shape `dims` and fills it with `value`.\n\nFor example:\n\n```\n# Output tensor has shape [2, 3].\nfill([2, 3], 9) ==> [[9, 9, 9]\n                     [9, 9, 9]]\n```\n\n`tf.fill` differs from `tf.constant` in a few ways:\n\n*   `tf.fill` only supports scalar contents, whereas `tf.constant` supports\n    Tensor values.\n*   `tf.fill` creates an Op in the computation graph that constructs the actual\n    Tensor value at runtime. This is in contrast to `tf.constant` which embeds\n    the entire Tensor into the graph with a `Const` node.\n*   Because `tf.fill` evaluates at graph runtime, it supports dynamic shapes\n    based on other runtime Tensors, unlike `tf.constant`.",
    "inputs": [
      { "name": "dims", "type": "Arg" },
      { "name": "value", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "TF_Tensor" }
    ]
  },
  {
    "name": "tf.FinalizeDataset",
    "summary": "Creates a dataset by applying `tf.data.Options` to `input_dataset`.",
    "inputs": [
      { "name": "input_dataset", "type": "Arg" }
    ],
    "outputs": [
      { "name": "handle", "type": "TF_VariantTensor" }
    ],
    "attributes": [
      { "name": "has_captured_ref", "type": "DefaultValuedOptionalAttr" },
      { "name": "output_types", "type": "ConfinedAttr" },
      { "name": "output_shapes", "type": "ConfinedAttr" }
    ]
  },
  {
    "name": "tf.FinalizeTPUEmbedding",
    "summary": "An op that finalizes the TPUEmbedding configuration.",
    "inputs": [
      { "name": "common_config", "type": "Arg" },
      { "name": "memory_config", "type": "Arg" }
    ]
  },
  {
    "name": "tf.FlatMapDataset",
    "summary": "Creates a dataset that applies `f` to the outputs of `input_dataset`.",
    "description": "Unlike MapDataset, the `f` in FlatMapDataset is expected to return a\nDataset variant, and FlatMapDataset will flatten successive results\ninto a single Dataset.",
    "inputs": [
      { "name": "input_dataset", "type": "TF_VariantTensor" },
      { "name": "other_arguments", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "handle", "type": "TF_VariantTensor" }
    ],
    "attributes": [
      { "name": "f", "type": "SymbolRefAttr" },
      { "name": "output_types", "type": "ConfinedAttr" },
      { "name": "output_shapes", "type": "ConfinedAttr" },
      { "name": "metadata", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.Floor",
    "summary": "Returns element-wise largest integer not greater than x.",
    "inputs": [
      { "name": "x", "type": "TF_FloatTensor" }
    ],
    "outputs": [
      { "name": "y", "type": "TF_FloatTensor" }
    ]
  },
  {
    "name": "tf.FloorDiv",
    "summary": "Returns x // y element-wise.",
    "description": "*NOTE*: `FloorDiv` supports broadcasting. More about broadcasting\n[here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)",
    "inputs": [
      { "name": "x", "type": "TensorOf" },
      { "name": "y", "type": "TensorOf" }
    ],
    "outputs": [
      { "name": "z", "type": "TensorOf" }
    ]
  },
  {
    "name": "tf.FloorMod",
    "summary": "Returns element-wise remainder of division.",
    "description": "This follows Python semantics in that the\nresult here is consistent with a flooring divide. E.g.\n`floor(x / y) * y + floormod(x, y) = x`, regardless of the signs of x and y.\n\n*NOTE*: `FloorMod` supports broadcasting. More about broadcasting\n[here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)",
    "inputs": [
      { "name": "x", "type": "TF_IntOrFpTensor" },
      { "name": "y", "type": "TF_IntOrFpTensor" }
    ],
    "outputs": [
      { "name": "z", "type": "TF_IntOrFpTensor" }
    ]
  },
  {
    "name": "tf.FlushSummaryWriter",
    "summary": "Flushes the writer's unwritten events.",
    "description": "writer: A handle to the summary writer resource.",
    "inputs": [
      { "name": "writer", "type": "Arg" }
    ]
  },
  {
    "name": "tf.FusedBatchNorm",
    "summary": "Batch normalization.",
    "description": "Note that the size of 4D Tensors are defined by either \"NHWC\" or \"NCHW\".\nThe size of 1D Tensors matches the dimension C of the 4D Tensors.",
    "inputs": [
      { "name": "x", "type": "Arg" },
      { "name": "scale", "type": "Arg" },
      { "name": "offset", "type": "Arg" },
      { "name": "mean", "type": "Arg" },
      { "name": "variance", "type": "Arg" }
    ],
    "outputs": [
      { "name": "y", "type": "Res" },
      { "name": "batch_mean", "type": "Res" },
      { "name": "batch_variance", "type": "Res" },
      { "name": "reserve_space_1", "type": "Res" },
      { "name": "reserve_space_2", "type": "Res" }
    ],
    "attributes": [
      { "name": "epsilon", "type": "DefaultValuedOptionalAttr" },
      { "name": "exponential_avg_factor", "type": "DefaultValuedOptionalAttr" },
      { "name": "data_format", "type": "DefaultValuedOptionalAttr" },
      { "name": "is_training", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.FusedBatchNormGrad",
    "summary": "Gradient for batch normalization.",
    "description": "Note that the size of 4D Tensors are defined by either \"NHWC\" or \"NCHW\".\nThe size of 1D Tensors matches the dimension C of the 4D Tensors.",
    "inputs": [
      { "name": "y_backprop", "type": "Arg" },
      { "name": "x", "type": "Arg" },
      { "name": "scale", "type": "Arg" },
      { "name": "reserve_space_1", "type": "Arg" },
      { "name": "reserve_space_2", "type": "Arg" }
    ],
    "outputs": [
      { "name": "x_backprop", "type": "Res" },
      { "name": "scale_backprop", "type": "Res" },
      { "name": "offset_backprop", "type": "Res" },
      { "name": "reserve_space_3", "type": "Res" },
      { "name": "reserve_space_4", "type": "Res" }
    ],
    "attributes": [
      { "name": "epsilon", "type": "DefaultValuedOptionalAttr" },
      { "name": "data_format", "type": "DefaultValuedOptionalAttr" },
      { "name": "is_training", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.FusedBatchNormGradV2",
    "summary": "Gradient for batch normalization.",
    "description": "Note that the size of 4D Tensors are defined by either \"NHWC\" or \"NCHW\".\nThe size of 1D Tensors matches the dimension C of the 4D Tensors.",
    "inputs": [
      { "name": "y_backprop", "type": "Arg" },
      { "name": "x", "type": "Arg" },
      { "name": "scale", "type": "Arg" },
      { "name": "reserve_space_1", "type": "Arg" },
      { "name": "reserve_space_2", "type": "Arg" }
    ],
    "outputs": [
      { "name": "x_backprop", "type": "Res" },
      { "name": "scale_backprop", "type": "Res" },
      { "name": "offset_backprop", "type": "Res" },
      { "name": "reserve_space_3", "type": "Res" },
      { "name": "reserve_space_4", "type": "Res" }
    ],
    "attributes": [
      { "name": "epsilon", "type": "DefaultValuedOptionalAttr" },
      { "name": "data_format", "type": "DefaultValuedOptionalAttr" },
      { "name": "is_training", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.FusedBatchNormGradV3",
    "summary": "Gradient for batch normalization.",
    "description": "Note that the size of 4D Tensors are defined by either \"NHWC\" or \"NCHW\".\nThe size of 1D Tensors matches the dimension C of the 4D Tensors.",
    "inputs": [
      { "name": "y_backprop", "type": "Arg" },
      { "name": "x", "type": "Arg" },
      { "name": "scale", "type": "Arg" },
      { "name": "reserve_space_1", "type": "Arg" },
      { "name": "reserve_space_2", "type": "Arg" },
      { "name": "reserve_space_3", "type": "Arg" }
    ],
    "outputs": [
      { "name": "x_backprop", "type": "Res" },
      { "name": "scale_backprop", "type": "Res" },
      { "name": "offset_backprop", "type": "Res" },
      { "name": "reserve_space_4", "type": "Res" },
      { "name": "reserve_space_5", "type": "Res" }
    ],
    "attributes": [
      { "name": "epsilon", "type": "DefaultValuedOptionalAttr" },
      { "name": "data_format", "type": "DefaultValuedOptionalAttr" },
      { "name": "is_training", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.FusedBatchNormV2",
    "summary": "Batch normalization.",
    "description": "Note that the size of 4D Tensors are defined by either \"NHWC\" or \"NCHW\".\nThe size of 1D Tensors matches the dimension C of the 4D Tensors.",
    "inputs": [
      { "name": "x", "type": "Arg" },
      { "name": "scale", "type": "Arg" },
      { "name": "offset", "type": "Arg" },
      { "name": "mean", "type": "Arg" },
      { "name": "variance", "type": "Arg" }
    ],
    "outputs": [
      { "name": "y", "type": "Res" },
      { "name": "batch_mean", "type": "Res" },
      { "name": "batch_variance", "type": "Res" },
      { "name": "reserve_space_1", "type": "Res" },
      { "name": "reserve_space_2", "type": "Res" }
    ],
    "attributes": [
      { "name": "epsilon", "type": "DefaultValuedOptionalAttr" },
      { "name": "exponential_avg_factor", "type": "DefaultValuedOptionalAttr" },
      { "name": "data_format", "type": "DefaultValuedOptionalAttr" },
      { "name": "is_training", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.FusedBatchNormV3",
    "summary": "Batch normalization.",
    "description": "Note that the size of 4D Tensors are defined by either \"NHWC\" or \"NCHW\".\nThe size of 1D Tensors matches the dimension C of the 4D Tensors.",
    "inputs": [
      { "name": "x", "type": "Arg" },
      { "name": "scale", "type": "Arg" },
      { "name": "offset", "type": "Arg" },
      { "name": "mean", "type": "Arg" },
      { "name": "variance", "type": "Arg" }
    ],
    "outputs": [
      { "name": "y", "type": "Res" },
      { "name": "batch_mean", "type": "Res" },
      { "name": "batch_variance", "type": "Res" },
      { "name": "reserve_space_1", "type": "Res" },
      { "name": "reserve_space_2", "type": "Res" },
      { "name": "reserve_space_3", "type": "Res" }
    ],
    "attributes": [
      { "name": "epsilon", "type": "DefaultValuedOptionalAttr" },
      { "name": "exponential_avg_factor", "type": "DefaultValuedOptionalAttr" },
      { "name": "data_format", "type": "DefaultValuedOptionalAttr" },
      { "name": "is_training", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.FusedConv2DBiasActivation",
    "summary": "Computes a fused kernel which implements: 2-D convolution, adds side input,",
    "description": "with separate scaling on convolution and side inputs, then adds bias and\n    applies the RELU activation function to the result. Supports both float and\n    qint8 data formats. In the case of qint8, the output is clipped to [0..127].\n\n    conv_input: A tensor with format as specified by `data_format` (see below).\n    filter: A tensor with format depending on `data_format` as follows:\n        \"NHWC\", \"NCHW\":\n             `float [ filter_height, filter_width, in_channels, out_channels ]`\n        \"NCHW_VECT_C\":\n             `qint8 [ out_channels, in_channels, filter_height, filter_width ]`\n    bias: 1-D float tensor with size matching the `out_channels` dimension of\n        `filter`.\n        Note: this tensor is still float, even if other inputs are qint8.\n    side_input: A tensor with format as specified by `data_format` (see below).\n        This tensor will be ignored and can be [] if side_input_scale == 0.\n        Otherwise, the size of each dimension must match the `output` tensor.\n    conv_input_scale: scalar float value to be multiplied by `conv_input`.\n        (conceptually.. in reality it is applied after convolution).\n        For the CPU version, this can also be a 1-D Tensor of per output-channel\n        scales.\n    side_input_scale: scalar float value to be multiplied by `side_input`.\n    output: A tensor with format as specified by `data_format` (see below).\n        The dimension sizes are determined automatically based on other inputs\n        and attributes.\n    T: The element data type of `conv_input`, `side_input` and `output` tensors.\n        Note: must match with the `data_format`.\n    Tbias: The element data type of `bias`.\n    strides: 1-D tensor of length 4.  The stride of the sliding window for each\n        dimension of `input`. The dimension order is determined by the value of\n        `data_format`, see below for details.\n        Note: the stride for batch and channel dimensions must be 1.\n    padding: The type of padding algorithm to use.\n    data_format: A string specifying the data format of `conv_input`,\n        `side_input` and `output` tensors with the following options:\n        \"NHWC\": `float [ batch, height, width, channels ]`\n        \"NCHW\": `float [ batch, channels, height, width ]`\n        \"NCHW_VECT_C\":\n            `qint8 [ batch, channels / 4, height, width, channels % 4 ]`\n        Note: for \"NCHW_VECT_C\", `channels` must be a multiple of 4.\n    filter_format: A string specifying the data format of `filter`,\n        \"HWIO\": `float [ kernel_height, kernel_width, input_channels,\n                         output_channels ]`\n        \"OIHW_VECT_I\":\n            `qint8 [ output_channels, input_channels / 4,\n                     kernel_height, kernel_width, input_channels % 4 ]`\n    activation_mode: The activation applied to the output.\n        Must be \"Relu\" or \"None\".\n    dilations: 1-D tensor of length 4.  The dilation factor for each dimension\n        of `input`. If set to k > 1, there will be k-1 skipped cells between\n        each filter element on that dimension. The dimension order is determined\n        by the value of `data_format`, see above for details. Dilations in the\n        batch and depth dimensions must be 1.",
    "inputs": [
      { "name": "conv_input", "type": "TensorOf" },
      { "name": "filter", "type": "TensorOf" },
      { "name": "bias", "type": "TensorOf" },
      { "name": "side_input", "type": "TensorOf" },
      { "name": "conv_input_scale", "type": "TF_Float32Tensor" },
      { "name": "side_input_scale", "type": "TF_Float32Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "TensorOf" }
    ],
    "attributes": [
      { "name": "strides", "type": "I64ArrayAttr" },
      { "name": "padding", "type": "TF_AnyStrAttrOf" },
      { "name": "data_format", "type": "DefaultValuedOptionalAttr" },
      { "name": "filter_format", "type": "DefaultValuedOptionalAttr" },
      { "name": "activation_mode", "type": "DefaultValuedOptionalAttr" },
      { "name": "dilations", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.Gather",
    "summary": "Gather slices from `params` according to `indices`.",
    "description": "`indices` must be an integer tensor of any dimension (usually 0-D or 1-D).\nProduces an output tensor with shape `indices.shape + params.shape[1:]` where:\n\n```python\n    # Scalar indices\n    output[:, ..., :] = params[indices, :, ... :]\n\n    # Vector indices\n    output[i, :, ..., :] = params[indices[i], :, ... :]\n\n    # Higher rank indices\n    output[i, ..., j, :, ... :] = params[indices[i, ..., j], :, ..., :]\n```\n\nIf `indices` is a permutation and `len(indices) == params.shape[0]` then\nthis operation will permute `params` accordingly.\n\n`validate_indices`: DEPRECATED. If this operation is assigned to CPU, values in\n`indices` are always validated to be within range. If assigned to GPU,\nout-of-bound indices result in safe but unspecified behavior, which may include\nraising an error.\n\n<div style=\"width:70%; margin:auto; margin-bottom:10px; margin-top:20px;\">\n<img style=\"width:100%\" src=\"https://www.tensorflow.org/images/Gather.png\" alt>\n</div>",
    "inputs": [
      { "name": "params", "type": "TF_Tensor" },
      { "name": "indices", "type": "TF_I32OrI64Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "TF_Tensor" }
    ],
    "attributes": [
      { "name": "validate_indices", "type": "DefaultValuedOptionalAttr" }
    ],
    "category": "Tensor"
  },
  {
    "name": "tf.GatherNd",
    "summary": "Gather slices from `params` into a Tensor with shape specified by `indices`.",
    "description": "`indices` is a K-dimensional integer tensor, best thought of as a\n(K-1)-dimensional tensor of indices into `params`, where each element defines a\nslice of `params`:\n\n    output[\\\\(i_0, ..., i_{K-2}\\\\)] = params[indices[\\\\(i_0, ..., i_{K-2}\\\\)]]\n\nWhereas in `tf.gather` `indices` defines slices into the `axis`\ndimension of `params`, in `tf.gather_nd`, `indices` defines slices into the\nfirst `N` dimensions of `params`, where `N = indices.shape[-1]`.\n\nThe last dimension of `indices` can be at most the rank of\n`params`:\n\n    indices.shape[-1] <= params.rank\n\nThe last dimension of `indices` corresponds to elements\n(if `indices.shape[-1] == params.rank`) or slices\n(if `indices.shape[-1] < params.rank`) along dimension `indices.shape[-1]`\nof `params`.  The output tensor has shape\n\n    indices.shape[:-1] + params.shape[indices.shape[-1]:]\n\nIf `indices` contains any out-of-bound indices, depending on\n`bad_indices_policy`, the op will either return an error or ignore the\nout-of-bound indices. `bad_indices_policy` can be one of the following values:\n1. \"\" or \"DEFAULT\": raises on CPU and ignore on GPU. This is because\n   historically on CPU and GPU we handle errors in different ways, and for\n   backward compatibility we keep the default behavior.\n2. \"ERROR\": raises error; GPU does not support this value.\n3. \"IGNORE\": ignore error and set the corresponding output to 0;\n   supported on both CPU and GPU.\n\nSome examples below.\n\nSimple indexing into a matrix:\n\n```python\n    indices = [[0, 0], [1, 1]]\n    params = [['a', 'b'], ['c', 'd']]\n    output = ['a', 'd']\n```\n\nSlice indexing into a matrix:\n\n```python\n    indices = [[1], [0]]\n    params = [['a', 'b'], ['c', 'd']]\n    output = [['c', 'd'], ['a', 'b']]\n```\n\nIndexing into a 3-tensor:\n\n```python\n    indices = [[1]]\n    params = [[['a0', 'b0'], ['c0', 'd0']],\n              [['a1', 'b1'], ['c1', 'd1']]]\n    output = [[['a1', 'b1'], ['c1', 'd1']]]\n\n\n    indices = [[0, 1], [1, 0]]\n    params = [[['a0', 'b0'], ['c0', 'd0']],\n              [['a1', 'b1'], ['c1', 'd1']]]\n    output = [['c0', 'd0'], ['a1', 'b1']]\n\n\n    indices = [[0, 0, 1], [1, 0, 1]]\n    params = [[['a0', 'b0'], ['c0', 'd0']],\n              [['a1', 'b1'], ['c1', 'd1']]]\n    output = ['b0', 'b1']\n```\n\nBatched indexing into a matrix:\n\n```python\n    indices = [[[0, 0]], [[0, 1]]]\n    params = [['a', 'b'], ['c', 'd']]\n    output = [['a'], ['b']]\n```\n\nBatched slice indexing into a matrix:\n\n```python\n    indices = [[[1]], [[0]]]\n    params = [['a', 'b'], ['c', 'd']]\n    output = [[['c', 'd']], [['a', 'b']]]\n```\n\nBatched indexing into a 3-tensor:\n\n```python\n    indices = [[[1]], [[0]]]\n    params = [[['a0', 'b0'], ['c0', 'd0']],\n              [['a1', 'b1'], ['c1', 'd1']]]\n    output = [[[['a1', 'b1'], ['c1', 'd1']]],\n              [[['a0', 'b0'], ['c0', 'd0']]]]\n\n    indices = [[[0, 1], [1, 0]], [[0, 0], [1, 1]]]\n    params = [[['a0', 'b0'], ['c0', 'd0']],\n              [['a1', 'b1'], ['c1', 'd1']]]\n    output = [[['c0', 'd0'], ['a1', 'b1']],\n              [['a0', 'b0'], ['c1', 'd1']]]\n\n\n    indices = [[[0, 0, 1], [1, 0, 1]], [[0, 1, 1], [1, 1, 0]]]\n    params = [[['a0', 'b0'], ['c0', 'd0']],\n              [['a1', 'b1'], ['c1', 'd1']]]\n    output = [['b0', 'b1'], ['d0', 'c1']]\n```\n\nSee also `tf.gather` and `tf.batch_gather`.",
    "inputs": [
      { "name": "params", "type": "Arg" },
      { "name": "indices", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "bad_indices_policy", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.GatherV2",
    "summary": "Gather slices from `params` axis `axis` according to `indices`.",
    "description": "`indices` must be an integer tensor of any dimension (usually 0-D or 1-D).\nProduces an output tensor with shape `params.shape[:axis] +\nindices.shape[batch_dims:] + params.shape[axis + 1:]` where:\n\n```python\n    # Scalar indices (output is rank(params) - 1).\n    output[a_0, ..., a_n, b_0, ..., b_n] =\n      params[a_0, ..., a_n, indices, b_0, ..., b_n]\n\n    # Vector indices (output is rank(params)).\n    output[a_0, ..., a_n, i, b_0, ..., b_n] =\n      params[a_0, ..., a_n, indices[i], b_0, ..., b_n]\n\n    # Higher rank indices (output is rank(params) + rank(indices) - 1).\n    output[a_0, ..., a_n, i, ..., j, b_0, ... b_n] =\n      params[a_0, ..., a_n, indices[i, ..., j], b_0, ..., b_n]\n```\n\n<div style=\"width:70%; margin:auto; margin-bottom:10px; margin-top:20px;\">\n<img style=\"width:100%\" src=\"https://www.tensorflow.org/images/Gather.png\" alt>\n</div>\n\nNote that on CPU, if an out of bound index is found, an error is returned.\nOn GPU, if an out of bound index is found, a 0 is stored in the\ncorresponding output value.\n\nNote that on TPU, if any dimension of `params` is of size 0 then the output will\nbe the expected shape filled with zeros. On CPU and GPU an error will be\nreturned.\n\nSee also `tf.batch_gather` and `tf.gather_nd`.",
    "inputs": [
      { "name": "params", "type": "Arg" },
      { "name": "indices", "type": "Arg" },
      { "name": "axis", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "batch_dims", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.GeneratorDataset",
    "summary": "Creates a dataset that invokes a function to generate elements.",
    "inputs": [
      { "name": "init_func_other_args", "type": "Variadic" },
      { "name": "next_func_other_args", "type": "Variadic" },
      { "name": "finalize_func_other_args", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "handle", "type": "TF_VariantTensor" }
    ],
    "attributes": [
      { "name": "init_func", "type": "SymbolRefAttr" },
      { "name": "next_func", "type": "SymbolRefAttr" },
      { "name": "finalize_func", "type": "SymbolRefAttr" },
      { "name": "output_types", "type": "ConfinedAttr" },
      { "name": "output_shapes", "type": "ConfinedAttr" },
      { "name": "metadata", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.GeneratorDatasetRegion",
    "summary": "Regional version of GeneratorDataset",
    "description": "Creates a dataset that invokes its 'next' region to generate elements. Conceptually,\nwithin MLIR, we treat this op as if it fills a buffer with all the results right away,\nand those results are then passed (through the variant tensor result) to\nMakeIterator / IteratorGetNext. Note that the actual TF implementation differs: It\ngenerates the next element just in time, during IteratorGetNext.\n\ninit_extra_args: Additional arguments to pass to 'init'.\nnext_extra_args: Additional arguments to pass to 'next'. (Passed after the\n                 normal arguments which are from the return values of 'init'.)\nfinalize_extra_args: Additional arguments to pass to 'finalize'. (Passed after\n                 the normal arguments which are from the return values of 'init'.)",
    "inputs": [
      { "name": "init_func_other_args", "type": "Variadic" },
      { "name": "next_func_other_args", "type": "Variadic" },
      { "name": "finalize_func_other_args", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "handle", "type": "TF_VariantTensor" }
    ],
    "attributes": [
      { "name": "output_types", "type": "ConfinedAttr" },
      { "name": "output_shapes", "type": "ConfinedAttr" },
      { "name": "metadata", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.GetMinibatchesInCsrWithPhysicalReplica",
    "inputs": [
      { "name": "program_key", "type": "TF_StrTensor" },
      { "name": "row_ids", "type": "TF_Int32Tensor" },
      { "name": "col_ids", "type": "TF_Int32Tensor" },
      { "name": "gains", "type": "TF_Float32Tensor" },
      { "name": "splits", "type": "TF_Int64Tensor" },
      { "name": "id_counts", "type": "TF_Int32Tensor" }
    ],
    "outputs": [
      { "name": "row_pointers", "type": "TF_Int32Tensor" },
      { "name": "sorted_sample_ids", "type": "TF_Int32Tensor" },
      { "name": "sorted_token_ids", "type": "TF_Int32Tensor" },
      { "name": "sorted_gains", "type": "TF_Float32Tensor" },
      { "name": "row_pointers_unpadded_size", "type": "TF_Int32Tensor" },
      { "name": "ids_unpadded_size", "type": "TF_Int32Tensor" },
      { "name": "num_minibatches_per_physical_sparse_core", "type": "TF_Int32Tensor" }
    ],
    "attributes": [
      { "name": "sample_count", "type": "ConfinedAttr" },
      { "name": "num_replica", "type": "ConfinedAttr" },
      { "name": "max_minibatches_per_sc", "type": "ConfinedAttr" },
      { "name": "max_ids_per_chip_per_sample", "type": "ConfinedAttr" },
      { "name": "table_vocab_size", "type": "ConfinedAttr" },
      { "name": "feature_width", "type": "ConfinedAttr" },
      { "name": "num_sc_per_chip", "type": "ConfinedAttr" },
      { "name": "table_name", "type": "StrAttr" },
      { "name": "mini_batch_in_csr", "type": "StrAttr" }
    ]
  },
  {
    "name": "tf.GetMinibatchSplitsWithPhysicalReplica",
    "inputs": [
      { "name": "program_key", "type": "TF_StrTensor" },
      { "name": "row_ids", "type": "TF_Int32Tensor" },
      { "name": "col_ids", "type": "TF_Int32Tensor" },
      { "name": "gains", "type": "TF_Float32Tensor" }
    ],
    "outputs": [
      { "name": "sorted_row_ids", "type": "TF_Int32Tensor" },
      { "name": "sorted_col_ids", "type": "TF_Int32Tensor" },
      { "name": "sorted_gains", "type": "TF_Float32Tensor" },
      { "name": "splits", "type": "TF_Int64Tensor" },
      { "name": "id_counts", "type": "TF_Int32Tensor" },
      { "name": "max_ids", "type": "TF_Int32Tensor" },
      { "name": "max_uniques", "type": "TF_Int32Tensor" }
    ],
    "attributes": [
      { "name": "sample_count", "type": "ConfinedAttr" },
      { "name": "num_replica", "type": "ConfinedAttr" },
      { "name": "table_vocab_size", "type": "ConfinedAttr" },
      { "name": "feature_width", "type": "ConfinedAttr" },
      { "name": "num_sc_per_chip", "type": "ConfinedAttr" },
      { "name": "table_name", "type": "StrAttr" },
      { "name": "mini_batch_splits", "type": "StrAttr" }
    ]
  },
  {
    "name": "tf.GetStatsFromListOfSparseCoreCooTensors",
    "summary": "An op which computes the max_ids/uniques for a given table.",
    "inputs": [
      { "name": "row_ids_list", "type": "Variadic" },
      { "name": "col_ids_list", "type": "Variadic" },
      { "name": "gains_list", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "max_ids_per_sparse_core", "type": "TF_Int32Tensor" },
      { "name": "max_unique_ids_per_sparse_core", "type": "TF_Int32Tensor" }
    ],
    "attributes": [
      { "name": "sample_count_list", "type": "I64ArrayAttr" },
      { "name": "col_offset_list", "type": "I64ArrayAttr" },
      { "name": "num_replica", "type": "ConfinedAttr" },
      { "name": "table_vocab_size", "type": "ConfinedAttr" },
      { "name": "feature_width", "type": "ConfinedAttr" },
      { "name": "num_sc_per_chip", "type": "ConfinedAttr" },
      { "name": "table_name", "type": "StrAttr" }
    ]
  },
  {
    "name": "tf.GlobalIterId",
    "summary": "Op that gets the global step id.",
    "description": "This op gets the step id for each loop iteration.",
    "outputs": [
      { "name": "iter_id", "type": "TF_Int64Tensor" }
    ]
  },
  {
    "name": "tf.Greater",
    "summary": "Returns the truth value of (x > y) element-wise.",
    "description": "*NOTE*: `Greater` supports broadcasting. More about broadcasting\n[here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n\nExample:\n\n```python\nx = tf.constant([5, 4, 6])\ny = tf.constant([5, 2, 5])\ntf.math.greater(x, y) ==> [False, True, True]\n\nx = tf.constant([5, 4, 6])\ny = tf.constant([5])\ntf.math.greater(x, y) ==> [False, False, True]\n```",
    "inputs": [
      { "name": "x", "type": "TF_IntOrFpTensor" },
      { "name": "y", "type": "TF_IntOrFpTensor" }
    ],
    "outputs": [
      { "name": "z", "type": "TF_BoolTensor" }
    ]
  },
  {
    "name": "tf.GreaterEqual",
    "summary": "Returns the truth value of (x >= y) element-wise.",
    "description": "*NOTE*: `GreaterEqual` supports broadcasting. More about broadcasting\n[here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n\nExample:\n\n```python\nx = tf.constant([5, 4, 6, 7])\ny = tf.constant([5, 2, 5, 10])\ntf.math.greater_equal(x, y) ==> [True, True, True, False]\n\nx = tf.constant([5, 4, 6, 7])\ny = tf.constant([5])\ntf.math.greater_equal(x, y) ==> [True, False, True, True]\n```",
    "inputs": [
      { "name": "x", "type": "TF_IntOrFpTensor" },
      { "name": "y", "type": "TF_IntOrFpTensor" }
    ],
    "outputs": [
      { "name": "z", "type": "TF_BoolTensor" }
    ]
  },
  {
    "name": "tf.HashTable",
    "summary": "Creates a non-initialized hash table.",
    "description": "This op creates a hash table, specifying the type of its keys and values.\nBefore using the table you will have to initialize it.  After initialization the\ntable will be immutable.",
    "outputs": [
      { "name": "table_handle", "type": "Res" }
    ],
    "attributes": [
      { "name": "container", "type": "DefaultValuedOptionalAttr" },
      { "name": "shared_name", "type": "DefaultValuedOptionalAttr" },
      { "name": "use_node_name_sharing", "type": "DefaultValuedOptionalAttr" },
      { "name": "key_dtype", "type": "TypeAttr" },
      { "name": "value_dtype", "type": "TypeAttr" }
    ]
  },
  {
    "name": "tf.HashTableV2",
    "summary": "Creates a non-initialized hash table.",
    "description": "This op creates a hash table, specifying the type of its keys and values.\nBefore using the table you will have to initialize it.  After initialization the\ntable will be immutable.",
    "outputs": [
      { "name": "table_handle", "type": "Res" }
    ],
    "attributes": [
      { "name": "container", "type": "DefaultValuedOptionalAttr" },
      { "name": "shared_name", "type": "DefaultValuedOptionalAttr" },
      { "name": "use_node_name_sharing", "type": "DefaultValuedOptionalAttr" },
      { "name": "key_dtype", "type": "TypeAttr" },
      { "name": "value_dtype", "type": "TypeAttr" }
    ]
  },
  {
    "name": "tf.HSVToRGB",
    "summary": "Convert one or more images from HSV to RGB.",
    "description": "Outputs a tensor of the same shape as the `images` tensor, containing the RGB\nvalue of the pixels. The output is only well defined if the value in `images`\nare in `[0,1]`.\n\nSee `rgb_to_hsv` for a description of the HSV encoding.",
    "inputs": [
      { "name": "images", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ]
  },
  {
    "name": "tf.Identity",
    "summary": "Return a tensor with the same shape and contents as the input tensor or value.",
    "inputs": [
      { "name": "input", "type": "TF_Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "TF_Tensor" }
    ]
  },
  {
    "name": "tf.IdentityN",
    "summary": "Returns a list of tensors with the same shapes and contents as the input",
    "description": "tensors.\n\nThis op can be used to override the gradient for complicated functions. For\nexample, suppose y = f(x) and we wish to apply a custom function g for backprop\nsuch that dx = g(dy). In Python,\n\n```python\nwith tf.get_default_graph().gradient_override_map(\n    {'IdentityN': 'OverrideGradientWithG'}):\n  y, _ = identity_n([f(x), x])\n\n@tf.RegisterGradient('OverrideGradientWithG')\ndef ApplyG(op, dy, _):\n  return [None, g(dy)]  # Do not backprop to f(x).\n```",
    "inputs": [
      { "name": "input", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "output", "type": "Variadic" }
    ]
  },
  {
    "name": "tf.If",
    "summary": "output = cond ? then_branch(input) : else_branch(input)",
    "description": "output = cond ? then_branch(input) : else_branch(input)\n\ncond: A Tensor. If the tensor is a scalar of non-boolean type, the\n    scalar is converted to a boolean according to the\n    following rule: if the scalar is a numerical value, non-zero means\n    True and zero means False; if the scalar is a string, non-empty\n    means True and empty means False. If the tensor is not a scalar,\n    being empty means False and being non-empty means True.\ninput: A list of input tensors.\nthen_branch: A function that takes 'inputs' and returns a list of\n    tensors, whose types are the same as what else_branch returns.\nelse_branch: A function that takes 'inputs' and returns a list of\n    tensors.  whose types are the same as what then_branch returns.",
    "inputs": [
      { "name": "cond", "type": "TF_Tensor" },
      { "name": "input", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "output", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "then_branch", "type": "FlatSymbolRefAttr" },
      { "name": "else_branch", "type": "FlatSymbolRefAttr" },
      { "name": "is_stateless", "type": "BoolAttr" }
    ]
  },
  {
    "name": "tf.IFFT",
    "summary": "Inverse fast Fourier transform.",
    "description": "Computes the inverse 1-dimensional discrete Fourier transform over the\ninner-most dimension of `input`.",
    "inputs": [
      { "name": "input", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ]
  },
  {
    "name": "tf.IFFT2D",
    "summary": "Inverse 2D fast Fourier transform.",
    "description": "Computes the inverse 2-dimensional discrete Fourier transform over the\ninner-most 2 dimensions of `input`.",
    "inputs": [
      { "name": "input", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ]
  },
  {
    "name": "tf.IFFT3D",
    "summary": "Inverse 3D fast Fourier transform.",
    "description": "Computes the inverse 3-dimensional discrete Fourier transform over the\ninner-most 3 dimensions of `input`.",
    "inputs": [
      { "name": "input", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ]
  },
  {
    "name": "tf.IfRegion",
    "summary": "output = cond ? then_branch output : else_branch output",
    "description": "\"output = cond ? then_branch output : else_branch output\"\n\ncond: A Tensor. If the tensor is a scalar of non-boolean type, the\n    scalar is converted to a boolean according to the\n    following rule: if the scalar is a numerical value, non-zero means\n    True and zero means False; if the scalar is a string, non-empty\n    means True and empty means False. If the tensor is not a scalar,\n    being empty means False and being non-empty means True.\nthen_branch: A region that computes the outputs of the op if cond = true.\n    It returns a list of tensors using tf.yield (as the terminator). The\n    types of these returned tensors is same as that of the else_branch\nelse_branch: A region that computes the outputs of the op if cond = false.\n    It returns a list of tensors using tf.yield (as the terminator). The\n    types of these returned tensors is same as that of the then_branch",
    "inputs": [
      { "name": "cond", "type": "DTensorOf" }
    ],
    "outputs": [
      { "name": "output", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "is_stateless", "type": "BoolAttr" },
      { "name": "_then_func_name", "type": "OptionalAttr" },
      { "name": "_else_func_name", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "tf.Igamma",
    "summary": "Compute the lower regularized incomplete Gamma function `P(a, x)`.",
    "description": "The lower regularized incomplete Gamma function is defined as:\n\n\n\\\\(P(a, x) = gamma(a, x) / Gamma(a) = 1 - Q(a, x)\\\\)\n\nwhere\n\n\\\\(gamma(a, x) = \\int_{0}^{x} t^{a-1} exp(-t) dt\\\\)\n\nis the lower incomplete Gamma function.\n\nNote, above `Q(a, x)` (`Igammac`) is the upper regularized complete\nGamma function.",
    "inputs": [
      { "name": "a", "type": "TF_FloatTensor" },
      { "name": "x", "type": "TF_FloatTensor" }
    ],
    "outputs": [
      { "name": "z", "type": "TF_FloatTensor" }
    ]
  },
  {
    "name": "tf.Igammac",
    "summary": "Compute the upper regularized incomplete Gamma function `Q(a, x)`.",
    "description": "The upper regularized incomplete Gamma function is defined as:\n\n\\\\(Q(a, x) = Gamma(a, x) / Gamma(a) = 1 - P(a, x)\\\\)\n\nwhere\n\n\\\\(Gamma(a, x) = \\int_{x}^{\\infty} t^{a-1} exp(-t) dt\\\\)\n\nis the upper incomplete Gamma function.\n\nNote, above `P(a, x)` (`Igamma`) is the lower regularized complete\nGamma function.",
    "inputs": [
      { "name": "a", "type": "TF_FloatTensor" },
      { "name": "x", "type": "TF_FloatTensor" }
    ],
    "outputs": [
      { "name": "z", "type": "TF_FloatTensor" }
    ]
  },
  {
    "name": "tf.IgammaGradA",
    "summary": "Computes the gradient of `igamma(a, x)` wrt `a`.",
    "inputs": [
      { "name": "a", "type": "TF_F32OrF64Tensor" },
      { "name": "x", "type": "TF_F32OrF64Tensor" }
    ],
    "outputs": [
      { "name": "z", "type": "TF_F32OrF64Tensor" }
    ]
  },
  {
    "name": "tf.Imag",
    "summary": "Returns the imaginary part of a complex number.",
    "description": "Given a tensor `input` of complex numbers, this operation returns a tensor of\ntype `float` that is the imaginary part of each element in `input`. All\nelements in `input` must be complex numbers of the form \\\\(a + bj\\\\), where *a*\nis the real part and *b* is the imaginary part returned by this operation.\n\nFor example:\n\n```\n# tensor 'input' is [-2.25 + 4.75j, 3.25 + 5.75j]\ntf.imag(input) ==> [4.75, 5.75]\n```",
    "inputs": [
      { "name": "input", "type": "TensorOf" }
    ],
    "outputs": [
      { "name": "output", "type": "TF_F32OrF64Tensor" }
    ]
  },
  {
    "name": "tf.ImportEvent",
    "summary": "Outputs a `tf.Event` protocol buffer.",
    "description": "When CreateSummaryDbWriter is being used, this op can be useful for\nimporting data from event logs.\n\nwriter: A handle to a summary writer.\nevent: A string containing a binary-encoded tf.Event proto.",
    "inputs": [
      { "name": "writer", "type": "Arg" },
      { "name": "event", "type": "TF_StrTensor" }
    ]
  },
  {
    "name": "tf.InfeedDequeue",
    "summary": "A placeholder op for a value that will be fed into the computation.",
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "shape", "type": "TF_ShapeAttr" }
    ]
  },
  {
    "name": "tf.InfeedDequeueTuple",
    "summary": "Fetches multiple values from infeed as an XLA tuple.",
    "outputs": [
      { "name": "outputs", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "_XlaSharding", "type": "OptionalAttr" },
      { "name": "layouts", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "tf.InfeedEnqueueTuple",
    "summary": "Feeds multiple Tensor values into the computation as an XLA tuple.",
    "inputs": [
      { "name": "inputs", "type": "Arg" }
    ],
    "attributes": [
      { "name": "dtypes", "type": "ConfinedAttr" },
      { "name": "shapes", "type": "TF_ShapeAttrArray" },
      { "name": "layouts", "type": "DefaultValuedOptionalAttr" },
      { "name": "device_ordinal", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.InitializeTable",
    "summary": "Table initializer that takes two tensors for keys and values respectively.",
    "inputs": [
      { "name": "table_handle", "type": "Arg" },
      { "name": "keys", "type": "Arg" },
      { "name": "values", "type": "Arg" }
    ]
  },
  {
    "name": "tf.InitializeTableFromDataset",
    "inputs": [
      { "name": "table_handle", "type": "Arg" },
      { "name": "dataset", "type": "TF_VariantTensor" }
    ]
  },
  {
    "name": "tf.InitializeTableFromTextFile",
    "summary": "Initializes a table from a text file.",
    "description": "It inserts one key-value pair into the table for each line of the file.\nThe key and value is extracted from the whole line content, elements from the\nsplit line based on `delimiter` or the line number (starting from zero).\nWhere to extract the key and value from a line is specified by `key_index` and\n`value_index`.\n\n- A value of -1 means use the line number(starting from zero), expects `int64`.\n- A value of -2 means use the whole line content, expects `string`.\n- A value >= 0 means use the index (starting at zero) of the split line based\n  on `delimiter`.",
    "inputs": [
      { "name": "table_handle", "type": "Arg" },
      { "name": "filename", "type": "Arg" }
    ],
    "attributes": [
      { "name": "key_index", "type": "ConfinedAttr" },
      { "name": "value_index", "type": "ConfinedAttr" },
      { "name": "vocab_size", "type": "ConfinedAttr" },
      { "name": "delimiter", "type": "DefaultValuedOptionalAttr" },
      { "name": "offset", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.InitializeTableFromTextFileV2",
    "summary": "Initializes a table from a text file.",
    "description": "It inserts one key-value pair into the table for each line of the file.\nThe key and value is extracted from the whole line content, elements from the\nsplit line based on `delimiter` or the line number (starting from zero).\nWhere to extract the key and value from a line is specified by `key_index` and\n`value_index`.\n\n- A value of -1 means use the line number(starting from zero), expects `int64`.\n- A value of -2 means use the whole line content, expects `string`.\n- A value >= 0 means use the index (starting at zero) of the split line based\n  on `delimiter`.",
    "inputs": [
      { "name": "table_handle", "type": "Arg" },
      { "name": "filename", "type": "Arg" }
    ],
    "attributes": [
      { "name": "key_index", "type": "ConfinedAttr" },
      { "name": "value_index", "type": "ConfinedAttr" },
      { "name": "vocab_size", "type": "ConfinedAttr" },
      { "name": "delimiter", "type": "DefaultValuedOptionalAttr" },
      { "name": "offset", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.InitializeTableV2",
    "summary": "Table initializer that takes two tensors for keys and values respectively.",
    "inputs": [
      { "name": "table_handle", "type": "Arg" },
      { "name": "keys", "type": "Arg" },
      { "name": "values", "type": "Arg" }
    ]
  },
  {
    "name": "tf.InplaceAdd",
    "summary": "Adds v into specified rows of x.",
    "description": "Computes y = x; y[i, :] += v; return y.",
    "inputs": [
      { "name": "x", "type": "Arg" },
      { "name": "i", "type": "Arg" },
      { "name": "v", "type": "Arg" }
    ],
    "outputs": [
      { "name": "y", "type": "Res" }
    ]
  },
  {
    "name": "tf.InplaceUpdate",
    "summary": "Updates specified rows 'i' with values 'v'.",
    "description": "Computes `x[i, :] = v; return x`.\n\nOriginally this function is mutative however for compilation we make this\noperation create / operate on a copy of `x`.",
    "inputs": [
      { "name": "x", "type": "Arg" },
      { "name": "i", "type": "Arg" },
      { "name": "v", "type": "Arg" }
    ],
    "outputs": [
      { "name": "y", "type": "Res" }
    ]
  },
  {
    "name": "tf.InTopKV2",
    "summary": "Says whether the targets are in the top `K` predictions.",
    "description": "This outputs a `batch_size` bool array, an entry `out[i]` is `true` if the\nprediction for the target class is among the top `k` predictions among\nall predictions for example `i`. Note that the behavior of `InTopK` differs\nfrom the `TopK` op in its handling of ties; if multiple classes have the\nsame prediction value and straddle the top-`k` boundary, all of those\nclasses are considered to be in the top `k`.\n\nMore formally, let\n\n  \\\\(predictions_i\\\\) be the predictions for all classes for example `i`,\n  \\\\(targets_i\\\\) be the target class for example `i`,\n  \\\\(out_i\\\\) be the output for example `i`,\n\n$$out_i = predictions_{i, targets_i} \\in TopKIncludingTies(predictions_i)$$",
    "inputs": [
      { "name": "predictions", "type": "Arg" },
      { "name": "targets", "type": "Arg" },
      { "name": "k", "type": "Arg" }
    ],
    "outputs": [
      { "name": "precision", "type": "Res" }
    ]
  },
  {
    "name": "tf.Inv",
    "summary": "Computes the reciprocal of x element-wise.",
    "description": "I.e., \\\\(y = 1 / x\\\\).",
    "inputs": [
      { "name": "x", "type": "TensorOf" }
    ],
    "outputs": [
      { "name": "y", "type": "TensorOf" }
    ]
  },
  {
    "name": "tf.Invert",
    "summary": "Invert (flip) each bit of supported types; for example, type `uint8` value 01010101 becomes 10101010.",
    "description": "Flip each bit of supported types.  For example, type `int8` (decimal 2) binary 00000010 becomes (decimal -3) binary 11111101.\nThis operation is performed on each element of the tensor argument `x`.\n\nExample:\n```python\nimport tensorflow as tf\nfrom tensorflow.python.ops import bitwise_ops\n\n# flip 2 (00000010) to -3 (11111101)\ntf.assert_equal(-3, bitwise_ops.invert(2))\n\ndtype_list = [dtypes.int8, dtypes.int16, dtypes.int32, dtypes.int64,\n              dtypes.uint8, dtypes.uint16, dtypes.uint32, dtypes.uint64]\n\ninputs = [0, 5, 3, 14]\nfor dtype in dtype_list:\n  # Because of issues with negative numbers, let's test this indirectly.\n  # 1. invert(a) and a = 0\n  # 2. invert(a) or a = invert(0)\n  input_tensor = tf.constant([0, 5, 3, 14], dtype=dtype)\n  not_a_and_a, not_a_or_a, not_0 = [bitwise_ops.bitwise_and(\n                                      input_tensor, bitwise_ops.invert(input_tensor)),\n                                    bitwise_ops.bitwise_or(\n                                      input_tensor, bitwise_ops.invert(input_tensor)),\n                                    bitwise_ops.invert(\n                                      tf.constant(0, dtype=dtype))]\n\n  expected = tf.constant([0, 0, 0, 0], dtype=tf.float32)\n  tf.assert_equal(tf.cast(not_a_and_a, tf.float32), expected)\n\n  expected = tf.cast([not_0] * 4, tf.float32)\n  tf.assert_equal(tf.cast(not_a_or_a, tf.float32), expected)\n\n  # For unsigned dtypes let's also check the result directly.\n  if dtype.is_unsigned:\n    inverted = bitwise_ops.invert(input_tensor)\n    expected = tf.constant([dtype.max - x for x in inputs], dtype=tf.float32)\n    tf.assert_equal(tf.cast(inverted, tf.float32), tf.cast(expected, tf.float32))\n```",
    "inputs": [
      { "name": "x", "type": "TF_IntTensor" }
    ],
    "outputs": [
      { "name": "y", "type": "TF_IntTensor" }
    ]
  },
  {
    "name": "tf.InvertPermutation",
    "summary": "Computes the inverse permutation of a tensor.",
    "description": "This operation computes the inverse of an index permutation. It takes a 1-D\ninteger tensor `x`, which represents the indices of a zero-based array, and\nswaps each value with its index position. In other words, for an output tensor\n`y` and an input tensor `x`, this operation computes the following:\n\n`y[x[i]] = i for i in [0, 1, ..., len(x) - 1]`\n\nThe values must include 0. There can be no duplicate values or negative values.\n\nFor example:\n\n```\n# tensor `x` is [3, 4, 0, 2, 1]\ninvert_permutation(x) ==> [2, 4, 3, 0, 1]\n```",
    "inputs": [
      { "name": "x", "type": "Arg" }
    ],
    "outputs": [
      { "name": "y", "type": "Res" }
    ]
  },
  {
    "name": "tf.IRFFT",
    "summary": "Inverse real-valued fast Fourier transform.",
    "description": "Computes the inverse 1-dimensional discrete Fourier transform of a real-valued\nsignal over the inner-most dimension of `input`.\n\nThe inner-most dimension of `input` is assumed to be the result of `RFFT`: the\n`fft_length / 2 + 1` unique components of the DFT of a real-valued signal. If\n`fft_length` is not provided, it is computed from the size of the inner-most\ndimension of `input` (`fft_length = 2 * (inner - 1)`). If the FFT length used to\ncompute `input` is odd, it should be provided since it cannot be inferred\nproperly.\n\nAlong the axis `IRFFT` is computed on, if `fft_length / 2 + 1` is smaller\nthan the corresponding dimension of `input`, the dimension is cropped. If it is\nlarger, the dimension is padded with zeros.",
    "inputs": [
      { "name": "input", "type": "Arg" },
      { "name": "fft_length", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ]
  },
  {
    "name": "tf.IRFFT2D",
    "summary": "Inverse 2D real-valued fast Fourier transform.",
    "description": "Computes the inverse 2-dimensional discrete Fourier transform of a real-valued\nsignal over the inner-most 2 dimensions of `input`.\n\nThe inner-most 2 dimensions of `input` are assumed to be the result of `RFFT2D`:\nThe inner-most dimension contains the `fft_length / 2 + 1` unique components of\nthe DFT of a real-valued signal. If `fft_length` is not provided, it is computed\nfrom the size of the inner-most 2 dimensions of `input`. If the FFT length used\nto compute `input` is odd, it should be provided since it cannot be inferred\nproperly.\n\nAlong each axis `IRFFT2D` is computed on, if `fft_length` (or\n`fft_length / 2 + 1` for the inner-most dimension) is smaller than the\ncorresponding dimension of `input`, the dimension is cropped. If it is larger,\nthe dimension is padded with zeros.",
    "inputs": [
      { "name": "input", "type": "Arg" },
      { "name": "fft_length", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ]
  },
  {
    "name": "tf.IRFFT3D",
    "summary": "Inverse 3D real-valued fast Fourier transform.",
    "description": "Computes the inverse 3-dimensional discrete Fourier transform of a real-valued\nsignal over the inner-most 3 dimensions of `input`.\n\nThe inner-most 3 dimensions of `input` are assumed to be the result of `RFFT3D`:\nThe inner-most dimension contains the `fft_length / 2 + 1` unique components of\nthe DFT of a real-valued signal. If `fft_length` is not provided, it is computed\nfrom the size of the inner-most 3 dimensions of `input`. If the FFT length used\nto compute `input` is odd, it should be provided since it cannot be inferred\nproperly.\n\nAlong each axis `IRFFT3D` is computed on, if `fft_length` (or\n`fft_length / 2 + 1` for the inner-most dimension) is smaller than the\ncorresponding dimension of `input`, the dimension is cropped. If it is larger,\nthe dimension is padded with zeros.",
    "inputs": [
      { "name": "input", "type": "Arg" },
      { "name": "fft_length", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ]
  },
  {
    "name": "tf.IsFinite",
    "summary": "Returns which elements of x are finite.",
    "description": "@compatibility(numpy)\nEquivalent to np.isfinite\n@end_compatibility\n\nExample:\n\n```python\nx = tf.constant([5.0, 4.8, 6.8, np.inf, np.nan])\ntf.math.is_finite(x) ==> [True, True, True, False, False]\n```",
    "inputs": [
      { "name": "x", "type": "TF_FloatTensor" }
    ],
    "outputs": [
      { "name": "y", "type": "TF_BoolTensor" }
    ]
  },
  {
    "name": "tf.IsInf",
    "summary": "Returns which elements of x are Inf.",
    "description": "@compatibility(numpy)\nEquivalent to np.isinf\n@end_compatibility\n\nExample:\n\n```python\nx = tf.constant([5.0, np.inf, 6.8, np.inf])\ntf.math.is_inf(x) ==> [False, True, False, True]\n```",
    "inputs": [
      { "name": "x", "type": "TF_FloatTensor" }
    ],
    "outputs": [
      { "name": "y", "type": "TF_BoolTensor" }
    ]
  },
  {
    "name": "tf.IsNan",
    "summary": "Returns which elements of x are NaN.",
    "description": "@compatibility(numpy)\nEquivalent to np.isnan\n@end_compatibility\n\nExample:\n\n```python\nx = tf.constant([5.0, np.nan, 6.8, np.nan, np.inf])\ntf.math.is_nan(x) ==> [False, True, False, True, False]\n```",
    "inputs": [
      { "name": "x", "type": "TF_FloatTensor" }
    ],
    "outputs": [
      { "name": "y", "type": "TF_BoolTensor" }
    ]
  },
  {
    "name": "tf.Iterator",
    "summary": "A container for an iterator resource.",
    "outputs": [
      { "name": "handle", "type": "Res" }
    ],
    "attributes": [
      { "name": "shared_name", "type": "StrAttr" },
      { "name": "container", "type": "StrAttr" },
      { "name": "output_types", "type": "ConfinedAttr" },
      { "name": "output_shapes", "type": "ConfinedAttr" }
    ]
  },
  {
    "name": "tf.IteratorFromStringHandle",
    "summary": "Converts the given string representing a handle to an iterator to a resource.",
    "inputs": [
      { "name": "string_handle", "type": "Arg" }
    ],
    "outputs": [
      { "name": "resource_handle", "type": "Res" }
    ],
    "attributes": [
      { "name": "output_types", "type": "DefaultValuedOptionalAttr" },
      { "name": "output_shapes", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.IteratorFromStringHandleV2",
    "inputs": [
      { "name": "string_handle", "type": "TF_StrTensor" }
    ],
    "outputs": [
      { "name": "resource_handle", "type": "Res" }
    ],
    "attributes": [
      { "name": "output_types", "type": "DefaultValuedOptionalAttr" },
      { "name": "output_shapes", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.IteratorGetNext",
    "summary": "Gets the next output from the given iterator .",
    "inputs": [
      { "name": "iterator", "type": "Arg" }
    ],
    "outputs": [
      { "name": "components", "type": "Variadic" }
    ]
  },
  {
    "name": "tf.IteratorGetNextAsOptional",
    "summary": "Gets the next output from the given iterator as an Optional variant.",
    "inputs": [
      { "name": "iterator", "type": "Arg" }
    ],
    "outputs": [
      { "name": "optional", "type": "TF_VariantTensor" }
    ],
    "attributes": [
      { "name": "output_types", "type": "ConfinedAttr" },
      { "name": "output_shapes", "type": "ConfinedAttr" }
    ]
  },
  {
    "name": "tf.IteratorGetNextSync",
    "summary": "Gets the next output from the given iterator.",
    "description": "This operation is a synchronous version IteratorGetNext. It should only be used\nin situations where the iterator does not block the calling thread, or where\nthe calling thread is not a member of the thread pool used to execute parallel\noperations (e.g. in eager mode).",
    "inputs": [
      { "name": "iterator", "type": "Arg" }
    ],
    "outputs": [
      { "name": "components", "type": "Variadic" }
    ]
  },
  {
    "name": "tf.IteratorToStringHandle",
    "summary": "Converts the given `resource_handle` representing an iterator to a string.",
    "inputs": [
      { "name": "resource_handle", "type": "Arg" }
    ],
    "outputs": [
      { "name": "string_handle", "type": "Res" }
    ]
  },
  {
    "name": "tf.IteratorV2",
    "outputs": [
      { "name": "handle", "type": "Res" }
    ],
    "attributes": [
      { "name": "shared_name", "type": "StrAttr" },
      { "name": "container", "type": "StrAttr" },
      { "name": "output_types", "type": "ConfinedAttr" },
      { "name": "output_shapes", "type": "ConfinedAttr" }
    ]
  },
  {
    "name": "tf.KthOrderStatistic",
    "summary": "Computes the Kth order statistic of a data set. The current",
    "description": "implementation uses a binary search requiring exactly 32 passes over\nthe input data. The running time is linear with respect to input\nsize. The median-of-medians algorithm is probably faster, but is\ndifficult to implement efficiently in XLA. The implementation imposes\na total ordering on floats. The ordering is consistent with the usual\npartial order.  Positive NaNs are greater than positive\ninfinity. Negative NaNs are less than negative infinity. NaNs with\ndistinct payloads are treated as distinct. Subnormal numbers are\npreserved (not flushed to zero). Positive infinity is greater than all\nnumbers. Negative infinity is less than all numbers. Positive is\ngreater than negative zero. There are less than k values greater than\nthe kth order statistic. There are at least k values greater than or\nequal to the Kth order statistic. The semantics are not the same as\ntop_k_unique.",
    "inputs": [
      { "name": "input", "type": "TF_Float32Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "TF_Float32Tensor" }
    ],
    "attributes": [
      { "name": "k", "type": "I64Attr" }
    ]
  },
  {
    "name": "tf.L2Loss",
    "summary": "L2 Loss.",
    "description": "Computes half the L2 norm of a tensor without the `sqrt`:\n\n    output = sum(t ** 2) / 2",
    "inputs": [
      { "name": "x", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ]
  },
  {
    "name": "tf.LeakyRelu",
    "summary": "Computes rectified linear: `max(features, features * alpha)`.",
    "inputs": [
      { "name": "features", "type": "TF_FloatTensor" }
    ],
    "outputs": [
      { "name": "activations", "type": "TF_FloatTensor" }
    ],
    "attributes": [
      { "name": "alpha", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.LeakyReluGrad",
    "summary": "Computes rectified linear gradients for a LeakyRelu operation.",
    "inputs": [
      { "name": "gradients", "type": "Arg" },
      { "name": "features", "type": "Arg" }
    ],
    "outputs": [
      { "name": "backprops", "type": "Res" }
    ],
    "attributes": [
      { "name": "alpha", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.LeftShift",
    "summary": "Elementwise computes the bitwise left-shift of `x` and `y`.",
    "description": "If `y` is negative, or greater than or equal to the width of `x` in bits the\nresult is implementation defined.\n\nExample:\n\n```python\nimport tensorflow as tf\nfrom tensorflow.python.ops import bitwise_ops\nimport numpy as np\ndtype_list = [tf.int8, tf.int16, tf.int32, tf.int64]\n\nfor dtype in dtype_list:\n  lhs = tf.constant([-1, -5, -3, -14], dtype=dtype)\n  rhs = tf.constant([5, 0, 7, 11], dtype=dtype)\n\n  left_shift_result = bitwise_ops.left_shift(lhs, rhs)\n\n  print(left_shift_result)\n\n# This will print:\n# tf.Tensor([ -32   -5 -128    0], shape=(4,), dtype=int8)\n# tf.Tensor([   -32     -5   -384 -28672], shape=(4,), dtype=int16)\n# tf.Tensor([   -32     -5   -384 -28672], shape=(4,), dtype=int32)\n# tf.Tensor([   -32     -5   -384 -28672], shape=(4,), dtype=int64)\n\nlhs = np.array([-2, 64, 101, 32], dtype=np.int8)\nrhs = np.array([-1, -5, -3, -14], dtype=np.int8)\nbitwise_ops.left_shift(lhs, rhs)\n# <tf.Tensor: shape=(4,), dtype=int8, numpy=array([ -2,  64, 101,  32], dtype=int8)>\n```",
    "inputs": [
      { "name": "x", "type": "TF_IntTensor" },
      { "name": "y", "type": "TF_IntTensor" }
    ],
    "outputs": [
      { "name": "z", "type": "TF_IntTensor" }
    ]
  },
  {
    "name": "tf.LegacyCall",
    "summary": "returns `f(inputs)`, where `f` is a function.",
    "description": "The LegacyCall operation represents a direct call to a function that is\n    within the same symbol scope as the call and is mapped to a GraphDef node\n    with the function name as the op name. Unlike a PartitionedCall which\n    represents asynchronously executing a function across multiple devices, a\n    LegacyCall ignores specification for ops in the attached function and\n    instead executes it on the device assigned to this op.",
    "inputs": [
      { "name": "args", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "output", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "arg_attrs", "type": "OptionalAttr" },
      { "name": "res_attrs", "type": "OptionalAttr" },
      { "name": "f", "type": "FlatSymbolRefAttr" },
      { "name": "_disable_call_shape_inference", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.Less",
    "summary": "Returns the truth value of (x < y) element-wise.",
    "description": "*NOTE*: `Less` supports broadcasting. More about broadcasting\n[here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n\nExample:\n\n```python\nx = tf.constant([5, 4, 6])\ny = tf.constant([5])\ntf.math.less(x, y) ==> [False, True, False]\n\nx = tf.constant([5, 4, 6])\ny = tf.constant([5, 6, 7])\ntf.math.less(x, y) ==> [False, True, True]\n```",
    "inputs": [
      { "name": "x", "type": "TF_IntOrFpTensor" },
      { "name": "y", "type": "TF_IntOrFpTensor" }
    ],
    "outputs": [
      { "name": "z", "type": "TF_BoolTensor" }
    ]
  },
  {
    "name": "tf.LessEqual",
    "summary": "Returns the truth value of (x <= y) element-wise.",
    "description": "*NOTE*: `LessEqual` supports broadcasting. More about broadcasting\n[here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n\nExample:\n\n```python\nx = tf.constant([5, 4, 6])\ny = tf.constant([5])\ntf.math.less_equal(x, y) ==> [True, True, False]\n\nx = tf.constant([5, 4, 6])\ny = tf.constant([5, 6, 6])\ntf.math.less_equal(x, y) ==> [True, True, True]\n```",
    "inputs": [
      { "name": "x", "type": "TF_IntOrFpTensor" },
      { "name": "y", "type": "TF_IntOrFpTensor" }
    ],
    "outputs": [
      { "name": "z", "type": "TF_BoolTensor" }
    ]
  },
  {
    "name": "tf.Lgamma",
    "summary": "Computes the log of the absolute value of `Gamma(x)` element-wise.",
    "description": "For positive numbers, this function computes log((input - 1)!) for every element in the tensor.\n  `lgamma(5) = log((5-1)!) = log(4!) = log(24) = 3.1780539`\n\nExample:\n\n```python\nx = tf.constant([0, 0.5, 1, 4.5, -4, -5.6])\ntf.math.lgamma(x) ==> [inf, 0.5723649, 0., 2.4537368, inf, -4.6477685]\n```",
    "inputs": [
      { "name": "x", "type": "TF_FloatTensor" }
    ],
    "outputs": [
      { "name": "y", "type": "TF_FloatTensor" }
    ]
  },
  {
    "name": "tf.LinSpace",
    "summary": "Generates values in an interval.",
    "description": "A sequence of `num` evenly-spaced values are generated beginning at `start`.\nIf `num > 1`, the values in the sequence increase by\n`(stop - start) / (num - 1)`, so that the last one is exactly `stop`.\n\nFor example:\n\n```\ntf.linspace(10.0, 12.0, 3, name=\"linspace\") => [ 10.0  11.0  12.0]\n```",
    "inputs": [
      { "name": "start", "type": "Arg" },
      { "name": "stop", "type": "Arg" },
      { "name": "num", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ]
  },
  {
    "name": "tf.ListDiff",
    "summary": "Computes the difference between two lists of numbers or strings.",
    "description": "Given a list `x` and a list `y`, this operation returns a list `out` that\nrepresents all values that are in `x` but not in `y`. The returned list `out`\nis sorted in the same order that the numbers appear in `x` (duplicates are\npreserved). This operation also returns a list `idx` that represents the\nposition of each `out` element in `x`. In other words:\n\n`out[i] = x[idx[i]] for i in [0, 1, ..., len(out) - 1]`\n\nFor example, given this input:\n\n```\nx = [1, 2, 3, 4, 5, 6]\ny = [1, 3, 5]\n```\n\nThis operation would return:\n\n```\nout ==> [2, 4, 6]\nidx ==> [1, 3, 5]\n```",
    "inputs": [
      { "name": "x", "type": "Arg" },
      { "name": "y", "type": "Arg" }
    ],
    "outputs": [
      { "name": "out", "type": "Res" },
      { "name": "idx", "type": "Res" }
    ]
  },
  {
    "name": "tf.LoadTPUEmbeddingAdadeltaParameters",
    "summary": "Load Adadelta embedding parameters.",
    "description": "An op that loads optimization parameters into HBM for embedding. Must be\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\nembedding table configuration. For example, this op is used to install\nparameters that are loaded from a checkpoint before a training loop is\nexecuted.",
    "inputs": [
      { "name": "parameters", "type": "Arg" },
      { "name": "accumulators", "type": "Arg" },
      { "name": "updates", "type": "Arg" }
    ],
    "attributes": [
      { "name": "table_id", "type": "DefaultValuedOptionalAttr" },
      { "name": "table_name", "type": "DefaultValuedOptionalAttr" },
      { "name": "num_shards", "type": "I64Attr" },
      { "name": "shard_id", "type": "I64Attr" },
      { "name": "config", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.LoadTPUEmbeddingAdadeltaParametersGradAccumDebug",
    "inputs": [
      { "name": "parameters", "type": "TF_Float32Tensor" },
      { "name": "accumulators", "type": "TF_Float32Tensor" },
      { "name": "updates", "type": "TF_Float32Tensor" },
      { "name": "gradient_accumulators", "type": "TF_Float32Tensor" }
    ],
    "attributes": [
      { "name": "table_id", "type": "DefaultValuedOptionalAttr" },
      { "name": "table_name", "type": "DefaultValuedOptionalAttr" },
      { "name": "num_shards", "type": "I64Attr" },
      { "name": "shard_id", "type": "I64Attr" },
      { "name": "config", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.LoadTPUEmbeddingAdagradParameters",
    "summary": "Load Adagrad embedding parameters.",
    "description": "An op that loads optimization parameters into HBM for embedding. Must be\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\nembedding table configuration. For example, this op is used to install\nparameters that are loaded from a checkpoint before a training loop is\nexecuted.",
    "inputs": [
      { "name": "parameters", "type": "Arg" },
      { "name": "accumulators", "type": "Arg" }
    ],
    "attributes": [
      { "name": "table_id", "type": "DefaultValuedOptionalAttr" },
      { "name": "table_name", "type": "DefaultValuedOptionalAttr" },
      { "name": "num_shards", "type": "I64Attr" },
      { "name": "shard_id", "type": "I64Attr" },
      { "name": "config", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.LoadTPUEmbeddingAdagradParametersGradAccumDebug",
    "inputs": [
      { "name": "parameters", "type": "TF_Float32Tensor" },
      { "name": "accumulators", "type": "TF_Float32Tensor" },
      { "name": "gradient_accumulators", "type": "TF_Float32Tensor" }
    ],
    "attributes": [
      { "name": "table_id", "type": "DefaultValuedOptionalAttr" },
      { "name": "table_name", "type": "DefaultValuedOptionalAttr" },
      { "name": "num_shards", "type": "I64Attr" },
      { "name": "shard_id", "type": "I64Attr" },
      { "name": "config", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.LoadTPUEmbeddingADAMParameters",
    "summary": "Load ADAM embedding parameters.",
    "description": "An op that loads optimization parameters into HBM for embedding. Must be\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\nembedding table configuration. For example, this op is used to install\nparameters that are loaded from a checkpoint before a training loop is\nexecuted.",
    "inputs": [
      { "name": "parameters", "type": "Arg" },
      { "name": "momenta", "type": "Arg" },
      { "name": "velocities", "type": "Arg" }
    ],
    "attributes": [
      { "name": "table_id", "type": "DefaultValuedOptionalAttr" },
      { "name": "table_name", "type": "DefaultValuedOptionalAttr" },
      { "name": "num_shards", "type": "I64Attr" },
      { "name": "shard_id", "type": "I64Attr" },
      { "name": "config", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.LoadTPUEmbeddingADAMParametersGradAccumDebug",
    "inputs": [
      { "name": "parameters", "type": "TF_Float32Tensor" },
      { "name": "momenta", "type": "TF_Float32Tensor" },
      { "name": "velocities", "type": "TF_Float32Tensor" },
      { "name": "gradient_accumulators", "type": "TF_Float32Tensor" }
    ],
    "attributes": [
      { "name": "table_id", "type": "DefaultValuedOptionalAttr" },
      { "name": "table_name", "type": "DefaultValuedOptionalAttr" },
      { "name": "num_shards", "type": "I64Attr" },
      { "name": "shard_id", "type": "I64Attr" },
      { "name": "config", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.LoadTPUEmbeddingCenteredRMSPropParameters",
    "summary": "Load centered RMSProp embedding parameters.",
    "description": "An op that loads optimization parameters into HBM for embedding. Must be\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\nembedding table configuration. For example, this op is used to install\nparameters that are loaded from a checkpoint before a training loop is\nexecuted.",
    "inputs": [
      { "name": "parameters", "type": "Arg" },
      { "name": "ms", "type": "Arg" },
      { "name": "mom", "type": "Arg" },
      { "name": "mg", "type": "Arg" }
    ],
    "attributes": [
      { "name": "table_id", "type": "DefaultValuedOptionalAttr" },
      { "name": "table_name", "type": "DefaultValuedOptionalAttr" },
      { "name": "num_shards", "type": "I64Attr" },
      { "name": "shard_id", "type": "I64Attr" },
      { "name": "config", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.LoadTPUEmbeddingFTRLParameters",
    "summary": "Load FTRL embedding parameters.",
    "description": "An op that loads optimization parameters into HBM for embedding. Must be\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\nembedding table configuration. For example, this op is used to install\nparameters that are loaded from a checkpoint before a training loop is\nexecuted.",
    "inputs": [
      { "name": "parameters", "type": "Arg" },
      { "name": "accumulators", "type": "Arg" },
      { "name": "linears", "type": "Arg" }
    ],
    "attributes": [
      { "name": "table_id", "type": "DefaultValuedOptionalAttr" },
      { "name": "table_name", "type": "DefaultValuedOptionalAttr" },
      { "name": "num_shards", "type": "I64Attr" },
      { "name": "shard_id", "type": "I64Attr" },
      { "name": "config", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.LoadTPUEmbeddingFTRLParametersGradAccumDebug",
    "inputs": [
      { "name": "parameters", "type": "TF_Float32Tensor" },
      { "name": "accumulators", "type": "TF_Float32Tensor" },
      { "name": "linears", "type": "TF_Float32Tensor" },
      { "name": "gradient_accumulators", "type": "TF_Float32Tensor" }
    ],
    "attributes": [
      { "name": "table_id", "type": "DefaultValuedOptionalAttr" },
      { "name": "table_name", "type": "DefaultValuedOptionalAttr" },
      { "name": "num_shards", "type": "I64Attr" },
      { "name": "shard_id", "type": "I64Attr" },
      { "name": "config", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.LoadTPUEmbeddingMDLAdagradLightParameters",
    "summary": "Load MDL Adagrad Light embedding parameters.",
    "description": "An op that loads optimization parameters into HBM for embedding. Must be\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\nembedding table configuration. For example, this op is used to install\nparameters that are loaded from a checkpoint before a training loop is\nexecuted.",
    "inputs": [
      { "name": "parameters", "type": "Arg" },
      { "name": "accumulators", "type": "Arg" },
      { "name": "weights", "type": "Arg" },
      { "name": "benefits", "type": "Arg" }
    ],
    "attributes": [
      { "name": "table_id", "type": "DefaultValuedOptionalAttr" },
      { "name": "table_name", "type": "DefaultValuedOptionalAttr" },
      { "name": "num_shards", "type": "I64Attr" },
      { "name": "shard_id", "type": "I64Attr" },
      { "name": "config", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.LoadTPUEmbeddingMomentumParameters",
    "summary": "Load Momentum embedding parameters.",
    "description": "An op that loads optimization parameters into HBM for embedding. Must be\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\nembedding table configuration. For example, this op is used to install\nparameters that are loaded from a checkpoint before a training loop is\nexecuted.",
    "inputs": [
      { "name": "parameters", "type": "Arg" },
      { "name": "momenta", "type": "Arg" }
    ],
    "attributes": [
      { "name": "table_id", "type": "DefaultValuedOptionalAttr" },
      { "name": "table_name", "type": "DefaultValuedOptionalAttr" },
      { "name": "num_shards", "type": "I64Attr" },
      { "name": "shard_id", "type": "I64Attr" },
      { "name": "config", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.LoadTPUEmbeddingMomentumParametersGradAccumDebug",
    "inputs": [
      { "name": "parameters", "type": "TF_Float32Tensor" },
      { "name": "momenta", "type": "TF_Float32Tensor" },
      { "name": "gradient_accumulators", "type": "TF_Float32Tensor" }
    ],
    "attributes": [
      { "name": "table_id", "type": "DefaultValuedOptionalAttr" },
      { "name": "table_name", "type": "DefaultValuedOptionalAttr" },
      { "name": "num_shards", "type": "I64Attr" },
      { "name": "shard_id", "type": "I64Attr" },
      { "name": "config", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.LoadTPUEmbeddingProximalAdagradParameters",
    "summary": "Load proximal Adagrad embedding parameters.",
    "description": "An op that loads optimization parameters into HBM for embedding. Must be\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\nembedding table configuration. For example, this op is used to install\nparameters that are loaded from a checkpoint before a training loop is\nexecuted.",
    "inputs": [
      { "name": "parameters", "type": "Arg" },
      { "name": "accumulators", "type": "Arg" }
    ],
    "attributes": [
      { "name": "table_id", "type": "DefaultValuedOptionalAttr" },
      { "name": "table_name", "type": "DefaultValuedOptionalAttr" },
      { "name": "num_shards", "type": "I64Attr" },
      { "name": "shard_id", "type": "I64Attr" },
      { "name": "config", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.LoadTPUEmbeddingProximalAdagradParametersGradAccumDebug",
    "inputs": [
      { "name": "parameters", "type": "TF_Float32Tensor" },
      { "name": "accumulators", "type": "TF_Float32Tensor" },
      { "name": "gradient_accumulators", "type": "TF_Float32Tensor" }
    ],
    "attributes": [
      { "name": "table_id", "type": "DefaultValuedOptionalAttr" },
      { "name": "table_name", "type": "DefaultValuedOptionalAttr" },
      { "name": "num_shards", "type": "I64Attr" },
      { "name": "shard_id", "type": "I64Attr" },
      { "name": "config", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.LoadTPUEmbeddingProximalYogiParameters",
    "inputs": [
      { "name": "parameters", "type": "TF_Float32Tensor" },
      { "name": "v", "type": "TF_Float32Tensor" },
      { "name": "m", "type": "TF_Float32Tensor" }
    ],
    "attributes": [
      { "name": "table_id", "type": "DefaultValuedOptionalAttr" },
      { "name": "table_name", "type": "DefaultValuedOptionalAttr" },
      { "name": "num_shards", "type": "I64Attr" },
      { "name": "shard_id", "type": "I64Attr" },
      { "name": "config", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.LoadTPUEmbeddingProximalYogiParametersGradAccumDebug",
    "inputs": [
      { "name": "parameters", "type": "TF_Float32Tensor" },
      { "name": "v", "type": "TF_Float32Tensor" },
      { "name": "m", "type": "TF_Float32Tensor" },
      { "name": "gradient_accumulators", "type": "TF_Float32Tensor" }
    ],
    "attributes": [
      { "name": "table_id", "type": "DefaultValuedOptionalAttr" },
      { "name": "table_name", "type": "DefaultValuedOptionalAttr" },
      { "name": "num_shards", "type": "I64Attr" },
      { "name": "shard_id", "type": "I64Attr" },
      { "name": "config", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.LoadTPUEmbeddingRMSPropParameters",
    "summary": "Load RMSProp embedding parameters.",
    "description": "An op that loads optimization parameters into HBM for embedding. Must be\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\nembedding table configuration. For example, this op is used to install\nparameters that are loaded from a checkpoint before a training loop is\nexecuted.",
    "inputs": [
      { "name": "parameters", "type": "Arg" },
      { "name": "ms", "type": "Arg" },
      { "name": "mom", "type": "Arg" }
    ],
    "attributes": [
      { "name": "table_id", "type": "DefaultValuedOptionalAttr" },
      { "name": "table_name", "type": "DefaultValuedOptionalAttr" },
      { "name": "num_shards", "type": "I64Attr" },
      { "name": "shard_id", "type": "I64Attr" },
      { "name": "config", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.LoadTPUEmbeddingRMSPropParametersGradAccumDebug",
    "inputs": [
      { "name": "parameters", "type": "TF_Float32Tensor" },
      { "name": "ms", "type": "TF_Float32Tensor" },
      { "name": "mom", "type": "TF_Float32Tensor" },
      { "name": "gradient_accumulators", "type": "TF_Float32Tensor" }
    ],
    "attributes": [
      { "name": "table_id", "type": "DefaultValuedOptionalAttr" },
      { "name": "table_name", "type": "DefaultValuedOptionalAttr" },
      { "name": "num_shards", "type": "I64Attr" },
      { "name": "shard_id", "type": "I64Attr" },
      { "name": "config", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.LoadTPUEmbeddingStochasticGradientDescentParameters",
    "summary": "Load SGD embedding parameters.",
    "description": "An op that loads optimization parameters into HBM for embedding. Must be\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\nembedding table configuration. For example, this op is used to install\nparameters that are loaded from a checkpoint before a training loop is\nexecuted.",
    "inputs": [
      { "name": "parameters", "type": "Arg" }
    ],
    "attributes": [
      { "name": "table_id", "type": "DefaultValuedOptionalAttr" },
      { "name": "table_name", "type": "DefaultValuedOptionalAttr" },
      { "name": "num_shards", "type": "I64Attr" },
      { "name": "shard_id", "type": "I64Attr" },
      { "name": "config", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.LoadTPUEmbeddingStochasticGradientDescentParametersGradAccumDebug",
    "inputs": [
      { "name": "parameters", "type": "TF_Float32Tensor" },
      { "name": "gradient_accumulators", "type": "TF_Float32Tensor" }
    ],
    "attributes": [
      { "name": "table_id", "type": "DefaultValuedOptionalAttr" },
      { "name": "table_name", "type": "DefaultValuedOptionalAttr" },
      { "name": "num_shards", "type": "I64Attr" },
      { "name": "shard_id", "type": "I64Attr" },
      { "name": "config", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.Log",
    "summary": "Computes natural logarithm of x element-wise.",
    "description": "I.e., \\\\(y = \\log_e x\\\\).\n\nExample:\n\n```python\nx = tf.constant([0, 0.5, 1, 5])\ntf.math.log(x) ==> [-inf, -0.6931472,  0. ,  1.609438]\n```",
    "inputs": [
      { "name": "x", "type": "TF_FpOrComplexTensor" }
    ],
    "outputs": [
      { "name": "y", "type": "TF_FpOrComplexTensor" }
    ]
  },
  {
    "name": "tf.Log1p",
    "summary": "Computes natural logarithm of (1 + x) element-wise.",
    "description": "I.e., \\\\(y = \\log_e (1 + x)\\\\).\n\nExample:\n\n```python\nx = tf.constant([0, 0.5, 1, 5])\ntf.math.log1p(x) ==> [0., 0.4054651, 0.6931472, 1.7917595]\n```",
    "inputs": [
      { "name": "x", "type": "TF_FpOrComplexTensor" }
    ],
    "outputs": [
      { "name": "y", "type": "TF_FpOrComplexTensor" }
    ]
  },
  {
    "name": "tf.LogicalAnd",
    "summary": "Returns the truth value of x AND y element-wise.",
    "description": "*NOTE*: `LogicalAnd` supports broadcasting. More about broadcasting\n[here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)",
    "inputs": [
      { "name": "x", "type": "TF_BoolTensor" },
      { "name": "y", "type": "TF_BoolTensor" }
    ],
    "outputs": [
      { "name": "z", "type": "TF_BoolTensor" }
    ]
  },
  {
    "name": "tf.LogicalNot",
    "summary": "Returns the truth value of `NOT x` element-wise.",
    "inputs": [
      { "name": "x", "type": "Arg" }
    ],
    "outputs": [
      { "name": "y", "type": "Res" }
    ]
  },
  {
    "name": "tf.LogicalOr",
    "summary": "Returns the truth value of x OR y element-wise.",
    "description": "*NOTE*: `LogicalOr` supports broadcasting. More about broadcasting\n[here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)",
    "inputs": [
      { "name": "x", "type": "TF_BoolTensor" },
      { "name": "y", "type": "TF_BoolTensor" }
    ],
    "outputs": [
      { "name": "z", "type": "TF_BoolTensor" }
    ]
  },
  {
    "name": "tf.LogSoftmax",
    "summary": "Computes log softmax activations.",
    "description": "For each batch `i` and class `j` we have\n\n    logsoftmax[i, j] = logits[i, j] - log(sum(exp(logits[i])))",
    "inputs": [
      { "name": "logits", "type": "Arg" }
    ],
    "outputs": [
      { "name": "logsoftmax", "type": "Res" }
    ]
  },
  {
    "name": "tf.LookupTableExportV2",
    "summary": "Outputs all keys and values in the table.",
    "inputs": [
      { "name": "table_handle", "type": "Arg" }
    ],
    "outputs": [
      { "name": "keys", "type": "Res" },
      { "name": "values", "type": "Res" }
    ]
  },
  {
    "name": "tf.LookupTableFind",
    "summary": "Looks up keys in a table, outputs the corresponding values.",
    "description": "The tensor `keys` must of the same type as the keys of the table.\nThe output `values` is of the type of the table values.\n\nThe scalar `default_value` is the value output for keys not present in the\ntable. It must also be of the same type as the table values.",
    "inputs": [
      { "name": "table_handle", "type": "Arg" },
      { "name": "keys", "type": "Arg" },
      { "name": "default_value", "type": "TF_Tensor" }
    ],
    "outputs": [
      { "name": "values", "type": "Res" }
    ]
  },
  {
    "name": "tf.LookupTableFindV2",
    "summary": "Looks up keys in a table, outputs the corresponding values.",
    "description": "The tensor `keys` must of the same type as the keys of the table.\nThe output `values` is of the type of the table values.\n\nThe scalar `default_value` is the value output for keys not present in the\ntable. It must also be of the same type as the table values.",
    "inputs": [
      { "name": "table_handle", "type": "Arg" },
      { "name": "keys", "type": "Arg" },
      { "name": "default_value", "type": "TF_Tensor" }
    ],
    "outputs": [
      { "name": "values", "type": "Res" }
    ]
  },
  {
    "name": "tf.LookupTableImportV2",
    "summary": "Replaces the contents of the table with the specified keys and values.",
    "description": "The tensor `keys` must be of the same type as the keys of the table.\nThe tensor `values` must be of the type of the table values.",
    "inputs": [
      { "name": "table_handle", "type": "Arg" },
      { "name": "keys", "type": "Arg" },
      { "name": "values", "type": "Arg" }
    ]
  },
  {
    "name": "tf.LookupTableInsertV2",
    "summary": "Updates the table to associates keys with values.",
    "description": "The tensor `keys` must be of the same type as the keys of the table.\nThe tensor `values` must be of the type of the table values.",
    "inputs": [
      { "name": "table_handle", "type": "Arg" },
      { "name": "keys", "type": "Arg" },
      { "name": "values", "type": "Arg" }
    ]
  },
  {
    "name": "tf.LookupTableRemoveV2",
    "summary": "Removes keys and its associated values from a table.",
    "description": "The tensor `keys` must of the same type as the keys of the table. Keys not\nalready in the table are silently ignored.",
    "inputs": [
      { "name": "table_handle", "type": "Arg" },
      { "name": "keys", "type": "Arg" }
    ]
  },
  {
    "name": "tf.LookupTableSize",
    "summary": "Computes the number of elements in the given table.",
    "inputs": [
      { "name": "table_handle", "type": "Arg" }
    ],
    "outputs": [
      { "name": "size", "type": "Res" }
    ]
  },
  {
    "name": "tf.LookupTableSizeV2",
    "summary": "Computes the number of elements in the given table.",
    "inputs": [
      { "name": "table_handle", "type": "Arg" }
    ],
    "outputs": [
      { "name": "size", "type": "Res" }
    ]
  },
  {
    "name": "tf.LowerBound",
    "summary": "Applies lower_bound(sorted_search_values, values) along each row.",
    "description": "Each set of rows with the same index in (sorted_inputs, values) is treated\nindependently.  The resulting row is the equivalent of calling\n`np.searchsorted(sorted_inputs, values, side='left')`.\n\nThe result is not a global index to the entire\n`Tensor`, but rather just the index in the last dimension.\n\nA 2-D example:\n  sorted_sequence = [[0, 3, 9, 9, 10],\n                     [1, 2, 3, 4, 5]]\n  values = [[2, 4, 9],\n            [0, 2, 6]]\n\n  result = LowerBound(sorted_sequence, values)\n\n  result == [[1, 2, 2],\n             [0, 1, 5]]",
    "inputs": [
      { "name": "sorted_inputs", "type": "Arg" },
      { "name": "values", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ]
  },
  {
    "name": "tf.LRN",
    "summary": "Local Response Normalization.",
    "description": "The 4-D `input` tensor is treated as a 3-D array of 1-D vectors (along the last\ndimension), and each vector is normalized independently.  Within a given vector,\neach component is divided by the weighted, squared sum of inputs within\n`depth_radius`.  In detail,\n\n    sqr_sum[a, b, c, d] =\n        sum(input[a, b, c, d - depth_radius : d + depth_radius + 1] ** 2)\n    output = input / (bias + alpha * sqr_sum) ** beta\n\nFor details, see [Krizhevsky et al., ImageNet classification with deep\nconvolutional neural networks (NIPS 2012)](http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks).",
    "inputs": [
      { "name": "input", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "TensorOf" }
    ],
    "attributes": [
      { "name": "depth_radius", "type": "DefaultValuedOptionalAttr" },
      { "name": "bias", "type": "DefaultValuedOptionalAttr" },
      { "name": "alpha", "type": "DefaultValuedOptionalAttr" },
      { "name": "beta", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.LRNGrad",
    "summary": "Gradients for Local Response Normalization.",
    "inputs": [
      { "name": "input_grads", "type": "Arg" },
      { "name": "input_image", "type": "Arg" },
      { "name": "output_image", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "depth_radius", "type": "DefaultValuedOptionalAttr" },
      { "name": "bias", "type": "DefaultValuedOptionalAttr" },
      { "name": "alpha", "type": "DefaultValuedOptionalAttr" },
      { "name": "beta", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.MakeIterator",
    "summary": "Makes a new iterator from the given `dataset` and stores it in `iterator`.",
    "description": "This operation may be executed multiple times. Each execution will reset the\niterator in `iterator` to the first element of `dataset`.",
    "inputs": [
      { "name": "dataset", "type": "TF_VariantTensor" },
      { "name": "iterator", "type": "Arg" }
    ]
  },
  {
    "name": "tf.MakeUnique",
    "summary": "Make all elements in the non-Batch dimension unique, but \\\"close\\\" to",
    "description": "their initial value. Never returns a sub-normal number. Never returns\nzero. The sign of each input element is always identical to the sign\nof the corresponding output element. Behavior for infinite elements is\nundefined. Behavior for subnormal elements is undefined.",
    "inputs": [
      { "name": "input", "type": "TF_Float32Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "TF_Float32Tensor" }
    ]
  },
  {
    "name": "tf.MapAndBatchDataset",
    "summary": "Creates a dataset that fuses mapping with batching.",
    "description": "Creates a dataset that applies `f` to the outputs of `input_dataset` and then\nbatches `batch_size` of them.\n\nUnlike a \"MapDataset\", which applies `f` sequentially, this dataset invokes up\nto `batch_size * num_parallel_batches` copies of `f` in parallel.",
    "inputs": [
      { "name": "input_dataset", "type": "Arg" },
      { "name": "other_arguments", "type": "Arg" },
      { "name": "batch_size", "type": "Arg" },
      { "name": "num_parallel_calls", "type": "Arg" },
      { "name": "drop_remainder", "type": "Arg" }
    ],
    "outputs": [
      { "name": "handle", "type": "TF_VariantTensor" }
    ],
    "attributes": [
      { "name": "f", "type": "SymbolRefAttr" },
      { "name": "output_types", "type": "ConfinedAttr" },
      { "name": "output_shapes", "type": "ConfinedAttr" },
      { "name": "preserve_cardinality", "type": "DefaultValuedOptionalAttr" },
      { "name": "metadata", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.MapDataset",
    "summary": "Creates a dataset that applies `f` to the outputs of `input_dataset`.",
    "inputs": [
      { "name": "input_dataset", "type": "TF_VariantTensor" },
      { "name": "other_arguments", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "handle", "type": "TF_VariantTensor" }
    ],
    "attributes": [
      { "name": "f", "type": "SymbolRefAttr" },
      { "name": "output_types", "type": "ConfinedAttr" },
      { "name": "output_shapes", "type": "ConfinedAttr" },
      { "name": "use_inter_op_parallelism", "type": "DefaultValuedOptionalAttr" },
      { "name": "preserve_cardinality", "type": "DefaultValuedOptionalAttr" },
      { "name": "force_synchronous", "type": "DefaultValuedOptionalAttr" },
      { "name": "metadata", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.MatMul",
    "summary": "Multiply the matrix \"a\" by the matrix \"b\".",
    "description": "The inputs must be two-dimensional matrices and the inner dimension of\n\"a\" (after being transposed if transpose_a is true) must match the\nouter dimension of \"b\" (after being transposed if transposed_b is\ntrue).\n\n*Note*: The default kernel implementation for MatMul on GPUs uses\ncublas.",
    "inputs": [
      { "name": "a", "type": "TensorOf" },
      { "name": "b", "type": "TensorOf" }
    ],
    "outputs": [
      { "name": "product", "type": "TensorOf" }
    ],
    "attributes": [
      { "name": "transpose_a", "type": "DefaultValuedOptionalAttr" },
      { "name": "transpose_b", "type": "DefaultValuedOptionalAttr" },
      { "name": "grad_a", "type": "DefaultValuedOptionalAttr" },
      { "name": "grad_b", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.MatrixBandPart",
    "summary": "Copy a tensor setting everything outside a central band in each innermost matrix to zero.",
    "description": "The `band` part is computed as follows:\nAssume `input` has `k` dimensions `[I, J, K, ..., M, N]`, then the output is a\ntensor with the same shape where\n\n`band[i, j, k, ..., m, n] = in_band(m, n) * input[i, j, k, ..., m, n]`.\n\nThe indicator function\n\n`in_band(m, n) = (num_lower < 0 || (m-n) <= num_lower)) &&\n                 (num_upper < 0 || (n-m) <= num_upper)`.\n\nFor example:\n\n```\n# if 'input' is [[ 0,  1,  2, 3]\n#                [-1,  0,  1, 2]\n#                [-2, -1,  0, 1]\n#                [-3, -2, -1, 0]],\n\ntf.linalg.band_part(input, 1, -1) ==> [[ 0,  1,  2, 3]\n                                       [-1,  0,  1, 2]\n                                       [ 0, -1,  0, 1]\n                                       [ 0,  0, -1, 0]],\n\ntf.linalg.band_part(input, 2, 1) ==> [[ 0,  1,  0, 0]\n                                      [-1,  0,  1, 0]\n                                      [-2, -1,  0, 1]\n                                      [ 0, -2, -1, 0]]\n```\n\nUseful special cases:\n\n```\n tf.linalg.band_part(input, 0, -1) ==> Upper triangular part.\n tf.linalg.band_part(input, -1, 0) ==> Lower triangular part.\n tf.linalg.band_part(input, 0, 0) ==> Diagonal.\n```",
    "inputs": [
      { "name": "input", "type": "Arg" },
      { "name": "num_lower", "type": "Arg" },
      { "name": "num_upper", "type": "Arg" }
    ],
    "outputs": [
      { "name": "band", "type": "Res" }
    ]
  },
  {
    "name": "tf.MatrixDiag",
    "summary": "Returns a batched diagonal tensor with a given batched diagonal values.",
    "description": "Given a `diagonal`, this operation returns a tensor with the `diagonal` and\neverything else padded with zeros. The diagonal is computed as follows:\n\nAssume `diagonal` has `k` dimensions `[I, J, K, ..., N]`, then the output is a\ntensor of rank `k+1` with dimensions [I, J, K, ..., N, N]` where:\n\n`output[i, j, k, ..., m, n] = 1{m=n} * diagonal[i, j, k, ..., n]`.\n\nFor example:\n\n```\n# 'diagonal' is [[1, 2, 3, 4], [5, 6, 7, 8]]\n\nand diagonal.shape = (2, 4)\n\ntf.matrix_diag(diagonal) ==> [[[1, 0, 0, 0]\n                                     [0, 2, 0, 0]\n                                     [0, 0, 3, 0]\n                                     [0, 0, 0, 4]],\n                                    [[5, 0, 0, 0]\n                                     [0, 6, 0, 0]\n                                     [0, 0, 7, 0]\n                                     [0, 0, 0, 8]]]\n\nwhich has shape (2, 4, 4)\n```",
    "inputs": [
      { "name": "diagonal", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ]
  },
  {
    "name": "tf.MatrixDiagPartV3",
    "summary": "Returns the batched diagonal part of a batched tensor.",
    "description": "Returns a tensor with the `k[0]`-th to `k[1]`-th diagonals of the batched\n`input`.\n\nAssume `input` has `r` dimensions `[I, J, ..., L, M, N]`.\nLet `max_diag_len` be the maximum length among all diagonals to be extracted,\n`max_diag_len = min(M + min(k[1], 0), N + min(-k[0], 0))`\nLet `num_diags` be the number of diagonals to extract,\n`num_diags = k[1] - k[0] + 1`.\n\nIf `num_diags == 1`, the output tensor is of rank `r - 1` with shape\n`[I, J, ..., L, max_diag_len]` and values:\n\n```\ndiagonal[i, j, ..., l, n]\n  = input[i, j, ..., l, n+y, n+x] ; if 0 <= n+y < M and 0 <= n+x < N,\n    padding_value                 ; otherwise.\n```\nwhere `y = max(-k[1], 0)`, `x = max(k[1], 0)`.\n\nOtherwise, the output tensor has rank `r` with dimensions\n`[I, J, ..., L, num_diags, max_diag_len]` with values:\n\n```\ndiagonal[i, j, ..., l, m, n]\n  = input[i, j, ..., l, n+y, n+x] ; if 0 <= n+y < M and 0 <= n+x < N,\n    padding_value                 ; otherwise.\n```\nwhere `d = k[1] - m`, `y = max(-d, 0) - offset`, and `x = max(d, 0) - offset`.\n\n`offset` is zero except when the alignment of the diagonal is to the right.\n```\noffset = max_diag_len - diag_len(d) ; if (`align` in {RIGHT_LEFT, RIGHT_RIGHT}\n                                           and `d >= 0`) or\n                                         (`align` in {LEFT_RIGHT, RIGHT_RIGHT}\n                                           and `d <= 0`)\n         0                          ; otherwise\n```\nwhere `diag_len(d) = min(cols - max(d, 0), rows + min(d, 0))`.\n\nThe input must be at least a matrix.\n\nFor example:\n\n```\ninput = np.array([[[1, 2, 3, 4],  # Input shape: (2, 3, 4)\n                   [5, 6, 7, 8],\n                   [9, 8, 7, 6]],\n                  [[5, 4, 3, 2],\n                   [1, 2, 3, 4],\n                   [5, 6, 7, 8]]])\n\n# A main diagonal from each batch.\ntf.matrix_diag_part(input) ==> [[1, 6, 7],  # Output shape: (2, 3)\n                                [5, 2, 7]]\n\n# A superdiagonal from each batch.\ntf.matrix_diag_part(input, k = 1)\n  ==> [[2, 7, 6],  # Output shape: (2, 3)\n       [4, 3, 8]]\n\n# A band from each batch.\ntf.matrix_diag_part(input, k = (-1, 2))\n  ==> [[[0, 3, 8],  # Output shape: (2, 4, 3)\n        [2, 7, 6],\n        [1, 6, 7],\n        [5, 8, 0]],\n       [[0, 3, 4],\n        [4, 3, 8],\n        [5, 2, 7],\n        [1, 6, 0]]]\n\n# LEFT_RIGHT alignment.\ntf.matrix_diag_part(input, k = (-1, 2), align=\"LEFT_RIGHT\")\n  ==> [[[3, 8, 0],  # Output shape: (2, 4, 3)\n        [2, 7, 6],\n        [1, 6, 7],\n        [0, 5, 8]],\n       [[3, 4, 0],\n        [4, 3, 8],\n        [5, 2, 7],\n        [0, 1, 6]]]\n\n# max_diag_len can be shorter than the main diagonal.\ntf.matrix_diag_part(input, k = (-2, -1))\n  ==> [[[5, 8],\n        [9, 0]],\n       [[1, 6],\n        [5, 0]]]\n\n# padding_value = 9\ntf.matrix_diag_part(input, k = (1, 3), padding_value = 9)\n  ==> [[[9, 9, 4],  # Output shape: (2, 3, 3)\n        [9, 3, 8],\n        [2, 7, 6]],\n       [[9, 9, 2],\n        [9, 3, 4],\n        [4, 3, 8]]]\n\n```",
    "inputs": [
      { "name": "input", "type": "Arg" },
      { "name": "k", "type": "Arg" },
      { "name": "padding_value", "type": "Arg" }
    ],
    "outputs": [
      { "name": "diagonal", "type": "Res" }
    ],
    "attributes": [
      { "name": "align", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.MatrixDiagV2",
    "summary": "Returns a batched diagonal tensor with given batched diagonal values.",
    "description": "Returns a tensor with the contents in `diagonal` as `k[0]`-th to `k[1]`-th\ndiagonals of a matrix, with everything else padded with `padding`. `num_rows`\nand `num_cols` specify the dimension of the innermost matrix of the output. If\nboth are not specified, the op assumes the innermost matrix is square and infers\nits size from `k` and the innermost dimension of `diagonal`. If only one of them\nis specified, the op assumes the unspecified value is the smallest possible\nbased on other criteria.\n\nLet `diagonal` have `r` dimensions `[I, J, ..., L, M, N]`. The output tensor has\nrank `r+1` with shape `[I, J, ..., L, M, num_rows, num_cols]` when only one\ndiagonal is given (`k` is an integer or `k[0] == k[1]`). Otherwise, it has rank\n`r` with shape `[I, J, ..., L, num_rows, num_cols]`.\n\nThe second innermost dimension of `diagonal` has double meaning.\nWhen `k` is scalar or `k[0] == k[1]`, `M` is part of the batch size\n[I, J, ..., M], and the output tensor is:\n\n```\noutput[i, j, ..., l, m, n]\n  = diagonal[i, j, ..., l, n-max(d_upper, 0)] ; if n - m == d_upper\n    padding_value                             ; otherwise\n```\n\nOtherwise, `M` is treated as the number of diagonals for the matrix in the\nsame batch (`M = k[1]-k[0]+1`), and the output tensor is:\n\n```\noutput[i, j, ..., l, m, n]\n  = diagonal[i, j, ..., l, diag_index, index_in_diag] ; if k[0] <= d <= k[1]\n    padding_value                                     ; otherwise\n```\nwhere `d = n - m`, `diag_index = k[1] - d`, and `index_in_diag = n - max(d, 0)`.\n\nFor example:\n\n```\n# The main diagonal.\ndiagonal = np.array([[1, 2, 3, 4],            # Input shape: (2, 4)\n                     [5, 6, 7, 8]])\ntf.matrix_diag(diagonal) ==> [[[1, 0, 0, 0],  # Output shape: (2, 4, 4)\n                               [0, 2, 0, 0],\n                               [0, 0, 3, 0],\n                               [0, 0, 0, 4]],\n                              [[5, 0, 0, 0],\n                               [0, 6, 0, 0],\n                               [0, 0, 7, 0],\n                               [0, 0, 0, 8]]]\n\n# A superdiagonal (per batch).\ndiagonal = np.array([[1, 2, 3],  # Input shape: (2, 3)\n                     [4, 5, 6]])\ntf.matrix_diag(diagonal, k = 1)\n  ==> [[[0, 1, 0, 0],  # Output shape: (2, 4, 4)\n        [0, 0, 2, 0],\n        [0, 0, 0, 3],\n        [0, 0, 0, 0]],\n       [[0, 4, 0, 0],\n        [0, 0, 5, 0],\n        [0, 0, 0, 6],\n        [0, 0, 0, 0]]]\n\n# A band of diagonals.\ndiagonals = np.array([[[1, 2, 3],  # Input shape: (2, 2, 3)\n                       [4, 5, 0]],\n                      [[6, 7, 9],\n                       [9, 1, 0]]])\ntf.matrix_diag(diagonals, k = (-1, 0))\n  ==> [[[1, 0, 0],  # Output shape: (2, 3, 3)\n        [4, 2, 0],\n        [0, 5, 3]],\n       [[6, 0, 0],\n        [9, 7, 0],\n        [0, 1, 9]]]\n\n# Rectangular matrix.\ndiagonal = np.array([1, 2])  # Input shape: (2)\ntf.matrix_diag(diagonal, k = -1, num_rows = 3, num_cols = 4)\n  ==> [[0, 0, 0, 0],  # Output shape: (3, 4)\n       [1, 0, 0, 0],\n       [0, 2, 0, 0]]\n\n# Rectangular matrix with inferred num_cols and padding_value = 9.\ntf.matrix_diag(diagonal, k = -1, num_rows = 3, padding_value = 9)\n  ==> [[9, 9],  # Output shape: (3, 2)\n       [1, 9],\n       [9, 2]]\n```",
    "inputs": [
      { "name": "diagonal", "type": "Arg" },
      { "name": "k", "type": "Arg" },
      { "name": "num_rows", "type": "Arg" },
      { "name": "num_cols", "type": "Arg" },
      { "name": "padding_value", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ]
  },
  {
    "name": "tf.MatrixDiagV3",
    "summary": "Returns a batched diagonal tensor with given batched diagonal values.",
    "description": "Returns a tensor with the contents in `diagonal` as `k[0]`-th to `k[1]`-th\ndiagonals of a matrix, with everything else padded with `padding`. `num_rows`\nand `num_cols` specify the dimension of the innermost matrix of the output. If\nboth are not specified, the op assumes the innermost matrix is square and infers\nits size from `k` and the innermost dimension of `diagonal`. If only one of them\nis specified, the op assumes the unspecified value is the smallest possible\nbased on other criteria.\n\nLet `diagonal` have `r` dimensions `[I, J, ..., L, M, N]`. The output tensor has\nrank `r+1` with shape `[I, J, ..., L, M, num_rows, num_cols]` when only one\ndiagonal is given (`k` is an integer or `k[0] == k[1]`). Otherwise, it has rank\n`r` with shape `[I, J, ..., L, num_rows, num_cols]`.\n\nThe second innermost dimension of `diagonal` has double meaning.\nWhen `k` is scalar or `k[0] == k[1]`, `M` is part of the batch size\n[I, J, ..., M], and the output tensor is:\n\n```\noutput[i, j, ..., l, m, n]\n  = diagonal[i, j, ..., l, n-max(d_upper, 0)] ; if n - m == d_upper\n    padding_value                             ; otherwise\n```\n\nOtherwise, `M` is treated as the number of diagonals for the matrix in the\nsame batch (`M = k[1]-k[0]+1`), and the output tensor is:\n\n```\noutput[i, j, ..., l, m, n]\n  = diagonal[i, j, ..., l, diag_index, index_in_diag] ; if k[0] <= d <= k[1]\n    padding_value                                     ; otherwise\n```\nwhere `d = n - m`, `diag_index = [k] - d`, and\n`index_in_diag = n - max(d, 0) + offset`.\n\n`offset` is zero except when the alignment of the diagonal is to the right.\n```\noffset = max_diag_len - diag_len(d) ; if (`align` in {RIGHT_LEFT, RIGHT_RIGHT}\n                                           and `d >= 0`) or\n                                         (`align` in {LEFT_RIGHT, RIGHT_RIGHT}\n                                           and `d <= 0`)\n         0                          ; otherwise\n```\nwhere `diag_len(d) = min(cols - max(d, 0), rows + min(d, 0))`.\n\nFor example:\n\n```\n# The main diagonal.\ndiagonal = np.array([[1, 2, 3, 4],            # Input shape: (2, 4)\n                     [5, 6, 7, 8]])\ntf.matrix_diag(diagonal) ==> [[[1, 0, 0, 0],  # Output shape: (2, 4, 4)\n                               [0, 2, 0, 0],\n                               [0, 0, 3, 0],\n                               [0, 0, 0, 4]],\n                              [[5, 0, 0, 0],\n                               [0, 6, 0, 0],\n                               [0, 0, 7, 0],\n                               [0, 0, 0, 8]]]\n\n# A superdiagonal (per batch).\ndiagonal = np.array([[1, 2, 3],  # Input shape: (2, 3)\n                     [4, 5, 6]])\ntf.matrix_diag(diagonal, k = 1)\n  ==> [[[0, 1, 0, 0],  # Output shape: (2, 4, 4)\n        [0, 0, 2, 0],\n        [0, 0, 0, 3],\n        [0, 0, 0, 0]],\n       [[0, 4, 0, 0],\n        [0, 0, 5, 0],\n        [0, 0, 0, 6],\n        [0, 0, 0, 0]]]\n\n# A tridiagonal band (per batch).\ndiagonals = np.array([[[0, 8, 9],  # Input shape: (2, 2, 3)\n                       [1, 2, 3],\n                       [4, 5, 0]],\n                      [[0, 2, 3],\n                       [6, 7, 9],\n                       [9, 1, 0]]])\ntf.matrix_diag(diagonals, k = (-1, 1))\n  ==> [[[1, 8, 0],  # Output shape: (2, 3, 3)\n        [4, 2, 9],\n        [0, 5, 3]],\n       [[6, 2, 0],\n        [9, 7, 3],\n        [0, 1, 9]]]\n\n# LEFT_RIGHT alignment.\ndiagonals = np.array([[[8, 9, 0],  # Input shape: (2, 2, 3)\n                       [1, 2, 3],\n                       [0, 4, 5]],\n                      [[2, 3, 0],\n                       [6, 7, 9],\n                       [0, 9, 1]]])\ntf.matrix_diag(diagonals, k = (-1, 1), align=\"LEFT_RIGHT\")\n  ==> [[[1, 8, 0],  # Output shape: (2, 3, 3)\n        [4, 2, 9],\n        [0, 5, 3]],\n       [[6, 2, 0],\n        [9, 7, 3],\n        [0, 1, 9]]]\n\n# Rectangular matrix.\ndiagonal = np.array([1, 2])  # Input shape: (2)\ntf.matrix_diag(diagonal, k = -1, num_rows = 3, num_cols = 4)\n  ==> [[0, 0, 0, 0],  # Output shape: (3, 4)\n       [1, 0, 0, 0],\n       [0, 2, 0, 0]]\n\n# Rectangular matrix with inferred num_cols and padding_value = 9.\ntf.matrix_diag(diagonal, k = -1, num_rows = 3, padding_value = 9)\n  ==> [[9, 9],  # Output shape: (3, 2)\n       [1, 9],\n       [9, 2]]\n\n```",
    "inputs": [
      { "name": "diagonal", "type": "Arg" },
      { "name": "k", "type": "Arg" },
      { "name": "num_rows", "type": "Arg" },
      { "name": "num_cols", "type": "Arg" },
      { "name": "padding_value", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "align", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.MatrixInverse",
    "summary": "Computes the inverse of one or more square invertible matrices or their adjoints (conjugate transposes).",
    "description": "The input is a tensor of shape `[..., M, M]` whose inner-most 2 dimensions\nform square matrices. The output is a tensor of the same shape as the input\ncontaining the inverse for all input submatrices `[..., :, :]`.\n\nThe op uses LU decomposition with partial pivoting to compute the inverses.\n\nIf a matrix is not invertible there is no guarantee what the op does. It\nmay detect the condition and raise an exception or it may simply return a\ngarbage result.",
    "inputs": [
      { "name": "input", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "adjoint", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.MatrixSetDiag",
    "summary": "Returns a batched matrix tensor with new batched diagonal values.",
    "description": "Given `input` and `diagonal`, this operation returns a tensor with the\nsame shape and values as `input`, except for the main diagonal of the\ninnermost matrices.  These will be overwritten by the values in `diagonal`.\n\nThe output is computed as follows:\n\nAssume `input` has `k+1` dimensions `[I, J, K, ..., M, N]` and `diagonal` has\n`k` dimensions `[I, J, K, ..., min(M, N)]`.  Then the output is a\ntensor of rank `k+1` with dimensions `[I, J, K, ..., M, N]` where:\n\n  * `output[i, j, k, ..., m, n] = diagonal[i, j, k, ..., n]` for `m == n`.\n  * `output[i, j, k, ..., m, n] = input[i, j, k, ..., m, n]` for `m != n`.",
    "inputs": [
      { "name": "input", "type": "Arg" },
      { "name": "diagonal", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ]
  },
  {
    "name": "tf.MatrixSetDiagV2",
    "summary": "Returns a batched matrix tensor with new batched diagonal values.",
    "description": "Given `input` and `diagonal`, this operation returns a tensor with the\nsame shape and values as `input`, except for the specified diagonals of the\ninnermost matrices. These will be overwritten by the values in `diagonal`.\n\n`input` has `r+1` dimensions `[I, J, ..., L, M, N]`. When `k` is scalar or\n`k[0] == k[1]`, `diagonal` has `r` dimensions `[I, J, ..., L, max_diag_len]`.\nOtherwise, it has `r+1` dimensions `[I, J, ..., L, num_diags, max_diag_len]`.\n`num_diags` is the number of diagonals, `num_diags = k[1] - k[0] + 1`.\n`max_diag_len` is the longest diagonal in the range `[k[0], k[1]]`,\n`max_diag_len = min(M + min(k[1], 0), N + min(-k[0], 0))`\n\nThe output is a tensor of rank `k+1` with dimensions `[I, J, ..., L, M, N]`.\nIf `k` is scalar or `k[0] == k[1]`:\n\n```\noutput[i, j, ..., l, m, n]\n  = diagonal[i, j, ..., l, n-max(k[1], 0)] ; if n - m == k[1]\n    input[i, j, ..., l, m, n]              ; otherwise\n```\n\nOtherwise,\n\n```\noutput[i, j, ..., l, m, n]\n  = diagonal[i, j, ..., l, diag_index, index_in_diag] ; if k[0] <= d <= k[1]\n    input[i, j, ..., l, m, n]                         ; otherwise\n```\nwhere `d = n - m`, `diag_index = k[1] - d`, and `index_in_diag = n - max(d, 0)`.\n\nFor example:\n\n```\n# The main diagonal.\ninput = np.array([[[7, 7, 7, 7],              # Input shape: (2, 3, 4)\n                   [7, 7, 7, 7],\n                   [7, 7, 7, 7]],\n                  [[7, 7, 7, 7],\n                   [7, 7, 7, 7],\n                   [7, 7, 7, 7]]])\ndiagonal = np.array([[1, 2, 3],               # Diagonal shape: (2, 3)\n                     [4, 5, 6]])\ntf.matrix_set_diag(diagonal) ==> [[[1, 7, 7, 7],  # Output shape: (2, 3, 4)\n                                   [7, 2, 7, 7],\n                                   [7, 7, 3, 7]],\n                                  [[4, 7, 7, 7],\n                                   [7, 5, 7, 7],\n                                   [7, 7, 6, 7]]]\n\n# A superdiagonal (per batch).\ntf.matrix_set_diag(diagonal, k = 1)\n  ==> [[[7, 1, 7, 7],  # Output shape: (2, 3, 4)\n        [7, 7, 2, 7],\n        [7, 7, 7, 3]],\n       [[7, 4, 7, 7],\n        [7, 7, 5, 7],\n        [7, 7, 7, 6]]]\n\n# A band of diagonals.\ndiagonals = np.array([[[1, 2, 3],  # Diagonal shape: (2, 2, 3)\n                       [4, 5, 0]],\n                      [[6, 1, 2],\n                       [3, 4, 0]]])\ntf.matrix_set_diag(diagonals, k = (-1, 0))\n  ==> [[[1, 7, 7, 7],  # Output shape: (2, 3, 4)\n        [4, 2, 7, 7],\n        [0, 5, 3, 7]],\n       [[6, 7, 7, 7],\n        [3, 1, 7, 7],\n        [7, 4, 2, 7]]]\n\n```",
    "inputs": [
      { "name": "input", "type": "Arg" },
      { "name": "diagonal", "type": "Arg" },
      { "name": "k", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ]
  },
  {
    "name": "tf.MatrixSetDiagV3",
    "summary": "Returns a batched matrix tensor with new batched diagonal values.",
    "description": "Given `input` and `diagonal`, this operation returns a tensor with the\nsame shape and values as `input`, except for the specified diagonals of the\ninnermost matrices. These will be overwritten by the values in `diagonal`.\n\n`input` has `r+1` dimensions `[I, J, ..., L, M, N]`. When `k` is scalar or\n`k[0] == k[1]`, `diagonal` has `r` dimensions `[I, J, ..., L, max_diag_len]`.\nOtherwise, it has `r+1` dimensions `[I, J, ..., L, num_diags, max_diag_len]`.\n`num_diags` is the number of diagonals, `num_diags = k[1] - k[0] + 1`.\n`max_diag_len` is the longest diagonal in the range `[k[0], k[1]]`,\n`max_diag_len = min(M + min(k[1], 0), N + min(-k[0], 0))`\n\nThe output is a tensor of rank `k+1` with dimensions `[I, J, ..., L, M, N]`.\nIf `k` is scalar or `k[0] == k[1]`:\n\n```\noutput[i, j, ..., l, m, n]\n  = diagonal[i, j, ..., l, n-max(k[1], 0)] ; if n - m == k[1]\n    input[i, j, ..., l, m, n]              ; otherwise\n```\n\nOtherwise,\n\n```\noutput[i, j, ..., l, m, n]\n  = diagonal[i, j, ..., l, diag_index, index_in_diag] ; if k[0] <= d <= k[1]\n    input[i, j, ..., l, m, n]                         ; otherwise\n```\nwhere `d = n - m`, `diag_index = k[1] - d`, and\n`index_in_diag = n - max(d, 0) + offset`.\n\n`offset` is zero except when the alignment of the diagonal is to the right.\n```\noffset = max_diag_len - diag_len(d) ; if (`align` in {RIGHT_LEFT, RIGHT_RIGHT}\n                                           and `d >= 0`) or\n                                         (`align` in {LEFT_RIGHT, RIGHT_RIGHT}\n                                           and `d <= 0`)\n         0                          ; otherwise\n```\nwhere `diag_len(d) = min(cols - max(d, 0), rows + min(d, 0))`.\n\nFor example:\n\n```\n# The main diagonal.\ninput = np.array([[[7, 7, 7, 7],              # Input shape: (2, 3, 4)\n                   [7, 7, 7, 7],\n                   [7, 7, 7, 7]],\n                  [[7, 7, 7, 7],\n                   [7, 7, 7, 7],\n                   [7, 7, 7, 7]]])\ndiagonal = np.array([[1, 2, 3],               # Diagonal shape: (2, 3)\n                     [4, 5, 6]])\ntf.matrix_set_diag(input, diagonal)\n  ==> [[[1, 7, 7, 7],  # Output shape: (2, 3, 4)\n        [7, 2, 7, 7],\n        [7, 7, 3, 7]],\n       [[4, 7, 7, 7],\n        [7, 5, 7, 7],\n        [7, 7, 6, 7]]]\n\n# A superdiagonal (per batch).\ntf.matrix_set_diag(input, diagonal, k = 1)\n  ==> [[[7, 1, 7, 7],  # Output shape: (2, 3, 4)\n        [7, 7, 2, 7],\n        [7, 7, 7, 3]],\n       [[7, 4, 7, 7],\n        [7, 7, 5, 7],\n        [7, 7, 7, 6]]]\n\n# A band of diagonals.\ndiagonals = np.array([[[0, 9, 1],  # Diagonal shape: (2, 4, 3)\n                       [6, 5, 8],\n                       [1, 2, 3],\n                       [4, 5, 0]],\n                      [[0, 1, 2],\n                       [5, 6, 4],\n                       [6, 1, 2],\n                       [3, 4, 0]]])\ntf.matrix_set_diag(input, diagonals, k = (-1, 2))\n  ==> [[[1, 6, 9, 7],  # Output shape: (2, 3, 4)\n        [4, 2, 5, 1],\n        [7, 5, 3, 8]],\n       [[6, 5, 1, 7],\n        [3, 1, 6, 2],\n        [7, 4, 2, 4]]]\n\n# LEFT_RIGHT alignment.\ndiagonals = np.array([[[9, 1, 0],  # Diagonal shape: (2, 4, 3)\n                       [6, 5, 8],\n                       [1, 2, 3],\n                       [0, 4, 5]],\n                      [[1, 2, 0],\n                       [5, 6, 4],\n                       [6, 1, 2],\n                       [0, 3, 4]]])\ntf.matrix_set_diag(input, diagonals, k = (-1, 2), align=\"LEFT_RIGHT\")\n  ==> [[[1, 6, 9, 7],  # Output shape: (2, 3, 4)\n        [4, 2, 5, 1],\n        [7, 5, 3, 8]],\n       [[6, 5, 1, 7],\n        [3, 1, 6, 2],\n        [7, 4, 2, 4]]]\n\n```",
    "inputs": [
      { "name": "input", "type": "Arg" },
      { "name": "diagonal", "type": "Arg" },
      { "name": "k", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "align", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.MatrixSolve",
    "summary": "Solves systems of linear equations.",
    "description": "`Matrix` is a tensor of shape `[..., M, M]` whose inner-most 2 dimensions\nform square matrices. `Rhs` is a tensor of shape `[..., M, K]`. The `output` is\na tensor shape `[..., M, K]`.  If `adjoint` is `False` then each output matrix\nsatisfies `matrix[..., :, :] * output[..., :, :] = rhs[..., :, :]`.\nIf `adjoint` is `True` then each output matrix satisfies\n`adjoint(matrix[..., :, :]) * output[..., :, :] = rhs[..., :, :]`.",
    "inputs": [
      { "name": "matrix", "type": "Arg" },
      { "name": "rhs", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "adjoint", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.MatrixTriangularSolve",
    "summary": "Solves systems of linear equations with upper or lower triangular matrices by backsubstitution.",
    "description": "`matrix` is a tensor of shape `[..., M, M]` whose inner-most 2 dimensions form\nsquare matrices. If `lower` is `True` then the strictly upper triangular part\nof each inner-most matrix is assumed to be zero and not accessed.\nIf `lower` is False then the strictly lower triangular part of each inner-most\nmatrix is assumed to be zero and not accessed.\n`rhs` is a tensor of shape `[..., M, N]`.\n\nThe output is a tensor of shape `[..., M, N]`. If `adjoint` is\n`True` then the innermost matrices in `output` satisfy matrix equations\n`matrix[..., :, :] * output[..., :, :] = rhs[..., :, :]`.\nIf `adjoint` is `False` then the strictly then the  innermost matrices in\n`output` satisfy matrix equations\n`adjoint(matrix[..., i, k]) * output[..., k, j] = rhs[..., i, j]`.\n\nNote, the batch shapes for the inputs only need to broadcast.\n\nExample:\n```python\n\na = tf.constant([[3,  0,  0,  0],\n                 [2,  1,  0,  0],\n                 [1,  0,  1,  0],\n                 [1,  1,  1,  1]], dtype=tf.float32)\n\nb = tf.constant([[4],\n                 [2],\n                 [4],\n                 [2]], dtype=tf.float32)\n\nx = tf.linalg.triangular_solve(a, b, lower=True)\nx\n# <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\n# array([[ 1.3333334 ],\n#        [-0.66666675],\n#        [ 2.6666665 ],\n#        [-1.3333331 ]], dtype=float32)>\n\n# in python3 one can use `a@x`\ntf.matmul(a, x)\n# <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\n# array([[4.       ],\n#        [2.       ],\n#        [4.       ],\n#        [1.9999999]], dtype=float32)>\n```",
    "inputs": [
      { "name": "matrix", "type": "Arg" },
      { "name": "rhs", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "lower", "type": "DefaultValuedOptionalAttr" },
      { "name": "adjoint", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.Max",
    "summary": "Computes the maximum of elements across dimensions of a tensor.",
    "description": "Reduces `input` along the dimensions given in `axis`. Unless\n`keep_dims` is true, the rank of the tensor is reduced by 1 for each entry in\n`axis`. If `keep_dims` is true, the reduced dimensions are\nretained with length 1.",
    "inputs": [
      { "name": "input", "type": "Arg" },
      { "name": "reduction_indices", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "keep_dims", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.Maximum",
    "summary": "Returns the max of x and y (i.e. x > y ? x : y) element-wise.",
    "description": "*NOTE*: `Maximum` supports broadcasting. More about broadcasting\n[here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)",
    "inputs": [
      { "name": "x", "type": "TF_IntOrFpTensor" },
      { "name": "y", "type": "TF_IntOrFpTensor" }
    ],
    "outputs": [
      { "name": "z", "type": "TF_IntOrFpTensor" }
    ]
  },
  {
    "name": "tf.MaxPool",
    "summary": "Performs max pooling on the input.",
    "inputs": [
      { "name": "input", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "ksize", "type": "ConfinedAttr" },
      { "name": "strides", "type": "ConfinedAttr" },
      { "name": "padding", "type": "TF_AnyStrAttrOf" },
      { "name": "explicit_paddings", "type": "DefaultValuedOptionalAttr" },
      { "name": "data_format", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.MaxPool3D",
    "summary": "Performs 3D max pooling on the input.",
    "inputs": [
      { "name": "input", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "ksize", "type": "ConfinedAttr" },
      { "name": "strides", "type": "ConfinedAttr" },
      { "name": "padding", "type": "TF_AnyStrAttrOf" },
      { "name": "data_format", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.MaxPool3DGrad",
    "summary": "Computes gradients of 3D max pooling function.",
    "inputs": [
      { "name": "orig_input", "type": "Arg" },
      { "name": "orig_output", "type": "Arg" },
      { "name": "grad", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "TensorOf" }
    ],
    "attributes": [
      { "name": "ksize", "type": "ConfinedAttr" },
      { "name": "strides", "type": "ConfinedAttr" },
      { "name": "padding", "type": "TF_AnyStrAttrOf" },
      { "name": "data_format", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.MaxPool3DGradGrad",
    "summary": "Computes second-order gradients of the maxpooling function.",
    "inputs": [
      { "name": "orig_input", "type": "Arg" },
      { "name": "orig_output", "type": "Arg" },
      { "name": "grad", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "ksize", "type": "ConfinedAttr" },
      { "name": "strides", "type": "ConfinedAttr" },
      { "name": "padding", "type": "TF_AnyStrAttrOf" },
      { "name": "data_format", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.MaxPoolGrad",
    "summary": "Computes gradients of the maxpooling function.",
    "inputs": [
      { "name": "orig_input", "type": "Arg" },
      { "name": "orig_output", "type": "Arg" },
      { "name": "grad", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "ksize", "type": "ConfinedAttr" },
      { "name": "strides", "type": "ConfinedAttr" },
      { "name": "padding", "type": "TF_AnyStrAttrOf" },
      { "name": "explicit_paddings", "type": "DefaultValuedOptionalAttr" },
      { "name": "data_format", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.MaxPoolGradGrad",
    "summary": "Computes second-order gradients of the maxpooling function.",
    "inputs": [
      { "name": "orig_input", "type": "Arg" },
      { "name": "orig_output", "type": "Arg" },
      { "name": "grad", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "ksize", "type": "ConfinedAttr" },
      { "name": "strides", "type": "ConfinedAttr" },
      { "name": "padding", "type": "TF_AnyStrAttrOf" },
      { "name": "data_format", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.MaxPoolGradGradV2",
    "summary": "Computes second-order gradients of the maxpooling function.",
    "inputs": [
      { "name": "orig_input", "type": "Arg" },
      { "name": "orig_output", "type": "Arg" },
      { "name": "grad", "type": "Arg" },
      { "name": "ksize", "type": "Arg" },
      { "name": "strides", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "padding", "type": "TF_AnyStrAttrOf" },
      { "name": "data_format", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.MaxPoolGradV2",
    "summary": "Computes gradients of the maxpooling function.",
    "inputs": [
      { "name": "orig_input", "type": "Arg" },
      { "name": "orig_output", "type": "Arg" },
      { "name": "grad", "type": "Arg" },
      { "name": "ksize", "type": "Arg" },
      { "name": "strides", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "padding", "type": "TF_AnyStrAttrOf" },
      { "name": "data_format", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.MaxPoolV2",
    "summary": "Performs max pooling on the input.",
    "inputs": [
      { "name": "input", "type": "Arg" },
      { "name": "ksize", "type": "Arg" },
      { "name": "strides", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "padding", "type": "TF_AnyStrAttrOf" },
      { "name": "data_format", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.Mean",
    "summary": "Computes the mean of elements across dimensions of a tensor.",
    "description": "Reduces `input` along the dimensions given in `axis`. Unless\n`keep_dims` is true, the rank of the tensor is reduced by 1 for each entry in\n`axis`. If `keep_dims` is true, the reduced dimensions are\nretained with length 1.",
    "inputs": [
      { "name": "input", "type": "Arg" },
      { "name": "reduction_indices", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "keep_dims", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.MergeSummary",
    "summary": "Merges summaries.",
    "description": "This op creates a\n[`Summary`](https://www.tensorflow.org/code/tensorflow/core/framework/summary.proto)\nprotocol buffer that contains the union of all the values in the input\nsummaries.\n\nWhen the Op is run, it reports an `InvalidArgument` error if multiple values\nin the summaries to merge use the same tag.",
    "inputs": [
      { "name": "inputs", "type": "Arg" }
    ],
    "outputs": [
      { "name": "summary", "type": "Res" }
    ]
  },
  {
    "name": "tf.MergeV2Checkpoints",
    "summary": "V2 format specific: merges the metadata files of sharded checkpoints.  The",
    "description": "result is one logical checkpoint, with one physical metadata file and renamed\ndata files.\n\nIntended for \"grouping\" multiple checkpoints in a sharded checkpoint setup.\n\nIf delete_old_dirs is true, attempts to delete recursively the dirname of each\npath in the input checkpoint_prefixes.  This is useful when those paths are non\nuser-facing temporary locations.\n\nIf allow_missing_files is true, merges the checkpoint prefixes as long as\nat least one file exists. Otherwise, if no files exist, an error will be thrown.\nThe default value for allow_missing_files is false.",
    "inputs": [
      { "name": "checkpoint_prefixes", "type": "Arg" },
      { "name": "destination_prefix", "type": "Arg" }
    ],
    "attributes": [
      { "name": "delete_old_dirs", "type": "DefaultValuedOptionalAttr" },
      { "name": "allow_missing_files", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.Min",
    "summary": "Computes the minimum of elements across dimensions of a tensor.",
    "description": "Reduces `input` along the dimensions given in `axis`. Unless\n`keep_dims` is true, the rank of the tensor is reduced by 1 for each entry in\n`axis`. If `keep_dims` is true, the reduced dimensions are\nretained with length 1.",
    "inputs": [
      { "name": "input", "type": "Arg" },
      { "name": "reduction_indices", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "keep_dims", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.Minimum",
    "summary": "Returns the min of x and y (i.e. x < y ? x : y) element-wise.",
    "description": "*NOTE*: `Minimum` supports broadcasting. More about broadcasting\n[here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)",
    "inputs": [
      { "name": "x", "type": "TF_IntOrFpTensor" },
      { "name": "y", "type": "TF_IntOrFpTensor" }
    ],
    "outputs": [
      { "name": "z", "type": "TF_IntOrFpTensor" }
    ]
  },
  {
    "name": "tf.MirrorPad",
    "summary": "Pads a tensor with mirrored values.",
    "description": "This operation pads a `input` with mirrored values according to the `paddings`\nyou specify. `paddings` is an integer tensor with shape `[n, 2]`, where n is\nthe rank of `input`. For each dimension D of `input`, `paddings[D, 0]` indicates\nhow many values to add before the contents of `input` in that dimension, and\n`paddings[D, 1]` indicates how many values to add after the contents of `input`\nin that dimension. Both `paddings[D, 0]` and `paddings[D, 1]` must be no greater\nthan `input.dim_size(D)` (or `input.dim_size(D) - 1`) if `copy_border` is true\n(if false, respectively).\n\nThe padded size of each dimension D of the output is:\n\n`paddings(D, 0) + input.dim_size(D) + paddings(D, 1)`\n\nFor example:\n\n```\n# 't' is [[1, 2, 3], [4, 5, 6]].\n# 'paddings' is [[1, 1]], [2, 2]].\n# 'mode' is SYMMETRIC.\n# rank of 't' is 2.\npad(t, paddings) ==> [[2, 1, 1, 2, 3, 3, 2]\n                      [2, 1, 1, 2, 3, 3, 2]\n                      [5, 4, 4, 5, 6, 6, 5]\n                      [5, 4, 4, 5, 6, 6, 5]]\n```",
    "inputs": [
      { "name": "input", "type": "Arg" },
      { "name": "paddings", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "mode", "type": "TF_AnyStrAttrOf" }
    ]
  },
  {
    "name": "tf.MirrorPadGrad",
    "summary": "Gradient op for `MirrorPad` op. This op folds a mirror-padded tensor.",
    "description": "This operation folds the padded areas of `input` by `MirrorPad` according to the\n`paddings` you specify. `paddings` must be the same as `paddings` argument\ngiven to the corresponding `MirrorPad` op.\n\nThe folded size of each dimension D of the output is:\n\n`input.dim_size(D) - paddings(D, 0) - paddings(D, 1)`\n\nFor example:\n\n```\n# 't' is [[1, 2, 3], [4, 5, 6], [7, 8, 9]].\n# 'paddings' is [[0, 1]], [0, 1]].\n# 'mode' is SYMMETRIC.\n# rank of 't' is 2.\npad(t, paddings) ==> [[ 1,  5]\n                      [11, 28]]\n```",
    "inputs": [
      { "name": "input", "type": "Arg" },
      { "name": "paddings", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "mode", "type": "TF_AnyStrAttrOf" }
    ]
  },
  {
    "name": "tf.MlirLocalVarOp",
    "summary": "Creates a handle to an in-scope variable.",
    "description": "Used by internal passes for temporary representation of local state, which will\nbe eventually removed.",
    "outputs": [
      { "name": "resource", "type": "Res" }
    ]
  },
  {
    "name": "tf.MlirPassthroughOp",
    "summary": "Wraps an arbitrary MLIR computation expressed as a module with a main() function.",
    "description": "This operation does not have an associated kernel and is not intended to be\nexecuted in a regular TensorFlow session. Instead it is intended to be used for\ntesting or for special case where a user intends to pass custom MLIR computation\nthrough a TensorFlow graph with the intent of having custom tooling processing\nit downstream (when targeting a different environment, like TensorFlow lite for\nexample).\nThe MLIR module is expected to have a main() function that will be used as an\nentry point. The inputs to the operations will be passed as argument to the\nmain() function and the returned values of the main function mapped to the\noutputs.\nExample usage:\n\n```\nimport tensorflow as tf\nfrom tensorflow.compiler.mlir.tensorflow.gen_mlir_passthrough_op import mlir_passthrough_op\n\nmlir_module = '''python\nfunc @main(%arg0 : tensor<10xf32>, %arg1 : tensor<10xf32>) -> tensor<10x10xf32> {\n   %add = \"magic.op\"(%arg0, %arg1) : (tensor<10xf32>, tensor<10xf32>) -> tensor<10x10xf32>\n   return %ret : tensor<10x10xf32>\n}\n'''\n\n@tf.function\ndef foo(x, y):\n  return mlir_passthrough_op([x, y], mlir_module, Toutputs=[tf.float32])\n\ngraph_def = foo.get_concrete_function(tf.TensorSpec([10], tf.float32), tf.TensorSpec([10], tf.float32)).graph.as_graph_def()\n```",
    "inputs": [
      { "name": "inputs", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "outputs", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "mlir_module", "type": "StrAttr" }
    ]
  },
  {
    "name": "tf.Mod",
    "summary": "Returns element-wise remainder of division. This emulates C semantics in that",
    "description": "the result here is consistent with a truncating divide. E.g.\n`tf.truncatediv(x, y) * y + truncate_mod(x, y) = x`.\n\n*NOTE*: `Mod` supports broadcasting. More about broadcasting\n[here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)",
    "inputs": [
      { "name": "x", "type": "TF_FpOrI32OrI64Tensor" },
      { "name": "y", "type": "TF_FpOrI32OrI64Tensor" }
    ],
    "outputs": [
      { "name": "z", "type": "TF_FpOrI32OrI64Tensor" }
    ]
  },
  {
    "name": "tf.ModelDataset",
    "summary": "Identity transformation that models performance.",
    "description": "Identity transformation that models performance.",
    "inputs": [
      { "name": "input_dataset", "type": "Arg" }
    ],
    "outputs": [
      { "name": "handle", "type": "TF_VariantTensor" }
    ],
    "attributes": [
      { "name": "algorithm", "type": "DefaultValuedOptionalAttr" },
      { "name": "cpu_budget", "type": "DefaultValuedOptionalAttr" },
      { "name": "ram_budget", "type": "DefaultValuedOptionalAttr" },
      { "name": "output_types", "type": "ConfinedAttr" },
      { "name": "output_shapes", "type": "ConfinedAttr" }
    ]
  },
  {
    "name": "tf.Mul",
    "summary": "Returns x * y element-wise.",
    "description": "*NOTE*: `Multiply` supports broadcasting. More about broadcasting\n[here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)",
    "inputs": [
      { "name": "x", "type": "TensorOf" },
      { "name": "y", "type": "TensorOf" }
    ],
    "outputs": [
      { "name": "z", "type": "TensorOf" }
    ]
  },
  {
    "name": "tf.MulNoNan",
    "summary": "Returns x * y element-wise. Returns zero if y is zero, even if x if infinite or NaN.",
    "description": "*NOTE*: `MulNoNan` supports broadcasting. More about broadcasting\n[here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)",
    "inputs": [
      { "name": "x", "type": "TF_FpOrComplexTensor" },
      { "name": "y", "type": "TF_FpOrComplexTensor" }
    ],
    "outputs": [
      { "name": "z", "type": "TF_FpOrComplexTensor" }
    ]
  },
  {
    "name": "tf.MultiDeviceIterator",
    "summary": "Creates a MultiDeviceIterator resource.",
    "outputs": [
      { "name": "handle", "type": "Res" }
    ],
    "attributes": [
      { "name": "devices", "type": "ConfinedAttr" },
      { "name": "shared_name", "type": "StrAttr" },
      { "name": "container", "type": "StrAttr" },
      { "name": "output_types", "type": "ConfinedAttr" },
      { "name": "output_shapes", "type": "ConfinedAttr" }
    ]
  },
  {
    "name": "tf.MultiDeviceIteratorFromStringHandle",
    "summary": "Generates a MultiDeviceIterator resource from its provided string handle.",
    "inputs": [
      { "name": "string_handle", "type": "Arg" }
    ],
    "outputs": [
      { "name": "multi_device_iterator", "type": "Res" }
    ],
    "attributes": [
      { "name": "output_types", "type": "DefaultValuedOptionalAttr" },
      { "name": "output_shapes", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.MultiDeviceIteratorGetNextFromShard",
    "summary": "Gets next element for the provided shard number.",
    "inputs": [
      { "name": "multi_device_iterator", "type": "Arg" },
      { "name": "shard_num", "type": "Arg" },
      { "name": "incarnation_id", "type": "Arg" }
    ],
    "outputs": [
      { "name": "components", "type": "Res" }
    ]
  },
  {
    "name": "tf.MultiDeviceIteratorInit",
    "summary": "Initializes the multi device iterator with the given dataset.",
    "inputs": [
      { "name": "dataset", "type": "Arg" },
      { "name": "multi_device_iterator", "type": "Arg" },
      { "name": "max_buffer_size", "type": "Arg" }
    ],
    "outputs": [
      { "name": "incarnation_id", "type": "Res" }
    ]
  },
  {
    "name": "tf.MultiDeviceIteratorToStringHandle",
    "summary": "Produces a string handle for the given MultiDeviceIterator.",
    "inputs": [
      { "name": "multi_device_iterator", "type": "Arg" }
    ],
    "outputs": [
      { "name": "string_handle", "type": "Res" }
    ]
  },
  {
    "name": "tf.Multinomial",
    "summary": "Draws samples from a multinomial distribution.",
    "inputs": [
      { "name": "logits", "type": "Arg" },
      { "name": "num_samples", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "seed", "type": "DefaultValuedOptionalAttr" },
      { "name": "seed2", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.MutableDenseHashTableV2",
    "summary": "Creates an empty hash table that uses tensors as the backing store.",
    "description": "It uses \"open addressing\" with quadratic reprobing to resolve\ncollisions.\n\nThis op creates a mutable hash table, specifying the type of its keys and\nvalues. Each value must be a scalar. Data can be inserted into the table using\nthe insert operations. It does not support the initialization operation.",
    "inputs": [
      { "name": "empty_key", "type": "Arg" },
      { "name": "deleted_key", "type": "TF_Tensor" }
    ],
    "outputs": [
      { "name": "table_handle", "type": "Res" }
    ],
    "attributes": [
      { "name": "container", "type": "DefaultValuedOptionalAttr" },
      { "name": "shared_name", "type": "DefaultValuedOptionalAttr" },
      { "name": "use_node_name_sharing", "type": "DefaultValuedOptionalAttr" },
      { "name": "value_dtype", "type": "TypeAttr" },
      { "name": "value_shape", "type": "DefaultValuedOptionalAttr" },
      { "name": "initial_num_buckets", "type": "DefaultValuedOptionalAttr" },
      { "name": "max_load_factor", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.MutableHashTableOfTensorsV2",
    "summary": "Creates an empty hash table.",
    "description": "This op creates a mutable hash table, specifying the type of its keys and\nvalues. Each value must be a vector. Data can be inserted into the table using\nthe insert operations. It does not support the initialization operation.",
    "outputs": [
      { "name": "table_handle", "type": "Res" }
    ],
    "attributes": [
      { "name": "container", "type": "DefaultValuedOptionalAttr" },
      { "name": "shared_name", "type": "DefaultValuedOptionalAttr" },
      { "name": "use_node_name_sharing", "type": "DefaultValuedOptionalAttr" },
      { "name": "key_dtype", "type": "TypeAttr" },
      { "name": "value_dtype", "type": "TypeAttr" },
      { "name": "value_shape", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.MutableHashTableV2",
    "summary": "Creates an empty hash table.",
    "description": "This op creates a mutable hash table, specifying the type of its keys and\nvalues. Each value must be a scalar. Data can be inserted into the table using\nthe insert operations. It does not support the initialization operation.",
    "outputs": [
      { "name": "table_handle", "type": "Res" }
    ],
    "attributes": [
      { "name": "container", "type": "DefaultValuedOptionalAttr" },
      { "name": "shared_name", "type": "DefaultValuedOptionalAttr" },
      { "name": "use_node_name_sharing", "type": "DefaultValuedOptionalAttr" },
      { "name": "key_dtype", "type": "TypeAttr" },
      { "name": "value_dtype", "type": "TypeAttr" }
    ]
  },
  {
    "name": "tf.NcclAllReduce",
    "summary": "Outputs a tensor containing the reduction across all input tensors.",
    "description": "Outputs a tensor containing the reduction across all input tensors passed to ops\nwithin the same `shared_name.\n\nThe graph should be constructed so if one op runs with shared_name value `c`,\nthen `num_devices` ops will run with shared_name value `c`.  Failure to do so\nwill cause the graph execution to fail to complete.\n\ninput: the input to the reduction\ndata: the value of the reduction across all `num_devices` devices.\nreduction: the reduction operation to perform.\nnum_devices: The number of devices participating in this reduction.\nshared_name: Identifier that shared between ops of the same reduction.",
    "inputs": [
      { "name": "input", "type": "TensorOf" }
    ],
    "outputs": [
      { "name": "data", "type": "TensorOf" }
    ],
    "attributes": [
      { "name": "reduction", "type": "TF_AnyStrAttrOf" },
      { "name": "num_devices", "type": "I64Attr" },
      { "name": "shared_name", "type": "StrAttr" }
    ]
  },
  {
    "name": "tf.Ndtri",
    "inputs": [
      { "name": "x", "type": "TF_FloatTensor" }
    ],
    "outputs": [
      { "name": "y", "type": "TF_FloatTensor" }
    ]
  },
  {
    "name": "tf.Neg",
    "summary": "Computes numerical negative value element-wise.",
    "description": "I.e., \\\\(y = -x\\\\).",
    "inputs": [
      { "name": "x", "type": "TensorOf" }
    ],
    "outputs": [
      { "name": "y", "type": "TensorOf" }
    ]
  },
  {
    "name": "tf.NextAfter",
    "summary": "Returns the next representable value of `x1` in the direction of `x2`, element-wise.",
    "description": "This operation returns the same result as the C++ std::nextafter function.\n\nIt can also return a subnormal number.\n\n@compatibility(cpp)\nEquivalent to C++ std::nextafter function.\n@end_compatibility",
    "inputs": [
      { "name": "x1", "type": "TF_F32OrF64Tensor" },
      { "name": "x2", "type": "TF_F32OrF64Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "TF_F32OrF64Tensor" }
    ]
  },
  {
    "name": "tf.NonMaxSuppressionV3",
    "summary": "Greedily selects a subset of bounding boxes in descending order of score,",
    "description": "pruning away boxes that have high intersection-over-union (IOU) overlap\nwith previously selected boxes.  Bounding boxes with score less than\n`score_threshold` are removed.  Bounding boxes are supplied as\n[y1, x1, y2, x2], where (y1, x1) and (y2, x2) are the coordinates of any\ndiagonal pair of box corners and the coordinates can be provided as normalized\n(i.e., lying in the interval [0, 1]) or absolute.  Note that this algorithm\nis agnostic to where the origin is in the coordinate system and more\ngenerally is invariant to orthogonal transformations and translations\nof the coordinate system; thus translating or reflections of the coordinate\nsystem result in the same boxes being selected by the algorithm.\nThe output of this operation is a set of integers indexing into the input\ncollection of bounding boxes representing the selected boxes.  The bounding\nbox coordinates corresponding to the selected indices can then be obtained\nusing the `tf.gather operation`.  For example:\n  selected_indices = tf.image.non_max_suppression_v2(\n      boxes, scores, max_output_size, iou_threshold, score_threshold)\n  selected_boxes = tf.gather(boxes, selected_indices)",
    "inputs": [
      { "name": "boxes", "type": "Arg" },
      { "name": "scores", "type": "Arg" },
      { "name": "max_output_size", "type": "Arg" },
      { "name": "iou_threshold", "type": "Arg" },
      { "name": "score_threshold", "type": "Arg" }
    ],
    "outputs": [
      { "name": "selected_indices", "type": "Res" }
    ]
  },
  {
    "name": "tf.NonMaxSuppressionV4",
    "summary": "Greedily selects a subset of bounding boxes in descending order of score,",
    "description": "pruning away boxes that have high intersection-over-union (IOU) overlap\nwith previously selected boxes.  Bounding boxes with score less than\n`score_threshold` are removed.  Bounding boxes are supplied as\n[y1, x1, y2, x2], where (y1, x1) and (y2, x2) are the coordinates of any\ndiagonal pair of box corners and the coordinates can be provided as normalized\n(i.e., lying in the interval [0, 1]) or absolute.  Note that this algorithm\nis agnostic to where the origin is in the coordinate system and more\ngenerally is invariant to orthogonal transformations and translations\nof the coordinate system; thus translating or reflections of the coordinate\nsystem result in the same boxes being selected by the algorithm.\nThe output of this operation is a set of integers indexing into the input\ncollection of bounding boxes representing the selected boxes.  The bounding\nbox coordinates corresponding to the selected indices can then be obtained\nusing the `tf.gather operation`.  For example:\n  selected_indices = tf.image.non_max_suppression_v2(\n      boxes, scores, max_output_size, iou_threshold, score_threshold)\n  selected_boxes = tf.gather(boxes, selected_indices)",
    "inputs": [
      { "name": "boxes", "type": "Arg" },
      { "name": "scores", "type": "Arg" },
      { "name": "max_output_size", "type": "Arg" },
      { "name": "iou_threshold", "type": "Arg" },
      { "name": "score_threshold", "type": "Arg" }
    ],
    "outputs": [
      { "name": "selected_indices", "type": "Res" },
      { "name": "valid_outputs", "type": "Res" }
    ],
    "attributes": [
      { "name": "pad_to_max_output_size", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.NonMaxSuppressionV5",
    "summary": "Greedily selects a subset of bounding boxes in descending order of score,",
    "description": "pruning away boxes that have high intersection-over-union (IOU) overlap\nwith previously selected boxes.  Bounding boxes with score less than\n`score_threshold` are removed.  Bounding boxes are supplied as\n[y1, x1, y2, x2], where (y1, x1) and (y2, x2) are the coordinates of any\ndiagonal pair of box corners and the coordinates can be provided as normalized\n(i.e., lying in the interval [0, 1]) or absolute.  Note that this algorithm\nis agnostic to where the origin is in the coordinate system and more\ngenerally is invariant to orthogonal transformations and translations\nof the coordinate system; thus translating or reflections of the coordinate\nsystem result in the same boxes being selected by the algorithm.\nThe output of this operation is a set of integers indexing into the input\ncollection of bounding boxes representing the selected boxes.  The bounding\nbox coordinates corresponding to the selected indices can then be obtained\nusing the `tf.gather operation`.  For example:\n  selected_indices = tf.image.non_max_suppression_v2(\n      boxes, scores, max_output_size, iou_threshold, score_threshold)\n  selected_boxes = tf.gather(boxes, selected_indices)\nThis op also supports a Soft-NMS (with Gaussian weighting) mode (c.f.\nBodla et al, https://arxiv.org/abs/1704.04503) where boxes reduce the score\nof other overlapping boxes instead of directly causing them to be pruned.\nTo enable this Soft-NMS mode, set the `soft_nms_sigma` parameter to be\nlarger than 0.",
    "inputs": [
      { "name": "boxes", "type": "Arg" },
      { "name": "scores", "type": "Arg" },
      { "name": "max_output_size", "type": "Arg" },
      { "name": "iou_threshold", "type": "Arg" },
      { "name": "score_threshold", "type": "Arg" },
      { "name": "soft_nms_sigma", "type": "Arg" }
    ],
    "outputs": [
      { "name": "selected_indices", "type": "Res" },
      { "name": "selected_scores", "type": "Res" },
      { "name": "valid_outputs", "type": "Res" }
    ],
    "attributes": [
      { "name": "pad_to_max_output_size", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.NoOp",
    "summary": "Does nothing. Only useful as a placeholder for control edges."
  },
  {
    "name": "tf.NotEqual",
    "summary": "Returns the truth value of (x != y) element-wise.",
    "description": "*NOTE*: `NotEqual` supports broadcasting. More about broadcasting\n[here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)",
    "inputs": [
      { "name": "x", "type": "TF_Tensor" },
      { "name": "y", "type": "TF_Tensor" }
    ],
    "outputs": [
      { "name": "z", "type": "TF_BoolTensor" }
    ],
    "attributes": [
      { "name": "incompatible_shape_error", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.OneHot",
    "summary": "Returns a one-hot tensor.",
    "description": "The locations represented by indices in `indices` take value `on_value`,\nwhile all other locations take value `off_value`.\n\nIf the input `indices` is rank `N`, the output will have rank `N+1`,\nThe new axis is created at dimension `axis` (default: the new axis is\nappended at the end).\n\nIf `indices` is a scalar the output shape will be a vector of length `depth`.\n\nIf `indices` is a vector of length `features`, the output shape will be:\n```\n  features x depth if axis == -1\n  depth x features if axis == 0\n```\n\nIf `indices` is a matrix (batch) with shape `[batch, features]`,\nthe output shape will be:\n```\n  batch x features x depth if axis == -1\n  batch x depth x features if axis == 1\n  depth x batch x features if axis == 0\n```\n\n\nExamples\n=========\n\nSuppose that\n```\n  indices = [0, 2, -1, 1]\n  depth = 3\n  on_value = 5.0\n  off_value = 0.0\n  axis = -1\n```\n\nThen output is `[4 x 3]`:\n```\noutput =\n  [5.0 0.0 0.0]  // one_hot(0)\n  [0.0 0.0 5.0]  // one_hot(2)\n  [0.0 0.0 0.0]  // one_hot(-1)\n  [0.0 5.0 0.0]  // one_hot(1)\n```\n\nSuppose that\n```\n  indices = [0, 2, -1, 1]\n  depth = 3\n  on_value = 0.0\n  off_value = 3.0\n  axis = 0\n```\n\nThen output is `[3 x 4]`:\n```\noutput =\n  [0.0 3.0 3.0 3.0]\n  [3.0 3.0 3.0 0.0]\n  [3.0 3.0 3.0 3.0]\n  [3.0 0.0 3.0 3.0]\n//  ^                one_hot(0)\n//      ^            one_hot(2)\n//          ^        one_hot(-1)\n//              ^    one_hot(1)\n```\n\nSuppose that\n```\n  indices = [[0, 2], [1, -1]]\n  depth = 3\n  on_value = 1.0\n  off_value = 0.0\n  axis = -1\n```\n\nThen output is `[2 x 2 x 3]`:\n```\noutput =\n  [\n    [1.0, 0.0, 0.0]  // one_hot(0)\n    [0.0, 0.0, 1.0]  // one_hot(2)\n  ][\n    [0.0, 1.0, 0.0]  // one_hot(1)\n    [0.0, 0.0, 0.0]  // one_hot(-1)\n  ]\n```",
    "inputs": [
      { "name": "indices", "type": "Arg" },
      { "name": "depth", "type": "Arg" },
      { "name": "on_value", "type": "Arg" },
      { "name": "off_value", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "axis", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.OneShotIterator",
    "summary": "Makes a \"one-shot\" iterator that can be iterated only once.",
    "description": "A one-shot iterator bundles the logic for defining the dataset and\nthe state of the iterator in a single op, which allows simple input\npipelines to be defined without an additional initialization\n(\"MakeIterator\") step.\n\nOne-shot iterators have the following limitations:\n\n* They do not support parameterization: all logic for creating the underlying\n  dataset must be bundled in the `dataset_factory` function.\n* They are not resettable. Once a one-shot iterator reaches the end of its\n  underlying dataset, subsequent \"IteratorGetNext\" operations on that\n  iterator will always produce an `OutOfRange` error.\n\nFor greater flexibility, use \"Iterator\" and \"MakeIterator\" to define\nan iterator using an arbitrary subgraph, which may capture tensors\n(including fed values) as parameters, and which may be reset multiple\ntimes by rerunning \"MakeIterator\".",
    "outputs": [
      { "name": "handle", "type": "Res" }
    ],
    "attributes": [
      { "name": "dataset_factory", "type": "SymbolRefAttr" },
      { "name": "output_types", "type": "ConfinedAttr" },
      { "name": "output_shapes", "type": "ConfinedAttr" },
      { "name": "container", "type": "DefaultValuedOptionalAttr" },
      { "name": "shared_name", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.OnesLike",
    "summary": "Returns a tensor of ones with the same shape and type as x.",
    "inputs": [
      { "name": "x", "type": "Arg" }
    ],
    "outputs": [
      { "name": "y", "type": "Res" }
    ]
  },
  {
    "name": "tf.OptimizeDatasetV2",
    "summary": "Creates a dataset by applying related optimizations to `input_dataset`.",
    "description": "Creates a dataset by applying related optimizations to `input_dataset`.",
    "inputs": [
      { "name": "input_dataset", "type": "Arg" },
      { "name": "optimizations_enabled", "type": "Arg" },
      { "name": "optimizations_disabled", "type": "Arg" },
      { "name": "optimizations_default", "type": "Arg" }
    ],
    "outputs": [
      { "name": "handle", "type": "TF_VariantTensor" }
    ],
    "attributes": [
      { "name": "output_types", "type": "ConfinedAttr" },
      { "name": "output_shapes", "type": "ConfinedAttr" },
      { "name": "optimization_configs", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.OptionalFromValue",
    "summary": "Constructs an Optional variant from a tuple of tensors.",
    "inputs": [
      { "name": "components", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "optional", "type": "TF_VariantTensor" }
    ]
  },
  {
    "name": "tf.OptionalGetValue",
    "summary": "Returns the value stored in an Optional variant or raises an error if none exists.",
    "inputs": [
      { "name": "optional", "type": "TF_VariantTensor" }
    ],
    "outputs": [
      { "name": "components", "type": "Variadic" }
    ]
  },
  {
    "name": "tf.OptionalHasValue",
    "summary": "Returns true if and only if the given Optional variant has a value.",
    "inputs": [
      { "name": "optional", "type": "TF_VariantTensor" }
    ],
    "outputs": [
      { "name": "has_value", "type": "TF_BoolTensor" }
    ]
  },
  {
    "name": "tf.OptionalNone",
    "summary": "Creates an Optional variant with no value.",
    "outputs": [
      { "name": "optional", "type": "TF_VariantTensor" }
    ]
  },
  {
    "name": "tf.OutfeedEnqueue",
    "summary": "Enqueue a Tensor on the computation outfeed.",
    "inputs": [
      { "name": "input", "type": "Arg" }
    ]
  },
  {
    "name": "tf.OutfeedEnqueueTuple",
    "summary": "Enqueue multiple Tensor values on the computation outfeed.",
    "inputs": [
      { "name": "inputs", "type": "Arg" }
    ]
  },
  {
    "name": "tf.Pack",
    "summary": "Packs a list of `N` rank-`R` tensors into one rank-`(R+1)` tensor.",
    "description": "Packs the `N` tensors in `values` into a tensor with rank one higher than each\ntensor in `values`, by packing them along the `axis` dimension.\nGiven a list of tensors of shape `(A, B, C)`;\n\nif `axis == 0` then the `output` tensor will have the shape `(N, A, B, C)`.\nif `axis == 1` then the `output` tensor will have the shape `(A, N, B, C)`.\nEtc.\n\nFor example:\n\n```\n# 'x' is [1, 4]\n# 'y' is [2, 5]\n# 'z' is [3, 6]\npack([x, y, z]) => [[1, 4], [2, 5], [3, 6]]  # Pack along first dim.\npack([x, y, z], axis=1) => [[1, 2, 3], [4, 5, 6]]\n```\n\nThis is the opposite of `unpack`.",
    "inputs": [
      { "name": "values", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "axis", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.Pad",
    "summary": "Pads a tensor with zeros.",
    "description": "This operation pads a `input` with zeros according to the `paddings` you\nspecify. `paddings` is an integer tensor with shape `[Dn, 2]`, where n is the\nrank of `input`. For each dimension D of `input`, `paddings[D, 0]` indicates\nhow many zeros to add before the contents of `input` in that dimension, and\n`paddings[D, 1]` indicates how many zeros to add after the contents of `input`\nin that dimension.\n\nThe padded size of each dimension D of the output is:\n\n`paddings(D, 0) + input.dim_size(D) + paddings(D, 1)`\n\nFor example:\n\n```\n# 't' is [[1, 1], [2, 2]]\n# 'paddings' is [[1, 1], [2, 2]]\n# rank of 't' is 2\npad(t, paddings) ==> [[0, 0, 0, 0, 0, 0]\n                      [0, 0, 1, 1, 0, 0]\n                      [0, 0, 2, 2, 0, 0]\n                      [0, 0, 0, 0, 0, 0]]\n```",
    "inputs": [
      { "name": "input", "type": "TF_Tensor" },
      { "name": "paddings", "type": "TF_I32OrI64Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "TF_Tensor" }
    ],
    "category": "Transform"
  },
  {
    "name": "tf.PadV2",
    "summary": "Pads a tensor.",
    "description": "This operation pads `input` according to the `paddings` and `constant_values`\nyou specify. `paddings` is an integer tensor with shape `[Dn, 2]`, where n is\nthe rank of `input`. For each dimension D of `input`, `paddings[D, 0]` indicates\nhow many padding values to add before the contents of `input` in that dimension,\nand `paddings[D, 1]` indicates how many padding values to add after the contents\nof `input` in that dimension. `constant_values` is a scalar tensor of the same\ntype as `input` that indicates the value to use for padding `input`.\n\nThe padded size of each dimension D of the output is:\n\n`paddings(D, 0) + input.dim_size(D) + paddings(D, 1)`\n\nFor example:\n\n```\n# 't' is [[1, 1], [2, 2]]\n# 'paddings' is [[1, 1], [2, 2]]\n# 'constant_values' is 0\n# rank of 't' is 2\npad(t, paddings) ==> [[0, 0, 0, 0, 0, 0]\n                      [0, 0, 1, 1, 0, 0]\n                      [0, 0, 2, 2, 0, 0]\n                      [0, 0, 0, 0, 0, 0]]\n```",
    "inputs": [
      { "name": "input", "type": "TF_Tensor" },
      { "name": "paddings", "type": "TF_I32OrI64Tensor" },
      { "name": "constant_values", "type": "TF_Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "TF_Tensor" }
    ]
  },
  {
    "name": "tf.ParallelDynamicStitch",
    "summary": "Interleave the values from the `data` tensors into a single tensor.",
    "description": "Builds a merged tensor such that\n\n```python\n    merged[indices[m][i, ..., j], ...] = data[m][i, ..., j, ...]\n```\n\nFor example, if each `indices[m]` is scalar or vector, we have\n\n```python\n    # Scalar indices:\n    merged[indices[m], ...] = data[m][...]\n\n    # Vector indices:\n    merged[indices[m][i], ...] = data[m][i, ...]\n```\n\nEach `data[i].shape` must start with the corresponding `indices[i].shape`,\nand the rest of `data[i].shape` must be constant w.r.t. `i`.  That is, we\nmust have `data[i].shape = indices[i].shape + constant`.  In terms of this\n`constant`, the output shape is\n\n    merged.shape = [max(indices)] + constant\n\nValues may be merged in parallel, so if an index appears in both `indices[m][i]`\nand `indices[n][j]`, the result may be invalid. This differs from the normal\nDynamicStitch operator that defines the behavior in that case.\n\nFor example:\n\n```python\n    indices[0] = 6\n    indices[1] = [4, 1]\n    indices[2] = [[5, 2], [0, 3]]\n    data[0] = [61, 62]\n    data[1] = [[41, 42], [11, 12]]\n    data[2] = [[[51, 52], [21, 22]], [[1, 2], [31, 32]]]\n    merged = [[1, 2], [11, 12], [21, 22], [31, 32], [41, 42],\n              [51, 52], [61, 62]]\n```\n\nThis method can be used to merge partitions created by `dynamic_partition`\nas illustrated on the following example:\n\n```python\n    # Apply function (increments x_i) on elements for which a certain condition\n    # apply (x_i != -1 in this example).\n    x=tf.constant([0.1, -1., 5.2, 4.3, -1., 7.4])\n    condition_mask=tf.not_equal(x,tf.constant(-1.))\n    partitioned_data = tf.dynamic_partition(\n        x, tf.cast(condition_mask, tf.int32) , 2)\n    partitioned_data[1] = partitioned_data[1] + 1.0\n    condition_indices = tf.dynamic_partition(\n        tf.range(tf.shape(x)[0]), tf.cast(condition_mask, tf.int32) , 2)\n    x = tf.dynamic_stitch(condition_indices, partitioned_data)\n    # Here x=[1.1, -1., 6.2, 5.3, -1, 8.4], the -1. values remain\n    # unchanged.\n```\n\n<div style=\"width:70%; margin:auto; margin-bottom:10px; margin-top:20px;\">\n<img style=\"width:100%\" src=\"https://www.tensorflow.org/images/DynamicStitch.png\" alt>\n</div>",
    "inputs": [
      { "name": "indices", "type": "Variadic" },
      { "name": "data", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "merged", "type": "TF_Tensor" }
    ]
  },
  {
    "name": "tf.ParallelMapDataset",
    "summary": "Creates a dataset that applies `f` to the outputs of `input_dataset`.",
    "description": "Unlike a \"MapDataset\", which applies `f` sequentially, this dataset invokes up\nto `num_parallel_calls` copies of `f` in parallel.",
    "inputs": [
      { "name": "input_dataset", "type": "TF_VariantTensor" },
      { "name": "other_arguments", "type": "Variadic" },
      { "name": "num_parallel_calls", "type": "Arg" }
    ],
    "outputs": [
      { "name": "handle", "type": "TF_VariantTensor" }
    ],
    "attributes": [
      { "name": "f", "type": "SymbolRefAttr" },
      { "name": "output_types", "type": "ConfinedAttr" },
      { "name": "output_shapes", "type": "ConfinedAttr" },
      { "name": "use_inter_op_parallelism", "type": "DefaultValuedOptionalAttr" },
      { "name": "sloppy", "type": "DefaultValuedOptionalAttr" },
      { "name": "preserve_cardinality", "type": "DefaultValuedOptionalAttr" },
      { "name": "metadata", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.ParallelMapDatasetV2",
    "summary": "Creates a dataset that applies `f` to the outputs of `input_dataset`.",
    "description": "Unlike a \"MapDataset\", which applies `f` sequentially, this dataset invokes up\nto `num_parallel_calls` copies of `f` in parallel.",
    "inputs": [
      { "name": "input_dataset", "type": "TF_VariantTensor" },
      { "name": "other_arguments", "type": "Variadic" },
      { "name": "num_parallel_calls", "type": "Arg" }
    ],
    "outputs": [
      { "name": "handle", "type": "TF_VariantTensor" }
    ],
    "attributes": [
      { "name": "f", "type": "SymbolRefAttr" },
      { "name": "output_types", "type": "ConfinedAttr" },
      { "name": "output_shapes", "type": "ConfinedAttr" },
      { "name": "use_inter_op_parallelism", "type": "DefaultValuedOptionalAttr" },
      { "name": "deterministic", "type": "DefaultValuedOptionalAttr" },
      { "name": "preserve_cardinality", "type": "DefaultValuedOptionalAttr" },
      { "name": "use_unbounded_threadpool", "type": "DefaultValuedOptionalAttr" },
      { "name": "metadata", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.ParameterizedTruncatedNormal",
    "summary": "Outputs random values from a normal distribution. The parameters may each be a",
    "description": "scalar which applies to the entire output, or a vector of length shape[0] which\nstores the parameters for each batch.",
    "inputs": [
      { "name": "shape", "type": "Arg" },
      { "name": "means", "type": "Arg" },
      { "name": "stdevs", "type": "Arg" },
      { "name": "minvals", "type": "Arg" },
      { "name": "maxvals", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "seed", "type": "DefaultValuedOptionalAttr" },
      { "name": "seed2", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.ParseExample",
    "summary": "Transforms a vector of tf.Example protos (as strings) into typed tensors.",
    "inputs": [
      { "name": "serialized", "type": "TF_StrTensor" },
      { "name": "names", "type": "TF_StrTensor" },
      { "name": "sparse_keys", "type": "Variadic" },
      { "name": "dense_keys", "type": "Variadic" },
      { "name": "dense_defaults", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "sparse_indices", "type": "Variadic" },
      { "name": "sparse_values", "type": "Variadic" },
      { "name": "sparse_shapes", "type": "Variadic" },
      { "name": "dense_values", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "dense_shapes", "type": "TF_ShapeAttrArray" }
    ]
  },
  {
    "name": "tf.ParseExampleV2",
    "summary": "Transforms a vector of tf.Example protos (as strings) into typed tensors.",
    "inputs": [
      { "name": "serialized", "type": "TF_StrTensor" },
      { "name": "names", "type": "TF_StrTensor" },
      { "name": "sparse_keys", "type": "TF_StrTensor" },
      { "name": "dense_keys", "type": "TF_StrTensor" },
      { "name": "ragged_keys", "type": "TF_StrTensor" },
      { "name": "dense_defaults", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "sparse_indices", "type": "Variadic" },
      { "name": "sparse_values", "type": "Variadic" },
      { "name": "sparse_shapes", "type": "Variadic" },
      { "name": "dense_values", "type": "Variadic" },
      { "name": "ragged_values", "type": "Variadic" },
      { "name": "ragged_row_splits", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "num_sparse", "type": "ConfinedAttr" },
      { "name": "dense_shapes", "type": "TF_ShapeAttrArray" }
    ]
  },
  {
    "name": "tf.PartitionedCall",
    "summary": "returns `f(inputs)`, where `f`'s body is placed and partitioned.",
    "description": "Asynchronously executes a function, potentially across multiple devices but\nwithin a single process. The kernel places and partitions a given function's\nunderlying graph, and executes each of the partitioned subgraphs as a function.",
    "inputs": [
      { "name": "args", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "arg_attrs", "type": "OptionalAttr" },
      { "name": "res_attrs", "type": "OptionalAttr" },
      { "name": "f", "type": "SymbolRefAttr" },
      { "name": "config", "type": "DefaultValuedOptionalAttr" },
      { "name": "config_proto", "type": "DefaultValuedOptionalAttr" },
      { "name": "executor_type", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.Placeholder",
    "summary": "Placeholder op",
    "description": "Inserts a placeholder for a tensor that will be always fed.",
    "outputs": [
      { "name": "output", "type": "TF_Tensor" }
    ]
  },
  {
    "name": "tf.PlaceholderWithDefault",
    "summary": "Placeholder op",
    "description": "A placeholder op that passes through input when its output is not fed.",
    "inputs": [
      { "name": "input", "type": "TF_Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "TF_Tensor" }
    ]
  },
  {
    "name": "tf.Polygamma",
    "summary": "Compute the polygamma function \\\\(\\psi^{(n)}(x)\\\\).",
    "description": "The polygamma function is defined as:\n\n\n\\\\(\\psi^{(a)}(x) = \\frac{d^a}{dx^a} \\psi(x)\\\\)\n\nwhere \\\\(\\psi(x)\\\\) is the digamma function.\nThe polygamma function is defined only for non-negative integer orders \\\\a\\\\.",
    "inputs": [
      { "name": "a", "type": "TF_F32OrF64Tensor" },
      { "name": "x", "type": "TF_F32OrF64Tensor" }
    ],
    "outputs": [
      { "name": "z", "type": "TF_F32OrF64Tensor" }
    ]
  },
  {
    "name": "tf.PopulationCount",
    "summary": "Computes element-wise population count (a.k.a. popcount, bitsum, bitcount).",
    "description": "For each entry in `x`, calculates the number of `1` (on) bits in the binary\nrepresentation of that entry.\n\n**NOTE**: It is more efficient to first `tf.bitcast` your tensors into\n`int32` or `int64` and perform the bitcount on the result, than to feed in\n8- or 16-bit inputs and then aggregate the resulting counts.",
    "inputs": [
      { "name": "x", "type": "TF_IntTensor" }
    ],
    "outputs": [
      { "name": "y", "type": "TF_Uint8Tensor" }
    ]
  },
  {
    "name": "tf.Pow",
    "summary": "Computes the power of one value to another.",
    "description": "Given a tensor `x` and a tensor `y`, this operation computes \\\\(x^y\\\\) for\ncorresponding elements in `x` and `y`. For example:\n\n```\n# tensor 'x' is [[2, 2]], [3, 3]]\n# tensor 'y' is [[8, 16], [2, 3]]\ntf.pow(x, y) ==> [[256, 65536], [9, 27]]\n```",
    "inputs": [
      { "name": "x", "type": "TensorOf" },
      { "name": "y", "type": "TensorOf" }
    ],
    "outputs": [
      { "name": "z", "type": "TensorOf" }
    ]
  },
  {
    "name": "tf.PrefetchDataset",
    "summary": "Creates a dataset that asynchronously prefetches elements from `input_dataset`.",
    "inputs": [
      { "name": "input_dataset", "type": "TF_VariantTensor" },
      { "name": "buffer_size", "type": "Arg" }
    ],
    "outputs": [
      { "name": "handle", "type": "TF_VariantTensor" }
    ],
    "attributes": [
      { "name": "output_types", "type": "ConfinedAttr" },
      { "name": "output_shapes", "type": "ConfinedAttr" },
      { "name": "slack_period", "type": "DefaultValuedOptionalAttr" },
      { "name": "legacy_autotune", "type": "DefaultValuedOptionalAttr" },
      { "name": "buffer_size_min", "type": "DefaultValuedOptionalAttr" },
      { "name": "metadata", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.PreventGradient",
    "summary": "An identity op that triggers an error if a gradient is requested.",
    "description": "When executed in a graph, this op outputs its input tensor as-is.\n\nWhen building ops to compute gradients, the TensorFlow gradient system\nwill return an error when trying to lookup the gradient of this op,\nbecause no gradient must ever be registered for this function.  This\nop exists to prevent subtle bugs from silently returning unimplemented\ngradients in some corner cases.",
    "inputs": [
      { "name": "input", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "message", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.Print",
    "summary": "Prints a list of tensors.",
    "description": "Passes `input` through to `output` and prints `data` when evaluating.",
    "inputs": [
      { "name": "input", "type": "Arg" },
      { "name": "data", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "message", "type": "DefaultValuedOptionalAttr" },
      { "name": "first_n", "type": "DefaultValuedOptionalAttr" },
      { "name": "summarize", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.PrintV2",
    "summary": "Prints a string scalar.",
    "description": "Prints a string scalar to the desired output_stream.",
    "inputs": [
      { "name": "input", "type": "Arg" }
    ],
    "attributes": [
      { "name": "output_stream", "type": "DefaultValuedOptionalAttr" },
      { "name": "end", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.Prod",
    "summary": "Computes the product of elements across dimensions of a tensor.",
    "description": "Reduces `input` along the dimensions given in `axis`. Unless\n`keep_dims` is true, the rank of the tensor is reduced by 1 for each entry in\n`axis`. If `keep_dims` is true, the reduced dimensions are\nretained with length 1.",
    "inputs": [
      { "name": "input", "type": "Arg" },
      { "name": "reduction_indices", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "keep_dims", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.Qr",
    "summary": "Computes the QR decompositions of one or more matrices.",
    "description": "Computes the QR decomposition of each inner matrix in `tensor` such that\n`tensor[..., :, :] = q[..., :, :] * r[..., :,:])`\n\nCurrently, the gradient for the QR decomposition is well-defined only when\nthe first `P` columns of the inner matrix are linearly independent, where\n`P` is the minimum of `M` and `N`, the 2 inner-most dimmensions of `tensor`.\n\n```python\n# a is a tensor.\n# q is a tensor of orthonormal matrices.\n# r is a tensor of upper triangular matrices.\nq, r = qr(a)\nq_full, r_full = qr(a, full_matrices=True)\n```",
    "inputs": [
      { "name": "input", "type": "Arg" }
    ],
    "outputs": [
      { "name": "q", "type": "Res" },
      { "name": "r", "type": "Res" }
    ],
    "attributes": [
      { "name": "full_matrices", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.QuantizeAndDequantize",
    "summary": "Use QuantizeAndDequantizeV2 instead.",
    "inputs": [
      { "name": "input", "type": "TF_FloatTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "TF_FloatTensor" }
    ],
    "attributes": [
      { "name": "signed_input", "type": "DefaultValuedOptionalAttr" },
      { "name": "num_bits", "type": "DefaultValuedOptionalAttr" },
      { "name": "range_given", "type": "DefaultValuedOptionalAttr" },
      { "name": "input_min", "type": "DefaultValuedOptionalAttr" },
      { "name": "input_max", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.QuantizeAndDequantizeV2",
    "summary": "Quantizes then dequantizes a tensor.",
    "description": "This op simulates the precision loss from the quantized forward pass by:\n\n1. Quantizing the tensor to fixed point numbers, which should match the target\n   quantization method when it is used in inference.\n2. Dequantizing it back to floating point numbers for the following ops, most\n   likely matmul.\n\nThere are different ways to quantize. This version uses only scaling, so 0.0\nmaps to 0.\n\nFrom the specified 'num_bits' in the quantized output type, it determines\nminimum and maximum representable quantized values.\n\ne.g.\n\n*   [-128, 127] for signed, num_bits = 8, or\n*   [0, 255] for unsigned, num_bits = 8.\n\nIf range_given == False, the initial input_min, input_max will be determined\nautomatically as the minimum and maximum values in the input tensor, otherwise\nthe specified values of input_min, input_max are used.\n\nNote: If the input_min, input_max are specified, they do not need to equal the\nactual minimum and maximum values in the tensor. e.g. in some cases it may be\nbeneficial to specify these values such that the low probability extremes of the\ninput distribution are clipped.\n\nThis op determines the maximum scale_factor that would map the initial\n[input_min, input_max] range to a range that lies within the representable\nquantized range.\n\nIt determines the scale from one of input_min and input_max, then updates the\nother one to maximize the representable range.\n\ne.g.\n\n*   if the output is signed, num_bits = 8, [input_min, input_max] = [-10.0,\n    5.0]: it would use a scale_factor of -128 / -10.0 = 12.8 In this case, it\n    would update input_max to be 127 / 12.8 = 9.921875\n*   if the output is signed, num_bits = 8, [input_min, input_max] = [-10.0,\n    10.0]: it would use a scale_factor of 127 / 10.0 = 12.7 In this case, it\n    would update input_min to be 128.0 / 12.7 = -10.07874\n*   if the output is unsigned, input_min is forced to be 0, and only the\n    specified input_max is used.\n\nAfter determining the scale_factor and updating the input range, it applies the\nfollowing to each value in the 'input' tensor.\n\noutput = round(clamp(value, input_min, input_max) * scale_factor) / scale_factor.\n\nThe above round function rounds the value based on the given round_mode.",
    "inputs": [
      { "name": "input", "type": "Arg" },
      { "name": "input_min", "type": "Arg" },
      { "name": "input_max", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "TF_FloatTensor" }
    ],
    "attributes": [
      { "name": "signed_input", "type": "DefaultValuedOptionalAttr" },
      { "name": "num_bits", "type": "DefaultValuedOptionalAttr" },
      { "name": "range_given", "type": "DefaultValuedOptionalAttr" },
      { "name": "round_mode", "type": "DefaultValuedOptionalAttr" },
      { "name": "narrow_range", "type": "DefaultValuedOptionalAttr" },
      { "name": "axis", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.QuantizeAndDequantizeV3",
    "summary": "Quantizes then dequantizes a tensor.",
    "description": "This is almost identical to QuantizeAndDequantizeV2, except that num_bits is a\ntensor, so its value can change during training.",
    "inputs": [
      { "name": "input", "type": "TF_FloatTensor" },
      { "name": "input_min", "type": "TF_FloatTensor" },
      { "name": "input_max", "type": "TF_FloatTensor" },
      { "name": "num_bits", "type": "TF_Int32Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "TF_FloatTensor" }
    ],
    "attributes": [
      { "name": "signed_input", "type": "DefaultValuedOptionalAttr" },
      { "name": "range_given", "type": "DefaultValuedOptionalAttr" },
      { "name": "narrow_range", "type": "DefaultValuedOptionalAttr" },
      { "name": "axis", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.QuantizeAndDequantizeV4",
    "summary": "Quantizes then dequantizes a tensor.",
    "description": "This is almost identical to QuantizeAndDequantizeV2, except that it returns a\ngradient of 1 for inputs that are within the quantization range, or 0 otherwise.",
    "inputs": [
      { "name": "input", "type": "Arg" },
      { "name": "input_min", "type": "Arg" },
      { "name": "input_max", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "TF_FloatTensor" }
    ],
    "attributes": [
      { "name": "signed_input", "type": "DefaultValuedOptionalAttr" },
      { "name": "num_bits", "type": "DefaultValuedOptionalAttr" },
      { "name": "range_given", "type": "DefaultValuedOptionalAttr" },
      { "name": "round_mode", "type": "DefaultValuedOptionalAttr" },
      { "name": "narrow_range", "type": "DefaultValuedOptionalAttr" },
      { "name": "axis", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.QuantizeV2",
    "summary": "Quantize the 'input' tensor of type float to 'output' tensor of type 'T'.",
    "description": "[min_range, max_range] are scalar floats that specify the range for\nthe 'input' data. The 'mode' attribute controls exactly which calculations are\nused to convert the float values to their quantized equivalents.  The\n'round_mode' attribute controls which rounding tie-breaking algorithm is used\nwhen rounding float values to their quantized equivalents.\n\nIn 'MIN_COMBINED' mode, each value of the tensor will undergo the following:\n\n```\nout[i] = (in[i] - min_range) * range(T) / (max_range - min_range)\nif T == qint8: out[i] -= (range(T) + 1) / 2.0\n```\n\nhere `range(T) = numeric_limits<T>::max() - numeric_limits<T>::min()`\n\n*MIN_COMBINED Mode Example*\n\nAssume the input is type float and has a possible range of [0.0, 6.0] and the\noutput type is quint8 ([0, 255]). The min_range and max_range values should be\nspecified as 0.0 and 6.0. Quantizing from float to quint8 will multiply each\nvalue of the input by 255/6 and cast to quint8.\n\nIf the output type was qint8 ([-128, 127]), the operation will additionally\nsubtract each value by 128 prior to casting, so that the range of values aligns\nwith the range of qint8.\n\nIf the mode is 'MIN_FIRST', then this approach is used:\n\n```\nnum_discrete_values = 1 << (# of bits in T)\nrange_adjust = num_discrete_values / (num_discrete_values - 1)\nrange = (range_max - range_min) * range_adjust\nrange_scale = num_discrete_values / range\nquantized = round(input * range_scale) - round(range_min * range_scale) +\n  numeric_limits<T>::min()\nquantized = max(quantized, numeric_limits<T>::min())\nquantized = min(quantized, numeric_limits<T>::max())\n```\n\nThe biggest difference between this and MIN_COMBINED is that the minimum range\nis rounded first, before it's subtracted from the rounded value. With\nMIN_COMBINED, a small bias is introduced where repeated iterations of quantizing\nand dequantizing will introduce a larger and larger error.\n\n*SCALED mode Example*\n\n`SCALED` mode matches the quantization approach used in\n`QuantizeAndDequantize{V2|V3}`.\n\nIf the mode is `SCALED`, the quantization is performed by multiplying each\ninput value by a scaling_factor.\nThe scaling_factor is determined from `min_range` and `max_range` to be as large\nas possible such that the range from `min_range` to `max_range` is representable\nwithin values of type T.\n\n```c++\n\n  const int min_T = std::numeric_limits<T>::min();\n  const int max_T = std::numeric_limits<T>::max();\n  const float max_float = std::numeric_limits<float>::max();\n\n  const float scale_factor_from_min_side =\n      (min_T * min_range > 0) ? min_T / min_range : max_float;\n  const float scale_factor_from_max_side =\n      (max_T * max_range > 0) ? max_T / max_range : max_float;\n\n  const float scale_factor = std::min(scale_factor_from_min_side,\n                                      scale_factor_from_max_side);\n```\n\nWe next use the scale_factor to adjust min_range and max_range as follows:\n\n```c++\n      min_range = min_T / scale_factor;\n      max_range = max_T / scale_factor;\n```\n\n\ne.g. if T = qint8, and initially min_range = -10, and max_range = 9, we would\ncompare -128/-10.0 = 12.8 to 127/9.0 = 14.11, and set scaling_factor = 12.8\nIn this case, min_range would remain -10, but max_range would be adjusted to\n127 / 12.8 = 9.921875\n\nSo we will quantize input values in the range (-10, 9.921875) to (-128, 127).\n\nThe input tensor can now be quantized by clipping values to the range\n`min_range` to `max_range`, then multiplying by scale_factor as follows:\n\n```c++\nresult = round(min(max_range, max(min_range, input)) * scale_factor)\n```\n\nThe adjusted `min_range` and `max_range` are returned as outputs 2 and 3 of\nthis operation. These outputs should be used as the range for any further\ncalculations.\n\n\n*narrow_range (bool) attribute*\n\nIf true, we do not use the minimum quantized value.\ni.e. for int8 the quantized output, it would be restricted to the range\n-127..127 instead of the full -128..127 range.\nThis is provided for compatibility with certain inference backends.\n(Only applies to SCALED mode)\n\n\n*axis (int) attribute*\n\nAn optional `axis` attribute can specify a dimension index of the input tensor,\nsuch that quantization ranges will be calculated and applied separately for each\nslice of the tensor along that dimension. This is useful for per-channel\nquantization.\n\nIf axis is specified, min_range and max_range\n\nif `axis`=None, per-tensor quantization is performed as normal.\n\n\n*ensure_minimum_range (float) attribute*\n\nEnsures the minimum quantization range is at least this value.\nThe legacy default value for this is 0.01, but it is strongly suggested to\nset it to 0 for new uses.",
    "inputs": [
      { "name": "input", "type": "TF_Float32Tensor" },
      { "name": "min_range", "type": "Arg" },
      { "name": "max_range", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" },
      { "name": "output_min", "type": "Res" },
      { "name": "output_max", "type": "Res" }
    ],
    "attributes": [
      { "name": "mode", "type": "DefaultValuedOptionalAttr" },
      { "name": "round_mode", "type": "DefaultValuedOptionalAttr" },
      { "name": "narrow_range", "type": "DefaultValuedOptionalAttr" },
      { "name": "axis", "type": "DefaultValuedOptionalAttr" },
      { "name": "ensure_minimum_range", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.QueueDequeueV2",
    "summary": "Dequeues a tuple of one or more tensors from the given queue.",
    "description": "This operation has k outputs, where k is the number of components\nin the tuples stored in the given queue, and output i is the ith\ncomponent of the dequeued tuple.\n\nN.B. If the queue is empty, this operation will block until an element\nhas been dequeued (or 'timeout_ms' elapses, if specified).",
    "inputs": [
      { "name": "handle", "type": "Arg" }
    ],
    "outputs": [
      { "name": "components", "type": "Res" }
    ],
    "attributes": [
      { "name": "timeout_ms", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.RaggedGather",
    "summary": "Gather ragged slices from `params` axis `0` according to `indices`.",
    "description": "Outputs a `RaggedTensor` output composed from `output_dense_values` and\n`output_nested_splits`, such that:\n\n```python\noutput.shape = indices.shape + params.shape[1:]\noutput.ragged_rank = indices.shape.ndims + params.ragged_rank\noutput[i...j, d0...dn] = params[indices[i...j], d0...dn]\n```\n\nwhere\n\n* `params =\n   ragged.from_nested_row_splits(params_dense_values, params_nested_splits)`\n   provides the values that should be gathered.\n* `indices` ia a dense tensor with dtype `int32` or `int64`, indicating which\n   values should be gathered.\n* `output =\n   ragged.from_nested_row_splits(output_dense_values, output_nested_splits)`\n   is the output tensor.\n\n(Note: This c++ op is used to implement the higher-level python\n`tf.ragged.gather` op, which also supports ragged indices.)",
    "inputs": [
      { "name": "params_nested_splits", "type": "Arg" },
      { "name": "params_dense_values", "type": "Arg" },
      { "name": "indices", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output_nested_splits", "type": "Res" },
      { "name": "output_dense_values", "type": "Res" }
    ]
  },
  {
    "name": "tf.RaggedRange",
    "summary": "Returns a `RaggedTensor` containing the specified sequences of numbers.",
    "description": "Returns a `RaggedTensor` `result` composed from `rt_dense_values` and\n`rt_nested_splits`, such that\n`result[i] = range(starts[i], limits[i], deltas[i])`.\n\n```python\n(rt_nested_splits, rt_dense_values) = ragged_range(\n      starts=[2, 5, 8], limits=[3, 5, 12], deltas=1)\nresult = tf.ragged.from_row_splits(rt_dense_values, rt_nested_splits)\nprint(result)\n<tf.RaggedTensor [[2], [], [8, 9, 10, 11]] >\n```\n\nThe input tensors `starts`, `limits`, and `deltas` may be scalars or vectors.\nThe vector inputs must all have the same size.  Scalar inputs are broadcast\nto match the size of the vector inputs.",
    "inputs": [
      { "name": "starts", "type": "Arg" },
      { "name": "limits", "type": "Arg" },
      { "name": "deltas", "type": "Arg" }
    ],
    "outputs": [
      { "name": "rt_nested_splits", "type": "Res" },
      { "name": "rt_dense_values", "type": "Res" }
    ]
  },
  {
    "name": "tf.RandomGamma",
    "summary": "Outputs random values from the Gamma distribution(s) described by alpha.",
    "description": "This op uses the algorithm by Marsaglia et al. to acquire samples via\ntransformation-rejection from pairs of uniform and normal random variables.\nSee http://dl.acm.org/citation.cfm?id=358414",
    "inputs": [
      { "name": "shape", "type": "Arg" },
      { "name": "alpha", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "seed", "type": "DefaultValuedOptionalAttr" },
      { "name": "seed2", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.RandomGammaGrad",
    "summary": "Computes the derivative of a Gamma random sample w.r.t. `alpha`.",
    "inputs": [
      { "name": "alpha", "type": "TF_F32OrF64Tensor" },
      { "name": "sample", "type": "TF_F32OrF64Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "TF_F32OrF64Tensor" }
    ]
  },
  {
    "name": "tf.RandomPoisson",
    "summary": "Use RandomPoissonV2 instead.",
    "inputs": [
      { "name": "shape", "type": "TF_I32OrI64Tensor" },
      { "name": "rate", "type": "TensorOf" }
    ],
    "outputs": [
      { "name": "output", "type": "TensorOf" }
    ],
    "attributes": [
      { "name": "seed", "type": "DefaultValuedOptionalAttr" },
      { "name": "seed2", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.RandomPoissonV2",
    "summary": "Outputs random values from the Poisson distribution(s) described by rate.",
    "description": "This op uses two algorithms, depending on rate. If rate >= 10, then\nthe algorithm by Hormann is used to acquire samples via\ntransformation-rejection.\nSee http://www.sciencedirect.com/science/article/pii/0167668793909974.\n\nOtherwise, Knuth's algorithm is used to acquire samples via multiplying uniform\nrandom variables.\nSee Donald E. Knuth (1969). Seminumerical Algorithms. The Art of Computer\nProgramming, Volume 2. Addison Wesley",
    "inputs": [
      { "name": "shape", "type": "Arg" },
      { "name": "rate", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "seed", "type": "DefaultValuedOptionalAttr" },
      { "name": "seed2", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.RandomShuffle",
    "summary": "Randomly shuffles a tensor along its first dimension.",
    "description": "The tensor is shuffled along dimension 0, such that each `value[j]` is mapped\n  to one and only one `output[i]`. For example, a mapping that might occur for a\n  3x2 tensor is:\n\n```\n[[1, 2],       [[5, 6],\n [3, 4],  ==>   [1, 2],\n [5, 6]]        [3, 4]]\n```",
    "inputs": [
      { "name": "value", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "seed", "type": "DefaultValuedOptionalAttr" },
      { "name": "seed2", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.RandomStandardNormal",
    "summary": "Outputs random values from a normal distribution.",
    "description": "The generated values will have mean 0 and standard deviation 1.",
    "inputs": [
      { "name": "shape", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "seed", "type": "DefaultValuedOptionalAttr" },
      { "name": "seed2", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.RandomUniform",
    "summary": "Outputs random values from a uniform distribution.",
    "description": "The generated values follow a uniform distribution in the range `[0, 1)`. The\nlower bound 0 is included in the range, while the upper bound 1 is excluded.",
    "inputs": [
      { "name": "shape", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "seed", "type": "DefaultValuedOptionalAttr" },
      { "name": "seed2", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.RandomUniformInt",
    "summary": "Outputs random integers from a uniform distribution.",
    "description": "The generated values are uniform integers in the range `[minval, maxval)`.\nThe lower bound `minval` is included in the range, while the upper bound\n`maxval` is excluded.\n\nThe random integers are slightly biased unless `maxval - minval` is an exact\npower of two.  The bias is small for values of `maxval - minval` significantly\nsmaller than the range of the output (either `2^32` or `2^64`).",
    "inputs": [
      { "name": "shape", "type": "Arg" },
      { "name": "minval", "type": "Arg" },
      { "name": "maxval", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "seed", "type": "DefaultValuedOptionalAttr" },
      { "name": "seed2", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.Range",
    "summary": "Creates a sequence of numbers.",
    "description": "This operation creates a sequence of numbers that begins at `start` and\nextends by increments of `delta` up to but not including `limit`.\n\nFor example:\n\n```\n# 'start' is 3\n# 'limit' is 18\n# 'delta' is 3\ntf.range(start, limit, delta) ==> [3, 6, 9, 12, 15]\n```",
    "inputs": [
      { "name": "start", "type": "Arg" },
      { "name": "limit", "type": "Arg" },
      { "name": "delta", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ]
  },
  {
    "name": "tf.RangeDataset",
    "summary": "Creates a dataset with a range of values. Corresponds to python's xrange.",
    "inputs": [
      { "name": "start", "type": "Arg" },
      { "name": "stop", "type": "Arg" },
      { "name": "step", "type": "Arg" }
    ],
    "outputs": [
      { "name": "handle", "type": "TF_VariantTensor" }
    ],
    "attributes": [
      { "name": "output_types", "type": "ConfinedAttr" },
      { "name": "output_shapes", "type": "ConfinedAttr" },
      { "name": "metadata", "type": "DefaultValuedOptionalAttr" },
      { "name": "replicate_on_split", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.Rank",
    "summary": "Returns the rank of a tensor.",
    "description": "This operation returns an integer representing the rank of `input`.\n\nFor example:\n\n```\n# 't' is [[[1, 1, 1], [2, 2, 2]], [[3, 3, 3], [4, 4, 4]]]\n# shape of tensor 't' is [2, 2, 3]\nrank(t) ==> 3\n```\n\n**Note**: The rank of a tensor is not the same as the rank of a matrix. The rank\nof a tensor is the number of indices required to uniquely select each element\nof the tensor. Rank is also known as \"order\", \"degree\", or \"ndims.\"",
    "inputs": [
      { "name": "input", "type": "TF_Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "TF_Int32Tensor" }
    ]
  },
  {
    "name": "tf.ReadFile",
    "summary": "Reads and outputs the entire contents of the input filename.",
    "inputs": [
      { "name": "filename", "type": "TF_StrTensor" }
    ],
    "outputs": [
      { "name": "contents", "type": "TF_StrTensor" }
    ]
  },
  {
    "name": "tf.ReadVariableOp",
    "summary": "Reads the value of a variable.",
    "description": "The tensor returned by this operation is immutable.\n\nThe value returned by this operation is guaranteed to be influenced by all the\nwrites on which this operation depends directly or indirectly, and to not be\ninfluenced by any of the writes which depend directly or indirectly on this\noperation.",
    "inputs": [
      { "name": "resource", "type": "Arg" }
    ],
    "outputs": [
      { "name": "value", "type": "TF_Tensor" }
    ]
  },
  {
    "name": "tf.Real",
    "summary": "Returns the real part of a complex number.",
    "description": "Given a tensor `input` of complex numbers, this operation returns a tensor of\ntype `float` that is the real part of each element in `input`. All elements in\n`input` must be complex numbers of the form \\\\(a + bj\\\\), where *a* is the real\n part returned by this operation and *b* is the imaginary part.\n\nFor example:\n\n```\n# tensor 'input' is [-2.25 + 4.75j, 3.25 + 5.75j]\ntf.real(input) ==> [-2.25, 3.25]\n```",
    "inputs": [
      { "name": "input", "type": "TensorOf" }
    ],
    "outputs": [
      { "name": "output", "type": "TF_F32OrF64Tensor" }
    ]
  },
  {
    "name": "tf.RealDiv",
    "summary": "Returns x / y element-wise for real types.",
    "description": "If `x` and `y` are reals, this will return the floating-point division.\n\n*NOTE*: `Div` supports broadcasting. More about broadcasting\n[here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)",
    "inputs": [
      { "name": "x", "type": "TensorOf" },
      { "name": "y", "type": "TensorOf" }
    ],
    "outputs": [
      { "name": "z", "type": "TensorOf" }
    ]
  },
  {
    "name": "tf.Reciprocal",
    "summary": "Computes the reciprocal of x element-wise.",
    "description": "I.e., \\\\(y = 1 / x\\\\).",
    "inputs": [
      { "name": "x", "type": "TensorOf" }
    ],
    "outputs": [
      { "name": "y", "type": "TensorOf" }
    ]
  },
  {
    "name": "tf.ReciprocalGrad",
    "summary": "Computes the gradient for the inverse of `x` wrt its input.",
    "description": "Specifically, `grad = -dy * y*y`, where `y = 1/x`, and `dy`\nis the corresponding input gradient.",
    "inputs": [
      { "name": "y", "type": "TF_FpOrComplexTensor" },
      { "name": "dy", "type": "TF_FpOrComplexTensor" }
    ],
    "outputs": [
      { "name": "z", "type": "TF_FpOrComplexTensor" }
    ]
  },
  {
    "name": "tf.Recv",
    "summary": "Receives the named tensor from send_device on recv_device.",
    "outputs": [
      { "name": "tensor", "type": "Res" }
    ],
    "attributes": [
      { "name": "tensor_name", "type": "StrAttr" },
      { "name": "send_device", "type": "StrAttr" },
      { "name": "send_device_incarnation", "type": "I64Attr" },
      { "name": "recv_device", "type": "StrAttr" },
      { "name": "client_terminated", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.RecvTPUEmbeddingActivations",
    "summary": "An op that receives embedding activations on the TPU.",
    "description": "The TPU system performs the embedding lookups and aggregations specified by\nthe arguments to TPUEmbeddingEnqueue(Integer/Sparse/SparseTensor)Batch. The\nresults of these aggregations are visible to the Tensorflow Graph as the\noutputs of a RecvTPUEmbeddingActivations op. This op returns a list containing\none Tensor of activations per table specified in the model. There can be at\nmost one RecvTPUEmbeddingActivations op in the TPU graph.",
    "outputs": [
      { "name": "outputs", "type": "Res" }
    ],
    "attributes": [
      { "name": "config", "type": "StrAttr" }
    ]
  },
  {
    "name": "tf.ReduceDataset",
    "summary": "Reduces the input dataset to a singleton using a reduce function.",
    "inputs": [
      { "name": "input_dataset", "type": "TF_VariantTensor" },
      { "name": "initial_state", "type": "Variadic" },
      { "name": "other_arguments", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "components", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "f", "type": "SymbolRefAttr" },
      { "name": "Tstate", "type": "ConfinedAttr" },
      { "name": "Targuments", "type": "ConfinedAttr" },
      { "name": "output_types", "type": "ConfinedAttr" },
      { "name": "output_shapes", "type": "ConfinedAttr" },
      { "name": "use_inter_op_parallelism", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.ReduceJoin",
    "summary": "Joins a string Tensor across the given dimensions.",
    "description": "Computes the string join across dimensions in the given string Tensor of shape\n`[\\\\(d_0, d_1, ..., d_{n-1}\\\\)]`.  Returns a new Tensor created by joining the input\nstrings with the given separator (default: empty string).  Negative indices are\ncounted backwards from the end, with `-1` being equivalent to `n - 1`.  If\nindices are not specified, joins across all dimensions beginning from `n - 1`\nthrough `0`.\n\nFor example:\n\n```python\n# tensor `a` is [[\"a\", \"b\"], [\"c\", \"d\"]]\ntf.reduce_join(a, 0) ==> [\"ac\", \"bd\"]\ntf.reduce_join(a, 1) ==> [\"ab\", \"cd\"]\ntf.reduce_join(a, -2) = tf.reduce_join(a, 0) ==> [\"ac\", \"bd\"]\ntf.reduce_join(a, -1) = tf.reduce_join(a, 1) ==> [\"ab\", \"cd\"]\ntf.reduce_join(a, 0, keep_dims=True) ==> [[\"ac\", \"bd\"]]\ntf.reduce_join(a, 1, keep_dims=True) ==> [[\"ab\"], [\"cd\"]]\ntf.reduce_join(a, 0, separator=\".\") ==> [\"a.c\", \"b.d\"]\ntf.reduce_join(a, [0, 1]) ==> \"acbd\"\ntf.reduce_join(a, [1, 0]) ==> \"abcd\"\ntf.reduce_join(a, []) ==> [[\"a\", \"b\"], [\"c\", \"d\"]]\ntf.reduce_join(a) = tf.reduce_join(a, [1, 0]) ==> \"abcd\"\n```",
    "inputs": [
      { "name": "inputs", "type": "Arg" },
      { "name": "reduction_indices", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "keep_dims", "type": "DefaultValuedOptionalAttr" },
      { "name": "separator", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.Relu",
    "summary": "Computes rectified linear: `max(features, 0)`.",
    "description": "See: https://en.wikipedia.org/wiki/Rectifier_(neural_networks)\nExample usage:\n>>> tf.nn.relu([-2., 0., 3.]).numpy()\narray([0., 0., 3.], dtype=float32)",
    "inputs": [
      { "name": "features", "type": "TensorOf" }
    ],
    "outputs": [
      { "name": "activations", "type": "TensorOf" }
    ],
    "category": "Activation"
  },
  {
    "name": "tf.Relu6",
    "summary": "Computes rectified linear 6: `min(max(features, 0), 6)`.",
    "inputs": [
      { "name": "features", "type": "TF_IntOrFpTensor" }
    ],
    "outputs": [
      { "name": "activations", "type": "TF_IntOrFpTensor" }
    ]
  },
  {
    "name": "tf.Relu6Grad",
    "summary": "Computes rectified linear 6 gradients for a Relu6 operation.",
    "inputs": [
      { "name": "gradients", "type": "Arg" },
      { "name": "features", "type": "Arg" }
    ],
    "outputs": [
      { "name": "backprops", "type": "Res" }
    ]
  },
  {
    "name": "tf.ReluGrad",
    "summary": "Computes rectified linear gradients for a Relu operation.",
    "inputs": [
      { "name": "gradients", "type": "Arg" },
      { "name": "features", "type": "Arg" }
    ],
    "outputs": [
      { "name": "backprops", "type": "Res" }
    ]
  },
  {
    "name": "tf.RemoteCall",
    "summary": "Runs function `f` on a remote device indicated by `target`.",
    "inputs": [
      { "name": "target", "type": "Arg" },
      { "name": "args", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "f", "type": "SymbolRefAttr" }
    ]
  },
  {
    "name": "tf.RepeatDataset",
    "summary": "Creates a dataset that emits the outputs of `input_dataset` `count` times.",
    "inputs": [
      { "name": "input_dataset", "type": "TF_VariantTensor" },
      { "name": "count", "type": "Arg" }
    ],
    "outputs": [
      { "name": "handle", "type": "TF_VariantTensor" }
    ],
    "attributes": [
      { "name": "output_types", "type": "ConfinedAttr" },
      { "name": "output_shapes", "type": "ConfinedAttr" },
      { "name": "metadata", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.Reshape",
    "summary": "Reshapes a tensor.",
    "description": "Given `tensor`, this operation returns a tensor that has the same values\nas `tensor` with shape `shape`.\n\nIf one component of 1-D tensor `shape` is the special value -1, the size of that\ndimension is computed so that the total size remains constant.  In particular, a\n`shape` of `[-1]` flattens into 1-D.  At most one component of `shape` may be\nunknown.\n\nThe `shape` must be 1-D and the operation returns a tensor with shape\n`shape` filled with the values of `tensor`. In this case, the number of elements\nimplied by `shape` must be the same as the number of elements in `tensor`.\n\nIt is an error if `shape` is not 1-D.\n\nFor example:\n\n```\n# tensor 't' is [1, 2, 3, 4, 5, 6, 7, 8, 9]\n# tensor 't' has shape [9]\nreshape(t, [3, 3]) ==> [[1, 2, 3],\n                        [4, 5, 6],\n                        [7, 8, 9]]\n\n# tensor 't' is [[[1, 1], [2, 2]],\n#                [[3, 3], [4, 4]]]\n# tensor 't' has shape [2, 2, 2]\nreshape(t, [2, 4]) ==> [[1, 1, 2, 2],\n                        [3, 3, 4, 4]]\n\n# tensor 't' is [[[1, 1, 1],\n#                 [2, 2, 2]],\n#                [[3, 3, 3],\n#                 [4, 4, 4]],\n#                [[5, 5, 5],\n#                 [6, 6, 6]]]\n# tensor 't' has shape [3, 2, 3]\n# pass '[-1]' to flatten 't'\nreshape(t, [-1]) ==> [1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4, 5, 5, 5, 6, 6, 6]\n\n# -1 can also be used to infer the shape\n\n# -1 is inferred to be 9:\nreshape(t, [2, -1]) ==> [[1, 1, 1, 2, 2, 2, 3, 3, 3],\n                         [4, 4, 4, 5, 5, 5, 6, 6, 6]]\n# -1 is inferred to be 2:\nreshape(t, [-1, 9]) ==> [[1, 1, 1, 2, 2, 2, 3, 3, 3],\n                         [4, 4, 4, 5, 5, 5, 6, 6, 6]]\n# -1 is inferred to be 3:\nreshape(t, [ 2, -1, 3]) ==> [[[1, 1, 1],\n                              [2, 2, 2],\n                              [3, 3, 3]],\n                             [[4, 4, 4],\n                              [5, 5, 5],\n                              [6, 6, 6]]]\n\n# tensor 't' is [7]\n# shape `[]` reshapes to a scalar\nreshape(t, []) ==> 7\n```",
    "inputs": [
      { "name": "tensor", "type": "TF_Tensor" },
      { "name": "shape", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "TF_Tensor" }
    ],
    "category": "Shape"
  },
  {
    "name": "tf.ResizeBilinear",
    "summary": "Resize `images` to `size` using bilinear interpolation.",
    "description": "Input images can be of different types but output images are always float.",
    "inputs": [
      { "name": "images", "type": "Arg" },
      { "name": "size", "type": "Arg" }
    ],
    "outputs": [
      { "name": "resized_images", "type": "Res" }
    ],
    "attributes": [
      { "name": "align_corners", "type": "DefaultValuedOptionalAttr" },
      { "name": "half_pixel_centers", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.ResizeBilinearGrad",
    "summary": "Computes the gradient of bilinear interpolation.",
    "inputs": [
      { "name": "grads", "type": "Arg" },
      { "name": "original_image", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "align_corners", "type": "DefaultValuedOptionalAttr" },
      { "name": "half_pixel_centers", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.ResizeNearestNeighbor",
    "summary": "Resize `images` to `size` using nearest neighbor interpolation.",
    "inputs": [
      { "name": "images", "type": "Arg" },
      { "name": "size", "type": "Arg" }
    ],
    "outputs": [
      { "name": "resized_images", "type": "Res" }
    ],
    "attributes": [
      { "name": "align_corners", "type": "DefaultValuedOptionalAttr" },
      { "name": "half_pixel_centers", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.ResizeNearestNeighborGrad",
    "summary": "Computes the gradient of nearest neighbor interpolation.",
    "inputs": [
      { "name": "grads", "type": "Arg" },
      { "name": "size", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "align_corners", "type": "DefaultValuedOptionalAttr" },
      { "name": "half_pixel_centers", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.ResourceApplyAdadelta",
    "summary": "Update '*var' according to the adadelta scheme.",
    "description": "accum = rho() * accum + (1 - rho()) * grad.square();\nupdate = (update_accum + epsilon).sqrt() * (accum + epsilon()).rsqrt() * grad;\nupdate_accum = rho() * update_accum + (1 - rho()) * update.square();\nvar -= update;",
    "inputs": [
      { "name": "var", "type": "Arg" },
      { "name": "accum", "type": "Arg" },
      { "name": "accum_update", "type": "Arg" },
      { "name": "lr", "type": "Arg" },
      { "name": "rho", "type": "Arg" },
      { "name": "epsilon", "type": "Arg" },
      { "name": "grad", "type": "Arg" }
    ],
    "attributes": [
      { "name": "use_locking", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.ResourceApplyAdagrad",
    "summary": "Update '*var' according to the adagrad scheme.",
    "description": "accum += grad * grad\nvar -= lr * grad * (1 / sqrt(accum))",
    "inputs": [
      { "name": "var", "type": "Arg" },
      { "name": "accum", "type": "Arg" },
      { "name": "lr", "type": "Arg" },
      { "name": "grad", "type": "Arg" }
    ],
    "attributes": [
      { "name": "use_locking", "type": "DefaultValuedOptionalAttr" },
      { "name": "update_slots", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.ResourceApplyAdagradDA",
    "summary": "Update '*var' according to the proximal adagrad scheme.",
    "inputs": [
      { "name": "var", "type": "Arg" },
      { "name": "gradient_accumulator", "type": "Arg" },
      { "name": "gradient_squared_accumulator", "type": "Arg" },
      { "name": "grad", "type": "Arg" },
      { "name": "lr", "type": "Arg" },
      { "name": "l1", "type": "Arg" },
      { "name": "l2", "type": "Arg" },
      { "name": "global_step", "type": "Arg" }
    ],
    "attributes": [
      { "name": "use_locking", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.ResourceApplyAdagradV2",
    "summary": "Update '*var' according to the adagrad scheme.",
    "description": "accum += grad * grad\nvar -= lr * grad * (1 / (sqrt(accum) + epsilon))",
    "inputs": [
      { "name": "var", "type": "Arg" },
      { "name": "accum", "type": "Arg" },
      { "name": "lr", "type": "Arg" },
      { "name": "epsilon", "type": "Arg" },
      { "name": "grad", "type": "Arg" }
    ],
    "attributes": [
      { "name": "use_locking", "type": "DefaultValuedOptionalAttr" },
      { "name": "update_slots", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.ResourceApplyAdam",
    "summary": "Update '*var' according to the Adam algorithm.",
    "description": "$$\\text{lr}_t := \\mathrm{lr} \\cdot \\frac{\\sqrt{1 - \\beta_2^t}}{1 - \\beta_1^t}$$\n$$m_t := \\beta_1 \\cdot m_{t-1} + (1 - \\beta_1) \\cdot g$$\n$$v_t := \\beta_2 \\cdot v_{t-1} + (1 - \\beta_2) \\cdot g^2$$\n$$\\text{var} := \\begin{cases} \\text{var} - (m_t \\beta_1 + g \\cdot (1 - \\beta_1))\\cdot\\text{lr}_t/(\\sqrt{v_t} + \\epsilon), &\\text{if use_nesterov}\\\\\\\\  \\text{var} - m_t \\cdot \\text{lr}_t /(\\sqrt{v_t} + \\epsilon), &\\text{otherwise} \\end{cases}$$",
    "inputs": [
      { "name": "var", "type": "Arg" },
      { "name": "m", "type": "Arg" },
      { "name": "v", "type": "Arg" },
      { "name": "beta1_power", "type": "Arg" },
      { "name": "beta2_power", "type": "Arg" },
      { "name": "lr", "type": "Arg" },
      { "name": "beta1", "type": "Arg" },
      { "name": "beta2", "type": "Arg" },
      { "name": "epsilon", "type": "Arg" },
      { "name": "grad", "type": "Arg" }
    ],
    "attributes": [
      { "name": "use_locking", "type": "DefaultValuedOptionalAttr" },
      { "name": "use_nesterov", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.ResourceApplyAdaMax",
    "summary": "Update '*var' according to the AdaMax algorithm.",
    "description": "m_t <- beta1 * m_{t-1} + (1 - beta1) * g\nv_t <- max(beta2 * v_{t-1}, abs(g))\nvariable <- variable - learning_rate / (1 - beta1^t) * m_t / (v_t + epsilon)",
    "inputs": [
      { "name": "var", "type": "Arg" },
      { "name": "m", "type": "Arg" },
      { "name": "v", "type": "Arg" },
      { "name": "beta1_power", "type": "Arg" },
      { "name": "lr", "type": "Arg" },
      { "name": "beta1", "type": "Arg" },
      { "name": "beta2", "type": "Arg" },
      { "name": "epsilon", "type": "Arg" },
      { "name": "grad", "type": "Arg" }
    ],
    "attributes": [
      { "name": "use_locking", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.ResourceApplyAddSign",
    "summary": "Update '*var' according to the AddSign update.",
    "description": "m_t <- beta1 * m_{t-1} + (1 - beta1) * g\nupdate <- (alpha + sign_decay * sign(g) *sign(m)) * g\nvariable <- variable - lr_t * update",
    "inputs": [
      { "name": "var", "type": "Arg" },
      { "name": "m", "type": "Arg" },
      { "name": "lr", "type": "Arg" },
      { "name": "alpha", "type": "Arg" },
      { "name": "sign_decay", "type": "Arg" },
      { "name": "beta", "type": "Arg" },
      { "name": "grad", "type": "Arg" }
    ],
    "attributes": [
      { "name": "use_locking", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.ResourceApplyCenteredRMSProp",
    "summary": "Update '*var' according to the centered RMSProp algorithm.",
    "description": "The centered RMSProp algorithm uses an estimate of the centered second moment\n(i.e., the variance) for normalization, as opposed to regular RMSProp, which\nuses the (uncentered) second moment. This often helps with training, but is\nslightly more expensive in terms of computation and memory.\n\nNote that in dense implementation of this algorithm, mg, ms, and mom will\nupdate even if the grad is zero, but in this sparse implementation, mg, ms,\nand mom will not update in iterations during which the grad is zero.\n\nmean_square = decay * mean_square + (1-decay) * gradient ** 2\nmean_grad = decay * mean_grad + (1-decay) * gradient\n\nDelta = learning_rate * gradient / sqrt(mean_square + epsilon - mean_grad ** 2)\n\nmg <- rho * mg_{t-1} + (1-rho) * grad\nms <- rho * ms_{t-1} + (1-rho) * grad * grad\nmom <- momentum * mom_{t-1} + lr * grad / sqrt(ms - mg * mg + epsilon)\nvar <- var - mom",
    "inputs": [
      { "name": "var", "type": "Arg" },
      { "name": "mg", "type": "Arg" },
      { "name": "ms", "type": "Arg" },
      { "name": "mom", "type": "Arg" },
      { "name": "lr", "type": "Arg" },
      { "name": "rho", "type": "Arg" },
      { "name": "momentum", "type": "Arg" },
      { "name": "epsilon", "type": "Arg" },
      { "name": "grad", "type": "Arg" }
    ],
    "attributes": [
      { "name": "use_locking", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.ResourceApplyFtrl",
    "summary": "Update '*var' according to the Ftrl-proximal scheme.",
    "description": "accum_new = accum + grad * grad\nlinear += grad - (accum_new^(-lr_power) - accum^(-lr_power)) / lr * var\nquadratic = 1.0 / (accum_new^(lr_power) * lr) + 2 * l2\nvar = (sign(linear) * l1 - linear) / quadratic if |linear| > l1 else 0.0\naccum = accum_new",
    "inputs": [
      { "name": "var", "type": "Arg" },
      { "name": "accum", "type": "Arg" },
      { "name": "linear", "type": "Arg" },
      { "name": "grad", "type": "Arg" },
      { "name": "lr", "type": "Arg" },
      { "name": "l1", "type": "Arg" },
      { "name": "l2", "type": "Arg" },
      { "name": "lr_power", "type": "Arg" }
    ],
    "attributes": [
      { "name": "use_locking", "type": "DefaultValuedOptionalAttr" },
      { "name": "multiply_linear_by_lr", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.ResourceApplyFtrlV2",
    "summary": "Update '*var' according to the Ftrl-proximal scheme.",
    "description": "accum_new = accum + grad * grad\ngrad_with_shrinkage = grad + 2 * l2_shrinkage * var\nlinear += grad_with_shrinkage +\n    (accum_new^(-lr_power) - accum^(-lr_power)) / lr * var\nquadratic = 1.0 / (accum_new^(lr_power) * lr) + 2 * l2\nvar = (sign(linear) * l1 - linear) / quadratic if |linear| > l1 else 0.0\naccum = accum_new",
    "inputs": [
      { "name": "var", "type": "Arg" },
      { "name": "accum", "type": "Arg" },
      { "name": "linear", "type": "Arg" },
      { "name": "grad", "type": "Arg" },
      { "name": "lr", "type": "Arg" },
      { "name": "l1", "type": "Arg" },
      { "name": "l2", "type": "Arg" },
      { "name": "l2_shrinkage", "type": "TF_NumberTensor" },
      { "name": "lr_power", "type": "Arg" }
    ],
    "attributes": [
      { "name": "use_locking", "type": "DefaultValuedOptionalAttr" },
      { "name": "multiply_linear_by_lr", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.ResourceApplyGradientDescent",
    "summary": "Update '*var' by subtracting 'alpha' * 'delta' from it.",
    "inputs": [
      { "name": "var", "type": "Arg" },
      { "name": "alpha", "type": "Arg" },
      { "name": "delta", "type": "Arg" }
    ],
    "attributes": [
      { "name": "use_locking", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.ResourceApplyKerasMomentum",
    "summary": "Update '*var' according to the momentum scheme.",
    "description": "Set use_nesterov = True if you want to use Nesterov momentum.\n\naccum = accum * momentum - lr * grad\nvar += accum",
    "inputs": [
      { "name": "var", "type": "Arg" },
      { "name": "accum", "type": "Arg" },
      { "name": "lr", "type": "Arg" },
      { "name": "grad", "type": "Arg" },
      { "name": "momentum", "type": "Arg" }
    ],
    "attributes": [
      { "name": "use_locking", "type": "DefaultValuedOptionalAttr" },
      { "name": "use_nesterov", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.ResourceApplyMomentum",
    "summary": "Update '*var' according to the momentum scheme.",
    "description": "Set use_nesterov = True if you want to use Nesterov momentum.\n\naccum = accum * momentum + grad\nvar -= lr * accum",
    "inputs": [
      { "name": "var", "type": "Arg" },
      { "name": "accum", "type": "Arg" },
      { "name": "lr", "type": "Arg" },
      { "name": "grad", "type": "Arg" },
      { "name": "momentum", "type": "Arg" }
    ],
    "attributes": [
      { "name": "use_locking", "type": "DefaultValuedOptionalAttr" },
      { "name": "use_nesterov", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.ResourceApplyPowerSign",
    "summary": "Update '*var' according to the AddSign update.",
    "description": "m_t <- beta1 * m_{t-1} + (1 - beta1) * g\nupdate <- exp(logbase * sign_decay * sign(g) * sign(m_t)) * g\nvariable <- variable - lr_t * update",
    "inputs": [
      { "name": "var", "type": "Arg" },
      { "name": "m", "type": "Arg" },
      { "name": "lr", "type": "Arg" },
      { "name": "logbase", "type": "Arg" },
      { "name": "sign_decay", "type": "Arg" },
      { "name": "beta", "type": "Arg" },
      { "name": "grad", "type": "Arg" }
    ],
    "attributes": [
      { "name": "use_locking", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.ResourceApplyProximalAdagrad",
    "summary": "Update '*var' and '*accum' according to FOBOS with Adagrad learning rate.",
    "description": "accum += grad * grad\nprox_v = var - lr * grad * (1 / sqrt(accum))\nvar = sign(prox_v)/(1+lr*l2) * max{|prox_v|-lr*l1,0}",
    "inputs": [
      { "name": "var", "type": "Arg" },
      { "name": "accum", "type": "Arg" },
      { "name": "lr", "type": "Arg" },
      { "name": "l1", "type": "Arg" },
      { "name": "l2", "type": "Arg" },
      { "name": "grad", "type": "Arg" }
    ],
    "attributes": [
      { "name": "use_locking", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.ResourceApplyProximalGradientDescent",
    "summary": "Update '*var' as FOBOS algorithm with fixed learning rate.",
    "description": "prox_v = var - alpha * delta\nvar = sign(prox_v)/(1+alpha*l2) * max{|prox_v|-alpha*l1,0}",
    "inputs": [
      { "name": "var", "type": "Arg" },
      { "name": "alpha", "type": "Arg" },
      { "name": "l1", "type": "Arg" },
      { "name": "l2", "type": "Arg" },
      { "name": "delta", "type": "Arg" }
    ],
    "attributes": [
      { "name": "use_locking", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.ResourceApplyRMSProp",
    "summary": "Update '*var' according to the RMSProp algorithm.",
    "description": "Note that in dense implementation of this algorithm, ms and mom will\nupdate even if the grad is zero, but in this sparse implementation, ms\nand mom will not update in iterations during which the grad is zero.\n\nmean_square = decay * mean_square + (1-decay) * gradient ** 2\nDelta = learning_rate * gradient / sqrt(mean_square + epsilon)\n\nms <- rho * ms_{t-1} + (1-rho) * grad * grad\nmom <- momentum * mom_{t-1} + lr * grad / sqrt(ms + epsilon)\nvar <- var - mom",
    "inputs": [
      { "name": "var", "type": "Arg" },
      { "name": "ms", "type": "Arg" },
      { "name": "mom", "type": "Arg" },
      { "name": "lr", "type": "Arg" },
      { "name": "rho", "type": "Arg" },
      { "name": "momentum", "type": "TF_NumberTensor" },
      { "name": "epsilon", "type": "Arg" },
      { "name": "grad", "type": "Arg" }
    ],
    "attributes": [
      { "name": "use_locking", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.ResourceGather",
    "summary": "Gather slices from the variable pointed to by `resource` according to `indices`.",
    "description": "`indices` must be an integer tensor of any dimension (usually 0-D or 1-D).\nProduces an output tensor with shape `indices.shape + params.shape[1:]` where:\n\n```python\n    # Scalar indices\n    output[:, ..., :] = params[indices, :, ... :]\n\n    # Vector indices\n    output[i, :, ..., :] = params[indices[i], :, ... :]\n\n    # Higher rank indices\n    output[i, ..., j, :, ... :] = params[indices[i, ..., j], :, ..., :]\n```",
    "inputs": [
      { "name": "resource", "type": "Arg" },
      { "name": "indices", "type": "TF_I32OrI64Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "TF_Tensor" }
    ],
    "attributes": [
      { "name": "batch_dims", "type": "DefaultValuedOptionalAttr" },
      { "name": "validate_indices", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.ResourceGatherNd",
    "summary": "GatherNd on a resource.",
    "description": "This op reads the variable referenced by the first argument, and\nthen performs a GatherNd operation on it.",
    "inputs": [
      { "name": "resource", "type": "Arg" },
      { "name": "indices", "type": "TF_I32OrI64Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "TF_Tensor" }
    ]
  },
  {
    "name": "tf.ResourceScatterAdd",
    "summary": "Adds sparse updates to the variable referenced by `resource`.",
    "description": "This operation computes\n\n    # Scalar indices\n    ref[indices, ...] += updates[...]\n\n    # Vector indices (for each i)\n    ref[indices[i], ...] += updates[i, ...]\n\n    # High rank indices (for each i, ..., j)\n    ref[indices[i, ..., j], ...] += updates[i, ..., j, ...]\n\nDuplicate entries are handled correctly: if multiple `indices` reference\nthe same location, their contributions add.\n\nRequires `updates.shape = indices.shape + ref.shape[1:]` or `updates.shape = []`.\n\n<div style=\"width:70%; margin:auto; margin-bottom:10px; margin-top:20px;\">\n<img style=\"width:100%\" src='https://www.tensorflow.org/images/ScatterAdd.png' alt>\n</div>",
    "inputs": [
      { "name": "resource", "type": "Arg" },
      { "name": "indices", "type": "Arg" },
      { "name": "updates", "type": "Arg" }
    ]
  },
  {
    "name": "tf.ResourceScatterDiv",
    "summary": "Divides sparse updates into the variable referenced by `resource`.",
    "description": "This operation computes\n\n    # Scalar indices\n    ref[indices, ...] /= updates[...]\n\n    # Vector indices (for each i)\n    ref[indices[i], ...] /= updates[i, ...]\n\n    # High rank indices (for each i, ..., j)\n    ref[indices[i, ..., j], ...] /= updates[i, ..., j, ...]\n\nDuplicate entries are handled correctly: if multiple `indices` reference\nthe same location, their contributions multiply.\n\nRequires `updates.shape = indices.shape + ref.shape[1:]` or `updates.shape = []`.\n\n<div style=\"width:70%; margin:auto; margin-bottom:10px; margin-top:20px;\">\n<img style=\"width:100%\" src='https://www.tensorflow.org/images/ScatterAdd.png' alt>\n</div>",
    "inputs": [
      { "name": "resource", "type": "Arg" },
      { "name": "indices", "type": "Arg" },
      { "name": "updates", "type": "Arg" }
    ]
  },
  {
    "name": "tf.ResourceScatterMax",
    "summary": "Reduces sparse updates into the variable referenced by `resource` using the `max` operation.",
    "description": "This operation computes\n\n    # Scalar indices\n    ref[indices, ...] = max(ref[indices, ...], updates[...])\n\n    # Vector indices (for each i)\n    ref[indices[i], ...] = max(ref[indices[i], ...], updates[i, ...])\n\n    # High rank indices (for each i, ..., j)\n    ref[indices[i, ..., j], ...] = max(ref[indices[i, ..., j], ...], updates[i, ..., j, ...])\n\nDuplicate entries are handled correctly: if multiple `indices` reference\nthe same location, their contributions are combined.\n\nRequires `updates.shape = indices.shape + ref.shape[1:]` or `updates.shape = []`.\n\n<div style=\"width:70%; margin:auto; margin-bottom:10px; margin-top:20px;\">\n<img style=\"width:100%\" src='https://www.tensorflow.org/images/ScatterAdd.png' alt>\n</div>",
    "inputs": [
      { "name": "resource", "type": "Arg" },
      { "name": "indices", "type": "Arg" },
      { "name": "updates", "type": "Arg" }
    ]
  },
  {
    "name": "tf.ResourceScatterMin",
    "summary": "Reduces sparse updates into the variable referenced by `resource` using the `min` operation.",
    "description": "This operation computes\n\n    # Scalar indices\n    ref[indices, ...] = min(ref[indices, ...], updates[...])\n\n    # Vector indices (for each i)\n    ref[indices[i], ...] = min(ref[indices[i], ...], updates[i, ...])\n\n    # High rank indices (for each i, ..., j)\n    ref[indices[i, ..., j], ...] = min(ref[indices[i, ..., j], ...], updates[i, ..., j, ...])\n\nDuplicate entries are handled correctly: if multiple `indices` reference\nthe same location, their contributions are combined.\n\nRequires `updates.shape = indices.shape + ref.shape[1:]` or `updates.shape = []`.\n\n<div style=\"width:70%; margin:auto; margin-bottom:10px; margin-top:20px;\">\n<img style=\"width:100%\" src='https://www.tensorflow.org/images/ScatterAdd.png' alt>\n</div>",
    "inputs": [
      { "name": "resource", "type": "Arg" },
      { "name": "indices", "type": "Arg" },
      { "name": "updates", "type": "Arg" }
    ]
  },
  {
    "name": "tf.ResourceScatterMul",
    "summary": "Multiplies sparse updates into the variable referenced by `resource`.",
    "description": "This operation computes\n\n    # Scalar indices\n    ref[indices, ...] *= updates[...]\n\n    # Vector indices (for each i)\n    ref[indices[i], ...] *= updates[i, ...]\n\n    # High rank indices (for each i, ..., j)\n    ref[indices[i, ..., j], ...] *= updates[i, ..., j, ...]\n\nDuplicate entries are handled correctly: if multiple `indices` reference\nthe same location, their contributions multiply.\n\nRequires `updates.shape = indices.shape + ref.shape[1:]` or `updates.shape = []`.\n\n<div style=\"width:70%; margin:auto; margin-bottom:10px; margin-top:20px;\">\n<img style=\"width:100%\" src='https://www.tensorflow.org/images/ScatterAdd.png' alt>\n</div>",
    "inputs": [
      { "name": "resource", "type": "Arg" },
      { "name": "indices", "type": "Arg" },
      { "name": "updates", "type": "Arg" }
    ]
  },
  {
    "name": "tf.ResourceScatterNdAdd",
    "summary": "Applies sparse addition to individual values or slices in a Variable.",
    "description": "`ref` is a `Tensor` with rank `P` and `indices` is a `Tensor` of rank `Q`.\n\n`indices` must be integer tensor, containing indices into `ref`.\nIt must be shape `[d_0, ..., d_{Q-2}, K]` where `0 < K <= P`.\n\nThe innermost dimension of `indices` (with length `K`) corresponds to\nindices into elements (if `K = P`) or slices (if `K < P`) along the `K`th\ndimension of `ref`.\n\n`updates` is `Tensor` of rank `Q-1+P-K` with shape:\n\n```\n[d_0, ..., d_{Q-2}, ref.shape[K], ..., ref.shape[P-1]]\n```\n\nFor example, say we want to add 4 scattered elements to a rank-1 tensor to\n8 elements. In Python, that addition would look like this:\n\n```python\nref = tf.Variable([1, 2, 3, 4, 5, 6, 7, 8], use_resource=True)\nindices = tf.constant([[4], [3], [1], [7]])\nupdates = tf.constant([9, 10, 11, 12])\nadd = tf.scatter_nd_add(ref, indices, updates)\nwith tf.Session() as sess:\n  print sess.run(add)\n```\n\nThe resulting update to ref would look like this:\n\n    [1, 13, 3, 14, 14, 6, 7, 20]\n\nSee `tf.scatter_nd` for more details about how to make updates to\nslices.",
    "inputs": [
      { "name": "ref", "type": "Arg" },
      { "name": "indices", "type": "Arg" },
      { "name": "updates", "type": "Arg" }
    ],
    "attributes": [
      { "name": "use_locking", "type": "DefaultValuedOptionalAttr" },
      { "name": "bad_indices_policy", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.ResourceScatterNdSub",
    "summary": "Applies sparse subtraction to individual values or slices in a Variable.",
    "description": "`ref` is a `Tensor` with rank `P` and `indices` is a `Tensor` of rank `Q`.\n\n`indices` must be integer tensor, containing indices into `ref`.\nIt must be shape `[d_0, ..., d_{Q-2}, K]` where `0 < K <= P`.\n\nThe innermost dimension of `indices` (with length `K`) corresponds to\nindices into elements (if `K = P`) or slices (if `K < P`) along the `K`th\ndimension of `ref`.\n\n`updates` is `Tensor` of rank `Q-1+P-K` with shape:\n\n```\n[d_0, ..., d_{Q-2}, ref.shape[K], ..., ref.shape[P-1]]\n```\n\nFor example, say we want to subtract 4 scattered elements from a rank-1 tensor\nwith 8 elements. In Python, that subtraction would look like this:\n\n```python\nref = tf.Variable([1, 2, 3, 4, 5, 6, 7, 8], use_resource=True)\nindices = tf.constant([[4], [3], [1], [7]])\nupdates = tf.constant([9, 10, 11, 12])\nsub = tf.scatter_nd_sub(ref, indices, updates)\nwith tf.Session() as sess:\n  print sess.run(sub)\n```\n\nThe resulting update to ref would look like this:\n\n    [1, -9, 3, -6, -4, 6, 7, -4]\n\nSee `tf.scatter_nd` for more details about how to make updates to\nslices.",
    "inputs": [
      { "name": "ref", "type": "Arg" },
      { "name": "indices", "type": "Arg" },
      { "name": "updates", "type": "Arg" }
    ],
    "attributes": [
      { "name": "use_locking", "type": "DefaultValuedOptionalAttr" },
      { "name": "bad_indices_policy", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.ResourceScatterNdUpdate",
    "summary": "Applies sparse `updates` to individual values or slices within a given",
    "description": "variable according to `indices`.\n\n`ref` is a `Tensor` with rank `P` and `indices` is a `Tensor` of rank `Q`.\n\n`indices` must be integer tensor, containing indices into `ref`.\nIt must be shape `[d_0, ..., d_{Q-2}, K]` where `0 < K <= P`.\n\nThe innermost dimension of `indices` (with length `K`) corresponds to\nindices into elements (if `K = P`) or slices (if `K < P`) along the `K`th\ndimension of `ref`.\n\n`updates` is `Tensor` of rank `Q-1+P-K` with shape:\n\n```\n[d_0, ..., d_{Q-2}, ref.shape[K], ..., ref.shape[P-1]].\n```\n\nFor example, say we want to update 4 scattered elements to a rank-1 tensor to\n8 elements. In Python, that update would look like this:\n\n```python\n    ref = tf.Variable([1, 2, 3, 4, 5, 6, 7, 8])\n    indices = tf.constant([[4], [3], [1] ,[7]])\n    updates = tf.constant([9, 10, 11, 12])\n    update = tf.scatter_nd_update(ref, indices, updates)\n    with tf.Session() as sess:\n      print sess.run(update)\n```\n\nThe resulting update to ref would look like this:\n\n    [1, 11, 3, 10, 9, 6, 7, 12]\n\nSee `tf.scatter_nd` for more details about how to make updates to\nslices.",
    "inputs": [
      { "name": "ref", "type": "Arg" },
      { "name": "indices", "type": "Arg" },
      { "name": "updates", "type": "Arg" }
    ],
    "attributes": [
      { "name": "use_locking", "type": "DefaultValuedOptionalAttr" },
      { "name": "bad_indices_policy", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.ResourceScatterSub",
    "summary": "Subtracts sparse updates from the variable referenced by `resource`.",
    "description": "This operation computes\n\n    # Scalar indices\n    ref[indices, ...] -= updates[...]\n\n    # Vector indices (for each i)\n    ref[indices[i], ...] -= updates[i, ...]\n\n    # High rank indices (for each i, ..., j)\n    ref[indices[i, ..., j], ...] -= updates[i, ..., j, ...]\n\nDuplicate entries are handled correctly: if multiple `indices` reference\nthe same location, their contributions add.\n\nRequires `updates.shape = indices.shape + ref.shape[1:]` or `updates.shape = []`.\n\n<div style=\"width:70%; margin:auto; margin-bottom:10px; margin-top:20px;\">\n<img style=\"width:100%\" src='https://www.tensorflow.org/images/ScatterAdd.png' alt>\n</div>",
    "inputs": [
      { "name": "resource", "type": "Arg" },
      { "name": "indices", "type": "Arg" },
      { "name": "updates", "type": "Arg" }
    ]
  },
  {
    "name": "tf.ResourceScatterUpdate",
    "summary": "Assigns sparse updates to the variable referenced by `resource`.",
    "description": "This operation computes\n\n    # Scalar indices\n    ref[indices, ...] = updates[...]\n\n    # Vector indices (for each i)\n    ref[indices[i], ...] = updates[i, ...]\n\n    # High rank indices (for each i, ..., j)\n    ref[indices[i, ..., j], ...] = updates[i, ..., j, ...]",
    "inputs": [
      { "name": "resource", "type": "Arg" },
      { "name": "indices", "type": "Arg" },
      { "name": "updates", "type": "Arg" }
    ]
  },
  {
    "name": "tf.ResourceSparseApplyAdagrad",
    "summary": "Update relevant entries in '*var' and '*accum' according to the adagrad scheme.",
    "description": "That is for rows we have grad for, we update var and accum as follows:\naccum += grad * grad\nvar -= lr * grad * (1 / sqrt(accum))",
    "inputs": [
      { "name": "var", "type": "Arg" },
      { "name": "accum", "type": "Arg" },
      { "name": "lr", "type": "Arg" },
      { "name": "grad", "type": "Arg" },
      { "name": "indices", "type": "Arg" }
    ],
    "attributes": [
      { "name": "use_locking", "type": "DefaultValuedOptionalAttr" },
      { "name": "update_slots", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.ResourceSparseApplyAdagradV2",
    "summary": "Update relevant entries in '*var' and '*accum' according to the adagrad scheme.",
    "description": "That is for rows we have grad for, we update var and accum as follows:\naccum += grad * grad\nvar -= lr * grad * (1 / sqrt(accum))",
    "inputs": [
      { "name": "var", "type": "Arg" },
      { "name": "accum", "type": "Arg" },
      { "name": "lr", "type": "Arg" },
      { "name": "epsilon", "type": "Arg" },
      { "name": "grad", "type": "Arg" },
      { "name": "indices", "type": "Arg" }
    ],
    "attributes": [
      { "name": "use_locking", "type": "DefaultValuedOptionalAttr" },
      { "name": "update_slots", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.ResourceSparseApplyFtrl",
    "summary": "Update relevant entries in '*var' according to the Ftrl-proximal scheme.",
    "description": "That is for rows we have grad for, we update var, accum and linear as follows:\naccum_new = accum + grad * grad\nlinear += grad - (accum_new^(-lr_power) - accum^(-lr_power)) / lr * var\nquadratic = 1.0 / (accum_new^(lr_power) * lr) + 2 * l2\nvar = (sign(linear) * l1 - linear) / quadratic if |linear| > l1 else 0.0\naccum = accum_new",
    "inputs": [
      { "name": "var", "type": "Arg" },
      { "name": "accum", "type": "Arg" },
      { "name": "linear", "type": "Arg" },
      { "name": "grad", "type": "Arg" },
      { "name": "indices", "type": "Arg" },
      { "name": "lr", "type": "Arg" },
      { "name": "l1", "type": "Arg" },
      { "name": "l2", "type": "Arg" },
      { "name": "lr_power", "type": "Arg" }
    ],
    "attributes": [
      { "name": "use_locking", "type": "DefaultValuedOptionalAttr" },
      { "name": "multiply_linear_by_lr", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.ResourceStridedSliceAssign",
    "summary": "Assign `value` to the sliced l-value reference of `ref`.",
    "description": "The values of `value` are assigned to the positions in the variable\n`ref` that are selected by the slice parameters. The slice parameters\n`begin, `end`, `strides`, etc. work exactly as in `StridedSlice`.\n\nNOTE this op currently does not support broadcasting and so `value`'s\nshape must be exactly the shape produced by the slice of `ref`.",
    "inputs": [
      { "name": "ref", "type": "Arg" },
      { "name": "begin", "type": "TF_I32OrI64Tensor" },
      { "name": "end", "type": "TF_I32OrI64Tensor" },
      { "name": "strides", "type": "TF_I32OrI64Tensor" },
      { "name": "value", "type": "TF_Tensor" }
    ],
    "attributes": [
      { "name": "begin_mask", "type": "DefaultValuedOptionalAttr" },
      { "name": "end_mask", "type": "DefaultValuedOptionalAttr" },
      { "name": "ellipsis_mask", "type": "DefaultValuedOptionalAttr" },
      { "name": "new_axis_mask", "type": "DefaultValuedOptionalAttr" },
      { "name": "shrink_axis_mask", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.Restore",
    "summary": "Restores a tensor from checkpoint files.",
    "description": "Reads a tensor stored in one or several files. If there are several files (for\ninstance because a tensor was saved as slices), `file_pattern` may contain\nwildcard symbols (`*` and `?`) in the filename portion only, not in the\ndirectory portion.\n\nIf a `file_pattern` matches several files, `preferred_shard` can be used to hint\nin which file the requested tensor is likely to be found. This op will first\nopen the file at index `preferred_shard` in the list of matching files and try\nto restore tensors from that file.  Only if some tensors or tensor slices are\nnot found in that first file, then the Op opens all the files. Setting\n`preferred_shard` to match the value passed as the `shard` input\nof a matching `Save` Op may speed up Restore.  This attribute only affects\nperformance, not correctness.  The default value -1 means files are processed in\norder.\n\nSee also `RestoreSlice`.",
    "inputs": [
      { "name": "file_pattern", "type": "Arg" },
      { "name": "tensor_name", "type": "Arg" }
    ],
    "outputs": [
      { "name": "tensor", "type": "Res" }
    ],
    "attributes": [
      { "name": "preferred_shard", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.RestoreV2",
    "summary": "Restores tensors from a V2 checkpoint.",
    "description": "For backward compatibility with the V1 format, this Op currently allows\nrestoring from a V1 checkpoint as well:\n  - This Op first attempts to find the V2 index file pointed to by \"prefix\", and\n    if found proceed to read it as a V2 checkpoint;\n  - Otherwise the V1 read path is invoked.\nRelying on this behavior is not recommended, as the ability to fall back to read\nV1 might be deprecated and eventually removed.\n\nBy default, restores the named tensors in full.  If the caller wishes to restore\nspecific slices of stored tensors, \"shape_and_slices\" should be non-empty\nstrings and correspondingly well-formed.\n\nCallers must ensure all the named tensors are indeed stored in the checkpoint.",
    "inputs": [
      { "name": "prefix", "type": "Arg" },
      { "name": "tensor_names", "type": "Arg" },
      { "name": "shape_and_slices", "type": "Arg" }
    ],
    "outputs": [
      { "name": "tensors", "type": "Res" }
    ]
  },
  {
    "name": "tf.RetrieveTPUEmbeddingAdadeltaParameters",
    "summary": "Retrieve Adadelta embedding parameters.",
    "description": "An op that retrieves optimization parameters from embedding to host\nmemory. Must be preceded by a ConfigureTPUEmbeddingHost op that sets up\nthe correct embedding table configuration. For example, this op is\nused to retrieve updated parameters before saving a checkpoint.",
    "outputs": [
      { "name": "parameters", "type": "Res" },
      { "name": "accumulators", "type": "Res" },
      { "name": "updates", "type": "Res" }
    ],
    "attributes": [
      { "name": "table_id", "type": "DefaultValuedOptionalAttr" },
      { "name": "table_name", "type": "DefaultValuedOptionalAttr" },
      { "name": "num_shards", "type": "I64Attr" },
      { "name": "shard_id", "type": "I64Attr" },
      { "name": "config", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.RetrieveTPUEmbeddingAdadeltaParametersGradAccumDebug",
    "outputs": [
      { "name": "parameters", "type": "TF_Float32Tensor" },
      { "name": "accumulators", "type": "TF_Float32Tensor" },
      { "name": "updates", "type": "TF_Float32Tensor" },
      { "name": "gradient_accumulators", "type": "TF_Float32Tensor" }
    ],
    "attributes": [
      { "name": "table_id", "type": "DefaultValuedOptionalAttr" },
      { "name": "table_name", "type": "DefaultValuedOptionalAttr" },
      { "name": "num_shards", "type": "I64Attr" },
      { "name": "shard_id", "type": "I64Attr" },
      { "name": "config", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.RetrieveTPUEmbeddingAdagradParameters",
    "summary": "Retrieve Adagrad embedding parameters.",
    "description": "An op that retrieves optimization parameters from embedding to host\nmemory. Must be preceded by a ConfigureTPUEmbeddingHost op that sets up\nthe correct embedding table configuration. For example, this op is\nused to retrieve updated parameters before saving a checkpoint.",
    "outputs": [
      { "name": "parameters", "type": "Res" },
      { "name": "accumulators", "type": "Res" }
    ],
    "attributes": [
      { "name": "table_id", "type": "DefaultValuedOptionalAttr" },
      { "name": "table_name", "type": "DefaultValuedOptionalAttr" },
      { "name": "num_shards", "type": "I64Attr" },
      { "name": "shard_id", "type": "I64Attr" },
      { "name": "config", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.RetrieveTPUEmbeddingAdagradParametersGradAccumDebug",
    "outputs": [
      { "name": "parameters", "type": "TF_Float32Tensor" },
      { "name": "accumulators", "type": "TF_Float32Tensor" },
      { "name": "gradient_accumulators", "type": "TF_Float32Tensor" }
    ],
    "attributes": [
      { "name": "table_id", "type": "DefaultValuedOptionalAttr" },
      { "name": "table_name", "type": "DefaultValuedOptionalAttr" },
      { "name": "num_shards", "type": "I64Attr" },
      { "name": "shard_id", "type": "I64Attr" },
      { "name": "config", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.RetrieveTPUEmbeddingADAMParameters",
    "summary": "Retrieve ADAM embedding parameters.",
    "description": "An op that retrieves optimization parameters from embedding to host\nmemory. Must be preceded by a ConfigureTPUEmbeddingHost op that sets up\nthe correct embedding table configuration. For example, this op is\nused to retrieve updated parameters before saving a checkpoint.",
    "outputs": [
      { "name": "parameters", "type": "Res" },
      { "name": "momenta", "type": "Res" },
      { "name": "velocities", "type": "Res" }
    ],
    "attributes": [
      { "name": "table_id", "type": "DefaultValuedOptionalAttr" },
      { "name": "table_name", "type": "DefaultValuedOptionalAttr" },
      { "name": "num_shards", "type": "I64Attr" },
      { "name": "shard_id", "type": "I64Attr" },
      { "name": "config", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.RetrieveTPUEmbeddingADAMParametersGradAccumDebug",
    "outputs": [
      { "name": "parameters", "type": "TF_Float32Tensor" },
      { "name": "momenta", "type": "TF_Float32Tensor" },
      { "name": "velocities", "type": "TF_Float32Tensor" },
      { "name": "gradient_accumulators", "type": "TF_Float32Tensor" }
    ],
    "attributes": [
      { "name": "table_id", "type": "DefaultValuedOptionalAttr" },
      { "name": "table_name", "type": "DefaultValuedOptionalAttr" },
      { "name": "num_shards", "type": "I64Attr" },
      { "name": "shard_id", "type": "I64Attr" },
      { "name": "config", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.RetrieveTPUEmbeddingCenteredRMSPropParameters",
    "summary": "Retrieve centered RMSProp embedding parameters.",
    "description": "An op that retrieves optimization parameters from embedding to host\nmemory. Must be preceded by a ConfigureTPUEmbeddingHost op that sets up\nthe correct embedding table configuration. For example, this op is\nused to retrieve updated parameters before saving a checkpoint.",
    "outputs": [
      { "name": "parameters", "type": "Res" },
      { "name": "ms", "type": "Res" },
      { "name": "mom", "type": "Res" },
      { "name": "mg", "type": "Res" }
    ],
    "attributes": [
      { "name": "table_id", "type": "DefaultValuedOptionalAttr" },
      { "name": "table_name", "type": "DefaultValuedOptionalAttr" },
      { "name": "num_shards", "type": "I64Attr" },
      { "name": "shard_id", "type": "I64Attr" },
      { "name": "config", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.RetrieveTPUEmbeddingFTRLParameters",
    "summary": "Retrieve FTRL embedding parameters.",
    "description": "An op that retrieves optimization parameters from embedding to host\nmemory. Must be preceded by a ConfigureTPUEmbeddingHost op that sets up\nthe correct embedding table configuration. For example, this op is\nused to retrieve updated parameters before saving a checkpoint.",
    "outputs": [
      { "name": "parameters", "type": "Res" },
      { "name": "accumulators", "type": "Res" },
      { "name": "linears", "type": "Res" }
    ],
    "attributes": [
      { "name": "table_id", "type": "DefaultValuedOptionalAttr" },
      { "name": "table_name", "type": "DefaultValuedOptionalAttr" },
      { "name": "num_shards", "type": "I64Attr" },
      { "name": "shard_id", "type": "I64Attr" },
      { "name": "config", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.RetrieveTPUEmbeddingFTRLParametersGradAccumDebug",
    "outputs": [
      { "name": "parameters", "type": "TF_Float32Tensor" },
      { "name": "accumulators", "type": "TF_Float32Tensor" },
      { "name": "linears", "type": "TF_Float32Tensor" },
      { "name": "gradient_accumulators", "type": "TF_Float32Tensor" }
    ],
    "attributes": [
      { "name": "table_id", "type": "DefaultValuedOptionalAttr" },
      { "name": "table_name", "type": "DefaultValuedOptionalAttr" },
      { "name": "num_shards", "type": "I64Attr" },
      { "name": "shard_id", "type": "I64Attr" },
      { "name": "config", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.RetrieveTPUEmbeddingMDLAdagradLightParameters",
    "summary": "Retrieve MDL Adagrad Light embedding parameters.",
    "description": "An op that retrieves optimization parameters from embedding to host\nmemory. Must be preceded by a ConfigureTPUEmbeddingHost op that sets up\nthe correct embedding table configuration. For example, this op is\nused to retrieve updated parameters before saving a checkpoint.",
    "outputs": [
      { "name": "parameters", "type": "Res" },
      { "name": "accumulators", "type": "Res" },
      { "name": "weights", "type": "Res" },
      { "name": "benefits", "type": "Res" }
    ],
    "attributes": [
      { "name": "table_id", "type": "DefaultValuedOptionalAttr" },
      { "name": "table_name", "type": "DefaultValuedOptionalAttr" },
      { "name": "num_shards", "type": "I64Attr" },
      { "name": "shard_id", "type": "I64Attr" },
      { "name": "config", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.RetrieveTPUEmbeddingMomentumParameters",
    "summary": "Retrieve Momentum embedding parameters.",
    "description": "An op that retrieves optimization parameters from embedding to host\nmemory. Must be preceded by a ConfigureTPUEmbeddingHost op that sets up\nthe correct embedding table configuration. For example, this op is\nused to retrieve updated parameters before saving a checkpoint.",
    "outputs": [
      { "name": "parameters", "type": "Res" },
      { "name": "momenta", "type": "Res" }
    ],
    "attributes": [
      { "name": "table_id", "type": "DefaultValuedOptionalAttr" },
      { "name": "table_name", "type": "DefaultValuedOptionalAttr" },
      { "name": "num_shards", "type": "I64Attr" },
      { "name": "shard_id", "type": "I64Attr" },
      { "name": "config", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.RetrieveTPUEmbeddingMomentumParametersGradAccumDebug",
    "outputs": [
      { "name": "parameters", "type": "TF_Float32Tensor" },
      { "name": "momenta", "type": "TF_Float32Tensor" },
      { "name": "gradient_accumulators", "type": "TF_Float32Tensor" }
    ],
    "attributes": [
      { "name": "table_id", "type": "DefaultValuedOptionalAttr" },
      { "name": "table_name", "type": "DefaultValuedOptionalAttr" },
      { "name": "num_shards", "type": "I64Attr" },
      { "name": "shard_id", "type": "I64Attr" },
      { "name": "config", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.RetrieveTPUEmbeddingProximalAdagradParameters",
    "summary": "Retrieve proximal Adagrad embedding parameters.",
    "description": "An op that retrieves optimization parameters from embedding to host\nmemory. Must be preceded by a ConfigureTPUEmbeddingHost op that sets up\nthe correct embedding table configuration. For example, this op is\nused to retrieve updated parameters before saving a checkpoint.",
    "outputs": [
      { "name": "parameters", "type": "Res" },
      { "name": "accumulators", "type": "Res" }
    ],
    "attributes": [
      { "name": "table_id", "type": "DefaultValuedOptionalAttr" },
      { "name": "table_name", "type": "DefaultValuedOptionalAttr" },
      { "name": "num_shards", "type": "I64Attr" },
      { "name": "shard_id", "type": "I64Attr" },
      { "name": "config", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.RetrieveTPUEmbeddingProximalAdagradParametersGradAccumDebug",
    "outputs": [
      { "name": "parameters", "type": "TF_Float32Tensor" },
      { "name": "accumulators", "type": "TF_Float32Tensor" },
      { "name": "gradient_accumulators", "type": "TF_Float32Tensor" }
    ],
    "attributes": [
      { "name": "table_id", "type": "DefaultValuedOptionalAttr" },
      { "name": "table_name", "type": "DefaultValuedOptionalAttr" },
      { "name": "num_shards", "type": "I64Attr" },
      { "name": "shard_id", "type": "I64Attr" },
      { "name": "config", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.RetrieveTPUEmbeddingProximalYogiParameters",
    "outputs": [
      { "name": "parameters", "type": "TF_Float32Tensor" },
      { "name": "v", "type": "TF_Float32Tensor" },
      { "name": "m", "type": "TF_Float32Tensor" }
    ],
    "attributes": [
      { "name": "table_id", "type": "DefaultValuedOptionalAttr" },
      { "name": "table_name", "type": "DefaultValuedOptionalAttr" },
      { "name": "num_shards", "type": "I64Attr" },
      { "name": "shard_id", "type": "I64Attr" },
      { "name": "config", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.RetrieveTPUEmbeddingProximalYogiParametersGradAccumDebug",
    "outputs": [
      { "name": "parameters", "type": "TF_Float32Tensor" },
      { "name": "v", "type": "TF_Float32Tensor" },
      { "name": "m", "type": "TF_Float32Tensor" },
      { "name": "gradient_accumulators", "type": "TF_Float32Tensor" }
    ],
    "attributes": [
      { "name": "table_id", "type": "DefaultValuedOptionalAttr" },
      { "name": "table_name", "type": "DefaultValuedOptionalAttr" },
      { "name": "num_shards", "type": "I64Attr" },
      { "name": "shard_id", "type": "I64Attr" },
      { "name": "config", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.RetrieveTPUEmbeddingRMSPropParameters",
    "summary": "Retrieve RMSProp embedding parameters.",
    "description": "An op that retrieves optimization parameters from embedding to host\nmemory. Must be preceded by a ConfigureTPUEmbeddingHost op that sets up\nthe correct embedding table configuration. For example, this op is\nused to retrieve updated parameters before saving a checkpoint.",
    "outputs": [
      { "name": "parameters", "type": "Res" },
      { "name": "ms", "type": "Res" },
      { "name": "mom", "type": "Res" }
    ],
    "attributes": [
      { "name": "table_id", "type": "DefaultValuedOptionalAttr" },
      { "name": "table_name", "type": "DefaultValuedOptionalAttr" },
      { "name": "num_shards", "type": "I64Attr" },
      { "name": "shard_id", "type": "I64Attr" },
      { "name": "config", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.RetrieveTPUEmbeddingRMSPropParametersGradAccumDebug",
    "outputs": [
      { "name": "parameters", "type": "TF_Float32Tensor" },
      { "name": "ms", "type": "TF_Float32Tensor" },
      { "name": "mom", "type": "TF_Float32Tensor" },
      { "name": "gradient_accumulators", "type": "TF_Float32Tensor" }
    ],
    "attributes": [
      { "name": "table_id", "type": "DefaultValuedOptionalAttr" },
      { "name": "table_name", "type": "DefaultValuedOptionalAttr" },
      { "name": "num_shards", "type": "I64Attr" },
      { "name": "shard_id", "type": "I64Attr" },
      { "name": "config", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.RetrieveTPUEmbeddingStochasticGradientDescentParameters",
    "summary": "Retrieve SGD embedding parameters.",
    "description": "An op that retrieves optimization parameters from embedding to host\nmemory. Must be preceded by a ConfigureTPUEmbeddingHost op that sets up\nthe correct embedding table configuration. For example, this op is\nused to retrieve updated parameters before saving a checkpoint.",
    "outputs": [
      { "name": "parameters", "type": "Res" }
    ],
    "attributes": [
      { "name": "table_id", "type": "DefaultValuedOptionalAttr" },
      { "name": "table_name", "type": "DefaultValuedOptionalAttr" },
      { "name": "num_shards", "type": "I64Attr" },
      { "name": "shard_id", "type": "I64Attr" },
      { "name": "config", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.RetrieveTPUEmbeddingStochasticGradientDescentParametersGradAccumDebug",
    "outputs": [
      { "name": "parameters", "type": "TF_Float32Tensor" },
      { "name": "gradient_accumulators", "type": "TF_Float32Tensor" }
    ],
    "attributes": [
      { "name": "table_id", "type": "DefaultValuedOptionalAttr" },
      { "name": "table_name", "type": "DefaultValuedOptionalAttr" },
      { "name": "num_shards", "type": "I64Attr" },
      { "name": "shard_id", "type": "I64Attr" },
      { "name": "config", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.Reverse",
    "summary": "Reverses specific dimensions of a tensor.",
    "description": "Given a `tensor`, and a `bool` tensor `dims` representing the dimensions\nof `tensor`, this operation reverses each dimension i of `tensor` where\n`dims[i]` is `True`.\n\n`tensor` can have up to 8 dimensions. The number of dimensions\nof `tensor` must equal the number of elements in `dims`. In other words:\n\n`rank(tensor) = size(dims)`\n\nFor example:\n\n```\n# tensor 't' is [[[[ 0,  1,  2,  3],\n#                  [ 4,  5,  6,  7],\n#                  [ 8,  9, 10, 11]],\n#                 [[12, 13, 14, 15],\n#                  [16, 17, 18, 19],\n#                  [20, 21, 22, 23]]]]\n# tensor 't' shape is [1, 2, 3, 4]\n\n# 'dims' is [False, False, False, True]\nreverse(t, dims) ==> [[[[ 3,  2,  1,  0],\n                        [ 7,  6,  5,  4],\n                        [ 11, 10, 9, 8]],\n                       [[15, 14, 13, 12],\n                        [19, 18, 17, 16],\n                        [23, 22, 21, 20]]]]\n\n# 'dims' is [False, True, False, False]\nreverse(t, dims) ==> [[[[12, 13, 14, 15],\n                        [16, 17, 18, 19],\n                        [20, 21, 22, 23]\n                       [[ 0,  1,  2,  3],\n                        [ 4,  5,  6,  7],\n                        [ 8,  9, 10, 11]]]]\n\n# 'dims' is [False, False, True, False]\nreverse(t, dims) ==> [[[[8, 9, 10, 11],\n                        [4, 5, 6, 7],\n                        [0, 1, 2, 3]]\n                       [[20, 21, 22, 23],\n                        [16, 17, 18, 19],\n                        [12, 13, 14, 15]]]]\n```",
    "inputs": [
      { "name": "tensor", "type": "Arg" },
      { "name": "dims", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ]
  },
  {
    "name": "tf.ReverseSequence",
    "summary": "Reverses variable length slices.",
    "description": "This op first slices `input` along the dimension `batch_dim`, and for each\nslice `i`, reverses the first `seq_lengths[i]` elements along\nthe dimension `seq_dim`.\n\nThe elements of `seq_lengths` must obey `seq_lengths[i] <= input.dims[seq_dim]`,\nand `seq_lengths` must be a vector of length `input.dims[batch_dim]`.\n\nThe output slice `i` along dimension `batch_dim` is then given by input\nslice `i`, with the first `seq_lengths[i]` slices along dimension\n`seq_dim` reversed.\n\nFor example:\n\n```\n# Given this:\nbatch_dim = 0\nseq_dim = 1\ninput.dims = (4, 8, ...)\nseq_lengths = [7, 2, 3, 5]\n\n# then slices of input are reversed on seq_dim, but only up to seq_lengths:\noutput[0, 0:7, :, ...] = input[0, 7:0:-1, :, ...]\noutput[1, 0:2, :, ...] = input[1, 2:0:-1, :, ...]\noutput[2, 0:3, :, ...] = input[2, 3:0:-1, :, ...]\noutput[3, 0:5, :, ...] = input[3, 5:0:-1, :, ...]\n\n# while entries past seq_lens are copied through:\noutput[0, 7:, :, ...] = input[0, 7:, :, ...]\noutput[1, 2:, :, ...] = input[1, 2:, :, ...]\noutput[2, 3:, :, ...] = input[2, 3:, :, ...]\noutput[3, 2:, :, ...] = input[3, 2:, :, ...]\n```\n\nIn contrast, if:\n\n```\n# Given this:\nbatch_dim = 2\nseq_dim = 0\ninput.dims = (8, ?, 4, ...)\nseq_lengths = [7, 2, 3, 5]\n\n# then slices of input are reversed on seq_dim, but only up to seq_lengths:\noutput[0:7, :, 0, :, ...] = input[7:0:-1, :, 0, :, ...]\noutput[0:2, :, 1, :, ...] = input[2:0:-1, :, 1, :, ...]\noutput[0:3, :, 2, :, ...] = input[3:0:-1, :, 2, :, ...]\noutput[0:5, :, 3, :, ...] = input[5:0:-1, :, 3, :, ...]\n\n# while entries past seq_lens are copied through:\noutput[7:, :, 0, :, ...] = input[7:, :, 0, :, ...]\noutput[2:, :, 1, :, ...] = input[2:, :, 1, :, ...]\noutput[3:, :, 2, :, ...] = input[3:, :, 2, :, ...]\noutput[2:, :, 3, :, ...] = input[2:, :, 3, :, ...]\n```",
    "inputs": [
      { "name": "input", "type": "Arg" },
      { "name": "seq_lengths", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "seq_dim", "type": "I64Attr" },
      { "name": "batch_dim", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.ReverseV2",
    "summary": "Reverses specific dimensions of a tensor.",
    "description": "Given a `tensor`, and a `int32` tensor `axis` representing the set of\ndimensions of `tensor` to reverse. This operation reverses each dimension\n`i` for which there exists `j` s.t. `axis[j] == i`.\n\n`tensor` can have up to 8 dimensions. The number of dimensions specified\nin `axis` may be 0 or more entries. If an index is specified more than\nonce, a InvalidArgument error is raised.\n\nFor example:\n\n```\n# tensor 't' is [[[[ 0,  1,  2,  3],\n#                  [ 4,  5,  6,  7],\n#                  [ 8,  9, 10, 11]],\n#                 [[12, 13, 14, 15],\n#                  [16, 17, 18, 19],\n#                  [20, 21, 22, 23]]]]\n# tensor 't' shape is [1, 2, 3, 4]\n\n# 'dims' is [3] or 'dims' is [-1]\nreverse(t, dims) ==> [[[[ 3,  2,  1,  0],\n                        [ 7,  6,  5,  4],\n                        [ 11, 10, 9, 8]],\n                       [[15, 14, 13, 12],\n                        [19, 18, 17, 16],\n                        [23, 22, 21, 20]]]]\n\n# 'dims' is '[1]' (or 'dims' is '[-3]')\nreverse(t, dims) ==> [[[[12, 13, 14, 15],\n                        [16, 17, 18, 19],\n                        [20, 21, 22, 23]\n                       [[ 0,  1,  2,  3],\n                        [ 4,  5,  6,  7],\n                        [ 8,  9, 10, 11]]]]\n\n# 'dims' is '[2]' (or 'dims' is '[-2]')\nreverse(t, dims) ==> [[[[8, 9, 10, 11],\n                        [4, 5, 6, 7],\n                        [0, 1, 2, 3]]\n                       [[20, 21, 22, 23],\n                        [16, 17, 18, 19],\n                        [12, 13, 14, 15]]]]\n```",
    "inputs": [
      { "name": "tensor", "type": "Arg" },
      { "name": "axis", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ]
  },
  {
    "name": "tf.RFFT",
    "summary": "Real-valued fast Fourier transform.",
    "description": "Computes the 1-dimensional discrete Fourier transform of a real-valued signal\nover the inner-most dimension of `input`.\n\nSince the DFT of a real signal is Hermitian-symmetric, `RFFT` only returns the\n`fft_length / 2 + 1` unique components of the FFT: the zero-frequency term,\nfollowed by the `fft_length / 2` positive-frequency terms.\n\nAlong the axis `RFFT` is computed on, if `fft_length` is smaller than the\ncorresponding dimension of `input`, the dimension is cropped. If it is larger,\nthe dimension is padded with zeros.",
    "inputs": [
      { "name": "input", "type": "Arg" },
      { "name": "fft_length", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ]
  },
  {
    "name": "tf.RFFT2D",
    "summary": "2D real-valued fast Fourier transform.",
    "description": "Computes the 2-dimensional discrete Fourier transform of a real-valued signal\nover the inner-most 2 dimensions of `input`.\n\nSince the DFT of a real signal is Hermitian-symmetric, `RFFT2D` only returns the\n`fft_length / 2 + 1` unique components of the FFT for the inner-most dimension\nof `output`: the zero-frequency term, followed by the `fft_length / 2`\npositive-frequency terms.\n\nAlong each axis `RFFT2D` is computed on, if `fft_length` is smaller than the\ncorresponding dimension of `input`, the dimension is cropped. If it is larger,\nthe dimension is padded with zeros.",
    "inputs": [
      { "name": "input", "type": "Arg" },
      { "name": "fft_length", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ]
  },
  {
    "name": "tf.RFFT3D",
    "summary": "3D real-valued fast Fourier transform.",
    "description": "Computes the 3-dimensional discrete Fourier transform of a real-valued signal\nover the inner-most 3 dimensions of `input`.\n\nSince the DFT of a real signal is Hermitian-symmetric, `RFFT3D` only returns the\n`fft_length / 2 + 1` unique components of the FFT for the inner-most dimension\nof `output`: the zero-frequency term, followed by the `fft_length / 2`\npositive-frequency terms.\n\nAlong each axis `RFFT3D` is computed on, if `fft_length` is smaller than the\ncorresponding dimension of `input`, the dimension is cropped. If it is larger,\nthe dimension is padded with zeros.",
    "inputs": [
      { "name": "input", "type": "Arg" },
      { "name": "fft_length", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ]
  },
  {
    "name": "tf.RGBToHSV",
    "summary": "Converts one or more images from RGB to HSV.",
    "description": "Outputs a tensor of the same shape as the `images` tensor, containing the HSV\nvalue of the pixels. The output is only well defined if the value in `images`\nare in `[0,1]`.\n\n`output[..., 0]` contains hue, `output[..., 1]` contains saturation, and\n`output[..., 2]` contains value. All HSV values are in `[0,1]`. A hue of 0\ncorresponds to pure red, hue 1/3 is pure green, and 2/3 is pure blue.\n\nUsage Example:\n\n>>> blue_image = tf.stack([\n...    tf.zeros([5,5]),\n...    tf.zeros([5,5]),\n...    tf.ones([5,5])],\n...    axis=-1)\n>>> blue_hsv_image = tf.image.rgb_to_hsv(blue_image)\n>>> blue_hsv_image[0,0].numpy()\narray([0.6666667, 1. , 1. ], dtype=float32)",
    "inputs": [
      { "name": "images", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ]
  },
  {
    "name": "tf.RightShift",
    "summary": "Elementwise computes the bitwise right-shift of `x` and `y`.",
    "description": "Performs a logical shift for unsigned integer types, and an arithmetic shift\nfor signed integer types.\n\nIf `y` is negative, or greater than or equal to than the width of `x` in bits\nthe result is implementation defined.\n\nExample:\n\n```python\nimport tensorflow as tf\nfrom tensorflow.python.ops import bitwise_ops\nimport numpy as np\ndtype_list = [tf.int8, tf.int16, tf.int32, tf.int64]\n\nfor dtype in dtype_list:\n  lhs = tf.constant([-1, -5, -3, -14], dtype=dtype)\n  rhs = tf.constant([5, 0, 7, 11], dtype=dtype)\n\n  right_shift_result = bitwise_ops.right_shift(lhs, rhs)\n\n  print(right_shift_result)\n\n# This will print:\n# tf.Tensor([-1 -5 -1 -1], shape=(4,), dtype=int8)\n# tf.Tensor([-1 -5 -1 -1], shape=(4,), dtype=int16)\n# tf.Tensor([-1 -5 -1 -1], shape=(4,), dtype=int32)\n# tf.Tensor([-1 -5 -1 -1], shape=(4,), dtype=int64)\n\nlhs = np.array([-2, 64, 101, 32], dtype=np.int8)\nrhs = np.array([-1, -5, -3, -14], dtype=np.int8)\nbitwise_ops.right_shift(lhs, rhs)\n# <tf.Tensor: shape=(4,), dtype=int8, numpy=array([ -2,  64, 101,  32], dtype=int8)>\n```",
    "inputs": [
      { "name": "x", "type": "TF_IntTensor" },
      { "name": "y", "type": "TF_IntTensor" }
    ],
    "outputs": [
      { "name": "z", "type": "TF_IntTensor" }
    ]
  },
  {
    "name": "tf.Rint",
    "summary": "Returns element-wise integer closest to x.",
    "description": "If the result is midway between two representable values,\nthe even representable is chosen.\nFor example:\n\n```\nrint(-1.5) ==> -2.0\nrint(0.5000001) ==> 1.0\nrint([-1.7, -1.5, -0.2, 0.2, 1.5, 1.7, 2.0]) ==> [-2., -2., -0., 0., 2., 2., 2.]\n```",
    "inputs": [
      { "name": "x", "type": "TF_FloatTensor" }
    ],
    "outputs": [
      { "name": "y", "type": "TF_FloatTensor" }
    ]
  },
  {
    "name": "tf.RiscAdd",
    "summary": "Returns x + y element-wise.",
    "description": "*NOTE*: `RiscAdd` does not supports broadcasting.\n\nGiven two input tensors, the `tf.risc_add` operation computes the sum for every element in the tensor.\n\nBoth input and output have a range `(-inf, inf)`.",
    "inputs": [
      { "name": "x", "type": "TF_FloatTensor" },
      { "name": "y", "type": "TF_FloatTensor" }
    ],
    "outputs": [
      { "name": "z", "type": "TF_FloatTensor" }
    ]
  },
  {
    "name": "tf.RiscDot",
    "inputs": [
      { "name": "a", "type": "TF_FloatTensor" },
      { "name": "b", "type": "TF_FloatTensor" }
    ],
    "outputs": [
      { "name": "product", "type": "TF_FloatTensor" }
    ],
    "attributes": [
      { "name": "transpose_a", "type": "DefaultValuedOptionalAttr" },
      { "name": "transpose_b", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.RngReadAndSkip",
    "summary": "Advance the counter of a counter-based RNG.",
    "description": "The state of the RNG after\n`rng_read_and_skip(n)` will be the same as that after `uniform([n])`\n(or any other distribution). The actual increment added to the\ncounter is an unspecified implementation choice.\n\nIn the case that the input algorithm is RNG_ALG_AUTO_SELECT, the counter in the state needs to be of size int64[2], the current maximal counter size among algorithms. In this case, this op will manage the counter as if it is an 128-bit integer with layout [lower_64bits, higher_64bits]. If an algorithm needs less than 128 bits for the counter, it should use the left portion of the int64[2]. In this way, the int64[2] is compatible with all current RNG algorithms (Philox, ThreeFry and xla::RandomAlgorithm::RNG_DEFAULT). Downstream RNG ops can thus use this counter with any RNG algorithm.",
    "inputs": [
      { "name": "resource", "type": "Arg" },
      { "name": "alg", "type": "Arg" },
      { "name": "delta", "type": "Arg" }
    ],
    "outputs": [
      { "name": "value", "type": "Res" }
    ]
  },
  {
    "name": "tf.Roll",
    "summary": "Rolls the elements of a tensor along an axis.",
    "description": "The elements are shifted positively (towards larger indices) by the offset of\n`shift` along the dimension of `axis`. Negative `shift` values will shift\nelements in the opposite direction. Elements that roll passed the last position\nwill wrap around to the first and vice versa. Multiple shifts along multiple\naxes may be specified.\n\nFor example:\n\n```\n# 't' is [0, 1, 2, 3, 4]\nroll(t, shift=2, axis=0) ==> [3, 4, 0, 1, 2]\n\n# shifting along multiple dimensions\n# 't' is [[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]]\nroll(t, shift=[1, -2], axis=[0, 1]) ==> [[7, 8, 9, 5, 6], [2, 3, 4, 0, 1]]\n\n# shifting along the same axis multiple times\n# 't' is [[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]]\nroll(t, shift=[2, -3], axis=[1, 1]) ==> [[1, 2, 3, 4, 0], [6, 7, 8, 9, 5]]\n```",
    "inputs": [
      { "name": "input", "type": "TF_Tensor" },
      { "name": "shift", "type": "Arg" },
      { "name": "axis", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ]
  },
  {
    "name": "tf.Round",
    "summary": "Rounds the values of a tensor to the nearest integer, element-wise.",
    "description": "Rounds half to even.  Also known as bankers rounding. If you want to round\naccording to the current system rounding mode use std::cint.",
    "inputs": [
      { "name": "x", "type": "TensorOf" }
    ],
    "outputs": [
      { "name": "y", "type": "TensorOf" }
    ]
  },
  {
    "name": "tf.Rsqrt",
    "summary": "Computes reciprocal of square root of x element-wise.",
    "description": "I.e., \\\\(y = 1 / \\sqrt{x}\\\\).",
    "inputs": [
      { "name": "x", "type": "TF_FpOrComplexTensor" }
    ],
    "outputs": [
      { "name": "y", "type": "TF_FpOrComplexTensor" }
    ]
  },
  {
    "name": "tf.RsqrtGrad",
    "summary": "Computes the gradient for the rsqrt of `x` wrt its input.",
    "description": "Specifically, `grad = dy * -0.5 * y^3`, where `y = rsqrt(x)`, and `dy`\nis the corresponding input gradient.",
    "inputs": [
      { "name": "y", "type": "TF_FpOrComplexTensor" },
      { "name": "dy", "type": "TF_FpOrComplexTensor" }
    ],
    "outputs": [
      { "name": "z", "type": "TF_FpOrComplexTensor" }
    ]
  },
  {
    "name": "tf.Save",
    "summary": "Saves the input tensors to disk.",
    "description": "The size of `tensor_names` must match the number of tensors in `data`. `data[i]`\nis written to `filename` with name `tensor_names[i]`.\n\nSee also `SaveSlices`.",
    "inputs": [
      { "name": "filename", "type": "Arg" },
      { "name": "tensor_names", "type": "Arg" },
      { "name": "data", "type": "Arg" }
    ]
  },
  {
    "name": "tf.SaveSlices",
    "summary": "Saves input tensors slices to disk.",
    "description": "This is like `Save` except that tensors can be listed in the saved file as being\na slice of a larger tensor.  `shapes_and_slices` specifies the shape of the\nlarger tensor and the slice that this tensor covers. `shapes_and_slices` must\nhave as many elements as `tensor_names`.\n\nElements of the `shapes_and_slices` input must either be:\n\n*  The empty string, in which case the corresponding tensor is\n   saved normally.\n*  A string of the form `dim0 dim1 ... dimN-1 slice-spec` where the\n   `dimI` are the dimensions of the larger tensor and `slice-spec`\n   specifies what part is covered by the tensor to save.\n\n`slice-spec` itself is a `:`-separated list: `slice0:slice1:...:sliceN-1`\nwhere each `sliceI` is either:\n\n*  The string `-` meaning that the slice covers all indices of this dimension\n*  `start,length` where `start` and `length` are integers.  In that\n   case the slice covers `length` indices starting at `start`.\n\nSee also `Save`.",
    "inputs": [
      { "name": "filename", "type": "Arg" },
      { "name": "tensor_names", "type": "Arg" },
      { "name": "shapes_and_slices", "type": "Arg" },
      { "name": "data", "type": "Arg" }
    ]
  },
  {
    "name": "tf.SaveV2",
    "summary": "Saves tensors in V2 checkpoint format.",
    "description": "By default, saves the named tensors in full.  If the caller wishes to save\nspecific slices of full tensors, \"shape_and_slices\" should be non-empty strings\nand correspondingly well-formed.",
    "inputs": [
      { "name": "prefix", "type": "Arg" },
      { "name": "tensor_names", "type": "Arg" },
      { "name": "shape_and_slices", "type": "Arg" },
      { "name": "tensors", "type": "Arg" }
    ]
  },
  {
    "name": "tf.ScatterNd",
    "summary": "Scatters `updates` into a tensor of shape `shape` according to `indices`.",
    "description": "Scatter sparse `updates` according to individual values at the specified\n`indices`. This op returns an output tensor with the `shape` you specify. This\nop is the inverse of the `tf.gather_nd` operator which extracts values or slices\nfrom a given tensor.\n\nThis operation is similar to `tf.tensor_scatter_nd_add`, except that the tensor\nis zero-initialized. Calling `tf.scatter_nd(indices, updates, shape)`\nis identical to calling\n`tf.tensor_scatter_nd_add(tf.zeros(shape, updates.dtype), indices, updates)`\n\nIf `indices` contains duplicates, the associated `updates` are accumulated\n(summed) into the output tensor.\n\n**WARNING**: For floating-point data types, the output may be nondeterministic.\nThis is because the order in which the updates are applied is nondeterministic\nand when floating-point numbers are added in different orders the resulting\nnumerical approximation error can be slightly different. However, the output\nwill be deterministic if op determinism is enabled via\n`tf.config.experimental.enable_op_determinism`.\n\n`indices` is an integer tensor containing indices into the output tensor. The\nlast dimension of `indices` can be at most the rank of `shape`:\n\n    indices.shape[-1] <= shape.rank\n\nThe last dimension of `indices` corresponds to indices of elements\n(if `indices.shape[-1] = shape.rank`) or slices\n(if `indices.shape[-1] < shape.rank`) along dimension `indices.shape[-1]` of\n`shape`.\n\n`updates` is a tensor with shape:\n\n    indices.shape[:-1] + shape[indices.shape[-1]:]\n\nThe simplest form of the scatter op is to insert individual elements in\na tensor by index. Consider an example where you want to insert 4 scattered\nelements in a rank-1 tensor with 8 elements.\n\n<div style=\"width:70%; margin:auto; margin-bottom:10px; margin-top:20px;\">\n<img style=\"width:100%\" src=\"https://www.tensorflow.org/images/ScatterNd1.png\" alt>\n</div>\n\nIn Python, this scatter operation would look like this:\n\n```python\n    indices = tf.constant([[4], [3], [1], [7]])\n    updates = tf.constant([9, 10, 11, 12])\n    shape = tf.constant([8])\n    scatter = tf.scatter_nd(indices, updates, shape)\n    print(scatter)\n```\n\nThe resulting tensor would look like this:\n\n    [0, 11, 0, 10, 9, 0, 0, 12]\n\nYou can also insert entire slices of a higher rank tensor all at once. For\nexample, you can insert two slices in the first dimension of a rank-3 tensor\nwith two matrices of new values.\n\n<div style=\"width:70%; margin:auto; margin-bottom:10px; margin-top:20px;\">\n<img style=\"width:100%\" src=\"https://www.tensorflow.org/images/ScatterNd2.png\" alt>\n</div>\n\nIn Python, this scatter operation would look like this:\n\n```python\n    indices = tf.constant([[1], [3]])\n    updates = tf.constant([[[5, 5, 5, 5], [6, 6, 6, 6],\n                            [7, 7, 7, 7], [8, 8, 8, 8]],\n                           [[5, 5, 5, 5], [6, 6, 6, 6],\n                            [7, 7, 7, 7], [8, 8, 8, 8]]])\n    shape = tf.constant([4, 4, 4])\n    scatter = tf.scatter_nd(indices, updates, shape)\n    print(scatter)\n```\n\nThe resulting tensor would look like this:\n\n    [[[0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]],\n     [[5, 5, 5, 5], [6, 6, 6, 6], [7, 7, 7, 7], [8, 8, 8, 8]],\n     [[0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]],\n     [[5, 5, 5, 5], [6, 6, 6, 6], [7, 7, 7, 7], [8, 8, 8, 8]]]\n\nIf `indices` contains any out-of-bound indices, depending on\n`bad_indices_policy`, the op will either return an error or ignore the\nout-of-bound indices. `bad_indices_policy` can be one of the following values:\n1. \"\" or \"DEFAULT\": raises on CPU and ignore on GPU. This is because\n   historically on CPU and GPU we handle errors in different ways, and for\n   backward compatibility we keep the default behavior.\n2. \"ERROR\": raises error; GPU does not support this value.\n3. \"IGNORE\": ignore the bad indices; supported on both CPU and GPU.",
    "inputs": [
      { "name": "indices", "type": "Arg" },
      { "name": "updates", "type": "Arg" },
      { "name": "shape", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "bad_indices_policy", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.SegmentMax",
    "summary": "Computes the maximum along segments of a tensor.",
    "description": "Read\n[the section on segmentation](https://tensorflow.org/api_docs/python/tf/math#Segmentation)\nfor an explanation of segments.\n\nComputes a tensor such that\n\\\\(output_i = \\max_j(data_j)\\\\) where `max` is over `j` such\nthat `segment_ids[j] == i`.\n\nIf the max is empty for a given segment ID `i`, `output[i] = 0`.\n\nCaution: On CPU, values in `segment_ids` are always validated to be sorted,\nand an error is thrown for indices that are not increasing. On GPU, this\ndoes not throw an error for unsorted indices. On GPU, out-of-order indices\nresult in safe but unspecified behavior, which may include treating\nout-of-order indices as the same as a smaller following index.\n\n<div style=\"width:70%; margin:auto; margin-bottom:10px; margin-top:20px;\">\n<img style=\"width:100%\" src=\"https://www.tensorflow.org/images/SegmentMax.png\" alt>\n</div>\n\nFor example:\n\n>>> c = tf.constant([[1,2,3,4], [4, 3, 2, 1], [5,6,7,8]])\n>>> tf.math.segment_max(c, tf.constant([0, 0, 1])).numpy()\narray([[4, 3, 3, 4],\n       [5, 6, 7, 8]], dtype=int32)",
    "inputs": [
      { "name": "data", "type": "TF_IntOrFpTensor" },
      { "name": "segment_ids", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ]
  },
  {
    "name": "tf.SegmentMaxV2",
    "summary": "Computes the maximum along segments of a tensor.",
    "description": "Read\n[the section on segmentation](https://tensorflow.org/api_docs/python/tf/math#Segmentation)\nfor an explanation of segments.\n\nComputes a tensor such that\n\\\\(output_i = \\max_j(data_j)\\\\) where `max` is over `j` such\nthat `segment_ids[j] == i`.\n\nIf the maximum is empty for a given segment ID `i`, it outputs the smallest\npossible value for the specific numeric type,\n`output[i] = numeric_limits<T>::lowest()`.\n\nNote: That this op is currently only supported with jit_compile=True.\n\nCaution: On CPU, values in `segment_ids` are always validated to be sorted,\nand an error is thrown for indices that are not increasing. On GPU, this\ndoes not throw an error for unsorted indices. On GPU, out-of-order indices\nresult in safe but unspecified behavior, which may include treating\nout-of-order indices as the same as a smaller following index.\n\nThe only difference with SegmentMax is the additional input  `num_segments`.\nThis helps in evaluating the output shape in compile time.\n`num_segments` should be consistent with segment_ids.\ne.g. Max(segment_ids) should be equal to `num_segments` - 1 for a 1-d segment_ids\nWith inconsistent num_segments, the op still runs. only difference is,\nthe output takes the size of num_segments irrespective of size of segment_ids and data.\nfor num_segments less than expected output size, the last elements are ignored\nfor num_segments more than the expected output size, last elements are assigned \nsmallest possible value for the specific numeric type.\n\nFor example:\n\n>>> @tf.function(jit_compile=True)\n... def test(c):\n...   return tf.raw_ops.SegmentMaxV2(data=c, segment_ids=tf.constant([0, 0, 1]), num_segments=2)\n>>> c = tf.constant([[1,2,3,4], [4, 3, 2, 1], [5,6,7,8]])\n>>> test(c).numpy()\narray([[4, 3, 3, 4],\n       [5, 6, 7, 8]], dtype=int32)",
    "inputs": [
      { "name": "data", "type": "TF_IntOrFpTensor" },
      { "name": "segment_ids", "type": "Arg" },
      { "name": "num_segments", "type": "TF_I32OrI64Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ]
  },
  {
    "name": "tf.SegmentMean",
    "summary": "Computes the mean along segments of a tensor.",
    "description": "Read\n[the section on segmentation](https://tensorflow.org/api_docs/python/tf/math#Segmentation)\nfor an explanation of segments.\n\nComputes a tensor such that\n\\\\(output_i = \\frac{\\sum_j data_j}{N}\\\\) where `mean` is\nover `j` such that `segment_ids[j] == i` and `N` is the total number of\nvalues summed.\n\nIf the mean is empty for a given segment ID `i`, `output[i] = 0`.\n\nCaution: On CPU, values in `segment_ids` are always validated to be sorted,\nand an error is thrown for indices that are not increasing. On GPU, this\ndoes not throw an error for unsorted indices. On GPU, out-of-order indices\nresult in safe but unspecified behavior, which may include treating\nout-of-order indices as a smaller following index when computing the numerator\nof the mean.\n\n<div style=\"width:70%; margin:auto; margin-bottom:10px; margin-top:20px;\">\n<img style=\"width:100%\" src=\"https://www.tensorflow.org/images/SegmentMean.png\" alt>\n</div>\n\nFor example:\n\n>>> c = tf.constant([[1.0,2,3,4], [4, 3, 2, 1], [5,6,7,8]])\n>>> tf.math.segment_mean(c, tf.constant([0, 0, 1])).numpy()\narray([[2.5, 2.5, 2.5, 2.5],\n       [5., 6., 7., 8.]], dtype=float32)",
    "inputs": [
      { "name": "data", "type": "TF_NumberTensor" },
      { "name": "segment_ids", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ]
  },
  {
    "name": "tf.SegmentMin",
    "summary": "Computes the minimum along segments of a tensor.",
    "description": "Read\n[the section on segmentation](https://tensorflow.org/api_docs/python/tf/math#Segmentation)\nfor an explanation of segments.\n\nComputes a tensor such that\n\\\\(output_i = \\min_j(data_j)\\\\) where `min` is over `j` such\nthat `segment_ids[j] == i`.\n\nIf the min is empty for a given segment ID `i`, `output[i] = 0`.\n\nCaution: On CPU, values in `segment_ids` are always validated to be sorted,\nand an error is thrown for indices that are not increasing. On GPU, this\ndoes not throw an error for unsorted indices. On GPU, out-of-order indices\nresult in safe but unspecified behavior, which may include treating\nout-of-order indices as the same as a smaller following index.\n\n<div style=\"width:70%; margin:auto; margin-bottom:10px; margin-top:20px;\">\n<img style=\"width:100%\" src=\"https://www.tensorflow.org/images/SegmentMin.png\" alt>\n</div>\n\nFor example:\n\n>>> c = tf.constant([[1,2,3,4], [4, 3, 2, 1], [5,6,7,8]])\n>>> tf.math.segment_min(c, tf.constant([0, 0, 1])).numpy()\narray([[1, 2, 2, 1],\n       [5, 6, 7, 8]], dtype=int32)",
    "inputs": [
      { "name": "data", "type": "TF_IntOrFpTensor" },
      { "name": "segment_ids", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ]
  },
  {
    "name": "tf.SegmentMinV2",
    "summary": "Computes the minimum along segments of a tensor.",
    "description": "Read\n[the section on segmentation](https://tensorflow.org/api_docs/python/tf/math#Segmentation)\nfor an explanation of segments.\n\nComputes a tensor such that\n\\\\(output_i = \\min_j(data_j)\\\\) where `min` is over `j` such\nthat `segment_ids[j] == i`.\n\nIf the minimum is empty for a given segment ID `i`, it outputs the largest\npossible value for the specific numeric type,\n`output[i] = numeric_limits<T>::max()`.\n\nNote: That this op is currently only supported with jit_compile=True.\n\nCaution: On CPU, values in `segment_ids` are always validated to be sorted,\nand an error is thrown for indices that are not increasing. On GPU, this\ndoes not throw an error for unsorted indices. On GPU, out-of-order indices\nresult in safe but unspecified behavior, which may include treating\nout-of-order indices as the same as a smaller following index.\n\nThe only difference with SegmentMin is the additional input  `num_segments`.\nThis helps in evaluating the output shape in compile time.\n`num_segments` should be consistent with segment_ids.\ne.g. Max(segment_ids) should be equal to `num_segments` - 1 for a 1-d segment_ids\nWith inconsistent num_segments, the op still runs. only difference is,\nthe output takes the size of num_segments irrespective of size of segment_ids and data.\nfor num_segments less than expected output size, the last elements are ignored\nfor num_segments more than the expected output size, last elements are assigned \nthe largest possible value for the specific numeric type.\n\nFor example:\n\n>>> @tf.function(jit_compile=True)\n... def test(c):\n...   return tf.raw_ops.SegmentMinV2(data=c, segment_ids=tf.constant([0, 0, 1]), num_segments=2)\n>>> c = tf.constant([[1,2,3,4], [4, 3, 2, 1], [5,6,7,8]])\n>>> test(c).numpy()\narray([[1, 2, 2, 1],\n       [5, 6, 7, 8]], dtype=int32)",
    "inputs": [
      { "name": "data", "type": "TF_IntOrFpTensor" },
      { "name": "segment_ids", "type": "Arg" },
      { "name": "num_segments", "type": "TF_I32OrI64Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ]
  },
  {
    "name": "tf.SegmentProd",
    "summary": "Computes the product along segments of a tensor.",
    "description": "Read\n[the section on segmentation](https://tensorflow.org/api_docs/python/tf/math#Segmentation)\nfor an explanation of segments.\n\nComputes a tensor such that\n\\\\(output_i = \\prod_j data_j\\\\) where the product is over `j` such\nthat `segment_ids[j] == i`.\n\nIf the product is empty for a given segment ID `i`, `output[i] = 1`.\n\nCaution: On CPU, values in `segment_ids` are always validated to be sorted,\nand an error is thrown for indices that are not increasing. On GPU, this\ndoes not throw an error for unsorted indices. On GPU, out-of-order indices\nresult in safe but unspecified behavior, which may include treating\nout-of-order indices as the same as a smaller following index.\n\n<div style=\"width:70%; margin:auto; margin-bottom:10px; margin-top:20px;\">\n<img style=\"width:100%\" src=\"https://www.tensorflow.org/images/SegmentProd.png\" alt>\n</div>\n\nFor example:\n\n>>> c = tf.constant([[1,2,3,4], [4, 3, 2, 1], [5,6,7,8]])\n>>> tf.math.segment_prod(c, tf.constant([0, 0, 1])).numpy()\narray([[4, 6, 6, 4],\n       [5, 6, 7, 8]], dtype=int32)",
    "inputs": [
      { "name": "data", "type": "TF_NumberTensor" },
      { "name": "segment_ids", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ]
  },
  {
    "name": "tf.SegmentProdV2",
    "summary": "Computes the product along segments of a tensor.",
    "description": "Read\n[the section on segmentation](https://tensorflow.org/api_docs/python/tf/math#Segmentation)\nfor an explanation of segments.\n\nComputes a tensor such that\n\\\\(output_i = \\prod_j data_j\\\\) where the product is over `j` such\nthat `segment_ids[j] == i`.\n\nIf the product is empty for a given segment ID `i`, `output[i] = 1`.\n\nNote: That this op is currently only supported with jit_compile=True.\n\nThe only difference with SegmentProd is the additional input  `num_segments`.\nThis helps in evaluating the output shape in compile time.\n`num_segments` should be consistent with segment_ids.\ne.g. Max(segment_ids) - 1 should be equal to `num_segments` for a 1-d segment_ids\nWith inconsistent num_segments, the op still runs. only difference is, \nthe output takes the size of num_segments irrespective of size of segment_ids and data.\nfor num_segments less than expected output size, the last elements are ignored\nfor num_segments more than the expected output size, last elements are assigned 1.\n\nFor example:\n\n>>> @tf.function(jit_compile=True)\n... def test(c):\n...   return tf.raw_ops.SegmentProdV2(data=c, segment_ids=tf.constant([0, 0, 1]), num_segments=2)\n>>> c = tf.constant([[1,2,3,4], [4, 3, 2, 1], [5,6,7,8]])\n>>> test(c).numpy()\narray([[4, 6, 6, 4],\n       [5, 6, 7, 8]], dtype=int32)",
    "inputs": [
      { "name": "data", "type": "TF_NumberTensor" },
      { "name": "segment_ids", "type": "Arg" },
      { "name": "num_segments", "type": "TF_I32OrI64Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ]
  },
  {
    "name": "tf.SegmentSum",
    "summary": "Computes the sum along segments of a tensor.",
    "description": "Read\n[the section on segmentation](https://tensorflow.org/api_docs/python/tf/math#Segmentation)\nfor an explanation of segments.\n\nComputes a tensor such that\n\\\\(output_i = \\sum_j data_j\\\\) where sum is over `j` such\nthat `segment_ids[j] == i`.\n\nIf the sum is empty for a given segment ID `i`, `output[i] = 0`.\n\nCaution: On CPU, values in `segment_ids` are always validated to be sorted,\nand an error is thrown for indices that are not increasing. On GPU, this\ndoes not throw an error for unsorted indices. On GPU, out-of-order indices\nresult in safe but unspecified behavior, which may include treating\nout-of-order indices as the same as a smaller following index.\n\n<div style=\"width:70%; margin:auto; margin-bottom:10px; margin-top:20px;\">\n<img style=\"width:100%\" src=\"https://www.tensorflow.org/images/SegmentSum.png\" alt>\n</div>\n\nFor example:\n\n>>> c = tf.constant([[1,2,3,4], [4, 3, 2, 1], [5,6,7,8]])\n>>> tf.math.segment_sum(c, tf.constant([0, 0, 1])).numpy()\narray([[5, 5, 5, 5],\n       [5, 6, 7, 8]], dtype=int32)",
    "inputs": [
      { "name": "data", "type": "TF_NumberTensor" },
      { "name": "segment_ids", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ]
  },
  {
    "name": "tf.SegmentSumV2",
    "summary": "Computes the sum along segments of a tensor.",
    "description": "Read\n[the section on segmentation](https://tensorflow.org/api_docs/python/tf/math#Segmentation)\nfor an explanation of segments.\n\nComputes a tensor such that\n\\\\(output_i = \\sum_j data_j\\\\) where sum is over `j` such\nthat `segment_ids[j] == i`.\n\nIf the sum is empty for a given segment ID `i`, `output[i] = 0`.\n\nNote that this op is currently only supported with jit_compile=True.\n</div>",
    "inputs": [
      { "name": "data", "type": "TF_NumberTensor" },
      { "name": "segment_ids", "type": "Arg" },
      { "name": "num_segments", "type": "TF_I32OrI64Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ]
  },
  {
    "name": "tf.Select",
    "summary": "Selects elements from `x` or `y`, depending on `condition`.",
    "description": "The `x`, and `y` tensors must all have the same shape, and the\noutput will also have that shape.\n\nThe `condition` tensor must be a scalar if `x` and `y` are scalars.\nIf `x` and `y` are vectors or higher rank, then `condition` must be either a\nscalar, a vector with size matching the first dimension of `x`, or must have\nthe same shape as `x`.\n\nThe `condition` tensor acts as a mask that chooses, based on the value at each\nelement, whether the corresponding element / row in the output should be\ntaken from `x` (if true) or `y` (if false).\n\nIf `condition` is a vector and `x` and `y` are higher rank matrices, then\nit chooses which row (outer dimension) to copy from `x` and `y`.\nIf `condition` has the same shape as `x` and `y`, then it chooses which\nelement to copy from `x` and `y`.\n\nFor example:\n\n```python\n# 'condition' tensor is [[True,  False]\n#                        [False, True]]\n# 't' is [[1, 2],\n#         [3, 4]]\n# 'e' is [[5, 6],\n#         [7, 8]]\nselect(condition, t, e)  # => [[1, 6], [7, 4]]\n\n\n# 'condition' tensor is [True, False]\n# 't' is [[1, 2],\n#         [3, 4]]\n# 'e' is [[5, 6],\n#         [7, 8]]\nselect(condition, t, e) ==> [[1, 2],\n                             [7, 8]]\n\n```",
    "inputs": [
      { "name": "condition", "type": "TF_BoolTensor" },
      { "name": "then_value", "type": "Arg" },
      { "name": "else_value", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ]
  },
  {
    "name": "tf.SelectV2",
    "inputs": [
      { "name": "condition", "type": "TF_BoolTensor" },
      { "name": "then_value", "type": "TF_Tensor" },
      { "name": "else_value", "type": "TF_Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "TF_Tensor" }
    ]
  },
  {
    "name": "tf.SelfAdjointEigV2",
    "summary": "Computes the eigen decomposition of one or more square self-adjoint matrices.",
    "description": "Computes the eigenvalues and (optionally) eigenvectors of each inner matrix in\n`input` such that `input[..., :, :] = v[..., :, :] * diag(e[..., :])`. The eigenvalues\nare sorted in non-decreasing order.\n\n```python\n# a is a tensor.\n# e is a tensor of eigenvalues.\n# v is a tensor of eigenvectors.\ne, v = self_adjoint_eig(a)\ne = self_adjoint_eig(a, compute_v=False)\n```",
    "inputs": [
      { "name": "input", "type": "Arg" }
    ],
    "outputs": [
      { "name": "e", "type": "Res" },
      { "name": "v", "type": "Res" }
    ],
    "attributes": [
      { "name": "compute_v", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.Selu",
    "summary": "Computes scaled exponential linear: `scale * alpha * (exp(features) - 1)`",
    "description": "if < 0, `scale * features` otherwise.\n\nTo be used together with\n`initializer = tf.variance_scaling_initializer(factor=1.0, mode='FAN_IN')`.\nFor correct dropout, use `tf.contrib.nn.alpha_dropout`.\n\nSee [Self-Normalizing Neural Networks](https://arxiv.org/abs/1706.02515)",
    "inputs": [
      { "name": "features", "type": "TF_FloatTensor" }
    ],
    "outputs": [
      { "name": "activations", "type": "TF_FloatTensor" }
    ]
  },
  {
    "name": "tf.SeluGrad",
    "summary": "Computes gradients for the scaled exponential linear (Selu) operation.",
    "inputs": [
      { "name": "gradients", "type": "Arg" },
      { "name": "outputs", "type": "Arg" }
    ],
    "outputs": [
      { "name": "backprops", "type": "Res" }
    ]
  },
  {
    "name": "tf.Send",
    "summary": "Sends the named tensor from send_device to recv_device.",
    "inputs": [
      { "name": "tensor", "type": "Arg" }
    ],
    "attributes": [
      { "name": "tensor_name", "type": "StrAttr" },
      { "name": "send_device", "type": "StrAttr" },
      { "name": "send_device_incarnation", "type": "I64Attr" },
      { "name": "recv_device", "type": "StrAttr" },
      { "name": "client_terminated", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.SendTPUEmbeddingGradients",
    "summary": "Performs gradient updates of embedding tables.",
    "inputs": [
      { "name": "inputs", "type": "Arg" },
      { "name": "learning_rates", "type": "Arg" }
    ],
    "attributes": [
      { "name": "config", "type": "StrAttr" }
    ]
  },
  {
    "name": "tf.SerializeIterator",
    "summary": "Converts the given `resource_handle` representing an iterator to a variant tensor.",
    "inputs": [
      { "name": "resource_handle", "type": "Arg" }
    ],
    "outputs": [
      { "name": "serialized", "type": "Res" }
    ],
    "attributes": [
      { "name": "external_state_policy", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.SerializeSparse",
    "summary": "Serialize a `SparseTensor` into a `[3]` `Tensor` object.",
    "inputs": [
      { "name": "sparse_indices", "type": "Arg" },
      { "name": "sparse_values", "type": "Arg" },
      { "name": "sparse_shape", "type": "Arg" }
    ],
    "outputs": [
      { "name": "serialized_sparse", "type": "TensorOf" }
    ]
  },
  {
    "name": "tf.SetStaticDimensionBounds",
    "summary": "Op used to indicate to the compiler and runtime the static bounds of a tensor.",
    "description": "The information passed through this op can possibly be used by the compiler and\nruntime to perform certain optimizations such as more efficient DMAs. The\nbounds passed via this op should be considered advisory only, and depending on\nthe implementation, might do nothing and simply be an identity\n\n`input`: The tensor that has dynamic dimensions.\n`static_shape`: The static shape of the tensor, corresponds to the maximum bounds of each dimension.\n`output` is the input tensor with no changes done to it.\n\nExample usage:\n\ndef tpu_call(args):\n  def model_fn(args):\n    # do something with dynamic tensor\n\n  @function.Defun(capture_resource_var_by_value=False)\n  def tpu_subgraph():\n      return tf.tpu.rewrite(model_fn, args)\n\n  return tf.raw_ops.TPUPartitionedCall(\n      args=tpu_subgraph.captured_inputs,\n      Tout=[o.type for o in tpu_subgraph.definition.signature.output_arg],\n      f=tpu_subgraph,\n      device_ordinal=[0])\n\nstatic_shape = tf.placeholder(tf.int32, shape=([3]), name='static_size')\n\nw = tf.Variable(tf.constant([[1.0], [2.0], [3.0]]), name='w')\n\nw_dyn = tf.SetDynamicDimensionBounds(w, static_size])\ntpu_call([w_dyn])",
    "inputs": [
      { "name": "input", "type": "TF_Tensor" },
      { "name": "static_shape", "type": "TF_I32OrI64Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "TF_Tensor" }
    ]
  },
  {
    "name": "tf.Shape",
    "summary": "Returns the shape of a tensor.",
    "description": "This operation returns a 1-D integer tensor representing the shape of `input`.\n\nFor example:\n\n```\n# 't' is [[[1, 1, 1], [2, 2, 2]], [[3, 3, 3], [4, 4, 4]]]\nshape(t) ==> [2, 2, 3]\n```",
    "inputs": [
      { "name": "input", "type": "TF_Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "TF_I32OrI64Tensor" }
    ],
    "category": "Shape"
  },
  {
    "name": "tf.ShapeN",
    "summary": "Returns shape of tensors.",
    "description": "This operation returns N 1-D integer tensors representing shape of `input[i]s`.",
    "inputs": [
      { "name": "input", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "output", "type": "Variadic" }
    ]
  },
  {
    "name": "tf.ShardedFilename",
    "summary": "Generate a sharded filename. The filename is printf formatted as",
    "description": "%s-%05d-of-%05d, basename, shard, num_shards.",
    "inputs": [
      { "name": "basename", "type": "TF_StrTensor" },
      { "name": "shard", "type": "TF_Int32Tensor" },
      { "name": "num_shards", "type": "TF_Int32Tensor" }
    ],
    "outputs": [
      { "name": "filename", "type": "TF_StrTensor" }
    ]
  },
  {
    "name": "tf.ShuffleAndRepeatDatasetV2",
    "inputs": [
      { "name": "input_dataset", "type": "TF_VariantTensor" },
      { "name": "buffer_size", "type": "TF_Int64Tensor" },
      { "name": "seed", "type": "TF_Int64Tensor" },
      { "name": "seed2", "type": "TF_Int64Tensor" },
      { "name": "count", "type": "TF_Int64Tensor" },
      { "name": "seed_generator", "type": "Arg" }
    ],
    "outputs": [
      { "name": "handle", "type": "TF_VariantTensor" }
    ],
    "attributes": [
      { "name": "reshuffle_each_iteration", "type": "DefaultValuedOptionalAttr" },
      { "name": "output_types", "type": "ConfinedAttr" },
      { "name": "output_shapes", "type": "ConfinedAttr" },
      { "name": "metadata", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.ShuffleDatasetV2",
    "inputs": [
      { "name": "input_dataset", "type": "TF_VariantTensor" },
      { "name": "buffer_size", "type": "TF_Int64Tensor" },
      { "name": "seed_generator", "type": "Arg" }
    ],
    "outputs": [
      { "name": "handle", "type": "TF_VariantTensor" }
    ],
    "attributes": [
      { "name": "output_types", "type": "ConfinedAttr" },
      { "name": "output_shapes", "type": "ConfinedAttr" },
      { "name": "metadata", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.ShuffleDatasetV3",
    "inputs": [
      { "name": "input_dataset", "type": "TF_VariantTensor" },
      { "name": "buffer_size", "type": "TF_Int64Tensor" },
      { "name": "seed", "type": "TF_Int64Tensor" },
      { "name": "seed2", "type": "TF_Int64Tensor" },
      { "name": "seed_generator", "type": "Arg" }
    ],
    "outputs": [
      { "name": "handle", "type": "TF_VariantTensor" }
    ],
    "attributes": [
      { "name": "reshuffle_each_iteration", "type": "DefaultValuedOptionalAttr" },
      { "name": "output_types", "type": "ConfinedAttr" },
      { "name": "output_shapes", "type": "ConfinedAttr" },
      { "name": "metadata", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.ShutdownDistributedTPU",
    "summary": "Shuts down a running distributed TPU system.",
    "description": "The op returns an error if no system is running."
  },
  {
    "name": "tf.ShutdownTPUSystem",
    "summary": "An op that shuts down the TPU system.",
    "outputs": [
      { "name": "success", "type": "TF_BoolTensor" }
    ]
  },
  {
    "name": "tf.Sigmoid",
    "summary": "Computes sigmoid of `x` element-wise.",
    "description": "Specifically, `y = 1 / (1 + exp(-x))`.",
    "inputs": [
      { "name": "x", "type": "TF_FpOrComplexTensor" }
    ],
    "outputs": [
      { "name": "y", "type": "TF_FpOrComplexTensor" }
    ],
    "category": "Activation"
  },
  {
    "name": "tf.SigmoidGrad",
    "summary": "Computes the gradient of the sigmoid of `x` wrt its input.",
    "description": "Specifically, `grad = dy * y * (1 - y)`, where `y = sigmoid(x)`, and\n`dy` is the corresponding input gradient.",
    "inputs": [
      { "name": "y", "type": "TF_FpOrComplexTensor" },
      { "name": "dy", "type": "TF_FpOrComplexTensor" }
    ],
    "outputs": [
      { "name": "z", "type": "TF_FpOrComplexTensor" }
    ]
  },
  {
    "name": "tf.Sign",
    "summary": "Returns an element-wise indication of the sign of a number.",
    "description": "`y = sign(x) = -1` if `x < 0`; 0 if `x == 0`; 1 if `x > 0`.\n\nFor complex numbers, `y = sign(x) = x / |x|` if `x != 0`, otherwise `y = 0`.\n\nExample usage:\n>>> tf.math.sign([0., 2., -3.])\n<tf.Tensor: shape=(3,), dtype=float32, numpy=array([ 0.,  1., -1.], dtype=float32)>",
    "inputs": [
      { "name": "x", "type": "TensorOf" }
    ],
    "outputs": [
      { "name": "y", "type": "TensorOf" }
    ]
  },
  {
    "name": "tf.Sin",
    "summary": "Computes sine of x element-wise.",
    "description": "Given an input tensor, this function computes sine of every\n  element in the tensor. Input range is `(-inf, inf)` and\n  output range is `[-1,1]`.\n\n  ```python\n  x = tf.constant([-float(\"inf\"), -9, -0.5, 1, 1.2, 200, 10, float(\"inf\")])\n  tf.math.sin(x) ==> [nan -0.4121185 -0.47942555 0.84147096 0.9320391 -0.87329733 -0.54402107 nan]\n  ```",
    "inputs": [
      { "name": "x", "type": "TF_FpOrComplexTensor" }
    ],
    "outputs": [
      { "name": "y", "type": "TF_FpOrComplexTensor" }
    ]
  },
  {
    "name": "tf.Sinh",
    "summary": "Computes hyperbolic sine of x element-wise.",
    "description": "Given an input tensor, this function computes hyperbolic sine of every\n  element in the tensor. Input range is `[-inf,inf]` and output range\n  is `[-inf,inf]`.\n\n  ```python\n  x = tf.constant([-float(\"inf\"), -9, -0.5, 1, 1.2, 2, 10, float(\"inf\")])\n  tf.math.sinh(x) ==> [-inf -4.0515420e+03 -5.2109528e-01 1.1752012e+00 1.5094614e+00 3.6268604e+00 1.1013232e+04 inf]\n  ```",
    "inputs": [
      { "name": "x", "type": "TF_FpOrComplexTensor" }
    ],
    "outputs": [
      { "name": "y", "type": "TF_FpOrComplexTensor" }
    ]
  },
  {
    "name": "tf.Size",
    "summary": "Returns the size of a tensor.",
    "description": "This operation returns an integer representing the number of elements in\n`input`.\n\nFor example:\n\n```\n# 't' is [[[1, 1,, 1], [2, 2, 2]], [[3, 3, 3], [4, 4, 4]]]]\nsize(t) ==> 12\n```",
    "inputs": [
      { "name": "input", "type": "TF_Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "TF_I32OrI64Tensor" }
    ],
    "category": "Shape"
  },
  {
    "name": "tf.Slice",
    "summary": "Return a slice from 'input'.",
    "description": "The output tensor is a tensor with dimensions described by 'size'\nwhose values are extracted from 'input' starting at the offsets in\n'begin'.\n\n*Requirements*:\n  0 <= begin[i] <= begin[i] + size[i] <= Di  for i in [0, n)",
    "inputs": [
      { "name": "input", "type": "TF_Tensor" },
      { "name": "begin", "type": "Arg" },
      { "name": "size", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "TF_Tensor" }
    ],
    "category": "Tensor"
  },
  {
    "name": "tf.Snapshot",
    "summary": "Returns a copy of the input tensor.",
    "inputs": [
      { "name": "input", "type": "TF_Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "TF_Tensor" }
    ]
  },
  {
    "name": "tf.Softmax",
    "summary": "Computes softmax activations.",
    "description": "For each batch `i` and class `j` we have\n\n    $$softmax[i, j] = exp(logits[i, j]) / sum_j(exp(logits[i, j]))$$",
    "inputs": [
      { "name": "logits", "type": "Arg" }
    ],
    "outputs": [
      { "name": "softmax", "type": "Res" }
    ],
    "category": "Activation"
  },
  {
    "name": "tf.SoftmaxCrossEntropyWithLogits",
    "summary": "Computes softmax cross entropy cost and gradients to backpropagate.",
    "description": "Inputs are the logits, not probabilities.",
    "inputs": [
      { "name": "features", "type": "Arg" },
      { "name": "labels", "type": "Arg" }
    ],
    "outputs": [
      { "name": "loss", "type": "Res" },
      { "name": "backprop", "type": "Res" }
    ]
  },
  {
    "name": "tf.Softplus",
    "inputs": [
      { "name": "features", "type": "TF_FloatTensor" }
    ],
    "outputs": [
      { "name": "activations", "type": "TF_FloatTensor" }
    ]
  },
  {
    "name": "tf.SoftplusGrad",
    "summary": "Computes softplus gradients for a softplus operation.",
    "inputs": [
      { "name": "gradients", "type": "Arg" },
      { "name": "features", "type": "Arg" }
    ],
    "outputs": [
      { "name": "backprops", "type": "Res" }
    ]
  },
  {
    "name": "tf.Softsign",
    "summary": "Computes softsign: `features / (abs(features) + 1)`.",
    "inputs": [
      { "name": "features", "type": "TF_FloatTensor" }
    ],
    "outputs": [
      { "name": "activations", "type": "TF_FloatTensor" }
    ]
  },
  {
    "name": "tf.SoftsignGrad",
    "summary": "Computes softsign gradients for a softsign operation.",
    "inputs": [
      { "name": "gradients", "type": "Arg" },
      { "name": "features", "type": "Arg" }
    ],
    "outputs": [
      { "name": "backprops", "type": "Res" }
    ]
  },
  {
    "name": "tf.SortListOfSparseCoreCooTensors",
    "summary": "An op which sorts each COO tensors in the list by which SparseCore the id will go to. This op should be used along with the ConvertToSparseCoreCsrWrappedCooTensorOp.",
    "inputs": [
      { "name": "row_ids_list", "type": "Variadic" },
      { "name": "col_ids_list", "type": "Variadic" },
      { "name": "gains_list", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "sorted_row_ids", "type": "TF_Int32Tensor" },
      { "name": "sorted_col_ids", "type": "TF_Int32Tensor" },
      { "name": "sorted_gains", "type": "TF_Float32Tensor" },
      { "name": "id_counts", "type": "TF_Int32Tensor" }
    ],
    "attributes": [
      { "name": "sample_count_list", "type": "I64ArrayAttr" },
      { "name": "col_offset_list", "type": "I64ArrayAttr" },
      { "name": "num_replica", "type": "ConfinedAttr" },
      { "name": "table_vocab_size", "type": "ConfinedAttr" },
      { "name": "feature_width", "type": "ConfinedAttr" },
      { "name": "num_sc_per_chip", "type": "ConfinedAttr" },
      { "name": "max_ids_per_sparse_core", "type": "ConfinedAttr" },
      { "name": "max_unique_ids_per_sparse_core", "type": "ConfinedAttr" },
      { "name": "table_name", "type": "StrAttr" }
    ]
  },
  {
    "name": "tf.SpaceToBatch",
    "summary": "SpaceToBatch for 4-D tensors of type T.",
    "description": "This is a legacy version of the more general SpaceToBatchND.\n\nZero-pads and then rearranges (permutes) blocks of spatial data into batch.\nMore specifically, this op outputs a copy of the input tensor where values from\nthe `height` and `width` dimensions are moved to the `batch` dimension. After\nthe zero-padding, both `height` and `width` of the input must be divisible by the\nblock size.\n\nThe attr `block_size` must be greater than one. It indicates the block size.\n\n  * Non-overlapping blocks of size `block_size x block size` in the height and\n    width dimensions are rearranged into the batch dimension at each location.\n  * The batch of the output tensor is `batch * block_size * block_size`.\n  * Both height_pad and width_pad must be divisible by block_size.\n\nThe shape of the output will be:\n\n    [batch*block_size*block_size, height_pad/block_size, width_pad/block_size,\n     depth]\n\nSome examples:\n\n(1) For the following input of shape `[1, 2, 2, 1]` and block_size of 2:\n\n```\nx = [[[[1], [2]], [[3], [4]]]]\n```\n\nThe output tensor has shape `[4, 1, 1, 1]` and value:\n\n```\n[[[[1]]], [[[2]]], [[[3]]], [[[4]]]]\n```\n\n(2) For the following input of shape `[1, 2, 2, 3]` and block_size of 2:\n\n```\nx = [[[[1, 2, 3], [4, 5, 6]],\n      [[7, 8, 9], [10, 11, 12]]]]\n```\n\nThe output tensor has shape `[4, 1, 1, 3]` and value:\n\n```\n[[[[1, 2, 3]]], [[[4, 5, 6]]], [[[7, 8, 9]]], [[[10, 11, 12]]]]\n```\n\n(3) For the following input of shape `[1, 4, 4, 1]` and block_size of 2:\n\n```\nx = [[[[1],   [2],  [3],  [4]],\n      [[5],   [6],  [7],  [8]],\n      [[9],  [10], [11],  [12]],\n      [[13], [14], [15],  [16]]]]\n```\n\nThe output tensor has shape `[4, 2, 2, 1]` and value:\n\n```\nx = [[[[1], [3]], [[9], [11]]],\n     [[[2], [4]], [[10], [12]]],\n     [[[5], [7]], [[13], [15]]],\n     [[[6], [8]], [[14], [16]]]]\n```\n\n(4) For the following input of shape `[2, 2, 4, 1]` and block_size of 2:\n\n```\nx = [[[[1],   [2],  [3],  [4]],\n      [[5],   [6],  [7],  [8]]],\n     [[[9],  [10], [11],  [12]],\n      [[13], [14], [15],  [16]]]]\n```\n\nThe output tensor has shape `[8, 1, 2, 1]` and value:\n\n```\nx = [[[[1], [3]]], [[[9], [11]]], [[[2], [4]]], [[[10], [12]]],\n     [[[5], [7]]], [[[13], [15]]], [[[6], [8]]], [[[14], [16]]]]\n```\n\nAmong others, this operation is useful for reducing atrous convolution into\nregular convolution.",
    "inputs": [
      { "name": "input", "type": "Arg" },
      { "name": "paddings", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "TF_Tensor" }
    ],
    "attributes": [
      { "name": "block_size", "type": "ConfinedAttr" }
    ]
  },
  {
    "name": "tf.SpaceToBatchND",
    "summary": "SpaceToBatch for N-D tensors of type T.",
    "description": "This operation divides \"spatial\" dimensions `[1, ..., M]` of the input into a\ngrid of blocks of shape `block_shape`, and interleaves these blocks with the\n\"batch\" dimension (0) such that in the output, the spatial dimensions\n`[1, ..., M]` correspond to the position within the grid, and the batch\ndimension combines both the position within a spatial block and the original\nbatch position.  Prior to division into blocks, the spatial dimensions of the\ninput are optionally zero padded according to `paddings`. See below for a\nprecise description.\n\nThis operation is equivalent to the following steps:\n\n1. Zero-pad the start and end of dimensions `[1, ..., M]` of the\n   input according to `paddings` to produce `padded` of shape `padded_shape`.\n\n2. Reshape `padded` to `reshaped_padded` of shape:\n\n     [batch] +\n     [padded_shape[1] / block_shape[0],\n       block_shape[0],\n      ...,\n      padded_shape[M] / block_shape[M-1],\n      block_shape[M-1]] +\n     remaining_shape\n\n3. Permute dimensions of `reshaped_padded` to produce\n   `permuted_reshaped_padded` of shape:\n\n     block_shape +\n     [batch] +\n     [padded_shape[1] / block_shape[0],\n      ...,\n      padded_shape[M] / block_shape[M-1]] +\n     remaining_shape\n\n4. Reshape `permuted_reshaped_padded` to flatten `block_shape` into the batch\n   dimension, producing an output tensor of shape:\n\n     [batch * prod(block_shape)] +\n     [padded_shape[1] / block_shape[0],\n      ...,\n      padded_shape[M] / block_shape[M-1]] +\n     remaining_shape\n\nSome examples:\n\n(1) For the following input of shape `[1, 2, 2, 1]`, `block_shape = [2, 2]`, and\n    `paddings = [[0, 0], [0, 0]]`:\n\n```\nx = [[[[1], [2]], [[3], [4]]]]\n```\n\nThe output tensor has shape `[4, 1, 1, 1]` and value:\n\n```\n[[[[1]]], [[[2]]], [[[3]]], [[[4]]]]\n```\n\n(2) For the following input of shape `[1, 2, 2, 3]`, `block_shape = [2, 2]`, and\n    `paddings = [[0, 0], [0, 0]]`:\n\n```\nx = [[[[1, 2, 3], [4, 5, 6]],\n      [[7, 8, 9], [10, 11, 12]]]]\n```\n\nThe output tensor has shape `[4, 1, 1, 3]` and value:\n\n```\n[[[[1, 2, 3]]], [[[4, 5, 6]]], [[[7, 8, 9]]], [[[10, 11, 12]]]]\n```\n\n(3) For the following input of shape `[1, 4, 4, 1]`, `block_shape = [2, 2]`, and\n    `paddings = [[0, 0], [0, 0]]`:\n\n```\nx = [[[[1],   [2],  [3],  [4]],\n      [[5],   [6],  [7],  [8]],\n      [[9],  [10], [11],  [12]],\n      [[13], [14], [15],  [16]]]]\n```\n\nThe output tensor has shape `[4, 2, 2, 1]` and value:\n\n```\nx = [[[[1], [3]], [[9], [11]]],\n     [[[2], [4]], [[10], [12]]],\n     [[[5], [7]], [[13], [15]]],\n     [[[6], [8]], [[14], [16]]]]\n```\n\n(4) For the following input of shape `[2, 2, 4, 1]`, block_shape = `[2, 2]`, and\n    paddings = `[[0, 0], [2, 0]]`:\n\n```\nx = [[[[1],   [2],  [3],  [4]],\n      [[5],   [6],  [7],  [8]]],\n     [[[9],  [10], [11],  [12]],\n      [[13], [14], [15],  [16]]]]\n```\n\nThe output tensor has shape `[8, 1, 3, 1]` and value:\n\n```\nx = [[[[0], [1], [3]]], [[[0], [9], [11]]],\n     [[[0], [2], [4]]], [[[0], [10], [12]]],\n     [[[0], [5], [7]]], [[[0], [13], [15]]],\n     [[[0], [6], [8]]], [[[0], [14], [16]]]]\n```\n\nAmong others, this operation is useful for reducing atrous convolution into\nregular convolution.",
    "inputs": [
      { "name": "input", "type": "Arg" },
      { "name": "block_shape", "type": "Arg" },
      { "name": "paddings", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "TF_Tensor" }
    ]
  },
  {
    "name": "tf.SpaceToDepth",
    "summary": "SpaceToDepth for tensors of type T.",
    "description": "Rearranges blocks of spatial data, into depth. More specifically,\nthis op outputs a copy of the input tensor where values from the `height`\nand `width` dimensions are moved to the `depth` dimension.\nThe attr `block_size` indicates the input block size.\n\n  * Non-overlapping blocks of size `block_size x block size` are rearranged\n    into depth at each location.\n  * The depth of the output tensor is `block_size * block_size * input_depth`.\n  * The Y, X coordinates within each block of the input become the high order\n    component of the output channel index.\n  * The input tensor's height and width must be divisible by block_size.\n\nThe `data_format` attr specifies the layout of the input and output tensors\nwith the following options:\n  \"NHWC\": `[ batch, height, width, channels ]`\n  \"NCHW\": `[ batch, channels, height, width ]`\n  \"NCHW_VECT_C\":\n      `qint8 [ batch, channels / 4, height, width, 4 ]`\n\nIt is useful to consider the operation as transforming a 6-D Tensor.\ne.g. for data_format = NHWC,\n     Each element in the input tensor can be specified via 6 coordinates,\n     ordered by decreasing memory layout significance as:\n     n,oY,bY,oX,bX,iC  (where n=batch index, oX, oY means X or Y coordinates\n                        within the output image, bX, bY means coordinates\n                        within the input block, iC means input channels).\n     The output would be a transpose to the following layout:\n     n,oY,oX,bY,bX,iC\n\nThis operation is useful for resizing the activations between convolutions\n(but keeping all data), e.g. instead of pooling. It is also useful for training\npurely convolutional models.\n\nFor example, given an input of shape `[1, 2, 2, 1]`, data_format = \"NHWC\" and\nblock_size = 2:\n\n```\nx = [[[[1], [2]],\n      [[3], [4]]]]\n```\n\nThis operation will output a tensor of shape `[1, 1, 1, 4]`:\n\n```\n[[[[1, 2, 3, 4]]]]\n```\n\nHere, the input has a batch of 1 and each batch element has shape `[2, 2, 1]`,\nthe corresponding output will have a single element (i.e. width and height are\nboth 1) and will have a depth of 4 channels (1 * block_size * block_size).\nThe output element shape is `[1, 1, 4]`.\n\nFor an input tensor with larger depth, here of shape `[1, 2, 2, 3]`, e.g.\n\n```\nx = [[[[1, 2, 3], [4, 5, 6]],\n      [[7, 8, 9], [10, 11, 12]]]]\n```\n\nThis operation, for block_size of 2, will return the following tensor of shape\n`[1, 1, 1, 12]`\n\n```\n[[[[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]]]]\n```\n\nSimilarly, for the following input of shape `[1 4 4 1]`, and a block size of 2:\n\n```\nx = [[[[1],   [2],  [5],  [6]],\n      [[3],   [4],  [7],  [8]],\n      [[9],  [10], [13],  [14]],\n      [[11], [12], [15],  [16]]]]\n```\n\nthe operator will return the following tensor of shape `[1 2 2 4]`:\n\n```\nx = [[[[1, 2, 3, 4],\n       [5, 6, 7, 8]],\n      [[9, 10, 11, 12],\n       [13, 14, 15, 16]]]]\n```",
    "inputs": [
      { "name": "input", "type": "TF_Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "TF_Tensor" }
    ],
    "attributes": [
      { "name": "block_size", "type": "ConfinedAttr" },
      { "name": "data_format", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.SparseAdd",
    "summary": "Adds two `SparseTensor` objects to produce another `SparseTensor`.",
    "description": "The input `SparseTensor` objects' indices are assumed ordered in standard\nlexicographic order.  If this is not the case, before this step run\n`SparseReorder` to restore index ordering.\n\nBy default, if two values sum to zero at some index, the output `SparseTensor`\nwould still include that particular location in its index, storing a zero in the\ncorresponding value slot.  To override this, callers can specify `thresh`,\nindicating that if the sum has a magnitude strictly smaller than `thresh`, its\ncorresponding value and index would then not be included.  In particular,\n`thresh == 0` (default) means everything is kept and actual thresholding happens\nonly for a positive value.\n\nIn the following shapes, `nnz` is the count after taking `thresh` into account.",
    "inputs": [
      { "name": "a_indices", "type": "Arg" },
      { "name": "a_values", "type": "Arg" },
      { "name": "a_shape", "type": "Arg" },
      { "name": "b_indices", "type": "Arg" },
      { "name": "b_values", "type": "Arg" },
      { "name": "b_shape", "type": "Arg" },
      { "name": "thresh", "type": "Arg" }
    ],
    "outputs": [
      { "name": "sum_indices", "type": "TF_Int64Tensor" },
      { "name": "sum_values", "type": "TF_NumberTensor" },
      { "name": "sum_shape", "type": "TF_Int64Tensor" }
    ]
  },
  {
    "name": "tf.SparseFillEmptyRows",
    "summary": "Fills empty rows in the input 2-D `SparseTensor` with a default value.",
    "description": "The input `SparseTensor` is represented via the tuple of inputs\n(`indices`, `values`, `dense_shape`).  The output `SparseTensor` has the\nsame `dense_shape` but with indices `output_indices` and values\n`output_values`.\n\nThis op inserts a single entry for every row that doesn't have any values.\nThe index is created as `[row, 0, ..., 0]` and the inserted value\nis `default_value`.\n\nFor example, suppose `sp_input` has shape `[5, 6]` and non-empty values:\n\n    [0, 1]: a\n    [0, 3]: b\n    [2, 0]: c\n    [3, 1]: d\n\nRows 1 and 4 are empty, so the output will be of shape `[5, 6]` with values:\n\n    [0, 1]: a\n    [0, 3]: b\n    [1, 0]: default_value\n    [2, 0]: c\n    [3, 1]: d\n    [4, 0]: default_value\n\nThe output `SparseTensor` will be in row-major order and will have the\nsame shape as the input.\n\nThis op also returns an indicator vector shaped `[dense_shape[0]]` such that\n\n    empty_row_indicator[i] = True iff row i was an empty row.\n\nAnd a reverse index map vector shaped `[indices.shape[0]]` that is used during\nbackpropagation,\n\n    reverse_index_map[j] = out_j s.t. indices[j, :] == output_indices[out_j, :]",
    "inputs": [
      { "name": "indices", "type": "Arg" },
      { "name": "values", "type": "Arg" },
      { "name": "dense_shape", "type": "Arg" },
      { "name": "default_value", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output_indices", "type": "TF_Int64Tensor" },
      { "name": "output_values", "type": "Res" },
      { "name": "empty_row_indicator", "type": "Res" },
      { "name": "reverse_index_map", "type": "Res" }
    ]
  },
  {
    "name": "tf.SparseMatMul",
    "summary": "Multiply matrix \"a\" by matrix \"b\".",
    "description": "The inputs must be two-dimensional matrices and the inner dimension of \"a\" must\nmatch the outer dimension of \"b\". Both \"a\" and \"b\" must be `Tensor`s not\n`SparseTensor`s.  This op is optimized for the case where at least one of \"a\" or\n\"b\" is sparse, in the sense that they have a large proportion of zero values.\nThe breakeven for using this versus a dense matrix multiply on one platform was\n30% zero values in the sparse matrix.\n\nThe gradient computation of this operation will only take advantage of sparsity\nin the input gradient when that gradient comes from a Relu.",
    "inputs": [
      { "name": "a", "type": "TensorOf" },
      { "name": "b", "type": "TensorOf" }
    ],
    "outputs": [
      { "name": "product", "type": "TF_Float32Tensor" }
    ],
    "attributes": [
      { "name": "transpose_a", "type": "DefaultValuedOptionalAttr" },
      { "name": "transpose_b", "type": "DefaultValuedOptionalAttr" },
      { "name": "a_is_sparse", "type": "DefaultValuedOptionalAttr" },
      { "name": "b_is_sparse", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.SparseReduceSum",
    "summary": "Computes the sum of elements across dimensions of a SparseTensor.",
    "description": "This Op takes a SparseTensor and is the sparse counterpart to\n`tf.reduce_sum()`.  In particular, this Op also returns a dense `Tensor`\ninstead of a sparse one.\n\nReduces `sp_input` along the dimensions given in `reduction_axes`.  Unless\n`keep_dims` is true, the rank of the tensor is reduced by 1 for each entry in\n`reduction_axes`. If `keep_dims` is true, the reduced dimensions are retained\nwith length 1.\n\nIf `reduction_axes` has no entries, all dimensions are reduced, and a tensor\nwith a single element is returned.  Additionally, the axes can be negative,\nwhich are interpreted according to the indexing rules in Python.",
    "inputs": [
      { "name": "input_indices", "type": "Arg" },
      { "name": "input_values", "type": "Arg" },
      { "name": "input_shape", "type": "Arg" },
      { "name": "reduction_axes", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "keep_dims", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.SparseReshape",
    "summary": "Reshapes a SparseTensor to represent values in a new dense shape.",
    "description": "This operation has the same semantics as reshape on the represented dense\ntensor.  The `input_indices` are recomputed based on the requested `new_shape`.\n\nIf one component of `new_shape` is the special value -1, the size of that\ndimension is computed so that the total dense size remains constant.  At\nmost one component of `new_shape` can be -1.  The number of dense elements\nimplied by `new_shape` must be the same as the number of dense elements\noriginally implied by `input_shape`.\n\nReshaping does not affect the order of values in the SparseTensor.\n\nIf the input tensor has rank `R_in` and `N` non-empty values, and `new_shape`\nhas length `R_out`, then `input_indices` has shape `[N, R_in]`,\n`input_shape` has length `R_in`, `output_indices` has shape `[N, R_out]`, and\n`output_shape` has length `R_out`.",
    "inputs": [
      { "name": "input_indices", "type": "Arg" },
      { "name": "input_shape", "type": "Arg" },
      { "name": "new_shape", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output_indices", "type": "Res" },
      { "name": "output_shape", "type": "Res" }
    ]
  },
  {
    "name": "tf.SparseSegmentMean",
    "summary": "Computes the mean along sparse segments of a tensor.",
    "description": "See `tf.sparse.segment_sum` for usage examples.\n\nLike `SegmentMean`, but `segment_ids` can have rank less than `data`'s first\ndimension, selecting a subset of dimension 0, specified by `indices`.",
    "inputs": [
      { "name": "data", "type": "TF_FloatTensor" },
      { "name": "indices", "type": "Arg" },
      { "name": "segment_ids", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "sparse_gradient", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.SparseSegmentMeanGrad",
    "summary": "Computes gradients for SparseSegmentMean.",
    "description": "Returns tensor \"output\" with same shape as grad, except for dimension 0 whose\nvalue is output_dim0.",
    "inputs": [
      { "name": "grad", "type": "Arg" },
      { "name": "indices", "type": "Arg" },
      { "name": "segment_ids", "type": "Arg" },
      { "name": "output_dim0", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "TF_FloatTensor" }
    ]
  },
  {
    "name": "tf.SparseSegmentMeanWithNumSegments",
    "summary": "Computes the mean along sparse segments of a tensor.",
    "description": "Like `SparseSegmentMean`, but allows missing ids in `segment_ids`. If an id is\nmissing, the `output` tensor at that position will be zeroed.\n\nRead\n[the section on segmentation](https://tensorflow.org/api_docs/python/tf/math#Segmentation)\nfor an explanation of segments.",
    "inputs": [
      { "name": "data", "type": "TF_FloatTensor" },
      { "name": "indices", "type": "Arg" },
      { "name": "segment_ids", "type": "Arg" },
      { "name": "num_segments", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "sparse_gradient", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.SparseSegmentSqrtN",
    "summary": "Computes the sum along sparse segments of a tensor divided by the sqrt of N.",
    "description": "N is the size of the segment being reduced.\n\nSee `tf.sparse.segment_sum` for usage examples.",
    "inputs": [
      { "name": "data", "type": "TF_FloatTensor" },
      { "name": "indices", "type": "Arg" },
      { "name": "segment_ids", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "sparse_gradient", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.SparseSegmentSqrtNGrad",
    "summary": "Computes gradients for SparseSegmentSqrtN.",
    "description": "Returns tensor \"output\" with same shape as grad, except for dimension 0 whose\nvalue is output_dim0.",
    "inputs": [
      { "name": "grad", "type": "Arg" },
      { "name": "indices", "type": "Arg" },
      { "name": "segment_ids", "type": "Arg" },
      { "name": "output_dim0", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "TF_FloatTensor" }
    ]
  },
  {
    "name": "tf.SparseSegmentSqrtNWithNumSegments",
    "summary": "Computes the sum along sparse segments of a tensor divided by the sqrt of N.",
    "description": "N is the size of the segment being reduced.\n\nLike `SparseSegmentSqrtN`, but allows missing ids in `segment_ids`. If an id is\nmissing, the `output` tensor at that position will be zeroed.\n\nRead\n[the section on segmentation](https://tensorflow.org/api_docs/python/tf/math#Segmentation)\nfor an explanation of segments.",
    "inputs": [
      { "name": "data", "type": "TF_FloatTensor" },
      { "name": "indices", "type": "Arg" },
      { "name": "segment_ids", "type": "Arg" },
      { "name": "num_segments", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "sparse_gradient", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.SparseSegmentSum",
    "summary": "Computes the sum along sparse segments of a tensor.",
    "description": "Read\n[the section on segmentation](https://tensorflow.org/api_docs/python/tf/math#Segmentation)\nfor an explanation of segments.\n\nLike `SegmentSum`, but `segment_ids` can have rank less than `data`'s first\ndimension, selecting a subset of dimension 0, specified by `indices`.\n\nFor example:\n\n```python\nc = tf.constant([[1,2,3,4], [-1,-2,-3,-4], [5,6,7,8]])\n\n# Select two rows, one segment.\ntf.sparse_segment_sum(c, tf.constant([0, 1]), tf.constant([0, 0]))\n# => [[0 0 0 0]]\n\n# Select two rows, two segment.\ntf.sparse_segment_sum(c, tf.constant([0, 1]), tf.constant([0, 1]))\n# => [[ 1  2  3  4]\n#     [-1 -2 -3 -4]]\n\n# Select all rows, two segments.\ntf.sparse_segment_sum(c, tf.constant([0, 1, 2]), tf.constant([0, 0, 1]))\n# => [[0 0 0 0]\n#     [5 6 7 8]]\n\n# Which is equivalent to:\ntf.segment_sum(c, tf.constant([0, 0, 1]))\n```",
    "inputs": [
      { "name": "data", "type": "TF_IntOrFpTensor" },
      { "name": "indices", "type": "Arg" },
      { "name": "segment_ids", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "sparse_gradient", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.SparseSoftmaxCrossEntropyWithLogits",
    "summary": "Computes softmax cross entropy cost and gradients to backpropagate.",
    "description": "Unlike `SoftmaxCrossEntropyWithLogits`, this operation does not accept\na matrix of label probabilities, but rather a single label per row\nof features.  This label is considered to have probability 1.0 for the\ngiven row.\n\nInputs are the logits, not probabilities.",
    "inputs": [
      { "name": "features", "type": "Arg" },
      { "name": "labels", "type": "Arg" }
    ],
    "outputs": [
      { "name": "loss", "type": "Res" },
      { "name": "backprop", "type": "Res" }
    ]
  },
  {
    "name": "tf.SparseTensorDenseMatMul",
    "summary": "Multiply SparseTensor (of rank 2) \"A\" by dense matrix \"B\".",
    "description": "No validity checking is performed on the indices of A.  However, the following\ninput format is recommended for optimal behavior:\n\nif adjoint_a == false:\n  A should be sorted in lexicographically increasing order.  Use SparseReorder\n  if you're not sure.\nif adjoint_a == true:\n  A should be sorted in order of increasing dimension 1 (i.e., \"column major\"\n  order instead of \"row major\" order).",
    "inputs": [
      { "name": "a_indices", "type": "Arg" },
      { "name": "a_values", "type": "Arg" },
      { "name": "a_shape", "type": "Arg" },
      { "name": "b", "type": "Arg" }
    ],
    "outputs": [
      { "name": "product", "type": "TF_Tensor" }
    ],
    "attributes": [
      { "name": "adjoint_a", "type": "DefaultValuedOptionalAttr" },
      { "name": "adjoint_b", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.SparseToDense",
    "summary": "Converts a sparse representation into a dense tensor.",
    "description": "Builds an array `dense` with shape `output_shape` such that\n\n```\n# If sparse_indices is scalar\ndense[i] = (i == sparse_indices ? sparse_values : default_value)\n\n# If sparse_indices is a vector, then for each i\ndense[sparse_indices[i]] = sparse_values[i]\n\n# If sparse_indices is an n by d matrix, then for each i in [0, n)\ndense[sparse_indices[i][0], ..., sparse_indices[i][d-1]] = sparse_values[i]\n```\n\nAll other values in `dense` are set to `default_value`.  If `sparse_values` is a\nscalar, all sparse indices are set to this single value.\n\nIndices should be sorted in lexicographic order, and indices must not\ncontain any repeats. If `validate_indices` is true, these properties\nare checked during execution.",
    "inputs": [
      { "name": "sparse_indices", "type": "Arg" },
      { "name": "output_shape", "type": "Arg" },
      { "name": "sparse_values", "type": "Arg" },
      { "name": "default_value", "type": "Arg" }
    ],
    "outputs": [
      { "name": "dense", "type": "Res" }
    ],
    "attributes": [
      { "name": "validate_indices", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.Split",
    "summary": "Splits a tensor into `num_split` tensors along one dimension.",
    "inputs": [
      { "name": "split_dim", "type": "Arg" },
      { "name": "value", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ]
  },
  {
    "name": "tf.SplitV",
    "summary": "Splits a tensor into `num_split` tensors along one dimension.",
    "inputs": [
      { "name": "value", "type": "Arg" },
      { "name": "size_splits", "type": "Arg" },
      { "name": "split_dim", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ]
  },
  {
    "name": "tf.Sqrt",
    "summary": "Computes square root of x element-wise.",
    "description": "I.e., \\\\(y = \\sqrt{x} = x^{1/2}\\\\).",
    "inputs": [
      { "name": "x", "type": "TF_FpOrComplexTensor" }
    ],
    "outputs": [
      { "name": "y", "type": "TF_FpOrComplexTensor" }
    ]
  },
  {
    "name": "tf.SqrtGrad",
    "summary": "Computes the gradient for the sqrt of `x` wrt its input.",
    "description": "Specifically, `grad = dy * 0.5 / y`, where `y = sqrt(x)`, and `dy`\nis the corresponding input gradient.",
    "inputs": [
      { "name": "y", "type": "TF_FpOrComplexTensor" },
      { "name": "dy", "type": "TF_FpOrComplexTensor" }
    ],
    "outputs": [
      { "name": "z", "type": "TF_FpOrComplexTensor" }
    ]
  },
  {
    "name": "tf.Square",
    "summary": "Computes square of x element-wise.",
    "description": "I.e., \\\\(y = x * x = x^2\\\\).",
    "inputs": [
      { "name": "x", "type": "TensorOf" }
    ],
    "outputs": [
      { "name": "y", "type": "TensorOf" }
    ]
  },
  {
    "name": "tf.SquaredDifference",
    "summary": "Returns conj(x - y)(x - y) element-wise.",
    "description": "*NOTE*: `SquaredDifference` supports broadcasting. More about broadcasting\n[here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)",
    "inputs": [
      { "name": "x", "type": "TensorOf" },
      { "name": "y", "type": "TensorOf" }
    ],
    "outputs": [
      { "name": "z", "type": "TensorOf" }
    ]
  },
  {
    "name": "tf.Squeeze",
    "summary": "Removes dimensions of size 1 from the shape of a tensor.",
    "description": "Given a tensor `input`, this operation returns a tensor of the same type with\nall dimensions of size 1 removed. If you don't want to remove all size 1\ndimensions, you can remove specific size 1 dimensions by specifying\n`axis`.\n\nFor example:\n\n```\n# 't' is a tensor of shape [1, 2, 1, 3, 1, 1]\nshape(squeeze(t)) ==> [2, 3]\n```\n\nOr, to remove specific size 1 dimensions:\n\n```\n# 't' is a tensor of shape [1, 2, 1, 3, 1, 1]\nshape(squeeze(t, [2, 4])) ==> [1, 2, 3, 1]\n```",
    "inputs": [
      { "name": "input", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "squeeze_dims", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.StackCloseV2",
    "summary": "Delete the stack from its resource container.",
    "inputs": [
      { "name": "handle", "type": "Arg" }
    ]
  },
  {
    "name": "tf.StackPopV2",
    "summary": "Pop the element at the top of the stack.",
    "inputs": [
      { "name": "handle", "type": "Arg" }
    ],
    "outputs": [
      { "name": "elem", "type": "Res" }
    ]
  },
  {
    "name": "tf.StackPushV2",
    "summary": "Push an element onto the stack.",
    "inputs": [
      { "name": "handle", "type": "Arg" },
      { "name": "elem", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "swap_memory", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.StackV2",
    "summary": "A stack that produces elements in first-in last-out order.",
    "inputs": [
      { "name": "max_size", "type": "Arg" }
    ],
    "outputs": [
      { "name": "handle", "type": "Res" }
    ],
    "attributes": [
      { "name": "elem_type", "type": "TypeAttr" },
      { "name": "stack_name", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.StatefulPartitionedCall",
    "summary": "returns `f(inputs)`, where `f`'s body is placed and partitioned.",
    "description": "Asynchronously executes a function, potentially across multiple devices but\nwithin a single process. The kernel places and partitions a given function's\nunderlying graph, and executes each of the partitioned subgraphs as a function.",
    "inputs": [
      { "name": "args", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "output", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "arg_attrs", "type": "OptionalAttr" },
      { "name": "res_attrs", "type": "OptionalAttr" },
      { "name": "f", "type": "FlatSymbolRefAttr" },
      { "name": "config", "type": "StrAttr" },
      { "name": "config_proto", "type": "StrAttr" },
      { "name": "executor_type", "type": "StrAttr" }
    ]
  },
  {
    "name": "tf.StatefulStandardNormalV2",
    "summary": "Outputs random values from a normal distribution.",
    "description": "The generated values will have mean 0 and standard deviation 1.",
    "inputs": [
      { "name": "resource", "type": "Arg" },
      { "name": "algorithm", "type": "Arg" },
      { "name": "shape", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ]
  },
  {
    "name": "tf.StatefulTruncatedNormal",
    "summary": "Outputs random values from a truncated normal distribution.",
    "description": "The generated values follow a normal distribution with mean 0 and standard\ndeviation 1, except that values whose magnitude is more than 2 standard\ndeviations from the mean are dropped and re-picked.",
    "inputs": [
      { "name": "resource", "type": "Arg" },
      { "name": "algorithm", "type": "Arg" },
      { "name": "shape", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ]
  },
  {
    "name": "tf.StatefulUniform",
    "summary": "Outputs random values from a uniform distribution.",
    "description": "The generated values follow a uniform distribution in the range `[0, 1)`. The\nlower bound 0 is included in the range, while the upper bound 1 is excluded.",
    "inputs": [
      { "name": "resource", "type": "Arg" },
      { "name": "algorithm", "type": "Arg" },
      { "name": "shape", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ]
  },
  {
    "name": "tf.StatefulUniformFullInt",
    "summary": "Outputs random integers from a uniform distribution.",
    "description": "The generated values are uniform integers covering the whole range of `dtype`.",
    "inputs": [
      { "name": "resource", "type": "Arg" },
      { "name": "algorithm", "type": "TF_Int64Tensor" },
      { "name": "shape", "type": "TF_I32OrI64Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "TensorOf" }
    ]
  },
  {
    "name": "tf.StatefulUniformInt",
    "summary": "Outputs random integers from a uniform distribution.",
    "description": "The generated values are uniform integers in the range `[minval, maxval)`.\nThe lower bound `minval` is included in the range, while the upper bound\n`maxval` is excluded.\n\nThe random integers are slightly biased unless `maxval - minval` is an exact\npower of two.  The bias is small for values of `maxval - minval` significantly\nsmaller than the range of the output (either `2^32` or `2^64`).",
    "inputs": [
      { "name": "resource", "type": "Arg" },
      { "name": "algorithm", "type": "TF_Int64Tensor" },
      { "name": "shape", "type": "TF_I32OrI64Tensor" },
      { "name": "minval", "type": "TensorOf" },
      { "name": "maxval", "type": "TensorOf" }
    ],
    "outputs": [
      { "name": "output", "type": "TensorOf" }
    ]
  },
  {
    "name": "tf.StatelessMultinomial",
    "summary": "Draws samples from a multinomial distribution.",
    "inputs": [
      { "name": "logits", "type": "Arg" },
      { "name": "num_samples", "type": "Arg" },
      { "name": "seed", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ]
  },
  {
    "name": "tf.StatelessParameterizedTruncatedNormal",
    "inputs": [
      { "name": "shape", "type": "Arg" },
      { "name": "seed", "type": "Arg" },
      { "name": "means", "type": "Arg" },
      { "name": "stddevs", "type": "Arg" },
      { "name": "minvals", "type": "Arg" },
      { "name": "maxvals", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ]
  },
  {
    "name": "tf.StatelessRandomBinomial",
    "summary": "Outputs deterministic pseudorandom random numbers from a binomial distribution.",
    "description": "Outputs random values from a binomial distribution.\n\nThe outputs are a deterministic function of `shape`, `seed`, `counts`, and `probs`.",
    "inputs": [
      { "name": "shape", "type": "Arg" },
      { "name": "seed", "type": "Arg" },
      { "name": "counts", "type": "Arg" },
      { "name": "probs", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ]
  },
  {
    "name": "tf.StatelessRandomGammaV2",
    "summary": "Outputs deterministic pseudorandom random numbers from a gamma distribution.",
    "description": "Outputs random values from a gamma distribution.\n\nThe outputs are a deterministic function of `shape`, `seed`, and `alpha`.",
    "inputs": [
      { "name": "shape", "type": "Arg" },
      { "name": "seed", "type": "Arg" },
      { "name": "alpha", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ]
  },
  {
    "name": "tf.StatelessRandomGetAlg",
    "summary": "Picks the best counter-based RNG algorithm based on device.",
    "description": "This op picks the best counter-based RNG algorithm based on device.",
    "outputs": [
      { "name": "alg", "type": "Res" }
    ]
  },
  {
    "name": "tf.StatelessRandomGetKeyCounter",
    "summary": "Scrambles seed into key and counter, using the best algorithm based on device.",
    "description": "This op scrambles a shape-[2] seed into a key and a counter, both needed by counter-based RNG algorithms. The scrambing uses the best algorithm based on device. The scrambling is opaque but approximately satisfies the property that different seed results in different key/counter pair (which will in turn result in different random numbers).",
    "inputs": [
      { "name": "seed", "type": "Arg" }
    ],
    "outputs": [
      { "name": "key", "type": "Res" },
      { "name": "counter", "type": "Res" }
    ]
  },
  {
    "name": "tf.StatelessRandomGetKeyCounterAlg",
    "summary": "Picks the best algorithm based on device, and scrambles seed into key and counter.",
    "description": "This op picks the best counter-based RNG algorithm based on device, and scrambles a shape-[2] seed into a key and a counter, both needed by the counter-based algorithm. The scrambling is opaque but approximately satisfies the property that different seed results in different key/counter pair (which will in turn result in different random numbers).",
    "inputs": [
      { "name": "seed", "type": "Arg" }
    ],
    "outputs": [
      { "name": "key", "type": "Res" },
      { "name": "counter", "type": "Res" },
      { "name": "alg", "type": "Res" }
    ]
  },
  {
    "name": "tf.StatelessRandomNormal",
    "summary": "Outputs deterministic pseudorandom values from a normal distribution.",
    "description": "The generated values will have mean 0 and standard deviation 1.\n\nThe outputs are a deterministic function of `shape` and `seed`.",
    "inputs": [
      { "name": "shape", "type": "Arg" },
      { "name": "seed", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ]
  },
  {
    "name": "tf.StatelessRandomNormalV2",
    "summary": "Outputs deterministic pseudorandom values from a normal distribution.",
    "description": "The generated values will have mean 0 and standard deviation 1.\n\nThe outputs are a deterministic function of `shape`, `key`, `counter` and `alg`.",
    "inputs": [
      { "name": "shape", "type": "Arg" },
      { "name": "key", "type": "Arg" },
      { "name": "counter", "type": "Arg" },
      { "name": "alg", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ]
  },
  {
    "name": "tf.StatelessRandomPoisson",
    "summary": "Outputs deterministic pseudorandom random numbers from a Poisson distribution.",
    "description": "Outputs random values from a Poisson distribution.\n\nThe outputs are a deterministic function of `shape`, `seed`, and `lam`.",
    "inputs": [
      { "name": "shape", "type": "Arg" },
      { "name": "seed", "type": "Arg" },
      { "name": "lam", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ]
  },
  {
    "name": "tf.StatelessRandomUniform",
    "summary": "Outputs deterministic pseudorandom random values from a uniform distribution.",
    "description": "The generated values follow a uniform distribution in the range `[0, 1)`. The\nlower bound 0 is included in the range, while the upper bound 1 is excluded.\n\nThe outputs are a deterministic function of `shape` and `seed`.",
    "inputs": [
      { "name": "shape", "type": "Arg" },
      { "name": "seed", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ]
  },
  {
    "name": "tf.StatelessRandomUniformFullInt",
    "summary": "Outputs deterministic pseudorandom random integers from a uniform distribution.",
    "description": "The generated values are uniform integers covering the whole range of `dtype`.\n\nThe outputs are a deterministic function of `shape` and `seed`.",
    "inputs": [
      { "name": "shape", "type": "Arg" },
      { "name": "seed", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ]
  },
  {
    "name": "tf.StatelessRandomUniformFullIntV2",
    "summary": "Outputs deterministic pseudorandom random integers from a uniform distribution.",
    "description": "The generated values are uniform integers covering the whole range of `dtype`.\n\nThe outputs are a deterministic function of `shape`, `key`, `counter` and `alg`.",
    "inputs": [
      { "name": "shape", "type": "Arg" },
      { "name": "key", "type": "Arg" },
      { "name": "counter", "type": "Arg" },
      { "name": "alg", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ]
  },
  {
    "name": "tf.StatelessRandomUniformInt",
    "summary": "Outputs deterministic pseudorandom random integers from a uniform distribution.",
    "description": "The generated values follow a uniform distribution in the range `[minval, maxval)`.\n\nThe outputs are a deterministic function of `shape`, `seed`, `minval`, and `maxval`.",
    "inputs": [
      { "name": "shape", "type": "Arg" },
      { "name": "seed", "type": "Arg" },
      { "name": "minval", "type": "Arg" },
      { "name": "maxval", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ]
  },
  {
    "name": "tf.StatelessRandomUniformIntV2",
    "summary": "Outputs deterministic pseudorandom random integers from a uniform distribution.",
    "description": "The generated values follow a uniform distribution in the range `[minval, maxval)`.\n\nThe outputs are a deterministic function of `shape`, `key`, `counter`, `alg`, `minval` and `maxval`.",
    "inputs": [
      { "name": "shape", "type": "Arg" },
      { "name": "key", "type": "Arg" },
      { "name": "counter", "type": "Arg" },
      { "name": "alg", "type": "Arg" },
      { "name": "minval", "type": "Arg" },
      { "name": "maxval", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ]
  },
  {
    "name": "tf.StatelessRandomUniformV2",
    "summary": "Outputs deterministic pseudorandom random values from a uniform distribution.",
    "description": "The generated values follow a uniform distribution in the range `[0, 1)`. The\nlower bound 0 is included in the range, while the upper bound 1 is excluded.\n\nThe outputs are a deterministic function of `shape`, `key`, `counter` and `alg`.",
    "inputs": [
      { "name": "shape", "type": "Arg" },
      { "name": "key", "type": "Arg" },
      { "name": "counter", "type": "Arg" },
      { "name": "alg", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ]
  },
  {
    "name": "tf.StatelessTruncatedNormal",
    "summary": "Outputs deterministic pseudorandom values from a truncated normal distribution.",
    "description": "The generated values follow a normal distribution with mean 0 and standard\ndeviation 1, except that values whose magnitude is more than 2 standard\ndeviations from the mean are dropped and re-picked.\n\nThe outputs are a deterministic function of `shape` and `seed`.",
    "inputs": [
      { "name": "shape", "type": "Arg" },
      { "name": "seed", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ]
  },
  {
    "name": "tf.StatelessTruncatedNormalV2",
    "summary": "Outputs deterministic pseudorandom values from a truncated normal distribution.",
    "description": "The generated values follow a normal distribution with mean 0 and standard\ndeviation 1, except that values whose magnitude is more than 2 standard\ndeviations from the mean are dropped and re-picked.\n\nThe outputs are a deterministic function of `shape`, `key`, `counter` and `alg`.",
    "inputs": [
      { "name": "shape", "type": "Arg" },
      { "name": "key", "type": "Arg" },
      { "name": "counter", "type": "Arg" },
      { "name": "alg", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ]
  },
  {
    "name": "tf.StaticRegexFullMatch",
    "summary": "Check if the input matches the regex pattern.",
    "description": "The input is a string tensor of any shape. The pattern is the\nregular expression to be matched with every element of the input tensor.\nThe boolean values (True or False) of the output tensor indicate\nif the input matches the regex pattern provided.\n\nThe pattern follows the re2 syntax (https://github.com/google/re2/wiki/Syntax)",
    "inputs": [
      { "name": "input", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "pattern", "type": "StrAttr" }
    ]
  },
  {
    "name": "tf.StopGradient",
    "summary": "Stops gradient computation.",
    "description": "When executed in a graph, this op outputs its input tensor as-is.\n\nWhen building ops to compute gradients, this op prevents the contribution of\nits inputs to be taken into account.  Normally, the gradient generator adds ops\nto a graph to compute the derivatives of a specified 'loss' by recursively\nfinding out inputs that contributed to its computation.  If you insert this op\nin the graph it inputs are masked from the gradient generator.  They are not\ntaken into account for computing gradients.\n\nThis is useful any time you want to compute a value with TensorFlow but need\nto pretend that the value was a constant. For example, the softmax function\nfor a vector x can be written as\n\n```python\n\n  def softmax(x):\n    numerator = tf.exp(x)\n    denominator = tf.reduce_sum(numerator)\n    return numerator / denominator\n```\n\nThis however is susceptible to overflow if the values in x are large. An\nalternative more stable way is to subtract the maximum of x from each of the\nvalues.\n\n```python\n\n  def stable_softmax(x):\n    z = x - tf.reduce_max(x)\n    numerator = tf.exp(z)\n    denominator = tf.reduce_sum(numerator)\n    return numerator / denominator\n```\n\nHowever, when we backprop through the softmax to x, we dont want to backprop\nthrough the `tf.reduce_max(x)` (if the max values are not unique then the\ngradient could flow to the wrong input) calculation and treat that as a\nconstant. Therefore, we should write this out as\n\n```python\n\n  def stable_softmax(x):\n    z = x - tf.stop_gradient(tf.reduce_max(x))\n    numerator = tf.exp(z)\n    denominator = tf.reduce_sum(numerator)\n    return numerator / denominator\n```\n\nSome other examples include:\n\n*  The *EM* algorithm where the *M-step* should not involve backpropagation\n   through the output of the *E-step*.\n*  Contrastive divergence training of Boltzmann machines where, when\n   differentiating the energy function, the training must not backpropagate\n   through the graph that generated the samples from the model.\n*  Adversarial training, where no backprop should happen through the adversarial\n   example generation process.",
    "inputs": [
      { "name": "input", "type": "TF_Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "TF_Tensor" }
    ]
  },
  {
    "name": "tf.StoreMinibatchStatisticsInFdo",
    "summary": "Store the number of IDs and unique IDs in an FDO table.",
    "inputs": [
      { "name": "program_key", "type": "TF_StrTensor" },
      { "name": "max_ids", "type": "TF_Int32Tensor" },
      { "name": "max_uniques", "type": "TF_Int32Tensor" }
    ],
    "attributes": [
      { "name": "sample_count", "type": "ConfinedAttr" },
      { "name": "num_replica", "type": "ConfinedAttr" },
      { "name": "feature_width", "type": "ConfinedAttr" },
      { "name": "num_sc_per_chip", "type": "ConfinedAttr" },
      { "name": "table_name", "type": "StrAttr" },
      { "name": "mini_batch_splits", "type": "StrAttr" }
    ]
  },
  {
    "name": "tf.StridedSlice",
    "summary": "Return a strided slice from `input`.",
    "description": "Note, most python users will want to use the Python `Tensor.__getitem__`\nor `Variable.__getitem__` rather than this op directly.\n\nThe goal of this op is to produce a new tensor with a subset of\nthe elements from the `n` dimensional `input` tensor. The subset is chosen using\na sequence of `m` sparse range specifications encoded into the arguments\nof this function. Note, in some cases\n`m` could be equal to `n`, but this need not be the case. Each\nrange specification entry can be one of the following:\n\n- An ellipsis (...). Ellipses are used to imply zero or more\n  dimensions of full-dimension selection and are produced using\n  `ellipsis_mask`. For example, `foo[...]` is the identity slice.\n\n- A new axis. This is used to insert a new shape=1 dimension and is\n  produced using `new_axis_mask`. For example, `foo[:, ...]` where\n  `foo` is shape `(3, 4)` produces a `(1, 3, 4)` tensor.\n\n\n- A range `begin:end:stride`. This is used to specify how much to choose from\n  a given dimension. `stride` can be any integer but 0.  `begin` is an integer\n  which represents the index of the first value to select while `end` represents\n  the index of the last value to select. The number of values selected in each\n  dimension is `end - begin` if `stride > 0` and `begin - end` if `stride < 0`.\n  `begin` and `end` can be negative where `-1` is the last element, `-2` is\n  the second to last. `begin_mask` controls whether to replace the explicitly\n  given `begin` with an implicit effective value of `0` if `stride > 0` and\n  `-1` if `stride < 0`. `end_mask` is analogous but produces the number\n  required to create the largest open interval. For example, given a shape\n  `(3,)` tensor `foo[:]`, the effective `begin` and `end` are `0` and `3`. Do\n  not assume this is equivalent to `foo[0:-1]` which has an effective `begin`\n  and `end` of `0` and `2`. Another example is `foo[-2::-1]` which reverses the\n  first dimension of a tensor while dropping the last two (in the original\n  order elements). For example `foo = [1,2,3,4]; foo[-2::-1]` is `[4,3]`.\n\n- A single index. This is used to keep only elements that have a given\n  index. For example (`foo[2, :]` on a shape `(5,6)` tensor produces a\n  shape `(6,)` tensor. This is encoded in `begin` and `end` and\n  `shrink_axis_mask`.\n\nEach conceptual range specification is encoded in the op's argument. This\nencoding is best understand by considering a non-trivial example. In\nparticular,\n`foo[1, 2:4, None, ..., :-3:-1, :]` will be encoded as\n\n```\nbegin = [1, 2, x, x, 0, x] # x denotes don't care (usually 0)\nend = [2, 4, x, x, -3, x]\nstrides = [1, 1, x, x, -1, 1]\nbegin_mask = 1<<4 | 1<<5 = 48\nend_mask = 1<<5 = 32\nellipsis_mask = 1<<3 = 8\nnew_axis_mask = 1<<2 = 4\nshrink_axis_mask = 1<<0 = 1\n```\n\nIn this case if `foo.shape` is (5, 5, 5, 5, 5, 5) the final shape of\nthe slice becomes (2, 1, 5, 5, 2, 5).\nLet us walk step by step through each argument specification.\n\n1.  The first argument in the example slice is turned into `begin = 1` and\n`end = begin + 1 = 2`. To disambiguate from the original spec `2:4` we\nalso set the appropriate bit in `shrink_axis_mask`.\n\n2. `2:4` is contributes 2, 4, 1 to begin, end, and stride. All masks have\nzero bits contributed.\n\n3. None is a synonym for `tf.newaxis`. This means insert a dimension of size 1\ndimension in the final shape. Dummy values are contributed to begin,\nend and stride, while the new_axis_mask bit is set.\n\n4. `...` grab the full ranges from as many dimensions as needed to\nfully specify a slice for every dimension of the input shape.\n\n5. `:-3:-1` shows the use of negative indices. A negative index `i` associated\nwith a dimension that has shape `s` is converted to a positive index\n`s + i`. So `-1` becomes `s-1` (i.e. the last element). This conversion\nis done internally so begin, end and strides receive x, -3, and -1.\nThe appropriate begin_mask bit is set to indicate the start range is the\nfull range (ignoring the x).\n\n6. `:` indicates that the entire contents of the corresponding dimension\nis selected. This is equivalent to `::` or `0::1`. begin, end, and strides\nreceive 0, 0, and 1, respectively. The appropriate bits in `begin_mask` and\n`end_mask` are also set.\n\n*Requirements*:\n  `0 != strides[i] for i in [0, m)`\n  `ellipsis_mask must be a power of two (only one ellipsis)`",
    "inputs": [
      { "name": "input", "type": "TF_Tensor" },
      { "name": "begin", "type": "Arg" },
      { "name": "end", "type": "Arg" },
      { "name": "strides", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "TF_Tensor" }
    ],
    "attributes": [
      { "name": "begin_mask", "type": "DefaultValuedOptionalAttr" },
      { "name": "end_mask", "type": "DefaultValuedOptionalAttr" },
      { "name": "ellipsis_mask", "type": "DefaultValuedOptionalAttr" },
      { "name": "new_axis_mask", "type": "DefaultValuedOptionalAttr" },
      { "name": "shrink_axis_mask", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.StridedSliceGrad",
    "summary": "Returns the gradient of `StridedSlice`.",
    "description": "Since `StridedSlice` cuts out pieces of its `input` which is size\n`shape`, its gradient will have the same shape (which is passed here\nas `shape`). The gradient will be zero in any element that the slice\ndoes not select.\n\nArguments are the same as StridedSliceGrad with the exception that\n`dy` is the input gradient to be propagated and `shape` is the\nshape of `StridedSlice`'s `input`.",
    "inputs": [
      { "name": "shape", "type": "TF_I32OrI64Tensor" },
      { "name": "begin", "type": "TF_I32OrI64Tensor" },
      { "name": "end", "type": "TF_I32OrI64Tensor" },
      { "name": "strides", "type": "TF_I32OrI64Tensor" },
      { "name": "dy", "type": "TF_Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "TF_Tensor" }
    ],
    "attributes": [
      { "name": "begin_mask", "type": "DefaultValuedOptionalAttr" },
      { "name": "end_mask", "type": "DefaultValuedOptionalAttr" },
      { "name": "ellipsis_mask", "type": "DefaultValuedOptionalAttr" },
      { "name": "new_axis_mask", "type": "DefaultValuedOptionalAttr" },
      { "name": "shrink_axis_mask", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.StringFormat",
    "summary": "Formats a string template using a list of tensors.",
    "description": "Formats a string template using a list of tensors, pretty-printing tensor summaries.",
    "inputs": [
      { "name": "inputs", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "output", "type": "TF_StrTensor" }
    ],
    "attributes": [
      { "name": "strtemplate", "type": "DefaultValuedStrAttr" },
      { "name": "placeholder", "type": "DefaultValuedStrAttr" },
      { "name": "summarize", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.StringJoin",
    "summary": "Joins the strings in the given list of string tensors into one tensor;",
    "description": "with the given separator (default is an empty separator).\n\nExamples:\n\n>>> s = [\"hello\", \"world\", \"tensorflow\"]\n>>> tf.strings.join(s, \" \")\n<tf.Tensor: shape=(), dtype=string, numpy=b'hello world tensorflow'>",
    "inputs": [
      { "name": "inputs", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "TF_StrTensor" }
    ],
    "attributes": [
      { "name": "separator", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.StringStrip",
    "summary": "Strip leading and trailing whitespaces from the Tensor.",
    "description": "Examples:\n\n>>> tf.strings.strip([\"\\nTensorFlow\", \"     The python library    \"]).numpy()\narray([b'TensorFlow', b'The python library'], dtype=object)",
    "inputs": [
      { "name": "input", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ]
  },
  {
    "name": "tf.StringToHashBucketFast",
    "summary": "Converts each string in the input Tensor to its hash mod by a number of buckets.",
    "description": "The hash function is deterministic on the content of the string within the\nprocess and will never change. However, it is not suitable for cryptography.\nThis function may be used when CPU time is scarce and inputs are trusted or\nunimportant. There is a risk of adversaries constructing inputs that all hash\nto the same bucket. To prevent this problem, use a strong hash function with\n`tf.string_to_hash_bucket_strong`.\n\nExamples:\n\n>>> tf.strings.to_hash_bucket_fast([\"Hello\", \"TensorFlow\", \"2.x\"], 3).numpy()\narray([0, 2, 2])",
    "inputs": [
      { "name": "input", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "num_buckets", "type": "ConfinedAttr" }
    ]
  },
  {
    "name": "tf.Sub",
    "summary": "Returns x - y element-wise.",
    "description": "*NOTE*: `Subtract` supports broadcasting. More about broadcasting\n[here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)",
    "inputs": [
      { "name": "x", "type": "TensorOf" },
      { "name": "y", "type": "TensorOf" }
    ],
    "outputs": [
      { "name": "z", "type": "TensorOf" }
    ]
  },
  {
    "name": "tf.Sum",
    "summary": "Computes the sum of elements across dimensions of a tensor.",
    "description": "Reduces `input` along the dimensions given in `axis`. Unless\n`keep_dims` is true, the rank of the tensor is reduced by 1 for each entry in\n`axis`. If `keep_dims` is true, the reduced dimensions are\nretained with length 1.",
    "inputs": [
      { "name": "input", "type": "Arg" },
      { "name": "reduction_indices", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "keep_dims", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.SummaryWriter",
    "summary": "Returns a handle to be used to access a summary writer.",
    "description": "The summary writer is an in-graph resource which can be used by ops to write\nsummaries to event files.\n\nwriter: the summary writer resource. Scalar handle.",
    "outputs": [
      { "name": "writer", "type": "Res" }
    ],
    "attributes": [
      { "name": "shared_name", "type": "StrAttr" },
      { "name": "container", "type": "StrAttr" }
    ]
  },
  {
    "name": "tf.Svd",
    "summary": "Computes the singular value decompositions of one or more matrices.",
    "description": "Computes the SVD of each inner matrix in `input` such that\n`input[..., :, :] = u[..., :, :] * diag(s[..., :, :]) * transpose(v[..., :, :])`\n\n```python\n# a is a tensor containing a batch of matrices.\n# s is a tensor of singular values for each matrix.\n# u is the tensor containing the left singular vectors for each matrix.\n# v is the tensor containing the right singular vectors for each matrix.\ns, u, v = svd(a)\ns, _, _ = svd(a, compute_uv=False)\n```",
    "inputs": [
      { "name": "input", "type": "Arg" }
    ],
    "outputs": [
      { "name": "s", "type": "Res" },
      { "name": "u", "type": "Res" },
      { "name": "v", "type": "Res" }
    ],
    "attributes": [
      { "name": "compute_uv", "type": "DefaultValuedOptionalAttr" },
      { "name": "full_matrices", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.SymbolicGradient",
    "summary": "Computes the gradient function for function f via backpropagation.",
    "inputs": [
      { "name": "input", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "f", "type": "SymbolRefAttr" }
    ]
  },
  {
    "name": "tf.TakeDataset",
    "summary": "Creates a dataset that contains `count` elements from the `input_dataset`.",
    "inputs": [
      { "name": "input_dataset", "type": "TF_VariantTensor" },
      { "name": "count", "type": "Arg" }
    ],
    "outputs": [
      { "name": "handle", "type": "TF_VariantTensor" }
    ],
    "attributes": [
      { "name": "output_types", "type": "ConfinedAttr" },
      { "name": "output_shapes", "type": "ConfinedAttr" },
      { "name": "metadata", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.TakeWhileDataset",
    "summary": "Creates a dataset that stops iteration when predicate` is false.",
    "description": "The `predicate` function must return a scalar boolean and accept the\nfollowing arguments:\n\n* One tensor for each component of an element of `input_dataset`.\n* One tensor for each value in `other_arguments`.",
    "inputs": [
      { "name": "input_dataset", "type": "TF_VariantTensor" },
      { "name": "other_arguments", "type": "Arg" }
    ],
    "outputs": [
      { "name": "handle", "type": "TF_VariantTensor" }
    ],
    "attributes": [
      { "name": "predicate", "type": "SymbolRefAttr" },
      { "name": "output_types", "type": "ConfinedAttr" },
      { "name": "output_shapes", "type": "ConfinedAttr" },
      { "name": "metadata", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.Tan",
    "summary": "Computes tan of x element-wise.",
    "description": "Given an input tensor, this function computes tangent of every\n  element in the tensor. Input range is `(-inf, inf)` and\n  output range is `(-inf, inf)`. If input lies outside the boundary, `nan`\n  is returned.\n\n  ```python\n  x = tf.constant([-float(\"inf\"), -9, -0.5, 1, 1.2, 200, 10000, float(\"inf\")])\n  tf.math.tan(x) ==> [nan 0.45231566 -0.5463025 1.5574077 2.572152 -1.7925274 0.32097113 nan]\n  ```",
    "inputs": [
      { "name": "x", "type": "TF_FpOrComplexTensor" }
    ],
    "outputs": [
      { "name": "y", "type": "TF_FpOrComplexTensor" }
    ]
  },
  {
    "name": "tf.Tanh",
    "summary": "Computes hyperbolic tangent of `x` element-wise.",
    "description": "Given an input tensor, this function computes hyperbolic tangent of every\n  element in the tensor. Input range is `[-inf, inf]` and\n  output range is `[-1,1]`.\n\n  >>> x = tf.constant([-float(\"inf\"), -5, -0.5, 1, 1.2, 2, 3, float(\"inf\")])\n  >>> tf.math.tanh(x)\n  <tf.Tensor: shape=(8,), dtype=float32, numpy=\n  array([-1.0, -0.99990916, -0.46211717,  0.7615942 ,  0.8336547 ,\n          0.9640276 ,  0.9950547 ,  1.0], dtype=float32)>",
    "inputs": [
      { "name": "x", "type": "TF_FpOrComplexTensor" }
    ],
    "outputs": [
      { "name": "y", "type": "TF_FpOrComplexTensor" }
    ],
    "category": "Activation"
  },
  {
    "name": "tf.TanhGrad",
    "summary": "Computes the gradient for the tanh of `x` wrt its input.",
    "description": "Specifically, `grad = dy * (1 - y*y)`, where `y = tanh(x)`, and `dy`\nis the corresponding input gradient.",
    "inputs": [
      { "name": "y", "type": "TF_FpOrComplexTensor" },
      { "name": "dy", "type": "TF_FpOrComplexTensor" }
    ],
    "outputs": [
      { "name": "z", "type": "TF_FpOrComplexTensor" }
    ]
  },
  {
    "name": "tf.TensorArrayCloseV3",
    "summary": "Delete the TensorArray from its resource container.",
    "description": "This enables the user to close and release the resource in the middle\nof a step/run.",
    "inputs": [
      { "name": "handle", "type": "Arg" }
    ]
  },
  {
    "name": "tf.TensorArrayConcatV3",
    "summary": "Concat the elements from the TensorArray into value `value`.",
    "description": "Takes `T` elements of shapes\n\n  ```\n  (n0 x d0 x d1 x ...), (n1 x d0 x d1 x ...), ..., (n(T-1) x d0 x d1 x ...)\n  ```\n\nand concatenates them into a Tensor of shape:\n\n  ```\n  (n0 + n1 + ... + n(T-1) x d0 x d1 x ...)\n  ```\n\nAll elements must have the same shape (excepting the first dimension).",
    "inputs": [
      { "name": "handle", "type": "Arg" },
      { "name": "flow_in", "type": "Arg" }
    ],
    "outputs": [
      { "name": "value", "type": "Res" },
      { "name": "lengths", "type": "Res" }
    ],
    "attributes": [
      { "name": "element_shape_except0", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.TensorArrayGatherV3",
    "summary": "Gather specific elements from the TensorArray into output `value`.",
    "description": "All elements selected by `indices` must have the same shape.",
    "inputs": [
      { "name": "handle", "type": "Arg" },
      { "name": "indices", "type": "Arg" },
      { "name": "flow_in", "type": "Arg" }
    ],
    "outputs": [
      { "name": "value", "type": "Res" }
    ],
    "attributes": [
      { "name": "element_shape", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.TensorArrayGradV3",
    "summary": "Creates a TensorArray for storing the gradients of values in the given handle.",
    "description": "If the given TensorArray gradient already exists, returns a reference to it.\n\nLocks the size of the original TensorArray by disabling its dynamic size flag.\n\n**A note about the input flow_in:**\n\nThe handle flow_in forces the execution of the gradient lookup to occur\nonly after certain other operations have occurred.  For example, when\nthe forward TensorArray is dynamically sized, writes to this TensorArray\nmay resize the object.  The gradient TensorArray is statically sized based\non the size of the forward TensorArray when this operation executes.\nFurthermore, the size of the forward TensorArray is frozen by this call.\nAs a result, the flow is used to ensure that the call to generate the gradient\nTensorArray only happens after all writes are executed.\n\nIn the case of dynamically sized TensorArrays, gradient computation should\nonly be performed on read operations that have themselves been chained via\nflow to occur only after all writes have executed. That way the final size\nof the forward TensorArray is known when this operation is called.\n\n**A note about the source attribute:**\n\nTensorArray gradient calls use an accumulator TensorArray object.  If\nmultiple gradients are calculated and run in the same session, the multiple\ngradient nodes may accidentally flow through the same accumulator TensorArray.\nThis double counts and generally breaks the TensorArray gradient flow.\n\nThe solution is to identify which gradient call this particular\nTensorArray gradient is being called in.  This is performed by identifying\na unique string (e.g. \"gradients\", \"gradients_1\", ...) from the input\ngradient Tensor's name.  This string is used as a suffix when creating\nthe TensorArray gradient object here (the attribute `source`).\n\nThe attribute `source` is added as a suffix to the forward TensorArray's\nname when performing the creation / lookup, so that each separate gradient\ncalculation gets its own TensorArray accumulator.",
    "inputs": [
      { "name": "handle", "type": "Arg" },
      { "name": "flow_in", "type": "Arg" }
    ],
    "outputs": [
      { "name": "grad_handle", "type": "Res" },
      { "name": "flow_out", "type": "TF_Float32Tensor" }
    ],
    "attributes": [
      { "name": "source", "type": "StrAttr" }
    ]
  },
  {
    "name": "tf.TensorArrayReadV3",
    "summary": "Read an element from the TensorArray into output `value`.",
    "inputs": [
      { "name": "handle", "type": "Arg" },
      { "name": "index", "type": "TF_Int32Tensor" },
      { "name": "flow_in", "type": "Arg" }
    ],
    "outputs": [
      { "name": "value", "type": "Res" }
    ]
  },
  {
    "name": "tf.TensorArrayScatterV3",
    "summary": "Scatter the data from the input value into specific TensorArray elements.",
    "description": "`indices` must be a vector, its length must match the first dim of `value`.",
    "inputs": [
      { "name": "handle", "type": "Arg" },
      { "name": "indices", "type": "Arg" },
      { "name": "value", "type": "Arg" },
      { "name": "flow_in", "type": "Arg" }
    ],
    "outputs": [
      { "name": "flow_out", "type": "Res" }
    ]
  },
  {
    "name": "tf.TensorArraySizeV3",
    "summary": "Get the current size of the TensorArray.",
    "inputs": [
      { "name": "handle", "type": "Arg" },
      { "name": "flow_in", "type": "Arg" }
    ],
    "outputs": [
      { "name": "size", "type": "Res" }
    ]
  },
  {
    "name": "tf.TensorArraySplitV3",
    "summary": "Split the data from the input value into TensorArray elements.",
    "description": "Assuming that `lengths` takes on values\n\n  ```\n  (n0, n1, ..., n(T-1))\n  ```\n\nand that `value` has shape\n\n  ```\n  (n0 + n1 + ... + n(T-1) x d0 x d1 x ...),\n  ```\n\nthis splits values into a TensorArray with T tensors.\n\nTensorArray index t will be the subtensor of values with starting position\n\n  ```\n  (n0 + n1 + ... + n(t-1), 0, 0, ...)\n  ```\n\nand having size\n\n  ```\n  nt x d0 x d1 x ...\n  ```",
    "inputs": [
      { "name": "handle", "type": "Arg" },
      { "name": "value", "type": "Arg" },
      { "name": "lengths", "type": "Arg" },
      { "name": "flow_in", "type": "Arg" }
    ],
    "outputs": [
      { "name": "flow_out", "type": "Res" }
    ]
  },
  {
    "name": "tf.TensorArrayV3",
    "summary": "An array of Tensors of given size.",
    "description": "Write data via Write and read via Read or Pack.",
    "inputs": [
      { "name": "size", "type": "Arg" }
    ],
    "outputs": [
      { "name": "handle", "type": "Res" },
      { "name": "flow", "type": "Res" }
    ],
    "attributes": [
      { "name": "dtype", "type": "TypeAttr" },
      { "name": "element_shape", "type": "DefaultValuedOptionalAttr" },
      { "name": "dynamic_size", "type": "DefaultValuedOptionalAttr" },
      { "name": "clear_after_read", "type": "DefaultValuedOptionalAttr" },
      { "name": "identical_element_shapes", "type": "DefaultValuedOptionalAttr" },
      { "name": "tensor_array_name", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.TensorArrayWriteV3",
    "summary": "Push an element onto the tensor_array.",
    "inputs": [
      { "name": "handle", "type": "Arg" },
      { "name": "index", "type": "Arg" },
      { "name": "value", "type": "Arg" },
      { "name": "flow_in", "type": "Arg" }
    ],
    "outputs": [
      { "name": "flow_out", "type": "Res" }
    ]
  },
  {
    "name": "tf.TensorListConcatV2",
    "summary": "Concats all tensors in the list along the 0th dimension.",
    "description": "Requires that all tensors have the same shape except the first dimension.\n\ninput_handle: The input list.\nelement_shape: The shape of the uninitialized elements in the list. If the first\n  dimension is not -1, it is assumed that all list elements have the same\n  leading dim.\nleading_dims: The list of leading dims of uninitialized list elements. Used if\n  the leading dim of input_handle.element_shape or the element_shape input arg\n  is not already set.\ntensor: The concated result.\nlengths: Output tensor containing sizes of the 0th dimension of tensors in the list, used for computing the gradient.",
    "inputs": [
      { "name": "input_handle", "type": "TF_VariantTensor" },
      { "name": "element_shape", "type": "TF_I32OrI64Tensor" },
      { "name": "leading_dims", "type": "TF_Int64Tensor" }
    ],
    "outputs": [
      { "name": "tensor", "type": "TF_Tensor" },
      { "name": "lengths", "type": "TF_Int64Tensor" }
    ]
  },
  {
    "name": "tf.TensorListElementShape",
    "summary": "The shape of the elements of the given list, as a tensor.",
    "description": "input_handle: the list\n  element_shape: the shape of elements of the list",
    "inputs": [
      { "name": "input_handle", "type": "TF_VariantTensor" }
    ],
    "outputs": [
      { "name": "element_shape", "type": "TF_I32OrI64Tensor" }
    ]
  },
  {
    "name": "tf.TensorListFromTensor",
    "summary": "Creates a TensorList which, when stacked, has the value of `tensor`.",
    "description": "Each tensor in the result list corresponds to one row of the input tensor.\n\ntensor: The input tensor.\noutput_handle: The list.",
    "inputs": [
      { "name": "tensor", "type": "TF_Tensor" },
      { "name": "element_shape", "type": "TF_I32OrI64Tensor" }
    ],
    "outputs": [
      { "name": "output_handle", "type": "TF_VariantTensor" }
    ]
  },
  {
    "name": "tf.TensorListGather",
    "summary": "Creates a Tensor by indexing into the TensorList.",
    "description": "Each row in the produced Tensor corresponds to the element in the TensorList\nspecified by the given index (see `tf.gather`).\n\ninput_handle: The input tensor list.\nindices: The indices used to index into the list.\nvalues: The tensor.",
    "inputs": [
      { "name": "input_handle", "type": "TF_VariantTensor" },
      { "name": "indices", "type": "TF_Int32Tensor" },
      { "name": "element_shape", "type": "TF_Int32Tensor" }
    ],
    "outputs": [
      { "name": "values", "type": "TF_Tensor" }
    ]
  },
  {
    "name": "tf.TensorListGetItem",
    "inputs": [
      { "name": "input_handle", "type": "TF_VariantTensor" },
      { "name": "index", "type": "TF_Int32Tensor" },
      { "name": "element_shape", "type": "TF_I32OrI64Tensor" }
    ],
    "outputs": [
      { "name": "item", "type": "TF_Tensor" }
    ]
  },
  {
    "name": "tf.TensorListLength",
    "summary": "Returns the number of tensors in the input tensor list.",
    "description": "input_handle: the input list\nlength: the number of tensors in the list",
    "inputs": [
      { "name": "input_handle", "type": "TF_VariantTensor" }
    ],
    "outputs": [
      { "name": "length", "type": "TF_Int32Tensor" }
    ]
  },
  {
    "name": "tf.TensorListPopBack",
    "summary": "Returns the last element of the input list as well as a list with all but that element.",
    "description": "Fails if the list is empty.\n\ninput_handle: the input list\ntensor: the withdrawn last element of the list\nelement_dtype: the type of elements in the list\nelement_shape: the shape of the output tensor",
    "inputs": [
      { "name": "input_handle", "type": "TF_VariantTensor" },
      { "name": "element_shape", "type": "TF_Int32Tensor" }
    ],
    "outputs": [
      { "name": "output_handle", "type": "TF_VariantTensor" },
      { "name": "tensor", "type": "TF_Tensor" }
    ]
  },
  {
    "name": "tf.TensorListPushBack",
    "summary": "Returns a list which has the passed-in `Tensor` as last element and the other elements of the given list in `input_handle`.",
    "description": "tensor: The tensor to put on the list.\ninput_handle: The old list.\noutput_handle: A list with the elements of the old list followed by tensor.\nelement_dtype: the type of elements in the list.\nelement_shape: a shape compatible with that of elements in the list.",
    "inputs": [
      { "name": "input_handle", "type": "TF_VariantTensor" },
      { "name": "tensor", "type": "TF_Tensor" }
    ],
    "outputs": [
      { "name": "output_handle", "type": "TF_VariantTensor" }
    ]
  },
  {
    "name": "tf.TensorListReserve",
    "summary": "List of the given size with empty elements.",
    "description": "element_shape: the shape of the future elements of the list\nnum_elements: the number of elements to reserve\nhandle: the output list\nelement_dtype: the desired type of elements in the list.",
    "inputs": [
      { "name": "element_shape", "type": "TF_I32OrI64Tensor" },
      { "name": "num_elements", "type": "TF_Int32Tensor" }
    ],
    "outputs": [
      { "name": "handle", "type": "TF_VariantTensor" }
    ]
  },
  {
    "name": "tf.TensorListResize",
    "summary": "Resizes the list.",
    "description": "input_handle: the input list\nsize: size of the output list",
    "inputs": [
      { "name": "input_handle", "type": "TF_VariantTensor" },
      { "name": "size", "type": "TF_Int32Tensor" }
    ],
    "outputs": [
      { "name": "output_handle", "type": "TF_VariantTensor" }
    ]
  },
  {
    "name": "tf.TensorListScatterIntoExistingList",
    "summary": "Scatters tensor at indices in an input list.",
    "description": "Each member of the TensorList corresponds to one row of the input tensor,\nspecified by the given index (see `tf.gather`).\n\ninput_handle: The list to scatter into.\ntensor: The input tensor.\nindices: The indices used to index into the list.\noutput_handle: The TensorList.",
    "inputs": [
      { "name": "input_handle", "type": "TF_VariantTensor" },
      { "name": "tensor", "type": "TF_Tensor" },
      { "name": "indices", "type": "TF_Int32Tensor" }
    ],
    "outputs": [
      { "name": "output_handle", "type": "TF_VariantTensor" }
    ]
  },
  {
    "name": "tf.TensorListSetItem",
    "inputs": [
      { "name": "input_handle", "type": "TF_VariantTensor" },
      { "name": "index", "type": "TF_Int32Tensor" },
      { "name": "item", "type": "TF_Tensor" }
    ],
    "outputs": [
      { "name": "output_handle", "type": "TF_VariantTensor" }
    ],
    "attributes": [
      { "name": "resize_if_index_out_of_bounds", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.TensorListStack",
    "summary": "Stacks all tensors in the list.",
    "description": "Requires that all tensors have the same shape.\n\ninput_handle: the input list\ntensor: the gathered result\nnum_elements: optional. If not -1, the number of elements in the list.",
    "inputs": [
      { "name": "input_handle", "type": "TF_VariantTensor" },
      { "name": "element_shape", "type": "TF_Int32Tensor" }
    ],
    "outputs": [
      { "name": "tensor", "type": "TF_Tensor" }
    ],
    "attributes": [
      { "name": "num_elements", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.TensorScatterAdd",
    "summary": "Adds sparse `updates` to an existing tensor according to `indices`.",
    "description": "This operation creates a new tensor by adding sparse `updates` to the passed\nin `tensor`.\nThis operation is very similar to `tf.compat.v1.scatter_nd_add`, except that the\nupdates are added onto an existing tensor (as opposed to a variable). If the\nmemory for the existing tensor cannot be re-used, a copy is made and updated.\n\n`indices` is an integer tensor containing indices into a new tensor of shape\n`tensor.shape`.  The last dimension of `indices` can be at most the rank of\n`tensor.shape`:\n\n```\nindices.shape[-1] <= tensor.shape.rank\n```\n\nThe last dimension of `indices` corresponds to indices into elements\n(if `indices.shape[-1] = tensor.shape.rank`) or slices\n(if `indices.shape[-1] < tensor.shape.rank`) along dimension\n`indices.shape[-1]` of `tensor.shape`.  `updates` is a tensor with shape\n\n```\nindices.shape[:-1] + tensor.shape[indices.shape[-1]:]\n```\n\nThe simplest form of `tensor_scatter_nd_add` is to add individual elements to a\ntensor by index. For example, say we want to add 4 elements in a rank-1\ntensor with 8 elements.\n\nIn Python, this scatter add operation would look like this:\n\n>>> indices = tf.constant([[4], [3], [1], [7]])\n>>> updates = tf.constant([9, 10, 11, 12])\n>>> tensor = tf.ones([8], dtype=tf.int32)\n>>> updated = tf.tensor_scatter_nd_add(tensor, indices, updates)\n>>> updated\n<tf.Tensor: shape=(8,), dtype=int32,\nnumpy=array([ 1, 12,  1, 11, 10,  1,  1, 13], dtype=int32)>\n\nWe can also, insert entire slices of a higher rank tensor all at once. For\nexample, if we wanted to insert two slices in the first dimension of a\nrank-3 tensor with two matrices of new values.\n\nIn Python, this scatter add operation would look like this:\n\n>>> indices = tf.constant([[0], [2]])\n>>> updates = tf.constant([[[5, 5, 5, 5], [6, 6, 6, 6],\n...                         [7, 7, 7, 7], [8, 8, 8, 8]],\n...                        [[5, 5, 5, 5], [6, 6, 6, 6],\n...                         [7, 7, 7, 7], [8, 8, 8, 8]]])\n>>> tensor = tf.ones([4, 4, 4],dtype=tf.int32)\n>>> updated = tf.tensor_scatter_nd_add(tensor, indices, updates)\n>>> updated\n<tf.Tensor: shape=(4, 4, 4), dtype=int32,\nnumpy=array([[[6, 6, 6, 6], [7, 7, 7, 7], [8, 8, 8, 8], [9, 9, 9, 9]],\n             [[1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1]],\n             [[6, 6, 6, 6], [7, 7, 7, 7], [8, 8, 8, 8], [9, 9, 9, 9]],\n             [[1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1]]], dtype=int32)>\n\n\nIf `indices` contains any out-of-bound indices, depending on\n`bad_indices_policy`, the op will either return an error or ignore the\nout-of-bound indices. `bad_indices_policy` can be one of the following values:\n1. \"\" or \"DEFAULT\": raises on CPU and ignore on GPU. This is because\n   historically on CPU and GPU we handle errors in different ways, and for\n   backward compatibility we keep the default behavior.\n2. \"ERROR\": raises error; GPU does not support this value.\n3. \"IGNORE\": ignore the bad indices; supported on both CPU and GPU.",
    "inputs": [
      { "name": "tensor", "type": "Arg" },
      { "name": "indices", "type": "Arg" },
      { "name": "updates", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "bad_indices_policy", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.TensorScatterMax",
    "summary": "Apply a sparse update to a tensor taking the element-wise maximum.",
    "description": "Returns a new tensor copied from `tensor` whose values are element-wise maximum between\ntensor and updates according to the indices.\n\n>>> tensor = [0, 0, 0, 0, 0, 0, 0, 0]\n>>> indices = [[1], [4], [5]]\n>>> updates = [1, -1, 1]\n>>> tf.tensor_scatter_nd_max(tensor, indices, updates).numpy()\narray([0, 1, 0, 0, 0, 1, 0, 0], dtype=int32)\n\nRefer to `tf.tensor_scatter_nd_update` for more details.",
    "inputs": [
      { "name": "tensor", "type": "Arg" },
      { "name": "indices", "type": "Arg" },
      { "name": "updates", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "bad_indices_policy", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.TensorScatterMin",
    "inputs": [
      { "name": "tensor", "type": "Arg" },
      { "name": "indices", "type": "Arg" },
      { "name": "updates", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "bad_indices_policy", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.TensorScatterSub",
    "summary": "Subtracts sparse `updates` from an existing tensor according to `indices`.",
    "description": "This operation creates a new tensor by subtracting sparse `updates` from the\npassed in `tensor`.\nThis operation is very similar to `tf.scatter_nd_sub`, except that the updates\nare subtracted from an existing tensor (as opposed to a variable). If the memory\nfor the existing tensor cannot be re-used, a copy is made and updated.\n\n`indices` is an integer tensor containing indices into a new tensor of shape\n`shape`.  The last dimension of `indices` can be at most the rank of `shape`:\n\n    indices.shape[-1] <= shape.rank\n\nThe last dimension of `indices` corresponds to indices into elements\n(if `indices.shape[-1] = shape.rank`) or slices\n(if `indices.shape[-1] < shape.rank`) along dimension `indices.shape[-1]` of\n`shape`.  `updates` is a tensor with shape\n\n    indices.shape[:-1] + shape[indices.shape[-1]:]\n\nThe simplest form of tensor_scatter_sub is to subtract individual elements\nfrom a tensor by index. For example, say we want to insert 4 scattered elements\nin a rank-1 tensor with 8 elements.\n\nIn Python, this scatter subtract operation would look like this:\n\n```python\n    indices = tf.constant([[4], [3], [1], [7]])\n    updates = tf.constant([9, 10, 11, 12])\n    tensor = tf.ones([8], dtype=tf.int32)\n    updated = tf.tensor_scatter_nd_sub(tensor, indices, updates)\n    print(updated)\n```\n\nThe resulting tensor would look like this:\n\n    [1, -10, 1, -9, -8, 1, 1, -11]\n\nWe can also, insert entire slices of a higher rank tensor all at once. For\nexample, if we wanted to insert two slices in the first dimension of a\nrank-3 tensor with two matrices of new values.\n\nIn Python, this scatter add operation would look like this:\n\n```python\n    indices = tf.constant([[0], [2]])\n    updates = tf.constant([[[5, 5, 5, 5], [6, 6, 6, 6],\n                            [7, 7, 7, 7], [8, 8, 8, 8]],\n                           [[5, 5, 5, 5], [6, 6, 6, 6],\n                            [7, 7, 7, 7], [8, 8, 8, 8]]])\n    tensor = tf.ones([4, 4, 4],dtype=tf.int32)\n    updated = tf.tensor_scatter_nd_sub(tensor, indices, updates)\n    print(updated)\n```\n\nThe resulting tensor would look like this:\n\n    [[[-4, -4, -4, -4], [-5, -5, -5, -5], [-6, -6, -6, -6], [-7, -7, -7, -7]],\n     [[1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1]],\n     [[-4, -4, -4, -4], [-5, -5, -5, -5], [-6, -6, -6, -6], [-7, -7, -7, -7]],\n     [[1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1]]]\n\nNote that on CPU, if an out of bound index is found, an error is returned.\nOn GPU, if an out of bound index is found, the index is ignored.",
    "inputs": [
      { "name": "tensor", "type": "Arg" },
      { "name": "indices", "type": "Arg" },
      { "name": "updates", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "bad_indices_policy", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.TensorScatterUpdate",
    "summary": "Scatter `updates` into an existing tensor according to `indices`.",
    "description": "This operation creates a new tensor by applying sparse `updates` to the passed\nin `tensor`.\nThis operation is very similar to `tf.scatter_nd`, except that the updates are\nscattered onto an existing tensor (as opposed to a zero-tensor). If the memory\nfor the existing tensor cannot be re-used, a copy is made and updated.\n\nIf `indices` contains duplicates, then we pick the last update for the index.\n\n**WARNING**: There are some GPU specific semantics for this operation.\n- If an out of bound index is found, the index is ignored.\n- The order in which updates are applied is nondeterministic, so the output\nwill be nondeterministic if `indices` contains duplicates.\n\n`indices` is an integer tensor containing indices into a new tensor of shape\n`shape`.\n\n* `indices` must have at least 2 axes: `(num_updates, index_depth)`.\n* The last axis of `indices` is how deep to index into `tensor` so  this index\n  depth must be less than the rank of `tensor`: `indices.shape[-1] <= tensor.ndim`\n\nif `indices.shape[-1] = tensor.rank` this Op indexes and updates scalar elements.\nif `indices.shape[-1] < tensor.rank` it indexes and updates slices of the input\n`tensor`.\n\nEach `update` has a rank of `tensor.rank - indices.shape[-1]`.\nThe overall shape of `updates` is:\n\n```\nindices.shape[:-1] + tensor.shape[indices.shape[-1]:]\n```\n\nIf `indices` contains any out-of-bound indices, depending on\n`bad_indices_policy`, the op will either return an error or ignore the\nout-of-bound indices. `bad_indices_policy` can be one of the following values:\n1. \"\" or \"DEFAULT\": raises on CPU and ignore on GPU. This is because\n   historically on CPU and GPU we handle errors in different ways, and for\n   backward compatibility we keep the default behavior.\n2. \"ERROR\": raises error; GPU does not support this value.\n3. \"IGNORE\": ignore the bad indices; supported on both CPU and GPU.\n\nFor usage examples see the python [tf.tensor_scatter_nd_update](\nhttps://www.tensorflow.org/api_docs/python/tf/tensor_scatter_nd_update) function",
    "inputs": [
      { "name": "tensor", "type": "Arg" },
      { "name": "indices", "type": "Arg" },
      { "name": "updates", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "bad_indices_policy", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.TensorSliceDataset",
    "summary": "Creates a dataset that emits each dim-0 slice of `components` once.",
    "inputs": [
      { "name": "components", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "handle", "type": "TF_VariantTensor" }
    ],
    "attributes": [
      { "name": "output_shapes", "type": "ConfinedAttr" },
      { "name": "is_files", "type": "DefaultValuedOptionalAttr" },
      { "name": "metadata", "type": "DefaultValuedOptionalAttr" },
      { "name": "replicate_on_split", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.TensorStridedSliceUpdate",
    "summary": "Assign `value` to the sliced l-value reference of `input`.",
    "description": "The values of `value` are assigned to the positions in the tensor `input` that\nare selected by the slice parameters. The slice parameters `begin` `end`\n`strides` etc. work exactly as in `StridedSlice`.\n\nNOTE this op currently does not support broadcasting and so `value`'s shape\nmust be exactly the shape produced by the slice of `input`.",
    "inputs": [
      { "name": "input", "type": "TF_Tensor" },
      { "name": "begin", "type": "TF_I32OrI64Tensor" },
      { "name": "end", "type": "TF_I32OrI64Tensor" },
      { "name": "strides", "type": "TF_I32OrI64Tensor" },
      { "name": "value", "type": "TF_Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "TF_Tensor" }
    ],
    "attributes": [
      { "name": "begin_mask", "type": "DefaultValuedOptionalAttr" },
      { "name": "end_mask", "type": "DefaultValuedOptionalAttr" },
      { "name": "ellipsis_mask", "type": "DefaultValuedOptionalAttr" },
      { "name": "new_axis_mask", "type": "DefaultValuedOptionalAttr" },
      { "name": "shrink_axis_mask", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.Tile",
    "summary": "Constructs a tensor by tiling a given tensor.",
    "description": "This operation creates a new tensor by replicating `input` `multiples` times.\nThe output tensor's i'th dimension has `input.dims(i) * multiples[i]` elements,\nand the values of `input` are replicated `multiples[i]` times along the 'i'th\ndimension. For example, tiling `[a b c d]` by `[2]` produces\n`[a b c d a b c d]`.\n\n>>> a = tf.constant([[1,2,3],[4,5,6]], tf.int32)\n>>> b = tf.constant([1,2], tf.int32)\n>>> tf.tile(a, b)\n<tf.Tensor: shape=(2, 6), dtype=int32, numpy=\narray([[1, 2, 3, 1, 2, 3],\n       [4, 5, 6, 4, 5, 6]], dtype=int32)>\n>>> c = tf.constant([2,1], tf.int32)\n>>> tf.tile(a, c)\n<tf.Tensor: shape=(4, 3), dtype=int32, numpy=\narray([[1, 2, 3],\n       [4, 5, 6],\n       [1, 2, 3],\n       [4, 5, 6]], dtype=int32)>\n>>> d = tf.constant([2,2], tf.int32)\n>>> tf.tile(a, d)\n<tf.Tensor: shape=(4, 6), dtype=int32, numpy=\narray([[1, 2, 3, 1, 2, 3],\n       [4, 5, 6, 4, 5, 6],\n       [1, 2, 3, 1, 2, 3],\n       [4, 5, 6, 4, 5, 6]], dtype=int32)>",
    "inputs": [
      { "name": "input", "type": "Arg" },
      { "name": "multiples", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "TF_Tensor" }
    ]
  },
  {
    "name": "tf.Timestamp",
    "summary": "Provides the time since epoch in seconds.",
    "description": "Returns the timestamp as a `float64` for seconds since the Unix epoch.\n\nCommon usages include:\n* Logging\n* Providing a random number seed\n* Debugging graph execution\n* Generating timing information, mainly through comparison of timestamps\n\nNote: In graph mode, the timestamp is computed when the op is executed,\nnot when it is added to the graph.  In eager mode, the timestamp is computed\nwhen the op is eagerly executed.",
    "outputs": [
      { "name": "ts", "type": "TF_Float64Tensor" }
    ]
  },
  {
    "name": "tf.ToBool",
    "summary": "Converts a tensor to a scalar predicate.",
    "description": "Converts a tensor to a scalar predicate with the following rules:\n\n- For 0D tensors, truthiness is determined by comparing against a \"zero\"\n  value. For numerical types it is the obvious zero. For strings it is the\n  empty string.\n\n- For >0D tensors, truthiness is determined by looking at the number of\n  elements. If has zero elements, then the result is false. Otherwise the\n  result is true.\n\nThis matches the behavior of If and While for determining if a tensor counts\nas true/false for a branch condition.",
    "inputs": [
      { "name": "input", "type": "TF_Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "I1Tensor" }
    ]
  },
  {
    "name": "tf.TopKUnique",
    "summary": "Returns the TopK unique values in the array in sorted order.",
    "description": "The running time is proportional to the product of K and the input\nsize. Sorting the whole array is more efficient for sufficiently large\nvalues of K. The median-of-medians algorithm is probably faster, but\ndifficult to implement efficiently in XLA. If there are fewer than K\nunique numbers (not NANs), the results are padded with negative\ninfinity. NaNs are never returned. Subnormal numbers are flushed to\nzero. If an element appears at multiple indices, the highest index is\nreturned. If a TopK element never appears in the input due to padding\nvalues, the indices are padded with negative one. If a padding value\nappears in the input and padding is needed, the highest index of the\npadding value will be returned. The semantics are not the same as\nkth_order_statistic.",
    "inputs": [
      { "name": "input", "type": "TF_Float32Tensor" }
    ],
    "outputs": [
      { "name": "topk", "type": "TF_Float32Tensor" },
      { "name": "topk_indices", "type": "TF_Int32Tensor" }
    ],
    "attributes": [
      { "name": "k", "type": "I64Attr" }
    ]
  },
  {
    "name": "tf.TopKV2",
    "summary": "Finds values and indices of the `k` largest elements for the last dimension.",
    "description": "If the input is a vector (rank-1), finds the `k` largest entries in the vector\nand outputs their values and indices as vectors.  Thus `values[j]` is the\n`j`-th largest entry in `input`, and its index is `indices[j]`.\n\nFor matrices (resp. higher rank input), computes the top `k` entries in each\nrow (resp. vector along the last dimension).  Thus,\n\n    values.shape = indices.shape = input.shape[:-1] + [k]\n\nIf two elements are equal, the lower-index element appears first.",
    "inputs": [
      { "name": "input", "type": "Arg" },
      { "name": "k", "type": "Arg" }
    ],
    "outputs": [
      { "name": "values", "type": "Res" },
      { "name": "indices", "type": "Res" }
    ],
    "attributes": [
      { "name": "sorted", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.TopKWithUnique",
    "summary": "Returns the TopK values in the array in sorted order.",
    "description": "This is a combination of MakeUnique and TopKUnique. The returned top-K will\nhave its lower bits replaced by iota, thus it will be close to the original\nvalue but not exactly the same. The running time is proportional to the product\nof K and the input size. NaNs are never returned. Subnormal numbers are flushed\nto zero.",
    "inputs": [
      { "name": "input", "type": "TF_Float32Tensor" }
    ],
    "outputs": [
      { "name": "topk", "type": "TF_Float32Tensor" },
      { "name": "topk_indices", "type": "TF_Int32Tensor" }
    ],
    "attributes": [
      { "name": "k", "type": "I64Attr" }
    ]
  },
  {
    "name": "tf.TPUAnnotateTensorsWithDynamicShape",
    "summary": "Placeholder op which takes the output of TPUCopyWithDynamicShapeOp and pass\nthem to the following tpu ops.",
    "description": "This op serves as an annotation for the dynamic shaped tensor and will be\nremoved during the bridge rewrite.",
    "inputs": [
      { "name": "tensors", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "tpu_tensors", "type": "Variadic" }
    ]
  },
  {
    "name": "tf.TPUCompilationResult",
    "summary": "Returns the result of a TPU compilation.",
    "description": "This operation returns the result of a TPU compilation as a serialized\nCompilationResultProto, which holds a status and an error message if an error\noccurred during compilation.",
    "outputs": [
      { "name": "output", "type": "TF_StrTensor" }
    ]
  },
  {
    "name": "tf.TPUCompileMlirAndExecute",
    "summary": "Op that compiles a computation in MLIR into a TPU program, and loads and executes it on a TPU device.",
    "description": "For the internal use of the TPU compiler.\n\n'static_shapes' are tensors specifying the maximum dimension sizes for the tensors specified in `dynamic_operands`.\n'args' are inputs to the TPU computation.\n'operands_with_static_shape' are the indices of the operands that have a maximal static shape specified.\n'mlir_module' is a serialized MLIR module with a `main` function that contains\ntarget computation.\n'metadata' is a serialized TPUCompileMetadataProto describing the shapes and\ntypes of the inputs to the computation, as well as a mapping onto the TPU pod\ntopology.\n'producer_name' is a string describing the name of the framework that add support for running this portion of the model on TPUs.",
    "inputs": [
      { "name": "args", "type": "Variadic" },
      { "name": "static_shapes", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "rendezvous_key_base", "type": "TF_Tensor" },
      { "name": "results", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "operands_with_static_shape", "type": "OptionalAttr" },
      { "name": "mlir_module", "type": "DefaultValuedStrAttr" },
      { "name": "metadata", "type": "StrAttr" },
      { "name": "producer_name", "type": "StrAttr" }
    ]
  },
  {
    "name": "tf.TPUCompileSucceededAssert",
    "summary": "Asserts that compilation succeeded.",
    "description": "This op produces no output and closes the device during failure to ensure all\npending device interactions fail.\n\n'compilation_status' is a serialized CompilationResultProto.",
    "inputs": [
      { "name": "compilation_status", "type": "TF_StrTensor" }
    ]
  },
  {
    "name": "tf.TPUCopyWithDynamicShape",
    "summary": "Op that copies host tensors to device with bounded dynamic shape support.",
    "description": "This op copies the padded tensor on cpu to TPU without the padded data. `tensors`\nis a list of cpu tensors with padded data. `unpadded_sizes` is a list of shape\ntensors which describes unpadded size of each dimension for each cpu tensor.\nThe size of the `unpadded_sizes` should be the same as `tensors`. They are both\non host. `tpu_tensors` are list of tpu device tensors without the padded data.\n`tpu_tensors` also has the same size of the `tensors` and the shapes of\n`tpu_tensors` are determined by the `unpadded_sizes`.",
    "inputs": [
      { "name": "tensors", "type": "Variadic" },
      { "name": "unpadded_sizes", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "tpu_tensors", "type": "Variadic" }
    ]
  },
  {
    "name": "tf.TPUCopyWithLayout",
    "summary": "Op that copies host tensor to device with specified layout.",
    "description": "For internal use only.",
    "inputs": [
      { "name": "input", "type": "TF_Tensor" },
      { "name": "layout", "type": "TF_Int64Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "TF_Tensor" }
    ]
  },
  {
    "name": "tf.TPUDummyInput",
    "summary": "Generates a zero-valued tensor for use as a dummy input to a TPU.",
    "description": "For the internal use of the TF2XLA bridge in the XLA Broadcast pass. This op",
    "outputs": [
      { "name": "output", "type": "TensorOf" }
    ],
    "attributes": [
      { "name": "shape", "type": "TF_ShapeAttr" }
    ]
  },
  {
    "name": "tf.TPUEmbeddingActivations",
    "summary": "An op enabling differentiation of TPU Embeddings.",
    "description": "This op simply returns its first input, which is assumed to have been sliced\nfrom the Tensors returned by TPUEmbeddingDequeueActivations. The presence of\nthis op, and its first argument being a trainable Variable, enables automatic\ndifferentiation of graphs containing embeddings via the TPU Embedding Python\nlibraries.",
    "inputs": [
      { "name": "embedding_variable", "type": "Arg" },
      { "name": "sliced_activations", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "TF_Float32Tensor" }
    ],
    "attributes": [
      { "name": "table_id", "type": "ConfinedAttr" },
      { "name": "lookup_id", "type": "ConfinedAttr" }
    ]
  },
  {
    "name": "tf.TPUExecute",
    "summary": "Op that loads and executes a TPU program on a TPU device.",
    "description": "For the internal use of the distributed TPU compiler.",
    "inputs": [
      { "name": "args", "type": "Variadic" },
      { "name": "key", "type": "TF_StrTensor" }
    ],
    "outputs": [
      { "name": "results", "type": "Variadic" }
    ]
  },
  {
    "name": "tf.TPUExecuteAndUpdateVariables",
    "summary": "Op that executes a program with optional in-place variable updates.",
    "description": "It (optionally) reads device variables, loads and executes a TPU program on a\nTPU device, and then (optionally) in-place updates variables using the program\noutputs, as specified in attributes device_var_reads_indices (program input\nindices from directly reading variables) and device_var_updates_indices (program\noutput indices used to update variables, -1 means no-update/read-only). Such\nprogram outputs are consumed by these variables will not appear in the op\noutput. For the internal use of the distributed TPU compiler.",
    "inputs": [
      { "name": "args", "type": "Variadic" },
      { "name": "key", "type": "TF_StrTensor" }
    ],
    "outputs": [
      { "name": "results", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "device_var_reads_indices", "type": "I64ArrayAttr" },
      { "name": "device_var_updates_indices", "type": "I64ArrayAttr" }
    ]
  },
  {
    "name": "tf.TPUGetLayoutOp",
    "summary": "Op that retrieves the layout of an input or output determined by TPUCompile.",
    "description": "For internal use only.",
    "inputs": [
      { "name": "cache_key", "type": "TF_StrTensor" }
    ],
    "outputs": [
      { "name": "layout", "type": "TF_Int64Tensor" }
    ],
    "attributes": [
      { "name": "index", "type": "I64Attr" },
      { "name": "is_output", "type": "BoolAttr" }
    ]
  },
  {
    "name": "tf.TPUOrdinalSelector",
    "summary": "A TPU core selector Op.",
    "description": "This Op produces a set of TPU cores (for warm-up) or a single TPU core\n(for regular inference) to execute the TPU program on. The output is\nconsumed by TPUPartitionedCall.",
    "outputs": [
      { "name": "device_ordinals", "type": "Res" }
    ]
  },
  {
    "name": "tf.TPUPartitionedCall",
    "summary": "Calls a function placed on a specified TPU device.",
    "inputs": [
      { "name": "args", "type": "Variadic" },
      { "name": "device_ordinal", "type": "TF_Int32Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "arg_attrs", "type": "OptionalAttr" },
      { "name": "res_attrs", "type": "OptionalAttr" },
      { "name": "f", "type": "SymbolRefAttr" },
      { "name": "autotuner_thresh", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.TPUPartitionedInput",
    "summary": "An op that groups a list of partitioned inputs together. This op",
    "inputs": [
      { "name": "inputs", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "output", "type": "TF_Tensor" }
    ],
    "attributes": [
      { "name": "partition_dim", "type": "DefaultValuedOptionalAttr" },
      { "name": "_XlaSharding", "type": "OptionalAttr" },
      { "name": "_XlaShardingV2", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "tf.TPUPartitionedInputV2",
    "summary": "An op that groups a list of partitioned inputs together. Supports ND sharding.",
    "inputs": [
      { "name": "inputs", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "output", "type": "TF_Tensor" }
    ],
    "attributes": [
      { "name": "partition_dims", "type": "I64ArrayAttr" },
      { "name": "is_packed", "type": "DefaultValuedOptionalAttr" },
      { "name": "_XlaSharding", "type": "OptionalAttr" },
      { "name": "_XlaShardingV2", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "tf.TPUPartitionedOutput",
    "summary": "An op that demultiplexes a tensor to be sharded by XLA to a list of partitioned",
    "description": "outputs outside the XLA computation.",
    "inputs": [
      { "name": "inputs", "type": "TF_Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "partition_dim", "type": "DefaultValuedOptionalAttr" },
      { "name": "_XlaSharding", "type": "OptionalAttr" },
      { "name": "_XlaShardingV2", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "tf.TPUPartitionedOutputV2",
    "summary": "An op that demultiplexes a tensor to be sharded by XLA to a list of partitioned",
    "description": "outputs outside the XLA computation. Supports ND sharding.",
    "inputs": [
      { "name": "inputs", "type": "TF_Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "partition_dims", "type": "I64ArrayAttr" },
      { "name": "_XlaSharding", "type": "OptionalAttr" },
      { "name": "_XlaShardingV2", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "tf.TPUReplicatedInput",
    "summary": "Connects N inputs to an N-way replicated TPU computation.",
    "description": "This operation holds a replicated input to a `tpu.replicate()` computation subgraph.\nEach replicated input has the same shape and type alongside the output.\n\nFor example:\n```\n%a = \"tf.opA\"()\n%b = \"tf.opB\"()\n%replicated_input = \"tf.TPUReplicatedInput\"(%a, %b)\n%computation = \"tf.Computation\"(%replicated_input)\n```\nThe above computation has a replicated input of two replicas.",
    "inputs": [
      { "name": "inputs", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "output", "type": "TF_Tensor" }
    ],
    "attributes": [
      { "name": "is_mirrored_variable", "type": "DefaultValuedOptionalAttr" },
      { "name": "index", "type": "DefaultValuedOptionalAttr" },
      { "name": "is_packed", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.TPUReplicatedOutput",
    "summary": "Connects N outputs from an N-way replicated TPU computation.",
    "description": "This operation holds a replicated output from a `tpu.replicate()` computation subgraph.\nEach replicated output has the same shape and type alongside the input.\n\nFor example:\n```\n%computation = \"tf.Computation\"()\n%replicated_output:2 = \"tf.TPUReplicatedOutput\"(%computation)\n```\nThe above computation has a replicated output of two replicas.",
    "inputs": [
      { "name": "input", "type": "TF_Tensor" }
    ],
    "outputs": [
      { "name": "outputs", "type": "Variadic" }
    ]
  },
  {
    "name": "tf.TPUReplicateMetadata",
    "summary": "Metadata indicating how the TPU computation should be replicated.",
    "description": "This operation holds the metadata common to operations of a `tpu.replicate()` computation subgraph.",
    "attributes": [
      { "name": "num_replicas", "type": "ConfinedAttr" },
      { "name": "num_cores_per_replica", "type": "DefaultValuedOptionalAttr" },
      { "name": "topology", "type": "DefaultValuedOptionalAttr" },
      { "name": "use_tpu", "type": "DefaultValuedOptionalAttr" },
      { "name": "device_assignment", "type": "DefaultValuedOptionalAttr" },
      { "name": "computation_shape", "type": "DefaultValuedOptionalAttr" },
      { "name": "host_compute_core", "type": "DefaultValuedOptionalAttr" },
      { "name": "padding_map", "type": "DefaultValuedOptionalAttr" },
      { "name": "step_marker_location", "type": "DefaultValuedOptionalAttr" },
      { "name": "allow_soft_placement", "type": "DefaultValuedOptionalAttr" },
      { "name": "use_spmd_for_xla_partitioning", "type": "DefaultValuedOptionalAttr" },
      { "name": "use_shardy_partitioner", "type": "DefaultValuedOptionalAttr" },
      { "name": "tpu_compile_options_proto", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.TPUReshardVariables",
    "summary": "Op that reshards on-device TPU variables to specified state.",
    "description": "Op that reshards on-device TPU variables to specified state. Internal use only.\n\nThe sharding state is represented as the key of the compilation that generated\nthe sharding/unsharding programs along with the main program. new_format_key\nspecifies the desired state, and format_state_var is the current state of the\nvariables.",
    "inputs": [
      { "name": "vars", "type": "Arg" },
      { "name": "new_format_key", "type": "TF_StrTensor" },
      { "name": "format_state_var", "type": "Arg" }
    ]
  },
  {
    "name": "tf.TPURoundRobin",
    "summary": "Round-robin load balancing on TPU cores.",
    "description": "A load balancing op that round-robins among TPU cores.\n\nThis op round-robins between the integers in [0, NumTPUCoresVisiblePerHost]. It\nis useful for interfacing with TensorFlow ops that take as input a TPU core on\nwhich to execute computations, such as `TPUPartitionedCall`.\n\ndevice_ordinal: An integer in [0, NumTPUCoresVisiblePerHost].",
    "outputs": [
      { "name": "device_ordinal", "type": "TF_Int32Tensor" }
    ]
  },
  {
    "name": "tf.Transpose",
    "summary": "Shuffle dimensions of x according to a permutation.",
    "description": "The output `y` has the same rank as `x`. The shapes of `x` and `y` satisfy:\n  `y.shape[i] == x.shape[perm[i]] for i in [0, 1, ..., rank(x) - 1]`",
    "inputs": [
      { "name": "x", "type": "TF_Tensor" },
      { "name": "perm", "type": "TF_I32OrI64Tensor" }
    ],
    "outputs": [
      { "name": "y", "type": "TF_Tensor" }
    ],
    "category": "Transform"
  },
  {
    "name": "tf.TridiagonalMatMul",
    "summary": "Calculate product with tridiagonal matrix.",
    "description": "Calculates product of two matrices, where left matrix is a tridiagonal matrix.",
    "inputs": [
      { "name": "superdiag", "type": "Arg" },
      { "name": "maindiag", "type": "Arg" },
      { "name": "subdiag", "type": "Arg" },
      { "name": "rhs", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ]
  },
  {
    "name": "tf.TridiagonalSolve",
    "summary": "Solves tridiagonal systems of equations.",
    "description": "Solves tridiagonal systems of equations.\n  Supports batch dimensions and multiple right-hand sides per each left-hand\n  side.\n  On CPU, solution is computed via Gaussian elimination with or without partial\n  pivoting, depending on `partial_pivoting` attribute. On GPU, Nvidia's cuSPARSE\n  library is used: https://docs.nvidia.com/cuda/cusparse/index.html#gtsv\n  Partial pivoting is not yet supported by XLA backends.",
    "inputs": [
      { "name": "diagonals", "type": "Arg" },
      { "name": "rhs", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "partial_pivoting", "type": "DefaultValuedOptionalAttr" },
      { "name": "perturb_singular", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.TruncateDiv",
    "summary": "Returns x / y element-wise, rounded towards zero.",
    "description": "Truncation designates that negative numbers will round fractional quantities\ntoward zero. I.e. -7 / 5 = -1. This matches C semantics but it is different\nthan Python semantics. See `FloorDiv` for a division function that matches\nPython Semantics.\n\n*NOTE*: `TruncateDiv` supports broadcasting. More about broadcasting\n[here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)",
    "inputs": [
      { "name": "x", "type": "TensorOf" },
      { "name": "y", "type": "TensorOf" }
    ],
    "outputs": [
      { "name": "z", "type": "TensorOf" }
    ]
  },
  {
    "name": "tf.TruncatedNormal",
    "summary": "Outputs random values from a truncated normal distribution.",
    "description": "The generated values follow a normal distribution with mean 0 and standard\ndeviation 1, except that values whose magnitude is more than 2 standard\ndeviations from the mean are dropped and re-picked.",
    "inputs": [
      { "name": "shape", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "seed", "type": "DefaultValuedOptionalAttr" },
      { "name": "seed2", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.TruncateMod",
    "summary": "Returns element-wise remainder of division. This emulates C semantics in that",
    "description": "the result here is consistent with a truncating divide. E.g. `truncate(x / y) *\ny + truncate_mod(x, y) = x`.\n\n*NOTE*: `TruncateMod` supports broadcasting. More about broadcasting\n[here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)",
    "inputs": [
      { "name": "x", "type": "TF_FpOrI32OrI64Tensor" },
      { "name": "y", "type": "TF_FpOrI32OrI64Tensor" }
    ],
    "outputs": [
      { "name": "z", "type": "TF_FpOrI32OrI64Tensor" }
    ]
  },
  {
    "name": "tf.UncompressElement",
    "summary": "Uncompresses a compressed dataset element.",
    "inputs": [
      { "name": "compressed", "type": "TF_VariantTensor" }
    ],
    "outputs": [
      { "name": "components", "type": "Variadic" }
    ]
  },
  {
    "name": "tf.UniformDequantize",
    "summary": "Perform dequantization on the quantized Tensor `input`.",
    "description": "Given quantized `input` which was quantized using `scales` and `zero_points`, performs dequantization using the formula:\ndequantized_data = (quantized_data - zero_point) * scale.",
    "inputs": [
      { "name": "input", "type": "Arg" },
      { "name": "scales", "type": "Arg" },
      { "name": "zero_points", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "quantization_axis", "type": "DefaultValuedOptionalAttr" },
      { "name": "quantization_min_val", "type": "I64Attr" },
      { "name": "quantization_max_val", "type": "I64Attr" }
    ]
  },
  {
    "name": "tf.UniformQuantize",
    "summary": "Perform quantization on Tensor `input`.",
    "description": "Given `input`, `scales` and `zero_points`, performs quantization using the formula:\nquantized_data = floor(input_data * (1.0f / scale) + 0.5f) + zero_point",
    "inputs": [
      { "name": "input", "type": "Arg" },
      { "name": "scales", "type": "Arg" },
      { "name": "zero_points", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "quantization_axis", "type": "DefaultValuedOptionalAttr" },
      { "name": "quantization_min_val", "type": "I64Attr" },
      { "name": "quantization_max_val", "type": "I64Attr" }
    ]
  },
  {
    "name": "tf.UniformQuantizedAdd",
    "summary": "Perform quantized add of quantized Tensor `lhs` and quantized Tensor `rhs` to make quantized `output`.",
    "description": "Given quantized `lhs` and quantized `rhs`, performs quantized add on `lhs` and `rhs` to make quantized `output`.\n\n`UniformQuantizedAdd` follows Numpy broadcasting rules.\nThe two input array shapes are compared element-wise.\nStarting with the trailing dimensions, the two dimensions either have to be equal or one of them needs to be 1.\n\n`lhs` and `rhs` must be quantized Tensor, where data value is quantized using the formula:\n```\nquantized_data = clip(original_data / scale + zero_point, quantization_min_val, quantization_max_val)\n```\n`output` is also quantized, using the same formula.\n\nIf `lhs` and `output` is both per-axis quantized, the quantization axis must match.\nAlso, if `rhs` and `output` is both per-axis quantized, the quantization axis must match.\n*Match* means the axis must match when adding, regarding the broadcasting.\ni.e. For both operands `lhs` and `rhs`,\nif `operand.quantization_axis` >= 0 and `output.quantization_axis` >= 0,\n`operand.dims` - `operand.quantization_axis` must be equal to `output.dims` - `output.quantization_axis`.",
    "inputs": [
      { "name": "lhs", "type": "Arg" },
      { "name": "rhs", "type": "Arg" },
      { "name": "lhs_scales", "type": "Arg" },
      { "name": "lhs_zero_points", "type": "Arg" },
      { "name": "rhs_scales", "type": "Arg" },
      { "name": "rhs_zero_points", "type": "Arg" },
      { "name": "output_scales", "type": "Arg" },
      { "name": "output_zero_points", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "lhs_quantization_axis", "type": "DefaultValuedOptionalAttr" },
      { "name": "lhs_quantization_min_val", "type": "I64Attr" },
      { "name": "lhs_quantization_max_val", "type": "I64Attr" },
      { "name": "rhs_quantization_axis", "type": "DefaultValuedOptionalAttr" },
      { "name": "rhs_quantization_min_val", "type": "I64Attr" },
      { "name": "rhs_quantization_max_val", "type": "I64Attr" },
      { "name": "output_quantization_axis", "type": "DefaultValuedOptionalAttr" },
      { "name": "output_quantization_min_val", "type": "I64Attr" },
      { "name": "output_quantization_max_val", "type": "I64Attr" }
    ]
  },
  {
    "name": "tf.UniformQuantizedClipByValue",
    "summary": "Perform clip by value on the quantized Tensor `operand`.",
    "description": "Given quantized `operand` which was quantized using `scales` and `zero_points`, performs clip by value using `min` and `max` values.\nIf quantization_axis is -1 (per-tensor quantized), the entire operand is clipped using scalar min, max.\nOtherwise (per-channel quantized), the clipping is also done per-channel.",
    "inputs": [
      { "name": "operand", "type": "Arg" },
      { "name": "min", "type": "Arg" },
      { "name": "max", "type": "Arg" },
      { "name": "scales", "type": "Arg" },
      { "name": "zero_points", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "quantization_axis", "type": "DefaultValuedOptionalAttr" },
      { "name": "quantization_min_val", "type": "I64Attr" },
      { "name": "quantization_max_val", "type": "I64Attr" }
    ]
  },
  {
    "name": "tf.UniformQuantizedConvolution",
    "summary": "Perform quantized convolution of quantized Tensor `lhs` and quantized Tensor `rhs`. to make quantized `output`.",
    "description": "Given quantized `lhs` and quantized `rhs`, performs quantized dot on `lhs` and `rhs` to make quantized `output`.\n\n`lhs` and `rhs` must be Tensors of same rank, and meet following shape conditions.\n- `lhs_feature` % `feature_group_count` == 0\n- `lhs_feature` % `rhs_input_feature` == 0\n- `lhs_feature` / `feature_group_count` == `rhs_input_feature`\n- `rhs_output_feature` % `feature_group_count` == 0\n- `lhs_batch` % `batch_group_count` == 0\n- `rhs_output_feature` % `batch_group_count` == 0\n\n`lhs` and `rhs` must be quantized Tensor, where data value is quantized using the formula:\n```\nquantized_data = clip(original_data / scale + zero_point, quantization_min_val, quantization_max_val)\n```\n`output` is also quantized, using the same formula.\nIf `rhs` is per-tensor quantized, `output` must be also per-tensor quantized.",
    "inputs": [
      { "name": "lhs", "type": "Arg" },
      { "name": "rhs", "type": "Arg" },
      { "name": "lhs_scales", "type": "Arg" },
      { "name": "lhs_zero_points", "type": "Arg" },
      { "name": "rhs_scales", "type": "Arg" },
      { "name": "rhs_zero_points", "type": "Arg" },
      { "name": "output_scales", "type": "Arg" },
      { "name": "output_zero_points", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "window_strides", "type": "DefaultValuedOptionalAttr" },
      { "name": "padding", "type": "StrAttr" },
      { "name": "explicit_padding", "type": "DefaultValuedOptionalAttr" },
      { "name": "lhs_dilation", "type": "DefaultValuedOptionalAttr" },
      { "name": "rhs_dilation", "type": "DefaultValuedOptionalAttr" },
      { "name": "batch_group_count", "type": "DefaultValuedOptionalAttr" },
      { "name": "feature_group_count", "type": "DefaultValuedOptionalAttr" },
      { "name": "dimension_numbers", "type": "DefaultValuedOptionalAttr" },
      { "name": "lhs_quantization_axis", "type": "DefaultValuedOptionalAttr" },
      { "name": "lhs_quantization_min_val", "type": "I64Attr" },
      { "name": "lhs_quantization_max_val", "type": "I64Attr" },
      { "name": "rhs_quantization_axis", "type": "DefaultValuedOptionalAttr" },
      { "name": "rhs_quantization_min_val", "type": "I64Attr" },
      { "name": "rhs_quantization_max_val", "type": "I64Attr" },
      { "name": "output_quantization_axis", "type": "DefaultValuedOptionalAttr" },
      { "name": "output_quantization_min_val", "type": "I64Attr" },
      { "name": "output_quantization_max_val", "type": "I64Attr" }
    ]
  },
  {
    "name": "tf.UniformQuantizedConvolutionHybrid",
    "summary": "Perform hybrid quantized convolution of float Tensor `lhs` and quantized Tensor `rhs`.",
    "description": "Given float `lhs` and quantized `rhs`, internally performs quantization on `lhs`,\nand then performs quantized convolution on quantized `lhs` and `rhs`.\n\nThe internal quantization on `lhs` is a quantization to `Trhs`, dynamic range,\nper-batch (per-axis along axis `dimension_numbers.input_batch_dimension`), asymmetric,\nand not narrow range (the range is [Trhs_MIN, Trhs_MAX]).\n\n`lhs` and `rhs` must be Tensors of same rank, and meet following shape conditions.\n- lhs_feature % feature_group_count == 0\n- lhs_feature % rhs_input_feature == 0\n- lhs_feature / feature_group_count == rhs_input_feature\n- rhs_output_feature % feature_group_count == 0\n- lhs_batch % batch_group_count == 0\n- rhs_output_feature % batch_group_count == 0\n\n`rhs` must be quantized Tensor, where its data value is quantized using the formula:\nquantized_data = clip(original_data / scale + zero_point, quantization_min_val, quantization_max_val).",
    "inputs": [
      { "name": "lhs", "type": "Arg" },
      { "name": "rhs", "type": "Arg" },
      { "name": "rhs_scales", "type": "Arg" },
      { "name": "rhs_zero_points", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "window_strides", "type": "DefaultValuedOptionalAttr" },
      { "name": "padding", "type": "StrAttr" },
      { "name": "explicit_padding", "type": "DefaultValuedOptionalAttr" },
      { "name": "lhs_dilation", "type": "DefaultValuedOptionalAttr" },
      { "name": "rhs_dilation", "type": "DefaultValuedOptionalAttr" },
      { "name": "batch_group_count", "type": "DefaultValuedOptionalAttr" },
      { "name": "feature_group_count", "type": "DefaultValuedOptionalAttr" },
      { "name": "dimension_numbers", "type": "DefaultValuedOptionalAttr" },
      { "name": "rhs_quantization_axis", "type": "DefaultValuedOptionalAttr" },
      { "name": "rhs_quantization_min_val", "type": "I64Attr" },
      { "name": "rhs_quantization_max_val", "type": "I64Attr" }
    ]
  },
  {
    "name": "tf.UniformQuantizedDot",
    "summary": "Perform quantized dot of quantized Tensor `lhs` and quantized Tensor `rhs` to make quantized `output`.",
    "description": "Given quantized `lhs` and quantized `rhs`, performs quantized dot on `lhs` and `rhs` to make quantized `output`.\n`lhs` and `rhs` must be 2D Tensors and the lhs.dim_size(1) must match rhs.dim_size(0).\n`lhs` and `rhs` must be quantized Tensor, where data value is quantized using the formula:\nquantized_data = clip(original_data / scale + zero_point, quantization_min_val, quantization_max_val).\n`output` is also quantized, using the same formula.\nIf `rhs` is per-tensor quantized, `output` must be also per-tensor quantized.",
    "inputs": [
      { "name": "lhs", "type": "Arg" },
      { "name": "rhs", "type": "Arg" },
      { "name": "lhs_scales", "type": "Arg" },
      { "name": "lhs_zero_points", "type": "Arg" },
      { "name": "rhs_scales", "type": "Arg" },
      { "name": "rhs_zero_points", "type": "Arg" },
      { "name": "output_scales", "type": "Arg" },
      { "name": "output_zero_points", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "lhs_quantization_axis", "type": "DefaultValuedOptionalAttr" },
      { "name": "lhs_quantization_min_val", "type": "I64Attr" },
      { "name": "lhs_quantization_max_val", "type": "I64Attr" },
      { "name": "rhs_quantization_axis", "type": "DefaultValuedOptionalAttr" },
      { "name": "rhs_quantization_min_val", "type": "I64Attr" },
      { "name": "rhs_quantization_max_val", "type": "I64Attr" },
      { "name": "output_quantization_axis", "type": "DefaultValuedOptionalAttr" },
      { "name": "output_quantization_min_val", "type": "I64Attr" },
      { "name": "output_quantization_max_val", "type": "I64Attr" }
    ]
  },
  {
    "name": "tf.UniformQuantizedDotHybrid",
    "summary": "Perform hybrid quantized dot of float Tensor `lhs` and quantized Tensor `rhs`.",
    "description": "Given float `lhs` and quantized `rhs`, internally performs quantization on `lhs`, and then performs quantized dot on quantized lhs and `rhs`.\nThe internal quantization on `lhs` is a quantization to qint8, dynamic range, per-batch (per-axis along axis 0), asymmetric, and not narrow range (the range is [-128, 127]).\n`lhs` and `rhs` must be 2D Tensors and the lhs.dim_size(1) must match rhs.dim_size(0).\n`rhs` must be quantized Tensor, where its data value is quantized using the formula:\nquantized_data = clip(original_data / scale + zero_point, quantization_min_val, quantization_max_val).",
    "inputs": [
      { "name": "lhs", "type": "Arg" },
      { "name": "rhs", "type": "Arg" },
      { "name": "rhs_scales", "type": "Arg" },
      { "name": "rhs_zero_points", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "rhs_quantization_axis", "type": "DefaultValuedOptionalAttr" },
      { "name": "rhs_quantization_min_val", "type": "I64Attr" },
      { "name": "rhs_quantization_max_val", "type": "I64Attr" }
    ]
  },
  {
    "name": "tf.UniformRequantize",
    "summary": "Given quantized tensor `input`, requantize it with new quantization parameters.",
    "description": "Given quantized tensor `input`, which was quantized using {input_scales, input_zero_points, input_quantization_axis, input_quantization_min_val, input_quantization_max_val},\nrequantize it to a tensor, which is quantized using {output_scales, output_zero_points, output_quantization_axis, output_quantization_min_val, output_quantization_max_val}.\nThe requantization is done by using the formula:\noutput_quantized_data = clip(\n  (input_quantized_data - input_zero_point) * (input_scale / output_scale) + output_zero_point,\n  output_quantization_min_val,\n  output_quantization_max_val)\n\nPer-tensor and per-axis quantization supported cases are followings:\n* per-tensor -> per-tensor\n* per-tensor -> per-axis\n* per-axis -> per-axis where input_quantization_axis equals output_quantization_axis.\ni.e. At least one among input_quantization_axis and output_quantization_axis must be -1, or two must be equal.",
    "inputs": [
      { "name": "input", "type": "Arg" },
      { "name": "input_scales", "type": "Arg" },
      { "name": "input_zero_points", "type": "Arg" },
      { "name": "output_scales", "type": "Arg" },
      { "name": "output_zero_points", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "input_quantization_axis", "type": "DefaultValuedOptionalAttr" },
      { "name": "input_quantization_min_val", "type": "I64Attr" },
      { "name": "input_quantization_max_val", "type": "I64Attr" },
      { "name": "output_quantization_axis", "type": "DefaultValuedOptionalAttr" },
      { "name": "output_quantization_min_val", "type": "I64Attr" },
      { "name": "output_quantization_max_val", "type": "I64Attr" }
    ]
  },
  {
    "name": "tf.Unique",
    "summary": "Finds unique elements in a 1-D tensor.",
    "description": "This operation returns a tensor `y` containing all of the unique elements of `x`\nsorted in the same order that they occur in `x`; `x` does not need to be sorted.\nThis operation also returns a tensor `idx` the same size as `x` that contains\nthe index of each value of `x` in the unique output `y`. In other words:\n\n`y[idx[i]] = x[i] for i in [0, 1,...,rank(x) - 1]`\n\nExamples:\n\n```\n# tensor 'x' is [1, 1, 2, 4, 4, 4, 7, 8, 8]\ny, idx = unique(x)\ny ==> [1, 2, 4, 7, 8]\nidx ==> [0, 0, 1, 2, 2, 2, 3, 4, 4]\n```\n\n```\n# tensor 'x' is [4, 5, 1, 2, 3, 3, 4, 5]\ny, idx = unique(x)\ny ==> [4, 5, 1, 2, 3]\nidx ==> [0, 1, 2, 3, 4, 4, 0, 1]\n```",
    "inputs": [
      { "name": "x", "type": "Arg" }
    ],
    "outputs": [
      { "name": "y", "type": "Res" },
      { "name": "idx", "type": "Res" }
    ]
  },
  {
    "name": "tf.UniqueV2",
    "summary": "Finds unique elements along an axis of a tensor.",
    "description": "This operation either returns a tensor `y` containing unique elements\nalong the `axis` of a tensor. The returned unique elements is sorted\nin the same order as they occur along `axis` in `x`.\nThis operation also returns a tensor `idx` that is the same size as\nthe number of the elements in `x` along the `axis` dimension. It\ncontains the index in the unique output `y`.\nIn other words, for an `1-D` tensor `x` with `axis = None:\n\n`y[idx[i]] = x[i] for i in [0, 1,...,rank(x) - 1]`\n\nFor example:\n\n```\n# tensor 'x' is [1, 1, 2, 4, 4, 4, 7, 8, 8]\ny, idx = unique(x)\ny ==> [1, 2, 4, 7, 8]\nidx ==> [0, 0, 1, 2, 2, 2, 3, 4, 4]\n```\n\nFor an `2-D` tensor `x` with `axis = 0`:\n\n```\n# tensor 'x' is [[1, 0, 0],\n#                [1, 0, 0],\n#                [2, 0, 0]]\ny, idx = unique(x, axis=0)\ny ==> [[1, 0, 0],\n       [2, 0, 0]]\nidx ==> [0, 0, 1]\n```\n\nFor an `2-D` tensor `x` with `axis = 1`:\n\n```\n# tensor 'x' is [[1, 0, 0],\n#                [1, 0, 0],\n#                [2, 0, 0]]\ny, idx = unique(x, axis=1)\ny ==> [[1, 0],\n       [1, 0],\n       [2, 0]]\nidx ==> [0, 1, 1]\n```",
    "inputs": [
      { "name": "x", "type": "Arg" },
      { "name": "axis", "type": "Arg" }
    ],
    "outputs": [
      { "name": "y", "type": "Res" },
      { "name": "idx", "type": "Res" }
    ]
  },
  {
    "name": "tf.Unpack",
    "summary": "Unpacks a given dimension of a rank-`R` tensor into `num` rank-`(R-1)` tensors.",
    "description": "Unpacks `num` tensors from `value` by chipping it along the `axis` dimension.\nFor example, given a tensor of shape `(A, B, C, D)`;\n\nIf `axis == 0` then the i'th tensor in `output` is the slice `value[i, :, :, :]`\n  and each tensor in `output` will have shape `(B, C, D)`. (Note that the\n  dimension unpacked along is gone, unlike `split`).\n\nIf `axis == 1` then the i'th tensor in `output` is the slice `value[:, i, :, :]`\n  and each tensor in `output` will have shape `(A, C, D)`.\nEtc.\n\nThis is the opposite of `pack`.",
    "inputs": [
      { "name": "value", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "axis", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.UnsortedSegmentMax",
    "summary": "Computes the maximum along segments of a tensor.",
    "description": "Read\n[the section on segmentation](https://tensorflow.org/api_docs/python/tf/math#Segmentation)\nfor an explanation of segments.\n\nThis operator is similar to `tf.math.unsorted_segment_sum`,\nInstead of computing the sum over segments, it computes the maximum such that:\n\n\\\\(output_i = \\max_{j...} data[j...]\\\\) where max is over tuples `j...` such\nthat `segment_ids[j...] == i`.\n\nIf the maximum is empty for a given segment ID `i`, it outputs the smallest\npossible value for the specific numeric type,\n`output[i] = numeric_limits<T>::lowest()`.\n\nIf the given segment ID `i` is negative, then the corresponding value is\ndropped, and will not be included in the result.\n\nCaution: On CPU, values in `segment_ids` are always validated to be less than\n`num_segments`, and an error is thrown for out-of-bound indices. On GPU, this\ndoes not throw an error for out-of-bound indices. On Gpu, out-of-bound indices\nresult in safe but unspecified behavior, which may include ignoring\nout-of-bound indices or outputting a tensor with a 0 stored in the first\ndimension of its shape if `num_segments` is 0.\n\n<div style=\"width:70%; margin:auto; margin-bottom:10px; margin-top:20px;\">\n<img style=\"width:100%\" src=\"https://www.tensorflow.org/images/UnsortedSegmentMax.png\" alt>\n</div>\n\nFor example:\n\n>>> c = tf.constant([[1,2,3,4], [5,6,7,8], [4,3,2,1]])\n>>> tf.math.unsorted_segment_max(c, tf.constant([0, 1, 0]), num_segments=2).numpy()\narray([[4, 3, 3, 4],\n       [5,  6, 7, 8]], dtype=int32)",
    "inputs": [
      { "name": "data", "type": "TF_IntOrFpTensor" },
      { "name": "segment_ids", "type": "Arg" },
      { "name": "num_segments", "type": "TF_I32OrI64Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ]
  },
  {
    "name": "tf.UnsortedSegmentMin",
    "summary": "Computes the minimum along segments of a tensor.",
    "description": "Read\n[the section on segmentation](https://tensorflow.org/api_docs/python/tf/math#Segmentation)\nfor an explanation of segments.\n\nThis operator is similar to `tf.math.unsorted_segment_sum`,\nInstead of computing the sum over segments, it computes the minimum such that:\n\n\\\\(output_i = \\min_{j...} data_[j...]\\\\) where min is over tuples `j...` such\nthat `segment_ids[j...] == i`.\n\nIf the minimum is empty for a given segment ID `i`, it outputs the largest\npossible value for the specific numeric type,\n`output[i] = numeric_limits<T>::max()`.\n\nFor example:\n\n>>> c = tf.constant([[1,2,3,4], [5,6,7,8], [4,3,2,1]])\n>>> tf.math.unsorted_segment_min(c, tf.constant([0, 1, 0]), num_segments=2).numpy()\narray([[1, 2, 2, 1],\n       [5, 6, 7, 8]], dtype=int32)\n\nIf the given segment ID `i` is negative, then the corresponding value is\ndropped, and will not be included in the result.\n\nCaution: On CPU, values in `segment_ids` are always validated to be less than\n`num_segments`, and an error is thrown for out-of-bound indices. On GPU, this\ndoes not throw an error for out-of-bound indices. On Gpu, out-of-bound indices\nresult in safe but unspecified behavior, which may include ignoring\nout-of-bound indices or outputting a tensor with a 0 stored in the first\ndimension of its shape if `num_segments` is 0.",
    "inputs": [
      { "name": "data", "type": "TF_IntOrFpTensor" },
      { "name": "segment_ids", "type": "Arg" },
      { "name": "num_segments", "type": "TF_I32OrI64Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ]
  },
  {
    "name": "tf.UnsortedSegmentProd",
    "summary": "Computes the product along segments of a tensor.",
    "description": "Read\n[the section on segmentation](https://tensorflow.org/api_docs/python/tf/math#Segmentation)\nfor an explanation of segments.\n\nThis operator is similar to `tf.math.unsorted_segment_sum`,\nInstead of computing the sum over segments, it computes the product of all\nentries belonging to a segment such that:\n\n\\\\(output_i = \\prod_{j...} data[j...]\\\\) where the product is over tuples\n`j...` such that `segment_ids[j...] == i`.\n\nFor example:\n\n>>> c = tf.constant([[1,2,3,4], [5,6,7,8], [4,3,2,1]])\n>>> tf.math.unsorted_segment_prod(c, tf.constant([0, 1, 0]), num_segments=2).numpy()\narray([[4, 6, 6, 4],\n       [5, 6, 7, 8]], dtype=int32)\n\nIf there is no entry for a given segment ID `i`, it outputs 1.\n\nIf the given segment ID `i` is negative, then the corresponding value is\ndropped, and will not be included in the result.\nCaution: On CPU, values in `segment_ids` are always validated to be less than\n`num_segments`, and an error is thrown for out-of-bound indices. On GPU, this\ndoes not throw an error for out-of-bound indices. On Gpu, out-of-bound indices\nresult in safe but unspecified behavior, which may include ignoring\nout-of-bound indices or outputting a tensor with a 0 stored in the first\ndimension of its shape if `num_segments` is 0.",
    "inputs": [
      { "name": "data", "type": "TF_NumberTensor" },
      { "name": "segment_ids", "type": "Arg" },
      { "name": "num_segments", "type": "TF_I32OrI64Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ]
  },
  {
    "name": "tf.UnsortedSegmentSum",
    "summary": "Computes the sum along segments of a tensor.",
    "description": "Read\n[the section on segmentation](https://tensorflow.org/api_docs/python/tf/math#Segmentation)\nfor an explanation of segments.\n\nComputes a tensor such that\n\\\\(output[i] = \\sum_{j...} data[j...]\\\\) where the sum is over tuples `j...` such\nthat `segment_ids[j...] == i`.  Unlike `SegmentSum`, `segment_ids`\nneed not be sorted and need not cover all values in the full\nrange of valid values.\n\nIf the sum is empty for a given segment ID `i`, `output[i] = 0`.\nIf the given segment ID `i` is negative, the value is dropped and will not be\nadded to the sum of the segment.\n\n`num_segments` should equal the number of distinct segment IDs.\n\nCaution: On CPU, values in `segment_ids` are always validated to be less than\n`num_segments`, and an error is thrown for out-of-bound indices. On GPU, this\ndoes not throw an error for out-of-bound indices. On Gpu, out-of-bound indices\nresult in safe but unspecified behavior, which may include ignoring\nout-of-bound indices or outputting a tensor with a 0 stored in the first\ndimension of its shape if `num_segments` is 0.\n\n<div style=\"width:70%; margin:auto; margin-bottom:10px; margin-top:20px;\">\n<img style=\"width:100%\" src=\"https://www.tensorflow.org/images/UnsortedSegmentSum.png\" alt>\n</div>\n\n>>> c = [[1,2,3,4], [5,6,7,8], [4,3,2,1]]\n>>> tf.math.unsorted_segment_sum(c, [0, 1, 0], num_segments=2).numpy()\narray([[5, 5, 5, 5],\n       [5, 6, 7, 8]], dtype=int32)",
    "inputs": [
      { "name": "data", "type": "TF_NumberTensor" },
      { "name": "segment_ids", "type": "Arg" },
      { "name": "num_segments", "type": "TF_I32OrI64Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ]
  },
  {
    "name": "tf.UpperBound",
    "summary": "Applies upper_bound(sorted_search_values, values) along each row.",
    "description": "Each set of rows with the same index in (sorted_inputs, values) is treated\nindependently.  The resulting row is the equivalent of calling\n`np.searchsorted(sorted_inputs, values, side='right')`.\n\nThe result is not a global index to the entire\n`Tensor`, but rather just the index in the last dimension.\n\nA 2-D example:\n  sorted_sequence = [[0, 3, 9, 9, 10],\n                     [1, 2, 3, 4, 5]]\n  values = [[2, 4, 9],\n            [0, 2, 6]]\n\n  result = UpperBound(sorted_sequence, values)\n\n  result == [[1, 2, 4],\n             [0, 2, 5]]",
    "inputs": [
      { "name": "sorted_inputs", "type": "Arg" },
      { "name": "values", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ]
  },
  {
    "name": "tf.VarHandleOp",
    "summary": "Creates a handle to a Variable resource from its name.",
    "description": "container: the container this variable is placed in.\nshared_name: the name by which this variable is referred to.\ndtype and shape: attributes representing the data type and shape held in the\n  variable.\n\nExample:\n    resource_variable_ops.var_handle_op(\n          dtype=dtypes.int32, shape=[8, 16], container=\"foo\", shared_name=\"bar\")\n  returns a handle for a variable with name \"bar\" in container \"foo\", and the\n  variable holds a tensor of shape [8, 16] and dtype int32.",
    "outputs": [
      { "name": "resource", "type": "Res" }
    ],
    "attributes": [
      { "name": "container", "type": "DefaultValuedStrAttr" },
      { "name": "shared_name", "type": "DefaultValuedStrAttr" }
    ]
  },
  {
    "name": "tf.Variable",
    "summary": "Use VariableV2 instead.",
    "outputs": [
      { "name": "ref", "type": "TF_Tensor" }
    ],
    "attributes": [
      { "name": "shape", "type": "TF_ShapeAttr" },
      { "name": "container", "type": "DefaultValuedOptionalAttr" },
      { "name": "shared_name", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.VariableShape",
    "summary": "Returns the shape of the variable pointed to by `resource`.",
    "description": "This operation returns a 1-D integer tensor representing the shape of `input`.\n\nFor example:\n\n```\n# 't' is [[[1, 1, 1], [2, 2, 2]], [[3, 3, 3], [4, 4, 4]]]\nshape(t) ==> [2, 2, 3]\n```",
    "inputs": [
      { "name": "input", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "TF_I32OrI64Tensor" }
    ]
  },
  {
    "name": "tf.VariableV2",
    "summary": "Holds state in the form of a tensor that persists across steps.",
    "description": "Outputs a ref to the tensor state so it may be read or modified.\nTODO(zhifengc/mrry): Adds a pointer to a more detail document\nabout sharing states in tensorflow.",
    "outputs": [
      { "name": "ref", "type": "Res" }
    ],
    "attributes": [
      { "name": "shape", "type": "TF_ShapeAttr" },
      { "name": "container", "type": "DefaultValuedOptionalAttr" },
      { "name": "shared_name", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.VarIsInitializedOp",
    "summary": "Checks whether a resource handle-based variable has been initialized.",
    "inputs": [
      { "name": "resource", "type": "Arg" }
    ],
    "outputs": [
      { "name": "is_initialized", "type": "Res" }
    ]
  },
  {
    "name": "tf.Where",
    "summary": "Returns locations of nonzero / true values in a tensor.",
    "description": "This operation returns the coordinates of true elements in `condition`. The\ncoordinates are returned in a 2-D tensor where the first dimension (rows)\nrepresents the number of true elements, and the second dimension (columns)\nrepresents the coordinates of the true elements. Keep in mind, the shape of\nthe output tensor can vary depending on how many true values there are in\n`condition`. Indices are output in row-major order.\n\nFor example:\n\n```\n# 'input' tensor is [[True, False]\n#                    [True, False]]\n# 'input' has two true values, so output has two coordinates.\n# 'input' has rank of 2, so coordinates have two indices.\nwhere(input) ==> [[0, 0],\n                  [1, 0]]\n\n# `condition` tensor is [[[True, False]\n#                     [True, False]]\n#                    [[False, True]\n#                     [False, True]]\n#                    [[False, False]\n#                     [False, True]]]\n# 'input' has 5 true values, so output has 5 coordinates.\n# 'input' has rank of 3, so coordinates have three indices.\nwhere(input) ==> [[0, 0, 0],\n                  [0, 1, 0],\n                  [1, 0, 1],\n                  [1, 1, 1],\n                  [2, 1, 1]]\n\n# `condition` tensor is [[[1.5,  0.0]\n#                     [-0.5, 0.0]]\n#                    [[0.0,  0.25]\n#                     [0.0,  0.75]]\n#                    [[0.0,  0.0]\n#                     [0.0,  0.01]]]\n# 'input' has 5 nonzero values, so output has 5 coordinates.\n# 'input' has rank of 3, so coordinates have three indices.\nwhere(input) ==> [[0, 0, 0],\n                  [0, 1, 0],\n                  [1, 0, 1],\n                  [1, 1, 1],\n                  [2, 1, 1]]\n\n# `condition` tensor is [[[1.5 + 0.0j, 0.0  + 0.0j]\n#                     [0.0 + 0.5j, 0.0  + 0.0j]]\n#                    [[0.0 + 0.0j, 0.25 + 1.5j]\n#                     [0.0 + 0.0j, 0.75 + 0.0j]]\n#                    [[0.0 + 0.0j, 0.0  + 0.0j]\n#                     [0.0 + 0.0j, 0.01 + 0.0j]]]\n# 'input' has 5 nonzero magnitude values, so output has 5 coordinates.\n# 'input' has rank of 3, so coordinates have three indices.\nwhere(input) ==> [[0, 0, 0],\n                  [0, 1, 0],\n                  [1, 0, 1],\n                  [1, 1, 1],\n                  [2, 1, 1]]\n```",
    "inputs": [
      { "name": "input", "type": "TensorOf" }
    ],
    "outputs": [
      { "name": "index", "type": "TF_Int64Tensor" }
    ]
  },
  {
    "name": "tf.While",
    "summary": "output = input; While (Cond(output)) { output = Body(output) }",
    "description": "output = input; While (Cond(output)) { output = Body(output) }\n\ninput: A list of input tensors whose types are T.\noutput: A list of output tensors whose types are T.\ncond: A function that takes 'input' and returns a tensor.  If the tensor is\n    a scalar of non-boolean, the scalar is converted to a boolean\n    according to the following rule: if the scalar is a numerical\n    value, non-zero means True and zero means False; if the scalar is\n    a string, non-empty means True and empty means False. If the\n    tensor is not a scalar, non-emptiness means True and False\n    otherwise.\nbody: A function that takes a list of tensors and returns another\n      list of tensors. Both lists have the same types as specified\n      by T.",
    "inputs": [
      { "name": "input", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "output", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "cond", "type": "FlatSymbolRefAttr" },
      { "name": "body", "type": "FlatSymbolRefAttr" },
      { "name": "parallel_iterations", "type": "ConfinedAttr" },
      { "name": "is_stateless", "type": "BoolAttr" },
      { "name": "shape_invariant", "type": "UnitAttr" }
    ]
  },
  {
    "name": "tf.WhileRegion",
    "summary": "while operation",
    "description": "The tf.WhileRegion op represents a while loop using 2 regions and a set of\n  iteration variables. The iteration variables maintained by this Op have the\n  same types as the inputs. The Op executes a while loop described by the\n  following pseudo code:\n\n  ```\n     func WhileRegionOp(inputs) {\n       iteration_vars = inputs;\n       while (cond(iteration_vars)) {\n           iteration_vars = body(iteration_vars);\n       }\n       return iteration_vars;\n     }\n  ```\n\n  `cond` is the condition region and `body` is the body region. Both these\n  regions accept the current value of the iteration variables as inputs.\n\n  The condition region yields a tensor<i1> which, if false, will exit the loop.\n  It can also, optionally and additionally, yield the iteration variables, which\n  must be unchanged.\n\n  The body region always has to yield the (possibly updated) iteration variables.\n\n  The iteration variables are initialized to the Op input, and the results of the\n  tf.WhileRegion op are the final values of the iteration variables.\n\n  This implies that the operand and result types for tf.WhileRegion should be\n  the same. Note that the condition and body regions can implicitly capture\n  loop invariant values directly. In canonical form, iteration variables that\n  pass through the loop body unmodified are converted to implicitly captured\n  references to their values outside the loop.",
    "inputs": [
      { "name": "input", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "output", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "parallel_iterations", "type": "ConfinedAttr" },
      { "name": "is_stateless", "type": "BoolAttr" },
      { "name": "shape_invariant", "type": "UnitAttr" }
    ]
  },
  {
    "name": "tf.WriteAudioSummary",
    "summary": "Writes a `Summary` protocol buffer with audio.",
    "description": "The summary has up to `max_outputs` summary values containing audio. The\naudio is built from `tensor` which must be 3-D with shape `[batch_size,\nframes, channels]` or 2-D with shape `[batch_size, frames]`. The values are\nassumed to be in the range of `[-1.0, 1.0]` with a sample rate of `sample_rate`.\n\nThe `tag` argument is a scalar `Tensor` of type `string`.  It is used to\nbuild the `tag` of the summary values:\n\n*  If `max_outputs` is 1, the summary value tag is '*tag*/audio'.\n*  If `max_outputs` is greater than 1, the summary value tags are\n   generated sequentially as '*tag*/audio/0', '*tag*/audio/1', etc.\n\nwriter: A handle to a summary writer.\nstep: The step to write the summary for.\ntag: Scalar. Used to build the `tag` attribute of the summary values.\ntensor: 2-D of shape `[batch_size, frames]`.\nsample_rate: The sample rate of the signal in hertz.\nmax_outputs: Max number of batch elements to generate audio for.",
    "inputs": [
      { "name": "writer", "type": "Arg" },
      { "name": "step", "type": "TF_Int64Tensor" },
      { "name": "tag", "type": "TF_StrTensor" },
      { "name": "tensor", "type": "TF_Float32Tensor" },
      { "name": "sample_rate", "type": "TF_Float32Tensor" }
    ],
    "attributes": [
      { "name": "max_outputs", "type": "ConfinedAttr" }
    ]
  },
  {
    "name": "tf.WriteGraphSummary",
    "summary": "Writes a `GraphDef` protocol buffer to a `SummaryWriter`.",
    "description": "writer: Handle of `SummaryWriter`.\nstep: The step to write the summary for.\ntensor: A scalar string of the serialized tf.GraphDef proto.",
    "inputs": [
      { "name": "writer", "type": "Arg" },
      { "name": "step", "type": "TF_Int64Tensor" },
      { "name": "tensor", "type": "TF_StrTensor" }
    ]
  },
  {
    "name": "tf.WriteHistogramSummary",
    "summary": "Writes a histogram summary.",
    "description": "The generated\n[`Summary`](https://www.tensorflow.org/code/tensorflow/core/framework/summary.proto)\nhas one summary value containing a histogram for `values`.\n\nThis op reports an `InvalidArgument` error if any value is not finite.\n\nwriter: A handle to a summary writer.\nstep: The step to write the summary for.\ntag: Scalar.  Tag to use for the `Summary.Value`.\nvalues: Any shape. Values to use to build the histogram.",
    "inputs": [
      { "name": "writer", "type": "Arg" },
      { "name": "step", "type": "TF_Int64Tensor" },
      { "name": "tag", "type": "TF_StrTensor" },
      { "name": "values", "type": "TF_IntOrFpTensor" }
    ]
  },
  {
    "name": "tf.WriteImageSummary",
    "summary": "Writes a `Summary` protocol buffer with images.",
    "description": "The summary has up to `max_images` summary values containing images. The\nimages are built from `tensor` which must be 4-D with shape `[batch_size,\nheight, width, channels]` and where `channels` can be:\n\n*  1: `tensor` is interpreted as Grayscale.\n*  3: `tensor` is interpreted as RGB.\n*  4: `tensor` is interpreted as RGBA.\n\nThe images have the same number of channels as the input tensor. For float\ninput, the values are normalized one image at a time to fit in the range\n`[0, 255]`.  `uint8` values are unchanged.  The op uses two different\nnormalization algorithms:\n\n*  If the input values are all positive, they are rescaled so the largest one\n   is 255.\n\n*  If any input value is negative, the values are shifted so input value 0.0\n   is at 127.  They are then rescaled so that either the smallest value is 0,\n   or the largest one is 255.\n\nThe `tag` argument is a scalar `Tensor` of type `string`.  It is used to\nbuild the `tag` of the summary values:\n\n*  If `max_images` is 1, the summary value tag is '*tag*/image'.\n*  If `max_images` is greater than 1, the summary value tags are\n   generated sequentially as '*tag*/image/0', '*tag*/image/1', etc.\n\nThe `bad_color` argument is the color to use in the generated images for\nnon-finite input values.  It is a `unit8` 1-D tensor of length `channels`.\nEach element must be in the range `[0, 255]` (It represents the value of a\npixel in the output image).  Non-finite values in the input tensor are\nreplaced by this tensor in the output image.  The default value is the color\nred.\n\nwriter: A handle to a summary writer.\nstep: The step to write the summary for.\ntag: Scalar. Used to build the `tag` attribute of the summary values.\ntensor: 4-D of shape `[batch_size, height, width, channels]` where\n  `channels` is 1, 3, or 4.\nmax_images: Max number of batch elements to generate images for.\nbad_color: Color to use for pixels with non-finite values.",
    "inputs": [
      { "name": "writer", "type": "Arg" },
      { "name": "step", "type": "TF_Int64Tensor" },
      { "name": "tag", "type": "TF_StrTensor" },
      { "name": "tensor", "type": "TensorOf" },
      { "name": "bad_color", "type": "TF_Uint8Tensor" }
    ],
    "attributes": [
      { "name": "max_images", "type": "ConfinedAttr" }
    ]
  },
  {
    "name": "tf.WriteRawProtoSummary",
    "summary": "Writes a `Summary` protocol buffer with serialized string `Summary` protocol buffers.",
    "description": "writer: A handle to a summary writer.\nstep: The step to write the summary for.\ntensor: A tensor holding one or more serialized `Summary` protobufs to write.",
    "inputs": [
      { "name": "writer", "type": "Arg" },
      { "name": "step", "type": "TF_Int64Tensor" },
      { "name": "tensor", "type": "TF_StrTensor" }
    ]
  },
  {
    "name": "tf.WriteScalarSummary",
    "summary": "Writes a `Summary` protocol buffer with scalar values.",
    "description": "The input `tag` and `value` must have the scalars.\n\nwriter: A handle to a summary writer.\nstep: The step to write the summary for.\ntag: Tag for the summary.\nvalue: Value for the summary.",
    "inputs": [
      { "name": "writer", "type": "Arg" },
      { "name": "step", "type": "TF_Int64Tensor" },
      { "name": "tag", "type": "TF_StrTensor" },
      { "name": "value", "type": "TF_IntOrFpTensor" }
    ]
  },
  {
    "name": "tf.WriteSummary",
    "summary": "Outputs a `Summary` protocol buffer with a tensor.",
    "description": "writer: A handle to a summary writer.\nstep: The step to write the summary for.\ntensor: A tensor to serialize.\ntag: The summary's tag.\nsummary_metadata: Serialized SummaryMetadata protocol buffer containing\n plugin-related metadata for this summary.",
    "inputs": [
      { "name": "writer", "type": "Arg" },
      { "name": "step", "type": "TF_Int64Tensor" },
      { "name": "tensor", "type": "TF_Tensor" },
      { "name": "tag", "type": "TF_StrTensor" },
      { "name": "summary_metadata", "type": "TF_StrTensor" }
    ]
  },
  {
    "name": "tf.WriteTrainingPredictions",
    "summary": "Writes the given predictions into a RecordIO file using a previously",
    "description": "initialized global TrainingPredictionWriter. The predictions are transformed\ninto a PredictionData proto before they are written to the file.",
    "inputs": [
      { "name": "keys", "type": "Arg" },
      { "name": "predictions_list", "type": "Arg" },
      { "name": "step", "type": "Arg" },
      { "name": "timestamp_usec", "type": "Arg" }
    ],
    "attributes": [
      { "name": "prediction_names", "type": "StrArrayAttr" },
      { "name": "training", "type": "BoolAttr" },
      { "name": "file_path", "type": "StrAttr" },
      { "name": "write_vector_predictions", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.Xdivy",
    "summary": "Returns 0 if x == 0, and x / y otherwise, elementwise.",
    "inputs": [
      { "name": "x", "type": "TF_FpOrComplexTensor" },
      { "name": "y", "type": "TF_FpOrComplexTensor" }
    ],
    "outputs": [
      { "name": "z", "type": "TF_FpOrComplexTensor" }
    ]
  },
  {
    "name": "tf.XlaAllReduce",
    "summary": "Wraps the XLA AllReduce operator",
    "description": "documented at https://www.tensorflow.org/xla/operation_semantics#allreduce.",
    "inputs": [
      { "name": "input", "type": "Arg" },
      { "name": "group_assignment", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "TensorOf" }
    ],
    "attributes": [
      { "name": "reduce_op", "type": "TF_AnyStrAttrOf" },
      { "name": "mode", "type": "TF_AnyStrAttrOf" }
    ]
  },
  {
    "name": "tf.XlaBroadcastHelper",
    "summary": "Helper operator for performing XLA-style broadcasts",
    "description": "Broadcasts `lhs` and `rhs` to the same rank, by adding size 1 dimensions to\nwhichever of `lhs` and `rhs` has the lower rank, using XLA's broadcasting rules\nfor binary operators.",
    "inputs": [
      { "name": "lhs", "type": "Arg" },
      { "name": "rhs", "type": "Arg" },
      { "name": "broadcast_dims", "type": "Arg" }
    ],
    "outputs": [
      { "name": "lhs_output", "type": "Res" },
      { "name": "rhs_output", "type": "Res" }
    ]
  },
  {
    "name": "tf.XlaCallModule",
    "summary": "Invokes a StableHLO module.",
    "description": "This op is used with JAX native serialization in a TensorFlow context with\nstability guarantees.",
    "inputs": [
      { "name": "args", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "version", "type": "I64Attr" },
      { "name": "module", "type": "StrAttr" },
      { "name": "Sout", "type": "TF_ShapeAttrArray" },
      { "name": "dim_args_spec", "type": "DefaultValuedOptionalAttr" },
      { "name": "platforms", "type": "DefaultValuedOptionalAttr" },
      { "name": "function_list", "type": "DefaultValuedOptionalAttr" },
      { "name": "has_token_input_output", "type": "DefaultValuedOptionalAttr" },
      { "name": "disabled_checks", "type": "DefaultValuedOptionalAttr" },
      { "name": "use_shardy_partitioner", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.XlaClusterOutput",
    "summary": "Operator that connects the output of an XLA computation to other consumer graph nodes.",
    "inputs": [
      { "name": "input", "type": "TF_Tensor" }
    ],
    "outputs": [
      { "name": "outputs", "type": "TF_Tensor" }
    ]
  },
  {
    "name": "tf.XlaConcatND",
    "summary": "Concats input tensor across all dimensions.",
    "description": "An op which merges slices the input tensor based on the given num_splits\nattribute, strips paddings optionally, and returns the merged tensor without\npaddings.\n\nThis op may be generated via the TPU bridge.\n\nFor example, with `input` tensor:\n```\n[[0, 1],\n [4, 5]]\n[[2, 3],\n [6, 7]]\n[[8, 9],\n [12, 13]]\n[[10, 11],\n [14, 15]]\n```\n`num_splits`:\n```\n[2, 2]\n```\nand `paddings`:\n```\n[1, 1]\n```\nthe expected `outputs` is:\n```\n[[0, 1, 2],\n [4, 5, 6],\n [8, 9, 10]]\n```",
    "inputs": [
      { "name": "inputs", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "num_concats", "type": "I64ArrayAttr" },
      { "name": "paddings", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.XlaConv",
    "summary": "Wraps the XLA ConvGeneralDilated operator, documented at",
    "description": "https://www.tensorflow.org/performance/xla/operation_semantics#conv_convolution\n.",
    "inputs": [
      { "name": "lhs", "type": "Arg" },
      { "name": "rhs", "type": "Arg" },
      { "name": "window_strides", "type": "Arg" },
      { "name": "padding", "type": "Arg" },
      { "name": "lhs_dilation", "type": "Arg" },
      { "name": "rhs_dilation", "type": "Arg" },
      { "name": "feature_group_count", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "TF_NumberTensor" }
    ],
    "attributes": [
      { "name": "dimension_numbers", "type": "StrAttr" },
      { "name": "precision_config", "type": "StrAttr" }
    ]
  },
  {
    "name": "tf.XlaConvV2",
    "summary": "Wraps the XLA ConvGeneralDilated operator, documented at",
    "description": "https://www.tensorflow.org/performance/xla/operation_semantics#conv_convolution\n.",
    "inputs": [
      { "name": "lhs", "type": "Arg" },
      { "name": "rhs", "type": "Arg" },
      { "name": "window_strides", "type": "Arg" },
      { "name": "padding", "type": "Arg" },
      { "name": "lhs_dilation", "type": "Arg" },
      { "name": "rhs_dilation", "type": "Arg" },
      { "name": "feature_group_count", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "TF_NumberTensor" }
    ],
    "attributes": [
      { "name": "dimension_numbers", "type": "StrAttr" },
      { "name": "precision_config", "type": "StrAttr" },
      { "name": "batch_group_count", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.XlaCustomCallV2",
    "summary": "Emits an HLO `CustomCall` operation with multiple outputs.",
    "description": "As opposed to `XlaCustomCall`, this operation supports multiple outputs.\n\nSee `CustomCall` specification at\n  https://tensorflow.org/xla/operation_semantics#customcall,\nand `mhlo.custom_call` specification at\n  https://tensorflow.org/mlir/hlo_ops#mhlocustom_call_mlirmhlocustomcallop.",
    "inputs": [
      { "name": "operands", "type": "Arg" }
    ],
    "outputs": [
      { "name": "results", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "call_target_name", "type": "StrAttr" },
      { "name": "backend_config", "type": "StrAttr" },
      { "name": "has_side_effect", "type": "BoolAttr" },
      { "name": "result_shapes", "type": "TF_ShapeAttrArray" }
    ]
  },
  {
    "name": "tf.XlaDot",
    "summary": "Wraps the XLA DotGeneral operator, documented at",
    "description": "https://www.tensorflow.org/performance/xla/operation_semantics#dotgeneral\n.",
    "inputs": [
      { "name": "lhs", "type": "Arg" },
      { "name": "rhs", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "TF_NumberTensor" }
    ],
    "attributes": [
      { "name": "dimension_numbers", "type": "StrAttr" },
      { "name": "precision_config", "type": "StrAttr" }
    ]
  },
  {
    "name": "tf.XlaDotV2",
    "summary": "Wraps the XLA DotGeneral operator, documented at",
    "description": "https://www.tensorflow.org/performance/xla/operation_semantics#dotgeneral\n.",
    "inputs": [
      { "name": "lhs", "type": "Arg" },
      { "name": "rhs", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "TF_NumberTensor" }
    ],
    "attributes": [
      { "name": "dimension_numbers", "type": "StrAttr" },
      { "name": "precision_config", "type": "StrAttr" }
    ]
  },
  {
    "name": "tf.XlaDynamicSlice",
    "summary": "Wraps the XLA DynamicSlice operator, documented at",
    "description": "https://www.tensorflow.org/performance/xla/operation_semantics#dynamicslice\n.\n\nDynamicSlice extracts a sub-array from the input array at dynamic\nstart_indices. The size of the slice in each dimension is passed in\nsize_indices, which specify the end point of exclusive slice intervals in each\ndimension -- [start, start + size). The shape of start_indices must have rank 1,\nwith dimension size equal to the rank of operand.",
    "inputs": [
      { "name": "input", "type": "Arg" },
      { "name": "start_indices", "type": "Arg" },
      { "name": "size_indices", "type": "TF_I32OrI64Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "TF_Tensor" }
    ]
  },
  {
    "name": "tf.XlaDynamicUpdateSlice",
    "summary": "Wraps the XLA DynamicUpdateSlice operator, documented at",
    "description": "https://www.tensorflow.org/performance/xla/operation_semantics#dynamicupdateslice\n.\n\nXlaDynamicUpdateSlice generates a result which is the value of the `input`\noperand, with a slice update overwritten at `indices`. The shape of `update`\ndetermines the shape of the sub-array of the result which is updated. The shape\nof indices must be rank == 1, with dimension size equal to the rank of `input`.\n\nHandling of out-of-bounds slice indices is implementation-defined.",
    "inputs": [
      { "name": "input", "type": "Arg" },
      { "name": "update", "type": "Arg" },
      { "name": "indices", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ]
  },
  {
    "name": "tf.XlaEinsum",
    "summary": "An op which supports basic einsum op with 2 inputs and 1 output.",
    "description": "This op has better TPU performance since it doesn't have explicitly reshape and\ntranspose operations as tf.einsum does.",
    "inputs": [
      { "name": "a", "type": "TensorOf" },
      { "name": "b", "type": "TensorOf" }
    ],
    "outputs": [
      { "name": "product", "type": "TensorOf" }
    ],
    "attributes": [
      { "name": "equation", "type": "StrAttr" }
    ]
  },
  {
    "name": "tf.XlaGather",
    "summary": "Wraps the XLA Gather operator documented at",
    "description": "https://www.tensorflow.org/xla/operation_semantics#gather",
    "inputs": [
      { "name": "operand", "type": "Arg" },
      { "name": "start_indices", "type": "Arg" },
      { "name": "slice_sizes", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "TensorOf" }
    ],
    "attributes": [
      { "name": "dimension_numbers", "type": "StrAttr" },
      { "name": "indices_are_sorted", "type": "BoolAttr" }
    ]
  },
  {
    "name": "tf.XlaHostCompute",
    "summary": "A pseudo-op to represent host-side computation in an XLA program.",
    "inputs": [
      { "name": "inputs", "type": "Arg" }
    ],
    "outputs": [
      { "name": "outputs", "type": "Res" }
    ],
    "attributes": [
      { "name": "ancestors", "type": "StrArrayAttr" },
      { "name": "shapes", "type": "TF_ShapeAttrArray" },
      { "name": "shape_inference_graph", "type": "OptionalAttr" },
      { "name": "key", "type": "StrAttr" },
      { "name": "send_key", "type": "DefaultValuedStrAttr" },
      { "name": "recv_key", "type": "DefaultValuedStrAttr" },
      { "name": "cost_estimate_ns", "type": "DefaultValuedOptionalAttr" },
      { "name": "tpu_core", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.XlaKeyValueSort",
    "summary": "Wraps the XLA Sort operator, documented at",
    "description": "https://www.tensorflow.org/performance/xla/operation_semantics#sort\n.\n\nSorts a tensor. Currently only sorts in ascending order are supported.",
    "inputs": [
      { "name": "keys", "type": "Arg" },
      { "name": "values", "type": "Arg" }
    ],
    "outputs": [
      { "name": "sorted_keys", "type": "Res" },
      { "name": "sorted_values", "type": "Res" }
    ]
  },
  {
    "name": "tf.XlaLaunch",
    "summary": "XLA Launch Op. For use by the XLA JIT only.",
    "inputs": [
      { "name": "constants", "type": "Variadic" },
      { "name": "args", "type": "Variadic" },
      { "name": "resources", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "results", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "function", "type": "SymbolRefAttr" }
    ]
  },
  {
    "name": "tf.XlaLaunchV2",
    "summary": "XLA Launch Op. For use by the XLA JIT only.",
    "inputs": [
      { "name": "args", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "results", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "constants", "type": "I64ArrayAttr" },
      { "name": "resources", "type": "I64ArrayAttr" },
      { "name": "function", "type": "SymbolRefAttr" }
    ]
  },
  {
    "name": "tf.XlaLocalSparseDenseMatmul",
    "summary": "Performs embedding lookup on SparseCore for a single table.",
    "inputs": [
      { "name": "embedding_ids", "type": "TF_Int32Tensor" },
      { "name": "sample_ids", "type": "TF_Int32Tensor" },
      { "name": "gains", "type": "TF_Float32Tensor" },
      { "name": "embedding_table", "type": "TF_Tensor" }
    ],
    "outputs": [
      { "name": "result", "type": "TF_Tensor" }
    ],
    "attributes": [
      { "name": "batch_size", "type": "ConfinedAttr" },
      { "name": "T", "type": "TypeAttr" }
    ]
  },
  {
    "name": "tf.XlaOptimizationBarrier",
    "summary": "Wraps the XLA OptimizationBarrier operator.",
    "description": "Documented at https://www.tensorflow.org/xla/operation_semantics#optimizationbarrier.",
    "inputs": [
      { "name": "input", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Variadic" }
    ]
  },
  {
    "name": "tf.XlaPad",
    "summary": "Wraps the XLA Pad operator, documented at",
    "description": "https://www.tensorflow.org/performance/xla/operation_semantics#pad\n.",
    "inputs": [
      { "name": "input", "type": "Arg" },
      { "name": "padding_value", "type": "Arg" },
      { "name": "padding_low", "type": "Arg" },
      { "name": "padding_high", "type": "Arg" },
      { "name": "padding_interior", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ]
  },
  {
    "name": "tf.XlaRecv",
    "summary": "Receives the named tensor from another XLA computation. Wraps the XLA Recv",
    "description": "operator documented at\n https://www.tensorflow.org/performance/xla/operation_semantics#recv .",
    "outputs": [
      { "name": "tensor", "type": "Res" }
    ],
    "attributes": [
      { "name": "tensor_name", "type": "StrAttr" },
      { "name": "shape", "type": "TF_ShapeAttr" }
    ]
  },
  {
    "name": "tf.XlaRecvFromHost",
    "summary": "An op to receive a tensor from the host.",
    "description": "output: the tensor that will be received from the host.\nToutput: element type for output.\nshape: shape for output.\nkey: A unique identifier for this region used to match up host transfers.",
    "outputs": [
      { "name": "output", "type": "TF_Tensor" }
    ],
    "attributes": [
      { "name": "shape", "type": "TF_ShapeAttr" },
      { "name": "key", "type": "StrAttr" }
    ]
  },
  {
    "name": "tf.XlaRecvTPUEmbeddingActivations",
    "summary": "An op that receives embedding activations on the TPU.",
    "description": "The TPU system performs the embedding lookups and aggregations. The results of\nthese aggregations are visible to the Tensorflow Graph as the outputs of a\nXlaRecvTPUEmbeddingActivations Op. This op returns a list containing one\nTensor of activations per table specified in the model.",
    "inputs": [
      { "name": "deduplication_data", "type": "Arg" }
    ],
    "outputs": [
      { "name": "outputs", "type": "Res" }
    ],
    "attributes": [
      { "name": "config", "type": "StrAttr" }
    ]
  },
  {
    "name": "tf.XlaRecvTPUEmbeddingDeduplicationData",
    "summary": "Receives deduplication data (indices and weights) from the embedding core.",
    "description": "The deduplication data is a Tensor with type=DT_VARIANT. The tensor itself is an\nXLA nested tuple containing N elements (where N is the ratio of the number of\nembedding to tensor cores per TPU chip). Each element of the nested tuple is a\ntuple of rank 1 tensors. Each tensor either contains indices (DT_UINT32) for\nembedding lookup on the TensorCore or weights (DT_FLOAT) to apply to the output\nof the embedding lookup operation.",
    "outputs": [
      { "name": "output", "type": "TF_VariantTensor" }
    ],
    "attributes": [
      { "name": "config", "type": "StrAttr" }
    ]
  },
  {
    "name": "tf.XlaReduce",
    "summary": "Wraps the XLA Reduce operator, documented at",
    "description": "https://www.tensorflow.org/performance/xla/operation_semantics#reduce .",
    "inputs": [
      { "name": "input", "type": "Arg" },
      { "name": "init_value", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "TensorOf" }
    ],
    "attributes": [
      { "name": "dimensions_to_reduce", "type": "I64ArrayAttr" },
      { "name": "reducer", "type": "SymbolRefAttr" }
    ]
  },
  {
    "name": "tf.XlaReducePrecision",
    "summary": "Wraps the XLA ReducePrecision operator",
    "description": "documented at https://www.tensorflow.org/xla/operation_semantics#reduceprecision.",
    "inputs": [
      { "name": "operand", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "TF_FloatTensor" }
    ],
    "attributes": [
      { "name": "exponent_bits", "type": "I64Attr" },
      { "name": "mantissa_bits", "type": "I64Attr" }
    ]
  },
  {
    "name": "tf.XlaReduceScatter",
    "summary": "Wraps the XLA ReduceScatter operator",
    "description": "documented at https://www.tensorflow.org/xla/operation_semantics#reducescatter.",
    "inputs": [
      { "name": "input", "type": "Arg" },
      { "name": "group_assignment", "type": "Arg" },
      { "name": "scatter_dimension", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "TensorOf" }
    ],
    "attributes": [
      { "name": "reduce_op", "type": "TF_AnyStrAttrOf" }
    ]
  },
  {
    "name": "tf.XlaReduceWindow",
    "summary": "Wraps the XLA ReduceWindow operator, documented at",
    "description": "https://www.tensorflow.org/performance/xla/operation_semantics#reducewindow .",
    "inputs": [
      { "name": "input", "type": "Arg" },
      { "name": "init_value", "type": "Arg" },
      { "name": "window_dimensions", "type": "Arg" },
      { "name": "window_strides", "type": "Arg" },
      { "name": "base_dilations", "type": "TF_I32OrI64Tensor" },
      { "name": "window_dilations", "type": "TF_I32OrI64Tensor" },
      { "name": "padding", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "TensorOf" }
    ],
    "attributes": [
      { "name": "computation", "type": "SymbolRefAttr" }
    ]
  },
  {
    "name": "tf.XlaRemoveDynamicDimensionSize",
    "summary": "Inverse of XlaSetDynamicDimensionSize.",
    "description": "Make an xla bounded dynamic dimension into a static dimension. The bound of the\nsize of dimension `dim_index` becomes the static dimension size.",
    "inputs": [
      { "name": "input", "type": "TF_Tensor" },
      { "name": "dim_index", "type": "TF_Int32Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "TF_Tensor" }
    ]
  },
  {
    "name": "tf.XlaReplicaId",
    "summary": "Replica ID.",
    "outputs": [
      { "name": "id", "type": "TF_Int32Tensor" }
    ]
  },
  {
    "name": "tf.XlaRngBitGenerator",
    "summary": "Stateless PRNG bit generator.",
    "description": "Wraps the XLA RngBitGenerator operator, documented at\n https://www.tensorflow.org/performance/xla/operation_semantics#rngbitgenerator.",
    "inputs": [
      { "name": "algorithm", "type": "Arg" },
      { "name": "initial_state", "type": "Arg" },
      { "name": "shape", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output_key", "type": "TF_Uint64Tensor" },
      { "name": "output", "type": "TensorOf" }
    ]
  },
  {
    "name": "tf.XlaScatter",
    "summary": "Wraps the XLA Scatter operator documented at",
    "description": "https://www.tensorflow.org/xla/operation_semantics#scatter.",
    "inputs": [
      { "name": "operand", "type": "Arg" },
      { "name": "scatter_indices", "type": "Arg" },
      { "name": "updates", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "TensorOf" }
    ],
    "attributes": [
      { "name": "update_computation", "type": "SymbolRefAttr" },
      { "name": "dimension_numbers", "type": "StrAttr" },
      { "name": "indices_are_sorted", "type": "BoolAttr" }
    ]
  },
  {
    "name": "tf.XlaSelectAndScatter",
    "summary": "Wraps the XLA SelectAndScatter operator, documented at",
    "description": "https://www.tensorflow.org/performance/xla/operation_semantics#selectandscatter\n.",
    "inputs": [
      { "name": "operand", "type": "Arg" },
      { "name": "window_dimensions", "type": "Arg" },
      { "name": "window_strides", "type": "Arg" },
      { "name": "padding", "type": "Arg" },
      { "name": "source", "type": "Arg" },
      { "name": "init_value", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "TF_NumberTensor" }
    ],
    "attributes": [
      { "name": "select", "type": "SymbolRefAttr" },
      { "name": "scatter", "type": "SymbolRefAttr" }
    ]
  },
  {
    "name": "tf.XlaSelfAdjointEig",
    "summary": "Computes the eigen decomposition of a batch of self-adjoint matrices",
    "description": "(Note: Only real inputs are supported).\n\nComputes the eigenvalues and eigenvectors of the innermost N-by-N matrices in\ntensor such that tensor[...,:,:] * v[..., :,i] = e[..., i] * v[...,:,i], for\ni=0...N-1.",
    "inputs": [
      { "name": "a", "type": "Arg" }
    ],
    "outputs": [
      { "name": "w", "type": "Res" },
      { "name": "v", "type": "Res" }
    ],
    "attributes": [
      { "name": "lower", "type": "BoolAttr" },
      { "name": "max_iter", "type": "I64Attr" },
      { "name": "epsilon", "type": "F32Attr" }
    ]
  },
  {
    "name": "tf.XlaSend",
    "summary": "Sends the named tensor to another XLA computation. Wraps the XLA Send operator",
    "description": "documented at\n https://www.tensorflow.org/performance/xla/operation_semantics#send .",
    "inputs": [
      { "name": "tensor", "type": "Arg" }
    ],
    "attributes": [
      { "name": "tensor_name", "type": "StrAttr" }
    ]
  },
  {
    "name": "tf.XlaSendToHost",
    "summary": "An op to send a tensor to the host.",
    "description": "input: the tensor that will be sent to the host.\nTinput: element type for input.\nkey: A unique identifier for this region used to match up host transfers.",
    "inputs": [
      { "name": "input", "type": "TF_Tensor" }
    ],
    "attributes": [
      { "name": "key", "type": "StrAttr" }
    ]
  },
  {
    "name": "tf.XlaSendTPUEmbeddingGradients",
    "summary": "An op that performs gradient updates of embedding tables.",
    "description": "The gradients argument is a TensorList having the same length and shapes as the\nreturn value of XlaRecvTPUEmbeddingActivations, but contains gradients of the\nmodel's loss with respect to the embedding activations. The embedding tables are\nupdated from these gradients via the optimizer specified in the\nTPUEmbeddingConfiguration proto given to tpu.initialize_system.",
    "inputs": [
      { "name": "gradients", "type": "Arg" },
      { "name": "learning_rates", "type": "Arg" },
      { "name": "deduplication_data", "type": "Arg" }
    ],
    "attributes": [
      { "name": "config", "type": "StrAttr" }
    ]
  },
  {
    "name": "tf.XlaSetBound",
    "summary": "Set a bound for the given input value as a hint to Xla compiler,",
    "description": "returns the same value.",
    "inputs": [
      { "name": "input", "type": "TF_Int32Tensor" },
      { "name": "bound", "type": "TF_Int32Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "TF_Int32Tensor" }
    ]
  },
  {
    "name": "tf.XlaSetDynamicDimensionSize",
    "summary": "Make a static dimension into a xla bounded dynamic dimension.",
    "description": "The current static dimension size will become the bound and the second\n        operand becomes the dynamic size of the dimension.",
    "inputs": [
      { "name": "input", "type": "TF_Tensor" },
      { "name": "dim_index", "type": "TF_Int32Tensor" },
      { "name": "size", "type": "TF_Int32Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "TF_Tensor" }
    ]
  },
  {
    "name": "tf.XlaSharding",
    "summary": "An op which shards the input based on the given sharding attribute.\n\nSince TF runtime still relies on V1 sharding but the compiler requires V2\nSharding to support the new partitioner Shardy, when the op can reach both the\ncompiler and the runtime, we record V1 sharding attribute in _XlaSharding and V2\nsharding attribute in _XlaShardingV2, and verify that they are equivalent when\nthey both exist and _XlaShardingV2 is used.",
    "inputs": [
      { "name": "input", "type": "TF_Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "TF_Tensor" }
    ],
    "attributes": [
      { "name": "sharding", "type": "DefaultValuedStrAttr" },
      { "name": "_XlaSharding", "type": "OptionalAttr" },
      { "name": "_XlaShardingV2", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "tf.XlaSort",
    "summary": "Wraps the XLA Sort operator, documented at",
    "description": "https://www.tensorflow.org/performance/xla/operation_semantics#sort\n.\n\nSorts a tensor. Currently only sorts in ascending order are supported.",
    "inputs": [
      { "name": "input", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ]
  },
  {
    "name": "tf.XlaSparseActivationsUnstack",
    "summary": "XlaSparseActivationsUnstackOp attempts to fuse transpose, relayout,\n    conversion, stacking and optionally interleaving of the embedding\n    activations, while also offloading this work to SparseCore.\n\n    The op assumes its operand is in SparseCore layout.\n    The output is a tuple of tensors in TensorCore layout.",
    "inputs": [
      { "name": "stacked_activations", "type": "TF_Tensor" }
    ],
    "outputs": [
      { "name": "unstacked_embedding_activations", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "num_tables", "type": "ConfinedAttr" },
      { "name": "sample_counts", "type": "ConfinedAttr" },
      { "name": "features", "type": "ConfinedAttr" },
      { "name": "interleaved", "type": "BoolAttr" }
    ]
  },
  {
    "name": "tf.XlaSparseCoreAdagrad",
    "summary": "aaa",
    "inputs": [
      { "name": "indices", "type": "TF_Int32Tensor" },
      { "name": "gradient", "type": "TF_Float32Tensor" },
      { "name": "learning_rate", "type": "TF_Float32Tensor" },
      { "name": "accumulator", "type": "TF_Float32Tensor" },
      { "name": "embedding_table", "type": "TF_Float32Tensor" }
    ],
    "outputs": [
      { "name": "updated_embedding_table", "type": "TF_Float32Tensor" },
      { "name": "updated_accumulator", "type": "TF_Float32Tensor" }
    ],
    "attributes": [
      { "name": "feature_width", "type": "I64Attr" }
    ]
  },
  {
    "name": "tf.XlaSparseCoreAdagradMomentum",
    "summary": "aaa",
    "inputs": [
      { "name": "indices", "type": "TF_Int32Tensor" },
      { "name": "gradient", "type": "TF_Float32Tensor" },
      { "name": "learning_rate", "type": "TF_Float32Tensor" },
      { "name": "beta_1", "type": "TF_Float32Tensor" },
      { "name": "epsilon", "type": "TF_Float32Tensor" },
      { "name": "accumulator", "type": "TF_Float32Tensor" },
      { "name": "momentum", "type": "TF_Float32Tensor" },
      { "name": "embedding_table", "type": "TF_Float32Tensor" }
    ],
    "outputs": [
      { "name": "updated_embedding_table", "type": "TF_Float32Tensor" },
      { "name": "updated_accumulator", "type": "TF_Float32Tensor" },
      { "name": "updated_momentum", "type": "TF_Float32Tensor" }
    ],
    "attributes": [
      { "name": "feature_width", "type": "I64Attr" },
      { "name": "use_nesterov", "type": "BoolAttr" },
      { "name": "beta_2", "type": "F32Attr" },
      { "name": "exponent", "type": "F32Attr" }
    ]
  },
  {
    "name": "tf.XlaSparseCoreAdam",
    "summary": "aaa",
    "inputs": [
      { "name": "embedding_table", "type": "TF_Float32Tensor" },
      { "name": "indices", "type": "TF_Int32Tensor" },
      { "name": "gradient", "type": "TF_Float32Tensor" },
      { "name": "learning_rate", "type": "TF_Float32Tensor" },
      { "name": "momentum", "type": "TF_Float32Tensor" },
      { "name": "velocity", "type": "TF_Float32Tensor" },
      { "name": "beta_1", "type": "TF_Float32Tensor" },
      { "name": "beta_2", "type": "TF_Float32Tensor" },
      { "name": "epsilon", "type": "TF_Float32Tensor" }
    ],
    "outputs": [
      { "name": "updated_embedding_table", "type": "TF_Float32Tensor" },
      { "name": "updated_velocity", "type": "TF_Float32Tensor" },
      { "name": "updated_momentum", "type": "TF_Float32Tensor" }
    ],
    "attributes": [
      { "name": "feature_width", "type": "I64Attr" },
      { "name": "use_sum_inside_sqrt", "type": "BoolAttr" }
    ]
  },
  {
    "name": "tf.XlaSparseCoreFtrl",
    "summary": "aaa",
    "inputs": [
      { "name": "embedding_table", "type": "TF_Float32Tensor" },
      { "name": "accumulator", "type": "TF_Float32Tensor" },
      { "name": "linear", "type": "TF_Float32Tensor" },
      { "name": "learning_rate", "type": "TF_Float32Tensor" },
      { "name": "indices", "type": "TF_Int32Tensor" },
      { "name": "gradient", "type": "TF_Float32Tensor" },
      { "name": "beta", "type": "TF_Float32Tensor" },
      { "name": "learning_rate_power", "type": "TF_Float32Tensor" },
      { "name": "l2_regularization_strength", "type": "TF_Float32Tensor" }
    ],
    "outputs": [
      { "name": "updated_embedding_table", "type": "TF_Float32Tensor" },
      { "name": "updated_accumulator", "type": "TF_Float32Tensor" },
      { "name": "updated_linear", "type": "TF_Float32Tensor" }
    ],
    "attributes": [
      { "name": "feature_width", "type": "I64Attr" },
      { "name": "multiply_linear_by_learning_rate", "type": "BoolAttr" },
      { "name": "l1_regularization_strength", "type": "F32Attr" }
    ]
  },
  {
    "name": "tf.XlaSparseCoreSgd",
    "summary": "aaa",
    "inputs": [
      { "name": "indices", "type": "TF_Int32Tensor" },
      { "name": "gradient", "type": "TF_Float32Tensor" },
      { "name": "learning_rate", "type": "TF_Float32Tensor" },
      { "name": "embedding_table", "type": "TF_Float32Tensor" }
    ],
    "outputs": [
      { "name": "updated_embedding_table", "type": "TF_Float32Tensor" }
    ],
    "attributes": [
      { "name": "feature_width", "type": "I64Attr" }
    ]
  },
  {
    "name": "tf.XlaSparseDenseMatmulCustomCombinerOnTcGradWithAdagradAndCsrInput",
    "summary": "This op back-propagates the activation gradients to the embedding table and the combiner weights.",
    "inputs": [
      { "name": "row_pointers", "type": "TF_Int32Tensor" },
      { "name": "sorted_sample_ids", "type": "TF_Int32Tensor" },
      { "name": "sorted_token_ids", "type": "TF_Int32Tensor" },
      { "name": "sorted_pos_ids", "type": "TF_Int32Tensor" },
      { "name": "sorted_gains", "type": "TF_Float32Tensor" },
      { "name": "weights", "type": "TF_Float32Tensor" },
      { "name": "preserved_valencies", "type": "TF_Int32Tensor" },
      { "name": "preserved_vectors", "type": "TF_Float32Tensor" },
      { "name": "preserved_weights", "type": "TF_Float32Tensor" },
      { "name": "activation_gradients", "type": "TF_Float32Tensor" },
      { "name": "learning_rate", "type": "TF_Float32Tensor" },
      { "name": "combiner_weights_learning_rate", "type": "TF_Float32Tensor" },
      { "name": "embedding_table", "type": "TF_Float32Tensor" },
      { "name": "accumulator", "type": "TF_Float32Tensor" }
    ],
    "outputs": [
      { "name": "updated_embedding_table", "type": "TF_Float32Tensor" },
      { "name": "updated_accumulator", "type": "TF_Float32Tensor" },
      { "name": "updated_weights", "type": "TF_Float32Tensor" }
    ],
    "attributes": [
      { "name": "max_valency", "type": "ConfinedAttr" },
      { "name": "num_weights", "type": "ConfinedAttr" },
      { "name": "clip_weight_min", "type": "F32Attr" },
      { "name": "clip_weight_max", "type": "F32Attr" },
      { "name": "combiner_table_vjp_computation", "type": "SymbolRefAttr" },
      { "name": "combiner_weights_vjp_computation", "type": "SymbolRefAttr" },
      { "name": "table_name", "type": "StrAttr" }
    ]
  },
  {
    "name": "tf.XlaSparseDenseMatmulCustomCombinerOnTcGradWithAdagradMomentumAndCsrInput",
    "summary": "This op back-propagates the activation gradients to the embedding table and the combiner weights.",
    "inputs": [
      { "name": "row_pointers", "type": "TF_Int32Tensor" },
      { "name": "sorted_sample_ids", "type": "TF_Int32Tensor" },
      { "name": "sorted_token_ids", "type": "TF_Int32Tensor" },
      { "name": "sorted_pos_ids", "type": "TF_Int32Tensor" },
      { "name": "sorted_gains", "type": "TF_Float32Tensor" },
      { "name": "weights", "type": "TF_Float32Tensor" },
      { "name": "preserved_valencies", "type": "TF_Int32Tensor" },
      { "name": "preserved_vectors", "type": "TF_Float32Tensor" },
      { "name": "preserved_weights", "type": "TF_Float32Tensor" },
      { "name": "activation_gradients", "type": "TF_Float32Tensor" },
      { "name": "learning_rate", "type": "TF_Float32Tensor" },
      { "name": "combiner_weights_learning_rate", "type": "TF_Float32Tensor" },
      { "name": "embedding_table", "type": "TF_Float32Tensor" },
      { "name": "accumulator", "type": "TF_Float32Tensor" },
      { "name": "momenta", "type": "TF_Float32Tensor" }
    ],
    "outputs": [
      { "name": "updated_embedding_table", "type": "TF_Float32Tensor" },
      { "name": "updated_accumulator", "type": "TF_Float32Tensor" },
      { "name": "updated_momenta", "type": "TF_Float32Tensor" },
      { "name": "updated_weights", "type": "TF_Float32Tensor" }
    ],
    "attributes": [
      { "name": "max_valency", "type": "ConfinedAttr" },
      { "name": "num_weights", "type": "ConfinedAttr" },
      { "name": "clip_weight_min", "type": "F32Attr" },
      { "name": "clip_weight_max", "type": "F32Attr" },
      { "name": "use_nesterov", "type": "BoolAttr" },
      { "name": "exponent", "type": "F32Attr" },
      { "name": "beta1", "type": "F32Attr" },
      { "name": "beta2", "type": "F32Attr" },
      { "name": "epsilon", "type": "F32Attr" },
      { "name": "combiner_table_vjp_computation", "type": "SymbolRefAttr" },
      { "name": "combiner_weights_vjp_computation", "type": "SymbolRefAttr" },
      { "name": "table_name", "type": "StrAttr" }
    ]
  },
  {
    "name": "tf.XlaSparseDenseMatmulCustomCombinerOnTcGradWithAdamAndCsrInput",
    "summary": "This op back-propagates the activation gradients to the embedding table and the combiner weights.",
    "inputs": [
      { "name": "row_pointers", "type": "TF_Int32Tensor" },
      { "name": "sorted_sample_ids", "type": "TF_Int32Tensor" },
      { "name": "sorted_token_ids", "type": "TF_Int32Tensor" },
      { "name": "sorted_pos_ids", "type": "TF_Int32Tensor" },
      { "name": "sorted_gains", "type": "TF_Float32Tensor" },
      { "name": "weights", "type": "TF_Float32Tensor" },
      { "name": "preserved_valencies", "type": "TF_Int32Tensor" },
      { "name": "preserved_vectors", "type": "TF_Float32Tensor" },
      { "name": "preserved_weights", "type": "TF_Float32Tensor" },
      { "name": "activation_gradients", "type": "TF_Float32Tensor" },
      { "name": "learning_rate", "type": "TF_Float32Tensor" },
      { "name": "combiner_weights_learning_rate", "type": "TF_Float32Tensor" },
      { "name": "embedding_table", "type": "TF_Float32Tensor" },
      { "name": "momenta", "type": "TF_Float32Tensor" },
      { "name": "velocity", "type": "TF_Float32Tensor" }
    ],
    "outputs": [
      { "name": "updated_embedding_table", "type": "TF_Float32Tensor" },
      { "name": "updated_momenta", "type": "TF_Float32Tensor" },
      { "name": "updated_velocity", "type": "TF_Float32Tensor" },
      { "name": "updated_weights", "type": "TF_Float32Tensor" }
    ],
    "attributes": [
      { "name": "max_valency", "type": "ConfinedAttr" },
      { "name": "num_weights", "type": "ConfinedAttr" },
      { "name": "clip_weight_min", "type": "F32Attr" },
      { "name": "clip_weight_max", "type": "F32Attr" },
      { "name": "use_sum_inside_sqrt", "type": "BoolAttr" },
      { "name": "beta1", "type": "F32Attr" },
      { "name": "beta2", "type": "F32Attr" },
      { "name": "epsilon", "type": "F32Attr" },
      { "name": "combiner_table_vjp_computation", "type": "SymbolRefAttr" },
      { "name": "combiner_weights_vjp_computation", "type": "SymbolRefAttr" },
      { "name": "table_name", "type": "StrAttr" }
    ]
  },
  {
    "name": "tf.XlaSparseDenseMatmulCustomCombinerOnTcGradWithCsrInput",
    "summary": "This op back-propagates the activation gradients to the embedding table and the combiner weights.",
    "inputs": [
      { "name": "row_pointers", "type": "TF_Int32Tensor" },
      { "name": "sorted_sample_ids", "type": "TF_Int32Tensor" },
      { "name": "sorted_token_ids", "type": "TF_Int32Tensor" },
      { "name": "sorted_pos_ids", "type": "TF_Int32Tensor" },
      { "name": "sorted_gains", "type": "TF_Float32Tensor" },
      { "name": "weights", "type": "TF_Float32Tensor" },
      { "name": "preserved_valencies", "type": "TF_Int32Tensor" },
      { "name": "preserved_vectors", "type": "TF_Float32Tensor" },
      { "name": "preserved_weights", "type": "TF_Float32Tensor" },
      { "name": "activation_gradients", "type": "TF_Float32Tensor" },
      { "name": "tables", "type": "Variadic" },
      { "name": "hyperparameters", "type": "Variadic" },
      { "name": "combiner_weights_learning_rate", "type": "TF_Float32Tensor" }
    ],
    "outputs": [
      { "name": "updated_tables", "type": "Variadic" },
      { "name": "updated_weights", "type": "TF_Float32Tensor" }
    ],
    "attributes": [
      { "name": "max_valency", "type": "ConfinedAttr" },
      { "name": "num_weights", "type": "ConfinedAttr" },
      { "name": "combiner_table_vjp_computation", "type": "SymbolRefAttr" },
      { "name": "combiner_weights_vjp_computation", "type": "SymbolRefAttr" },
      { "name": "optimizer_custom_computation", "type": "SymbolRefAttr" },
      { "name": "table_name", "type": "StrAttr" }
    ]
  },
  {
    "name": "tf.XlaSparseDenseMatmulCustomCombinerOnTcGradWithFtrlAndCsrInput",
    "summary": "This op back-propagates the activation gradients to the embedding table and the combiner weights.",
    "inputs": [
      { "name": "row_pointers", "type": "TF_Int32Tensor" },
      { "name": "sorted_sample_ids", "type": "TF_Int32Tensor" },
      { "name": "sorted_token_ids", "type": "TF_Int32Tensor" },
      { "name": "sorted_pos_ids", "type": "TF_Int32Tensor" },
      { "name": "sorted_gains", "type": "TF_Float32Tensor" },
      { "name": "weights", "type": "TF_Float32Tensor" },
      { "name": "preserved_valencies", "type": "TF_Int32Tensor" },
      { "name": "preserved_vectors", "type": "TF_Float32Tensor" },
      { "name": "preserved_weights", "type": "TF_Float32Tensor" },
      { "name": "activation_gradients", "type": "TF_Float32Tensor" },
      { "name": "learning_rate", "type": "TF_Float32Tensor" },
      { "name": "combiner_weights_learning_rate", "type": "TF_Float32Tensor" },
      { "name": "embedding_table", "type": "TF_Float32Tensor" },
      { "name": "accumulator", "type": "TF_Float32Tensor" },
      { "name": "linear", "type": "TF_Float32Tensor" }
    ],
    "outputs": [
      { "name": "updated_embedding_table", "type": "TF_Float32Tensor" },
      { "name": "updated_accumulator", "type": "TF_Float32Tensor" },
      { "name": "updated_linear", "type": "TF_Float32Tensor" },
      { "name": "updated_weights", "type": "TF_Float32Tensor" }
    ],
    "attributes": [
      { "name": "max_valency", "type": "ConfinedAttr" },
      { "name": "num_weights", "type": "ConfinedAttr" },
      { "name": "clip_weight_min", "type": "F32Attr" },
      { "name": "clip_weight_max", "type": "F32Attr" },
      { "name": "multiply_linear_by_learning_rate", "type": "BoolAttr" },
      { "name": "beta", "type": "F32Attr" },
      { "name": "learning_rate_power", "type": "F32Attr" },
      { "name": "l1_regularization_strength", "type": "F32Attr" },
      { "name": "l2_regularization_strength", "type": "F32Attr" },
      { "name": "combiner_table_vjp_computation", "type": "SymbolRefAttr" },
      { "name": "combiner_weights_vjp_computation", "type": "SymbolRefAttr" },
      { "name": "table_name", "type": "StrAttr" }
    ]
  },
  {
    "name": "tf.XlaSparseDenseMatmulCustomCombinerOnTcGradWithSgdAndCsrInput",
    "summary": "This op back-propagates the activation gradients to the embedding table and the combiner weights.",
    "inputs": [
      { "name": "row_pointers", "type": "TF_Int32Tensor" },
      { "name": "sorted_sample_ids", "type": "TF_Int32Tensor" },
      { "name": "sorted_token_ids", "type": "TF_Int32Tensor" },
      { "name": "sorted_pos_ids", "type": "TF_Int32Tensor" },
      { "name": "sorted_gains", "type": "TF_Float32Tensor" },
      { "name": "weights", "type": "TF_Float32Tensor" },
      { "name": "preserved_valencies", "type": "TF_Int32Tensor" },
      { "name": "preserved_vectors", "type": "TF_Float32Tensor" },
      { "name": "preserved_weights", "type": "TF_Float32Tensor" },
      { "name": "activation_gradients", "type": "TF_Float32Tensor" },
      { "name": "learning_rate", "type": "TF_Float32Tensor" },
      { "name": "combiner_weights_learning_rate", "type": "TF_Float32Tensor" },
      { "name": "embedding_table", "type": "TF_Float32Tensor" }
    ],
    "outputs": [
      { "name": "updated_embedding_table", "type": "TF_Float32Tensor" },
      { "name": "updated_weights", "type": "TF_Float32Tensor" }
    ],
    "attributes": [
      { "name": "max_valency", "type": "ConfinedAttr" },
      { "name": "num_weights", "type": "ConfinedAttr" },
      { "name": "clip_weight_min", "type": "F32Attr" },
      { "name": "clip_weight_max", "type": "F32Attr" },
      { "name": "combiner_table_vjp_computation", "type": "SymbolRefAttr" },
      { "name": "combiner_weights_vjp_computation", "type": "SymbolRefAttr" },
      { "name": "table_name", "type": "StrAttr" }
    ]
  },
  {
    "name": "tf.XlaSparseDenseMatmulCustomCombinerOnTcWithCsrInput",
    "summary": "This op looks up the embedding vectors on SparseCores and performs the given combiner computation on TensorCores.",
    "inputs": [
      { "name": "row_pointers", "type": "TF_Int32Tensor" },
      { "name": "sorted_sample_ids", "type": "TF_Int32Tensor" },
      { "name": "sorted_token_ids", "type": "TF_Int32Tensor" },
      { "name": "sorted_pos_ids", "type": "TF_Int32Tensor" },
      { "name": "sorted_gains", "type": "TF_Float32Tensor" },
      { "name": "embedding_table", "type": "TF_Float32Tensor" },
      { "name": "weights", "type": "TF_Float32Tensor" }
    ],
    "outputs": [
      { "name": "activations", "type": "TF_Float32Tensor" },
      { "name": "preserved_valencies", "type": "TF_Int32Tensor" },
      { "name": "preserved_vectors", "type": "TF_Float32Tensor" }
    ],
    "attributes": [
      { "name": "input_size", "type": "ConfinedAttr" },
      { "name": "max_valency", "type": "ConfinedAttr" },
      { "name": "num_weights", "type": "ConfinedAttr" },
      { "name": "quantization_config_low", "type": "OptionalAttr" },
      { "name": "quantization_config_high", "type": "OptionalAttr" },
      { "name": "quantization_config_num_buckets", "type": "OptionalAttr" },
      { "name": "combiner_computation", "type": "SymbolRefAttr" },
      { "name": "table_name", "type": "StrAttr" }
    ]
  },
  {
    "name": "tf.XlaSparseDenseMatmulGradWithAdagradAndCsrInput",
    "inputs": [
      { "name": "row_pointers", "type": "TF_Int32Tensor" },
      { "name": "sorted_sample_ids", "type": "TF_Int32Tensor" },
      { "name": "sorted_token_ids", "type": "TF_Int32Tensor" },
      { "name": "sorted_gains", "type": "TF_Float32Tensor" },
      { "name": "activation_gradients", "type": "TF_Float32Tensor" },
      { "name": "learning_rate", "type": "TF_Float32Tensor" },
      { "name": "embedding_table", "type": "TF_Float32Tensor" },
      { "name": "accumulator", "type": "TF_Float32Tensor" },
      { "name": "num_minibatches_per_physical_sparse_core", "type": "TF_Int32Tensor" }
    ],
    "outputs": [
      { "name": "updated_embedding_table", "type": "TF_Float32Tensor" },
      { "name": "updated_accumulator", "type": "TF_Float32Tensor" }
    ],
    "attributes": [
      { "name": "clip_weight_min", "type": "F32Attr" },
      { "name": "clip_weight_max", "type": "F32Attr" },
      { "name": "table_name", "type": "StrAttr" }
    ]
  },
  {
    "name": "tf.XlaSparseDenseMatmulGradWithAdagradAndStaticBufferSize",
    "summary": "A XLA op which performs the Adagrad optimizer update for the dense-sparse matrix multiplication.",
    "inputs": [
      { "name": "row_pointers", "type": "TF_Int32Tensor" },
      { "name": "sorted_sample_ids", "type": "TF_Int32Tensor" },
      { "name": "sorted_token_ids", "type": "TF_Int32Tensor" },
      { "name": "sorted_gains", "type": "TF_Float32Tensor" },
      { "name": "activation_gradients", "type": "TF_Float32Tensor" },
      { "name": "learning_rate", "type": "TF_Float32Tensor" },
      { "name": "embedding_table", "type": "TF_Float32Tensor" },
      { "name": "accumulator", "type": "TF_Float32Tensor" },
      { "name": "num_minibatches_per_physical_sparse_core", "type": "TF_Int32Tensor" }
    ],
    "outputs": [
      { "name": "updated_embedding_table", "type": "TF_Float32Tensor" },
      { "name": "updated_accumulator", "type": "TF_Float32Tensor" }
    ],
    "attributes": [
      { "name": "clip_weight_min", "type": "F32Attr" },
      { "name": "clip_weight_max", "type": "F32Attr" },
      { "name": "max_ids_per_sparse_core", "type": "ConfinedAttr" },
      { "name": "max_unique_ids_per_sparse_core", "type": "ConfinedAttr" },
      { "name": "table_name", "type": "StrAttr" }
    ]
  },
  {
    "name": "tf.XlaSparseDenseMatmulGradWithAdagradMomentumAndCsrInput",
    "inputs": [
      { "name": "row_pointers", "type": "TF_Int32Tensor" },
      { "name": "sorted_sample_ids", "type": "TF_Int32Tensor" },
      { "name": "sorted_token_ids", "type": "TF_Int32Tensor" },
      { "name": "sorted_gains", "type": "TF_Float32Tensor" },
      { "name": "activation_gradients", "type": "TF_Float32Tensor" },
      { "name": "learning_rate", "type": "TF_Float32Tensor" },
      { "name": "embedding_table", "type": "TF_Float32Tensor" },
      { "name": "accumulator", "type": "TF_Float32Tensor" },
      { "name": "momenta", "type": "TF_Float32Tensor" },
      { "name": "num_minibatches_per_physical_sparse_core", "type": "TF_Int32Tensor" }
    ],
    "outputs": [
      { "name": "updated_embedding_table", "type": "TF_Float32Tensor" },
      { "name": "updated_accumulator", "type": "TF_Float32Tensor" },
      { "name": "updated_momenta", "type": "TF_Float32Tensor" }
    ],
    "attributes": [
      { "name": "use_nesterov", "type": "BoolAttr" },
      { "name": "exponent", "type": "F32Attr" },
      { "name": "beta1", "type": "F32Attr" },
      { "name": "beta2", "type": "F32Attr" },
      { "name": "epsilon", "type": "F32Attr" },
      { "name": "clip_weight_min", "type": "F32Attr" },
      { "name": "clip_weight_max", "type": "F32Attr" },
      { "name": "table_name", "type": "StrAttr" }
    ]
  },
  {
    "name": "tf.XlaSparseDenseMatmulGradWithAdagradMomentumAndStaticBufferSize",
    "summary": "A XLA op which performs the Adagrad momentumoptimizer update for the dense-sparse matrix multiplication.",
    "inputs": [
      { "name": "row_pointers", "type": "TF_Int32Tensor" },
      { "name": "sorted_sample_ids", "type": "TF_Int32Tensor" },
      { "name": "sorted_token_ids", "type": "TF_Int32Tensor" },
      { "name": "sorted_gains", "type": "TF_Float32Tensor" },
      { "name": "activation_gradients", "type": "TF_Float32Tensor" },
      { "name": "learning_rate", "type": "TF_Float32Tensor" },
      { "name": "embedding_table", "type": "TF_Float32Tensor" },
      { "name": "accumulator", "type": "TF_Float32Tensor" },
      { "name": "momenta", "type": "TF_Float32Tensor" },
      { "name": "num_minibatches_per_physical_sparse_core", "type": "TF_Int32Tensor" }
    ],
    "outputs": [
      { "name": "updated_embedding_table", "type": "TF_Float32Tensor" },
      { "name": "updated_accumulator", "type": "TF_Float32Tensor" },
      { "name": "updated_momenta", "type": "TF_Float32Tensor" }
    ],
    "attributes": [
      { "name": "use_nesterov", "type": "BoolAttr" },
      { "name": "exponent", "type": "F32Attr" },
      { "name": "beta1", "type": "F32Attr" },
      { "name": "beta2", "type": "F32Attr" },
      { "name": "epsilon", "type": "F32Attr" },
      { "name": "clip_weight_min", "type": "F32Attr" },
      { "name": "clip_weight_max", "type": "F32Attr" },
      { "name": "max_ids_per_sparse_core", "type": "ConfinedAttr" },
      { "name": "max_unique_ids_per_sparse_core", "type": "ConfinedAttr" },
      { "name": "table_name", "type": "StrAttr" }
    ]
  },
  {
    "name": "tf.XlaSparseDenseMatmulGradWithAdamAndCsrInput",
    "inputs": [
      { "name": "row_pointers", "type": "TF_Int32Tensor" },
      { "name": "sorted_sample_ids", "type": "TF_Int32Tensor" },
      { "name": "sorted_token_ids", "type": "TF_Int32Tensor" },
      { "name": "sorted_gains", "type": "TF_Float32Tensor" },
      { "name": "activation_gradients", "type": "TF_Float32Tensor" },
      { "name": "learning_rate", "type": "TF_Float32Tensor" },
      { "name": "embedding_table", "type": "TF_Float32Tensor" },
      { "name": "momenta", "type": "TF_Float32Tensor" },
      { "name": "velocity", "type": "TF_Float32Tensor" },
      { "name": "num_minibatches_per_physical_sparse_core", "type": "TF_Int32Tensor" }
    ],
    "outputs": [
      { "name": "updated_embedding_table", "type": "TF_Float32Tensor" },
      { "name": "updated_momenta", "type": "TF_Float32Tensor" },
      { "name": "updated_velocity", "type": "TF_Float32Tensor" }
    ],
    "attributes": [
      { "name": "use_sum_inside_sqrt", "type": "BoolAttr" },
      { "name": "beta1", "type": "F32Attr" },
      { "name": "beta2", "type": "F32Attr" },
      { "name": "epsilon", "type": "F32Attr" },
      { "name": "clip_weight_min", "type": "F32Attr" },
      { "name": "clip_weight_max", "type": "F32Attr" },
      { "name": "table_name", "type": "StrAttr" }
    ]
  },
  {
    "name": "tf.XlaSparseDenseMatmulGradWithAdamAndStaticBufferSize",
    "summary": "A XLA op which performs the Adam optimizer update for the dense-sparse matrix multiplication.",
    "inputs": [
      { "name": "row_pointers", "type": "TF_Int32Tensor" },
      { "name": "sorted_sample_ids", "type": "TF_Int32Tensor" },
      { "name": "sorted_token_ids", "type": "TF_Int32Tensor" },
      { "name": "sorted_gains", "type": "TF_Float32Tensor" },
      { "name": "activation_gradients", "type": "TF_Float32Tensor" },
      { "name": "learning_rate", "type": "TF_Float32Tensor" },
      { "name": "embedding_table", "type": "TF_Float32Tensor" },
      { "name": "momenta", "type": "TF_Float32Tensor" },
      { "name": "velocity", "type": "TF_Float32Tensor" },
      { "name": "num_minibatches_per_physical_sparse_core", "type": "TF_Int32Tensor" }
    ],
    "outputs": [
      { "name": "updated_embedding_table", "type": "TF_Float32Tensor" },
      { "name": "updated_momenta", "type": "TF_Float32Tensor" },
      { "name": "updated_velocity", "type": "TF_Float32Tensor" }
    ],
    "attributes": [
      { "name": "use_sum_inside_sqrt", "type": "BoolAttr" },
      { "name": "beta1", "type": "F32Attr" },
      { "name": "beta2", "type": "F32Attr" },
      { "name": "epsilon", "type": "F32Attr" },
      { "name": "clip_weight_min", "type": "F32Attr" },
      { "name": "clip_weight_max", "type": "F32Attr" },
      { "name": "max_ids_per_sparse_core", "type": "ConfinedAttr" },
      { "name": "max_unique_ids_per_sparse_core", "type": "ConfinedAttr" },
      { "name": "table_name", "type": "StrAttr" }
    ]
  },
  {
    "name": "tf.XlaSparseDenseMatmulGradWithCsrInput",
    "summary": "A XLA op which performs the custom optimizer per-row update for the dense-sparse matrix multiplication.",
    "inputs": [
      { "name": "row_pointers", "type": "TF_Int32Tensor" },
      { "name": "sorted_sample_ids", "type": "TF_Int32Tensor" },
      { "name": "sorted_token_ids", "type": "TF_Int32Tensor" },
      { "name": "sorted_gains", "type": "TF_Float32Tensor" },
      { "name": "activation_gradients", "type": "TF_Float32Tensor" },
      { "name": "tables", "type": "Variadic" },
      { "name": "hyperparameters", "type": "Variadic" },
      { "name": "num_minibatches_per_physical_sparse_core", "type": "TF_Int32Tensor" }
    ],
    "outputs": [
      { "name": "updated_tables", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "custom_computation", "type": "SymbolRefAttr" },
      { "name": "table_name", "type": "StrAttr" }
    ]
  },
  {
    "name": "tf.XlaSparseDenseMatmulGradWithFtrlAndCsrInput",
    "inputs": [
      { "name": "row_pointers", "type": "TF_Int32Tensor" },
      { "name": "sorted_sample_ids", "type": "TF_Int32Tensor" },
      { "name": "sorted_token_ids", "type": "TF_Int32Tensor" },
      { "name": "sorted_gains", "type": "TF_Float32Tensor" },
      { "name": "activation_gradients", "type": "TF_Float32Tensor" },
      { "name": "learning_rate", "type": "TF_Float32Tensor" },
      { "name": "embedding_table", "type": "TF_Float32Tensor" },
      { "name": "accumulator", "type": "TF_Float32Tensor" },
      { "name": "linear", "type": "TF_Float32Tensor" },
      { "name": "num_minibatches_per_physical_sparse_core", "type": "TF_Int32Tensor" }
    ],
    "outputs": [
      { "name": "updated_embedding_table", "type": "TF_Float32Tensor" },
      { "name": "updated_accumulator", "type": "TF_Float32Tensor" },
      { "name": "updated_linear", "type": "TF_Float32Tensor" }
    ],
    "attributes": [
      { "name": "multiply_linear_by_learning_rate", "type": "BoolAttr" },
      { "name": "beta", "type": "F32Attr" },
      { "name": "learning_rate_power", "type": "F32Attr" },
      { "name": "l1_regularization_strength", "type": "F32Attr" },
      { "name": "l2_regularization_strength", "type": "F32Attr" },
      { "name": "clip_weight_min", "type": "F32Attr" },
      { "name": "clip_weight_max", "type": "F32Attr" },
      { "name": "table_name", "type": "StrAttr" }
    ]
  },
  {
    "name": "tf.XlaSparseDenseMatmulGradWithFtrlAndStaticBufferSize",
    "summary": "A XLA op which performs the Ftrl optimizer update for the dense-sparse matrix multiplication.",
    "inputs": [
      { "name": "row_pointers", "type": "TF_Int32Tensor" },
      { "name": "sorted_sample_ids", "type": "TF_Int32Tensor" },
      { "name": "sorted_token_ids", "type": "TF_Int32Tensor" },
      { "name": "sorted_gains", "type": "TF_Float32Tensor" },
      { "name": "activation_gradients", "type": "TF_Float32Tensor" },
      { "name": "learning_rate", "type": "TF_Float32Tensor" },
      { "name": "embedding_table", "type": "TF_Float32Tensor" },
      { "name": "accumulator", "type": "TF_Float32Tensor" },
      { "name": "linear", "type": "TF_Float32Tensor" },
      { "name": "num_minibatches_per_physical_sparse_core", "type": "TF_Int32Tensor" }
    ],
    "outputs": [
      { "name": "updated_embedding_table", "type": "TF_Float32Tensor" },
      { "name": "updated_accumulator", "type": "TF_Float32Tensor" },
      { "name": "updated_linear", "type": "TF_Float32Tensor" }
    ],
    "attributes": [
      { "name": "multiply_linear_by_learning_rate", "type": "BoolAttr" },
      { "name": "beta", "type": "F32Attr" },
      { "name": "learning_rate_power", "type": "F32Attr" },
      { "name": "l1_regularization_strength", "type": "F32Attr" },
      { "name": "l2_regularization_strength", "type": "F32Attr" },
      { "name": "clip_weight_min", "type": "F32Attr" },
      { "name": "clip_weight_max", "type": "F32Attr" },
      { "name": "max_ids_per_sparse_core", "type": "ConfinedAttr" },
      { "name": "max_unique_ids_per_sparse_core", "type": "ConfinedAttr" },
      { "name": "table_name", "type": "StrAttr" }
    ]
  },
  {
    "name": "tf.XlaSparseDenseMatmulGradWithSgdAndCsrInput",
    "inputs": [
      { "name": "row_pointers", "type": "TF_Int32Tensor" },
      { "name": "sorted_sample_ids", "type": "TF_Int32Tensor" },
      { "name": "sorted_token_ids", "type": "TF_Int32Tensor" },
      { "name": "sorted_gains", "type": "TF_Float32Tensor" },
      { "name": "activation_gradients", "type": "TF_Float32Tensor" },
      { "name": "learning_rate", "type": "TF_Float32Tensor" },
      { "name": "embedding_table", "type": "TF_Float32Tensor" },
      { "name": "num_minibatches_per_physical_sparse_core", "type": "TF_Int32Tensor" }
    ],
    "outputs": [
      { "name": "updated_embedding_table", "type": "TF_Float32Tensor" }
    ],
    "attributes": [
      { "name": "clip_weight_min", "type": "F32Attr" },
      { "name": "clip_weight_max", "type": "F32Attr" },
      { "name": "table_name", "type": "StrAttr" }
    ]
  },
  {
    "name": "tf.XlaSparseDenseMatmulGradWithSgdAndStaticBufferSize",
    "summary": "A XLA op which performs the SGD optimizer update for the dense-sparse matrix multiplication.",
    "inputs": [
      { "name": "row_pointers", "type": "TF_Int32Tensor" },
      { "name": "sorted_sample_ids", "type": "TF_Int32Tensor" },
      { "name": "sorted_token_ids", "type": "TF_Int32Tensor" },
      { "name": "sorted_gains", "type": "TF_Float32Tensor" },
      { "name": "activation_gradients", "type": "TF_Float32Tensor" },
      { "name": "learning_rate", "type": "TF_Float32Tensor" },
      { "name": "embedding_table", "type": "TF_Float32Tensor" },
      { "name": "num_minibatches_per_physical_sparse_core", "type": "TF_Int32Tensor" }
    ],
    "outputs": [
      { "name": "updated_embedding_table", "type": "TF_Float32Tensor" }
    ],
    "attributes": [
      { "name": "clip_weight_min", "type": "F32Attr" },
      { "name": "clip_weight_max", "type": "F32Attr" },
      { "name": "max_ids_per_sparse_core", "type": "ConfinedAttr" },
      { "name": "max_unique_ids_per_sparse_core", "type": "ConfinedAttr" },
      { "name": "table_name", "type": "StrAttr" }
    ]
  },
  {
    "name": "tf.XlaSparseDenseMatmulWithCsrInput",
    "summary": "aaa",
    "inputs": [
      { "name": "row_pointers", "type": "TF_Int32Tensor" },
      { "name": "sorted_sample_ids", "type": "TF_Int32Tensor" },
      { "name": "sorted_token_ids", "type": "TF_Int32Tensor" },
      { "name": "sorted_gains", "type": "TF_Float32Tensor" },
      { "name": "embedding_table", "type": "TF_Tensor" },
      { "name": "num_minibatches_per_physical_sparse_core", "type": "TF_Int32Tensor" }
    ],
    "outputs": [
      { "name": "activations", "type": "TF_Tensor" }
    ],
    "attributes": [
      { "name": "input_size", "type": "ConfinedAttr" },
      { "name": "quantization_config_low", "type": "OptionalAttr" },
      { "name": "quantization_config_high", "type": "OptionalAttr" },
      { "name": "quantization_config_num_buckets", "type": "OptionalAttr" },
      { "name": "table_name", "type": "StrAttr" }
    ]
  },
  {
    "name": "tf.XlaSparseDenseMatmulWithStaticBufferSize",
    "summary": "A XLA op which performs the dense-sparse matrix multiplication.",
    "inputs": [
      { "name": "row_pointers", "type": "TF_Int32Tensor" },
      { "name": "sorted_sample_ids", "type": "TF_Int32Tensor" },
      { "name": "sorted_token_ids", "type": "TF_Int32Tensor" },
      { "name": "sorted_gains", "type": "TF_Float32Tensor" },
      { "name": "embedding_table", "type": "TF_Float32Tensor" },
      { "name": "num_minibatches_per_physical_sparse_core", "type": "TF_Int32Tensor" }
    ],
    "outputs": [
      { "name": "activations", "type": "TF_Float32Tensor" }
    ],
    "attributes": [
      { "name": "input_size", "type": "ConfinedAttr" },
      { "name": "quantization_config_low", "type": "OptionalAttr" },
      { "name": "quantization_config_high", "type": "OptionalAttr" },
      { "name": "quantization_config_num_buckets", "type": "OptionalAttr" },
      { "name": "max_ids_per_sparse_core", "type": "ConfinedAttr" },
      { "name": "max_unique_ids_per_sparse_core", "type": "ConfinedAttr" },
      { "name": "table_name", "type": "StrAttr" }
    ]
  },
  {
    "name": "tf.XlaSparseGradientsStack",
    "summary": "XlaSparseGradientsStackOp attempts to fuse transpose, relayout, conversion,\n    unstacking and optionally interleaving of the embedding gradients, while\n    also offloading this work to SparseCore.\n\n    The op assumes its operands are in TensoreCore layout.\n    The output is in SparseCore layout.",
    "inputs": [
      { "name": "unstacked_gradients", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "stacked_gradients", "type": "TF_Tensor" }
    ],
    "attributes": [
      { "name": "num_tables", "type": "ConfinedAttr" },
      { "name": "interleaved", "type": "BoolAttr" }
    ]
  },
  {
    "name": "tf.XlaSplitND",
    "summary": "Splits input tensor across all dimensions.",
    "description": "An op which slices the input tensor based on the given num_splits attribute,\npads slices optionally, and returned the slices. Slices are returned in\nrow-major order.\n\nThis op may be generated via the TPU bridge.\n\nFor example, with `input` tensor:\n```\n[[0, 1, 2],\n [3, 4, 5],\n [6, 7, 8]]\n```\n`num_splits`:\n```\n[2, 2]\n```\nand `paddings`:\n```\n[1, 1]\n```\nthe expected `outputs` is:\n```\n[[0, 1],\n [3, 4]]\n[[2, 0],\n [5, 0]]\n[[6, 7],\n [0, 0]]\n[[8, 0],\n [0, 0]]\n```",
    "inputs": [
      { "name": "input", "type": "Arg" }
    ],
    "outputs": [
      { "name": "outputs", "type": "Res" }
    ],
    "attributes": [
      { "name": "num_splits", "type": "I64ArrayAttr" },
      { "name": "paddings", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.XlaSpmdFullToShardShape",
    "summary": "An op used by XLA SPMD partitioner to switch from automatic partitioning to",
    "description": "manual partitioning. It annotates the input (full-shape, to be automatically\npartitioned) with the same sharding used by manual partitioning, and outputs a\nshard-shaped tensor to be consumed by later manually-partitioned ops. If the\nshape is not evenly partitionable, the padding region will be masked with 0s.\nThe conversion can happen partially in subgroups, by specifying the dim\nattribute, where only that dim will be converted.",
    "inputs": [
      { "name": "input", "type": "TF_Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "TF_Tensor" }
    ],
    "attributes": [
      { "name": "manual_sharding", "type": "StrAttr" },
      { "name": "dim", "type": "DefaultValuedOptionalAttr" },
      { "name": "unspecified_dims", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.XlaSpmdShardToFullShape",
    "summary": "An op used by XLA SPMD partitioner to switch from manual partitioning to",
    "description": "automatic partitioning. It converts the shard-shaped, manually partitioned input\ninto full-shaped tensor to be partitioned automatically with the same sharding\nused by manual partitioning. The conversion can happen partially in subgroups,\nby specifying the dim attribute, where only that dim will be converted.",
    "inputs": [
      { "name": "input", "type": "TF_Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "TF_Tensor" }
    ],
    "attributes": [
      { "name": "manual_sharding", "type": "StrAttr" },
      { "name": "full_shape", "type": "TF_ShapeAttr" },
      { "name": "dim", "type": "DefaultValuedOptionalAttr" },
      { "name": "unspecified_dims", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.XlaSvd",
    "summary": "Computes the eigen decomposition of a batch of self-adjoint matrices",
    "description": "(Note: Only real inputs are supported).\n\nComputes the eigenvalues and eigenvectors of the innermost M-by-N matrices in\ntensor such that tensor[...,:,:] = u[..., :, :] * Diag(s[..., :]) * Transpose(v[...,:,:]).",
    "inputs": [
      { "name": "a", "type": "Arg" }
    ],
    "outputs": [
      { "name": "s", "type": "Res" },
      { "name": "u", "type": "Res" },
      { "name": "v", "type": "Res" }
    ],
    "attributes": [
      { "name": "max_iter", "type": "I64Attr" },
      { "name": "epsilon", "type": "F32Attr" },
      { "name": "precision_config", "type": "StrAttr" }
    ]
  },
  {
    "name": "tf.XlaVariadicReduce",
    "summary": "Wraps the variadic XLA Reduce operator.",
    "description": "Semantics are documented at\n https://www.tensorflow.org/performance/xla/operation_semantics#variadic_reduce.\n\nThis version is limited to operands of the same dtype.\nXlaVariadicReduceV2 is a version that supports heterogeneous operands.",
    "inputs": [
      { "name": "input", "type": "Arg" },
      { "name": "init_value", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "dimensions_to_reduce", "type": "I64ArrayAttr" },
      { "name": "reducer", "type": "SymbolRefAttr" }
    ]
  },
  {
    "name": "tf.XlaVariadicReduceV2",
    "summary": "Wraps the variadic XLA Reduce operator.",
    "description": "Semantics are documented at\n https://www.tensorflow.org/performance/xla/operation_semantics#variadic_reduce.\n\nThis is an expanded version of XlaVariadicReduce, with support for\noperands of different dtypes, and improved shape inference.",
    "inputs": [
      { "name": "inputs", "type": "Arg" },
      { "name": "init_values", "type": "Arg" }
    ],
    "outputs": [
      { "name": "outputs", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "dimensions_to_reduce", "type": "I64ArrayAttr" },
      { "name": "reducer", "type": "SymbolRefAttr" }
    ]
  },
  {
    "name": "tf.XlaVariadicSort",
    "summary": "Wraps the XLA Sort operator, documented at",
    "description": "https://www.tensorflow.org/performance/xla/operation_semantics#sort\n.\n\nSorts one or more tensors, with support for custom comparator, dimension, and\nis_stable attributes.",
    "inputs": [
      { "name": "inputs", "type": "Arg" },
      { "name": "dimension", "type": "Arg" }
    ],
    "outputs": [
      { "name": "outputs", "type": "Res" }
    ],
    "attributes": [
      { "name": "comparator", "type": "SymbolRefAttr" },
      { "name": "is_stable", "type": "BoolAttr" }
    ]
  },
  {
    "name": "tf.Xlog1py",
    "summary": "Returns 0 if x == 0, and x * log1p(y) otherwise, elementwise.",
    "inputs": [
      { "name": "x", "type": "TF_FpOrComplexTensor" },
      { "name": "y", "type": "TF_FpOrComplexTensor" }
    ],
    "outputs": [
      { "name": "z", "type": "TF_FpOrComplexTensor" }
    ]
  },
  {
    "name": "tf.Xlogy",
    "summary": "Returns 0 if x == 0, and x * log(y) otherwise, elementwise.",
    "inputs": [
      { "name": "x", "type": "TF_FpOrComplexTensor" },
      { "name": "y", "type": "TF_FpOrComplexTensor" }
    ],
    "outputs": [
      { "name": "z", "type": "TF_FpOrComplexTensor" }
    ]
  },
  {
    "name": "tf.Yield",
    "summary": "Yield operation",
    "description": "The \"yield\" operation represents a return operation within the conditional\n    and body of structured control flow (e.g., if and while). The operation\n    takes a variable number of operands and produces no results. The number and\n    types of inputs must match the signature of the operation that contains the\n    region."
  },
  {
    "name": "tf.ZerosLike",
    "summary": "Returns a tensor of zeros with the same shape and type as x.",
    "inputs": [
      { "name": "x", "type": "Arg" }
    ],
    "outputs": [
      { "name": "y", "type": "Res" }
    ]
  },
  {
    "name": "tf.Zeta",
    "summary": "Compute the Hurwitz zeta function \\\\(\\zeta(x, q)\\\\).",
    "description": "The Hurwitz zeta function is defined as:\n\n\n\\\\(\\zeta(x, q) = \\sum_{n=0}^{\\infty} (q + n)^{-x}\\\\)",
    "inputs": [
      { "name": "x", "type": "TF_F32OrF64Tensor" },
      { "name": "q", "type": "TF_F32OrF64Tensor" }
    ],
    "outputs": [
      { "name": "z", "type": "TF_F32OrF64Tensor" }
    ]
  },
  {
    "name": "tfl.abs",
    "summary": "Absolute value operator",
    "description": "Given a tensor `x`, this operation returns a tensor containing the absolute\nvalue of each element in `x`. For example, if x is an input element and y is\nan output element, this operation computes \\\\(y = |x|\\\\).",
    "inputs": [
      { "name": "x", "type": "TFL_TensorOf" }
    ],
    "outputs": [
      { "name": "y", "type": "TFL_TensorOf" }
    ]
  },
  {
    "name": "tfl.add",
    "summary": "Addition operator",
    "description": "Element-wise addition operation.",
    "inputs": [
      { "name": "lhs", "type": "TFL_TensorOf" },
      { "name": "rhs", "type": "TFL_TensorOf" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_TensorOf" }
    ],
    "attributes": [
      { "name": "fused_activation_function", "type": "TFL_AFAttr" }
    ]
  },
  {
    "name": "tfl.add_n",
    "summary": "add_n operator",
    "description": "Adds all input tensors element-wise.",
    "inputs": [
      { "name": "inputs", "type": "TFL_VariadicTensorOf" }
    ],
    "outputs": [
      { "name": "sum", "type": "TFL_TensorOf" }
    ]
  },
  {
    "name": "tfl.arg_max",
    "summary": "ArgMax operator",
    "description": "Returns the index with the largest value across dimensions of a tensor.",
    "inputs": [
      { "name": "input", "type": "TFL_TensorOf" },
      { "name": "dim", "type": "TFL_I32OrI64Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_I32OrI64Tensor" }
    ]
  },
  {
    "name": "tfl.arg_min",
    "summary": "ArgMin operator",
    "description": "Returns the index with the smallest value across dimensions of a tensor.\n      a = [1, 10, 26.9, 2.8, 166.32, 62.3]\n      b = tf.math.argmin(input = a)\n      c = tf.keras.backend.eval(b)",
    "inputs": [
      { "name": "input", "type": "TFL_TensorOf" },
      { "name": "dim", "type": "TFL_I32OrI64Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_I32OrI64Tensor" }
    ]
  },
  {
    "name": "tfl.assign_variable",
    "summary": "Assigns a new value to a variable.",
    "description": "Any ReadVariableOp with a control dependency on this op is guaranteed to return\nthis value or a subsequent newer value of the variable.",
    "inputs": [
      { "name": "resource_id", "type": "TFL_ResourceTensor" },
      { "name": "value", "type": "TFL_TensorOf" }
    ]
  },
  {
    "name": "tfl.atan2",
    "summary": "Atan2 operation",
    "description": "The \"atan2\" operation computes the arctangent of y/x element-wise,\n    respecting signs of the arguments.",
    "inputs": [
      { "name": "y", "type": "TFL_TensorOf" },
      { "name": "x", "type": "TFL_TensorOf" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_TensorOf" }
    ]
  },
  {
    "name": "tfl.average_pool_2d",
    "summary": "Average_pool_2d operator",
    "description": "Performs average-pooling operation on input.",
    "inputs": [
      { "name": "input", "type": "TFL_TensorOf" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_TensorOf" }
    ],
    "attributes": [
      { "name": "filter_height", "type": "I32Attr" },
      { "name": "filter_width", "type": "I32Attr" },
      { "name": "padding", "type": "TFL_PaddingAttr" },
      { "name": "stride_h", "type": "I32Attr" },
      { "name": "stride_w", "type": "I32Attr" },
      { "name": "fused_activation_function", "type": "TFL_AFAttr" }
    ]
  },
  {
    "name": "tfl.basic_lstm",
    "summary": "The basic lstm operator",
    "description": "basic LSTM Cell Operator.",
    "inputs": [
      { "name": "data_input", "type": "TFL_TensorOf" },
      { "name": "prev_activ_input", "type": "TFL_TensorOf" },
      { "name": "weights_input", "type": "TFL_TensorOf" },
      { "name": "biases_input", "type": "TFL_TensorOf" },
      { "name": "prev_state_input", "type": "TFL_TensorOf" }
    ],
    "outputs": [
      { "name": "activ_output", "type": "TFL_2DTensorOf" },
      { "name": "state_output", "type": "TFL_2DTensorOf" },
      { "name": "concat_temp", "type": "TFL_2DTensorOf" },
      { "name": "activ_temp", "type": "TFL_2DTensorOf" }
    ],
    "attributes": [
      { "name": "fused_activation_function", "type": "DefaultValuedStrAttr" },
      { "name": "cell_clip", "type": "ConfinedAttr" },
      { "name": "proj_clip", "type": "ConfinedAttr" },
      { "name": "kernel_type", "type": "ConfinedAttr" }
    ]
  },
  {
    "name": "tfl.batch_matmul",
    "summary": "Batch Matrix Multiply Operator",
    "description": "Performs a batched matrix multiplication on the inputs. Follows the\nconventions of TensorFlow BatchMatMulV2, with support for unknown dimensions\nin the batch dimensions and broadcasting.\n\n    Inputs:\n      `inputs[0]`: required: input LHS\n      `inputs[1]`: required: input RHS\n      `adjoint_lhs`: optional: Transpose LHS (default false)\n      `adjoint_rhs`: optional: Transpose RHS (default false)",
    "inputs": [
      { "name": "x", "type": "TFL_TensorOf" },
      { "name": "y", "type": "TFL_TensorOf" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_TensorOf" }
    ],
    "attributes": [
      { "name": "adj_x", "type": "DefaultValuedOptionalAttr" },
      { "name": "adj_y", "type": "DefaultValuedOptionalAttr" },
      { "name": "asymmetric_quantize_inputs", "type": "OptionalAttr" }
    ],
    "category": "Layer"
  },
  {
    "name": "tfl.batch_to_space_nd",
    "summary": "BatchToSpaceNd operator",
    "description": "This operation reshapes the \"batch\" dimension 0 into space dimensions.",
    "inputs": [
      { "name": "input", "type": "TFL_TensorOf" },
      { "name": "block_shape", "type": "TFL_TensorOf" },
      { "name": "indices", "type": "TFL_TensorOf" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_TensorOf" }
    ]
  },
  {
    "name": "tfl.bidirectional_sequence_lstm",
    "summary": "Bidirectional sequence lstm operator",
    "description": "Bidirectional lstm is essentially two lstms, one running forward & the\n    other running backward. And the output is the concatenation of the two\n    lstms.",
    "inputs": [
      { "name": "input", "type": "TFL_TensorOf" },
      { "name": "fw_input_to_input_weights", "type": "TFL_TensorOfOrNone" },
      { "name": "fw_input_to_forget_weights", "type": "TFL_TensorOf" },
      { "name": "fw_input_to_cell_weights", "type": "TFL_TensorOf" },
      { "name": "fw_input_to_output_weights", "type": "TFL_TensorOf" },
      { "name": "fw_recurrent_to_input_weights", "type": "TFL_TensorOfOrNone" },
      { "name": "fw_recurrent_to_forget_weights", "type": "TFL_TensorOf" },
      { "name": "fw_recurrent_to_cell_weights", "type": "TFL_TensorOf" },
      { "name": "fw_recurrent_to_output_weights", "type": "TFL_TensorOf" },
      { "name": "fw_cell_to_input_weights", "type": "TFL_TensorOfOrNone" },
      { "name": "fw_cell_to_forget_weights", "type": "TFL_TensorOfOrNone" },
      { "name": "fw_cell_to_output_weights", "type": "TFL_TensorOfOrNone" },
      { "name": "fw_input_gate_bias", "type": "TFL_TensorOfOrNone" },
      { "name": "fw_forget_gate_bias", "type": "TFL_TensorOf" },
      { "name": "fw_cell_bias", "type": "TFL_TensorOf" },
      { "name": "fw_output_gate_bias", "type": "TFL_TensorOf" },
      { "name": "fw_projection_weights", "type": "TFL_TensorOfOrNone" },
      { "name": "fw_projection_bias", "type": "TFL_TensorOfOrNone" },
      { "name": "bw_input_to_input_weights", "type": "TFL_TensorOfOrNone" },
      { "name": "bw_input_to_forget_weights", "type": "TFL_TensorOf" },
      { "name": "bw_input_to_cell_weights", "type": "TFL_TensorOf" },
      { "name": "bw_input_to_output_weights", "type": "TFL_TensorOf" },
      { "name": "bw_recurrent_to_input_weights", "type": "TFL_TensorOfOrNone" },
      { "name": "bw_recurrent_to_forget_weights", "type": "TFL_TensorOf" },
      { "name": "bw_recurrent_to_cell_weights", "type": "TFL_TensorOf" },
      { "name": "bw_recurrent_to_output_weights", "type": "TFL_TensorOf" },
      { "name": "bw_cell_to_input_weights", "type": "TFL_TensorOfOrNone" },
      { "name": "bw_cell_to_forget_weights", "type": "TFL_TensorOfOrNone" },
      { "name": "bw_cell_to_output_weights", "type": "TFL_TensorOfOrNone" },
      { "name": "bw_input_gate_bias", "type": "TFL_TensorOfOrNone" },
      { "name": "bw_forget_gate_bias", "type": "TFL_TensorOf" },
      { "name": "bw_cell_bias", "type": "TFL_TensorOf" },
      { "name": "bw_output_gate_bias", "type": "TFL_TensorOf" },
      { "name": "bw_projection_weights", "type": "TFL_TensorOfOrNone" },
      { "name": "bw_projection_bias", "type": "TFL_TensorOfOrNone" },
      { "name": "fw_input_activation_state", "type": "TFL_StatefulTensor" },
      { "name": "fw_input_cell_state", "type": "TFL_StatefulTensor" },
      { "name": "bw_input_activation_state", "type": "TFL_StatefulTensor" },
      { "name": "bw_input_cell_state", "type": "TFL_StatefulTensor" },
      { "name": "aux_input", "type": "TFL_TensorOfOrNone" },
      { "name": "fw_aux_input_to_input_weights", "type": "TFL_TensorOfOrNone" },
      { "name": "fw_aux_input_to_forget_weights", "type": "TFL_TensorOfOrNone" },
      { "name": "fw_aux_input_to_cell_weights", "type": "TFL_TensorOfOrNone" },
      { "name": "fw_aux_input_to_output_weights", "type": "TFL_TensorOfOrNone" },
      { "name": "bw_aux_input_to_input_weights", "type": "TFL_TensorOfOrNone" },
      { "name": "bw_aux_input_to_forget_weights", "type": "TFL_TensorOfOrNone" },
      { "name": "bw_aux_input_to_cell_weights", "type": "TFL_TensorOfOrNone" },
      { "name": "bw_aux_input_to_output_weights", "type": "TFL_TensorOfOrNone" }
    ],
    "outputs": [
      { "name": "fw_output", "type": "AnyTensor" },
      { "name": "bw_output", "type": "AnyTensor" }
    ],
    "attributes": [
      { "name": "fused_activation_function", "type": "TFL_AFAttr" },
      { "name": "cell_clip", "type": "ConfinedAttr" },
      { "name": "proj_clip", "type": "ConfinedAttr" },
      { "name": "merge_outputs", "type": "BoolAttr" },
      { "name": "time_major", "type": "BoolAttr" },
      { "name": "asymmetric_quantize_inputs", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "tfl.bitcast",
    "summary": "Bitcast operator",
    "description": "Bitcasts a tensor from one type to another.",
    "inputs": [
      { "name": "input", "type": "AnyTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ]
  },
  {
    "name": "tfl.bitwise_xor",
    "summary": "Bitwise Xor operator",
    "description": "Elementwise computes the bitwise XOR of `lhs` and `rhs`.",
    "inputs": [
      { "name": "lhs", "type": "TFL_TensorOf" },
      { "name": "rhs", "type": "TFL_TensorOf" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_TensorOf" }
    ]
  },
  {
    "name": "tfl.broadcast_args",
    "summary": "Return the shape of s0 op s1 with broadcast.",
    "description": "Given `s0` and `s1`, tensors that represent shapes, compute `r0`, the\nbroadcasted shape. `s0`, `s1` and `r0` are all integer vectors.",
    "inputs": [
      { "name": "s0", "type": "TFL_I32OrI64Tensor" },
      { "name": "s1", "type": "TFL_I32OrI64Tensor" }
    ],
    "outputs": [
      { "name": "r0", "type": "TFL_I32OrI64Tensor" }
    ]
  },
  {
    "name": "tfl.broadcast_to",
    "summary": "Broadcast an array for a compatible shape.",
    "description": "Broadcasting is the process of making arrays to have compatible shapes\nfor arithmetic operations. Two shapes are compatible if for each\ndimension pair they are either equal or one of them is one. When trying\nto broadcast a Tensor to a shape, it starts with the trailing dimensions,\nand works its way forward.\n\nFor example,\n\n>>> x = tf.constant([1, 2, 3])\n>>> y = tf.broadcast_to(x, [3, 3])\n>>> print(y)\ntf.Tensor(\n    [[1 2 3]\n     [1 2 3]\n     [1 2 3]], shape=(3, 3), dtype=int32)\n\nIn the above example, the input Tensor with the shape of `[1, 3]`\nis broadcasted to output Tensor with shape of `[3, 3]`.\n\nWhen doing broadcasted operations such as multiplying a tensor\nby a scalar, broadcasting (usually) confers some time or space\nbenefit, as the broadcasted tensor is never materialized.\n\nHowever, `broadcast_to` does not carry with it any such benefits.\nThe newly-created tensor takes the full memory of the broadcasted\nshape. (In a graph context, `broadcast_to` might be fused to\nsubsequent operation and then be optimized away, however.)",
    "inputs": [
      { "name": "input", "type": "TFL_TensorOf" },
      { "name": "shape", "type": "TFL_I32OrI64Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_TensorOf" }
    ]
  },
  {
    "name": "tfl.bucketize",
    "summary": "Bucketizes 'input' based on 'boundaries'.",
    "description": "Example:\n\nIf the inputs are `boundaries = [0, 10, 100]` and\n`input = [[-5, 10000][150, 10][5, 100]]`,\nthen the output will be `output = [[0, 3][3, 2][1, 3]]`.",
    "inputs": [
      { "name": "input", "type": "TFL_TensorOf" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_TensorOf" }
    ],
    "attributes": [
      { "name": "boundaries", "type": "F32ArrayAttr" }
    ]
  },
  {
    "name": "tfl.call_once",
    "summary": "Invokes an initialization function",
    "description": "This operation invokes the given initialization function for the session\ninitializer in tf saved model dialect.",
    "attributes": [
      { "name": "session_init_function", "type": "StrAttr" }
    ]
  },
  {
    "name": "tfl.cast",
    "summary": "Cast operator",
    "description": "Casts input from input type to output type.",
    "inputs": [
      { "name": "input", "type": "TFL_TensorOf" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_TensorOf" }
    ]
  },
  {
    "name": "tfl.ceil",
    "summary": "Ceil operator",
    "description": "Returns element-wise ceil value of the input.",
    "inputs": [
      { "name": "x", "type": "TFL_FpTensor" }
    ],
    "outputs": [
      { "name": "y", "type": "TFL_FpTensor" }
    ]
  },
  {
    "name": "tfl.complex_abs",
    "summary": "Computes the complex absolute value of a tensor.",
    "description": "Given a tensor `x` of complex numbers, this operation returns a tensor of type\n`float` or `double` that is the absolute value of each element in `x`. All\nelements in `x` must be complex numbers of the form \\\\(a + bj\\\\). The absolute\nvalue is computed as \\\\( \\sqrt{a^2 + b^2}\\\\).",
    "inputs": [
      { "name": "input", "type": "TFL_TensorOf" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_TensorOf" }
    ]
  },
  {
    "name": "tfl.concatenation",
    "summary": "Concatenation operator",
    "description": "Concatenates tensors along one dimension",
    "inputs": [
      { "name": "values", "type": "TFL_VariadicTensorOf" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_TensorOf" }
    ],
    "attributes": [
      { "name": "axis", "type": "I32Attr" },
      { "name": "fused_activation_function", "type": "TFL_AFAttr" }
    ]
  },
  {
    "name": "tfl.control_node",
    "summary": "The `TFL.control_node` operation wraps single-block operations in order to attach control edges.",
    "description": "This is used to wrap regions and attach control dependencies to them. Typically,\n    this will happen in one of the last steps before emitting the flatbuffer model\n    in order to enable optimizations that rely on a fixed order of operations (such\n    as rematerialization.)\n    The flatbuffer exporter will unwrap the wrapped region and annotate the generated\n    model with metadata such that any runtime reorderings will respect the order\n    given by the control dependencies.",
    "inputs": [
      { "name": "controlInputs", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "outputs", "type": "Variadic" },
      { "name": "control", "type": "TFL_Control" }
    ]
  },
  {
    "name": "tfl.conv_2d",
    "summary": "operator",
    "description": "Performs convolution operation on inputs.\n\n    Inputs:\n      `inputs[0]`: required: the input activation tensor\n      `inputs[1]`: required: the filter weight tensor\n      `inputs[2]`: optional: the bias tensor",
    "inputs": [
      { "name": "input", "type": "TFL_TensorOf" },
      { "name": "filter", "type": "TFL_TensorOf" },
      { "name": "bias", "type": "TFL_1DTensorOfOrNone" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_TensorOf" }
    ],
    "attributes": [
      { "name": "dilation_h_factor", "type": "I32Attr" },
      { "name": "dilation_w_factor", "type": "I32Attr" },
      { "name": "fused_activation_function", "type": "TFL_AFAttr" },
      { "name": "padding", "type": "TFL_PaddingAttr" },
      { "name": "stride_h", "type": "I32Attr" },
      { "name": "stride_w", "type": "I32Attr" }
    ],
    "category": "Layer"
  },
  {
    "name": "tfl.conv_3d",
    "summary": "Convolution 3D operator",
    "description": "Performs convolution operation on 3D inputs.\n    Inputs:\n      `inputs[0]`: required: the input activation tensor\n      `inputs[1]`: required: the filter weight tensor\n      `inputs[2]`: optional: the bias tensor",
    "inputs": [
      { "name": "input", "type": "TFL_TensorOf" },
      { "name": "filter", "type": "TFL_TensorOf" },
      { "name": "bias", "type": "TFL_TensorOfOrNone" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_TensorOf" }
    ],
    "attributes": [
      { "name": "dilation_d_factor", "type": "I32Attr" },
      { "name": "dilation_h_factor", "type": "I32Attr" },
      { "name": "dilation_w_factor", "type": "I32Attr" },
      { "name": "fused_activation_function", "type": "TFL_AFAttr" },
      { "name": "padding", "type": "TFL_PaddingAttr" },
      { "name": "stride_d", "type": "I32Attr" },
      { "name": "stride_h", "type": "I32Attr" },
      { "name": "stride_w", "type": "I32Attr" }
    ]
  },
  {
    "name": "tfl.conv_3d_transpose",
    "summary": "Transposed Convolution 3D operator",
    "description": "Performs transposed convolution operation on 3D inputs.\n    Inputs:\n      `inputs[0]`: required: the shape of output tensor\n      `inputs[1]`: required: the filter weight tensor\n      `inputs[2]`: required: the input activation tensor\n      `inputs[3]`: optional: the bias tensor",
    "inputs": [
      { "name": "output_shape", "type": "TFL_I32Tensor" },
      { "name": "filter", "type": "TFL_TensorOf" },
      { "name": "input", "type": "TFL_TensorOf" },
      { "name": "bias", "type": "TFL_TensorOfOrNone" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_TensorOf" }
    ],
    "attributes": [
      { "name": "dilation_d_factor", "type": "I32Attr" },
      { "name": "dilation_h_factor", "type": "I32Attr" },
      { "name": "dilation_w_factor", "type": "I32Attr" },
      { "name": "fused_activation_function", "type": "TFL_AFAttr" },
      { "name": "padding", "type": "TFL_PaddingAttr" },
      { "name": "stride_d", "type": "I32Attr" },
      { "name": "stride_h", "type": "I32Attr" },
      { "name": "stride_w", "type": "I32Attr" }
    ]
  },
  {
    "name": "tfl.cos",
    "summary": "Cosine operator",
    "description": "Computes element-wise Cosine of input",
    "inputs": [
      { "name": "x", "type": "TFL_FpTensor" }
    ],
    "outputs": [
      { "name": "y", "type": "TFL_FpTensor" }
    ]
  },
  {
    "name": "tfl.cumsum",
    "summary": "Cumsum operator",
    "description": "Compute the cumulative sum of the tensor x along axis.",
    "inputs": [
      { "name": "input", "type": "TFL_TensorOf" },
      { "name": "axis", "type": "TFL_I32Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_TensorOf" }
    ],
    "attributes": [
      { "name": "exclusive", "type": "DefaultValuedOptionalAttr" },
      { "name": "reverse", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tfl.custom",
    "summary": "Custom op",
    "description": "A generic op for any TFLite custom operation.\n\n    input: A list of inputs in the original op.\n    custom_code: A string used to identify which exactly this op is, which\n                 corresponds to operator_codes.custom_code in the flatbuffer.\n    custom_option: a holder to save the op attributes in bytes fashion.\n    output: A list of outputs in the original op.",
    "inputs": [
      { "name": "input", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "output", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "custom_code", "type": "StrAttr" },
      { "name": "custom_option", "type": "TFL_ConstBytesAttr" }
    ]
  },
  {
    "name": "tfl.custom_tf",
    "summary": "Wrapper Op for TF custom ops.",
    "description": "A wrapper op around any Custom TF op. These includes ops defined using\n    custom_opdefs or linked which are not defined in TF dialect.\n    This Op just wraps the custom op inside a region.\n    Note #1, this Op will not include TF Lite custom ops defined using CustomOp.\n    Note #2, this op is just internal representation inside the converter and\n    are not exposed/exported when the model is exported to Flatbuffer.",
    "inputs": [
      { "name": "input", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "output", "type": "Variadic" }
    ]
  },
  {
    "name": "tfl.densify",
    "summary": "Densify operator",
    "description": "Converts sparse tensor to dense format.",
    "inputs": [
      { "name": "input", "type": "TFL_TensorOf" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_TensorOf" }
    ]
  },
  {
    "name": "tfl.depth_to_space",
    "summary": "DepthToSpace operator",
    "description": "Rearranges data from depth into blocks of spatial data.\n    This is the reverse transformation of SpaceToDepth. More specifically,\n    this op outputs a copy of the input tensor where values from the `depth`\n    dimension are moved in spatial blocks to the `height` and `width`\n    dimensions. The attr `block_size` indicates the input block size and how\n    the data is moved.",
    "inputs": [
      { "name": "input", "type": "TFL_TensorOf" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_TensorOf" }
    ],
    "attributes": [
      { "name": "block_size", "type": "ConfinedAttr" }
    ]
  },
  {
    "name": "tfl.depthwise_conv_2d",
    "summary": "operator",
    "description": "Performs convolution operation on inputs.\n\n    Inputs:\n      `inputs[0]`: required: the input activation tensor\n      `inputs[1]`: required: the filter weight tensor\n      `inputs[2]`: optional: the bias tensor",
    "inputs": [
      { "name": "input", "type": "TFL_TensorOf" },
      { "name": "filter", "type": "TFL_TensorOf" },
      { "name": "bias", "type": "TFL_1DTensorOfOrNone" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_TensorOf" }
    ],
    "attributes": [
      { "name": "dilation_h_factor", "type": "I32Attr" },
      { "name": "dilation_w_factor", "type": "I32Attr" },
      { "name": "fused_activation_function", "type": "TFL_AFAttr" },
      { "name": "padding", "type": "TFL_PaddingAttr" },
      { "name": "stride_h", "type": "I32Attr" },
      { "name": "stride_w", "type": "I32Attr" },
      { "name": "depth_multiplier", "type": "I32Attr" }
    ]
  },
  {
    "name": "tfl.dequantize",
    "summary": "Dequantize operator",
    "description": "Converts quantized array of integers to floating-points according to the\n    quantization parameters.",
    "inputs": [
      { "name": "input", "type": "TFL_TensorOf" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_FpTensor" }
    ]
  },
  {
    "name": "tfl.dilate",
    "summary": "Dilation operator",
    "description": "Extends a tensor by adding new elements between the existing ones.",
    "inputs": [
      { "name": "input", "type": "TFL_TensorOf" },
      { "name": "dilations", "type": "TFL_TensorOf" },
      { "name": "padding_value", "type": "TFL_0DTensorOf" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_TensorOf" }
    ]
  },
  {
    "name": "tfl.div",
    "summary": "Division operator",
    "description": "Element-wise division operation.",
    "inputs": [
      { "name": "lhs", "type": "TFL_TensorOf" },
      { "name": "rhs", "type": "TFL_TensorOf" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_TensorOf" }
    ],
    "attributes": [
      { "name": "fused_activation_function", "type": "TFL_AFAttr" }
    ]
  },
  {
    "name": "tfl.dynamic_update_slice",
    "summary": "DynamicUpdateSlice.",
    "description": "DynamicUpdateSlice op that have the same semantics with XLA\n    DynamicUpdateSlice.\n    Generates a result which is the value of the input array\n    operand, with a slice update overwritten at start_indices.\n\n    See https://www.tensorflow.org/xla/operation_semantics#dynamicupdateslice.",
    "inputs": [
      { "name": "operand", "type": "TFL_TensorOf" },
      { "name": "update", "type": "TFL_TensorOf" },
      { "name": "start_indices", "type": "TFL_I32OrI64Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_TensorOf" }
    ]
  },
  {
    "name": "tfl.elu",
    "summary": "Exponential Linear Unit operator",
    "description": "Computes the exponential linear\n      f(x) -> exp(x) - 1 for x < 0, x for x >= 0.\n    element-wise.",
    "inputs": [
      { "name": "x", "type": "TFL_TensorOf" }
    ],
    "outputs": [
      { "name": "y", "type": "TFL_TensorOf" }
    ]
  },
  {
    "name": "tfl.embedding_lookup",
    "summary": "Embedding lookup operator",
    "description": "Looks up ids in a list of embedding tensors.",
    "inputs": [
      { "name": "lookup", "type": "TFL_TensorOf" },
      { "name": "value", "type": "TFL_TensorOf" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_TensorOf" }
    ]
  },
  {
    "name": "tfl.equal",
    "summary": "Equal operator",
    "description": "Returns the truth element of x == y element-wise",
    "inputs": [
      { "name": "x", "type": "TFL_TensorOf" },
      { "name": "y", "type": "TFL_TensorOf" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_BoolTensor" }
    ]
  },
  {
    "name": "tfl.exp",
    "summary": "Natural exponentiation operator",
    "description": "Performs element-wise natural exponentiation operation on input.",
    "inputs": [
      { "name": "x", "type": "TFL_TensorOf" }
    ],
    "outputs": [
      { "name": "y", "type": "TFL_TensorOf" }
    ]
  },
  {
    "name": "tfl.expand_dims",
    "summary": "Inserts a dimension of 1 into a tensor's shape.",
    "description": "Given a tensor `input`, this operation inserts a dimension of 1 at the\ndimension index `axis` of `input`'s shape. The dimension index `axis` starts at\nzero; if you specify a negative number for `axis` it is counted backward from\nthe end.\n\nThis operation is useful if you want to add a batch dimension to a single\nelement. For example, if you have a single image of shape `[height, width,\nchannels]`, you can make it a batch of 1 image with `expand_dims(image, 0)`,\nwhich will make the shape `[1, height, width, channels]`.\n\nOther examples:\n\n```\n# 't' is a tensor of shape [2]\nshape(expand_dims(t, 0)) ==> [1, 2]\nshape(expand_dims(t, 1)) ==> [2, 1]\nshape(expand_dims(t, -1)) ==> [2, 1]\n\n# 't2' is a tensor of shape [2, 3, 5]\nshape(expand_dims(t2, 0)) ==> [1, 2, 3, 5]\nshape(expand_dims(t2, 2)) ==> [2, 3, 1, 5]\nshape(expand_dims(t2, 3)) ==> [2, 3, 5, 1]\n```\n\nThis operation requires that:\n\n`-1-input.dims() <= dim <= input.dims()`\n\nThis operation is related to `squeeze()`, which removes dimensions of\nsize 1.",
    "inputs": [
      { "name": "input", "type": "AnyTensor" },
      { "name": "dim", "type": "TFL_I32OrI64Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ]
  },
  {
    "name": "tfl.external_const",
    "summary": "External const op.",
    "description": "External const op holds a `buffer_index` which points to a constant\n    in the flatbuffer.",
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ],
    "attributes": [
      { "name": "buffer_index", "type": "I32Attr" }
    ]
  },
  {
    "name": "tfl.fake_quant",
    "summary": "FakeQuant operator",
    "description": "Fake-quantize the 'inputs' tensor of type float via float scalars min and\n    max to 'outputs' tensor of same shape as inputs.",
    "inputs": [
      { "name": "input", "type": "TFL_FpTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_FpTensor" }
    ],
    "attributes": [
      { "name": "min", "type": "F32Attr" },
      { "name": "max", "type": "F32Attr" },
      { "name": "num_bits", "type": "ConfinedAttr" },
      { "name": "narrow_range", "type": "ConfinedAttr" }
    ]
  },
  {
    "name": "tfl.fill",
    "summary": "Fill the tensor with given value.",
    "description": "Fill the tensor with given value.",
    "inputs": [
      { "name": "dims", "type": "TFL_I32OrI64Tensor" },
      { "name": "input", "type": "TFL_TensorOf" }
    ],
    "outputs": [
      { "name": "result", "type": "TFL_TensorOf" }
    ]
  },
  {
    "name": "tfl.floor",
    "summary": "Floor operator",
    "description": "Returns element-wise floor value of the input.",
    "inputs": [
      { "name": "x", "type": "TFL_FpTensor" }
    ],
    "outputs": [
      { "name": "y", "type": "TFL_FpTensor" }
    ]
  },
  {
    "name": "tfl.floor_div",
    "summary": "Floor div operator",
    "description": "Element-wise floor div operation.",
    "inputs": [
      { "name": "lhs", "type": "TFL_TensorOf" },
      { "name": "rhs", "type": "TFL_TensorOf" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_TensorOf" }
    ]
  },
  {
    "name": "tfl.floor_mod",
    "summary": "Division reminder",
    "description": "Element-wise division reminder operation.",
    "inputs": [
      { "name": "lhs", "type": "TFL_TensorOf" },
      { "name": "rhs", "type": "TFL_TensorOf" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_TensorOf" }
    ]
  },
  {
    "name": "tfl.fully_connected",
    "summary": "Fully connected op",
    "inputs": [
      { "name": "input", "type": "TFL_TensorOf" },
      { "name": "filter", "type": "TFL_TensorOf" },
      { "name": "bias", "type": "TFL_TensorOfOrNone" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_VariadicTensorOf" }
    ],
    "attributes": [
      { "name": "fused_activation_function", "type": "TFL_AFAttr" },
      { "name": "weights_format", "type": "TFL_FullyConnectedOptionsWeightFormatAttr" },
      { "name": "keep_num_dims", "type": "BoolAttr" },
      { "name": "asymmetric_quantize_inputs", "type": "OptionalAttr" }
    ],
    "category": "Layer"
  },
  {
    "name": "tfl.gather",
    "summary": "Gather operator",
    "description": "Gather slices from `params` axis `axis` according to `indices`.",
    "inputs": [
      { "name": "params", "type": "TFL_TensorOf" },
      { "name": "indices", "type": "TFL_TensorOf" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_TensorOf" }
    ],
    "attributes": [
      { "name": "axis", "type": "I32Attr" },
      { "name": "batch_dims", "type": "DefaultValuedOptionalAttr" }
    ],
    "category": "Tensor"
  },
  {
    "name": "tfl.gather_nd",
    "summary": "Gather_nd operator",
    "description": "Gather slices from `params` into a Tensor with shape specified by `indices`.",
    "inputs": [
      { "name": "params", "type": "TFL_TensorOf" },
      { "name": "indices", "type": "TFL_TensorOf" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_TensorOf" }
    ]
  },
  {
    "name": "tfl.gelu",
    "summary": "GELU activation function.",
    "description": "Computes GELU activation function element-wise.",
    "inputs": [
      { "name": "input", "type": "TFL_TensorOf" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_TensorOf" }
    ],
    "attributes": [
      { "name": "approximate", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tfl.greater",
    "summary": "Greater operator",
    "description": "Element-wise greater operation.",
    "inputs": [
      { "name": "lhs", "type": "TFL_TensorOf" },
      { "name": "rhs", "type": "TFL_TensorOf" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_BoolTensor" }
    ]
  },
  {
    "name": "tfl.greater_equal",
    "summary": "Greater_equal operator",
    "description": "Element-wise greater_equal operation.",
    "inputs": [
      { "name": "lhs", "type": "TFL_TensorOf" },
      { "name": "rhs", "type": "TFL_TensorOf" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_BoolTensor" }
    ]
  },
  {
    "name": "tfl.hard_swish",
    "summary": "Hardswish activation function.",
    "description": "Computes hard-swish activation function\n      f(x) -> (x * relu6(x+3))/6\n    element-wise.",
    "inputs": [
      { "name": "input", "type": "TFL_TensorOf" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_TensorOf" }
    ]
  },
  {
    "name": "tfl.hashtable",
    "summary": "Creates a non-initialized hash table.",
    "description": "This op creates a hash table, specifying the type of its keys and values.\nBefore using the table you will have to initialize it.  After initialization the\ntable will be immutable.",
    "outputs": [
      { "name": "out", "type": "TFL_ResourceTensor" }
    ],
    "attributes": [
      { "name": "table_id", "type": "I32Attr" },
      { "name": "key_dtype", "type": "TypeAttr" },
      { "name": "value_dtype", "type": "TypeAttr" }
    ]
  },
  {
    "name": "tfl.hashtable_find",
    "summary": "Looks up keys in a table, outputs the corresponding values.",
    "description": "The tensor `keys` must of the same type as the keys of the table.\nThe output `values` is of the type of the table values.\n\nThe scalar `default_value` is the value output for keys not present in the\ntable. It must also be of the same type as the table values.",
    "inputs": [
      { "name": "hash_table", "type": "TFL_ResourceTensor" },
      { "name": "keys", "type": "TFL_TensorOf" },
      { "name": "default_value", "type": "TFL_TensorOf" }
    ],
    "outputs": [
      { "name": "out", "type": "TFL_TensorOf" }
    ]
  },
  {
    "name": "tfl.hashtable_import",
    "summary": "Replaces the contents of the table with the specified keys and values.",
    "description": "The tensor `keys` must be of the same type as the keys of the table.\nThe tensor `values` must be of the type of the table values.",
    "inputs": [
      { "name": "hash_table", "type": "TFL_ResourceTensor" },
      { "name": "keys", "type": "TFL_TensorOf" },
      { "name": "values", "type": "TFL_TensorOf" }
    ]
  },
  {
    "name": "tfl.hashtable_size",
    "summary": "Computes the number of elements in the given table.",
    "inputs": [
      { "name": "hash_table", "type": "TFL_ResourceTensor" }
    ],
    "outputs": [
      { "name": "out", "type": "TFL_I64Tensor" }
    ]
  },
  {
    "name": "tfl.if",
    "summary": "if-then-else operation",
    "description": "The `tfl.if` operation represents an if-then-else construct for\n    conditionally executing two regions of code. The operand to an if operation\n    is a boolean value. For example:\n\n    ```mlir\n    tfl.if %b  {\n      ...\n    } else {\n      ...\n    }\n    ```\n\n    `tfl.if` may also return results that are defined in its regions. The\n    values defined are determined by which execution path is taken.\n\n    Example:\n\n    ```mlir\n    %x, %y = tfl.if %b -> (tensor<f32>, tensor<f32>) {\n      %x_true = ...\n      %y_true = ...\n      tfl.yield %x_true, %y_true : tensor<f32>, tensor<f32>\n    } else {\n      %x_false = ...\n      %y_false = ...\n      tfl.yield %x_false, %y_false : tensor<f32>, tensor<f32>\n    }\n    ```\n\n    `tfl.if` regions are always terminated with \"tfl.yield\". If \"tfl.if\"\n    defines no values, the \"tfl.yield\" can be left out, and will be inserted\n    implicitly. Otherwise, it must be explicit.\n    Also, if \"tfl.if\" defines one or more values, the 'else' block cannot be\n    omitted.\n\n    Example:\n\n    ```mlir\n    tfl.if %b  {\n      ...\n    }\n    ```",
    "inputs": [
      { "name": "cond", "type": "TFL_BoolTensor" }
    ],
    "outputs": [
      { "name": "results", "type": "Variadic" }
    ]
  },
  {
    "name": "tfl.imag",
    "summary": "Returns the imaginary part of a complex number.",
    "description": "Given a tensor `input` of complex numbers, this operation returns a tensor of\ntype `float` that is the imaginary part of each element in `input`. All\nelements in `input` must be complex numbers of the form \\\\(a + bj\\\\), where *a*\nis the real part and *b* is the imaginary part returned by this operation.",
    "inputs": [
      { "name": "input", "type": "TFL_TensorOf" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_TensorOf" }
    ]
  },
  {
    "name": "tfl.l2_normalization",
    "summary": "L2 Normalize Operator",
    "description": "L2Normalization Op",
    "inputs": [
      { "name": "input", "type": "TFL_TensorOf" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_TensorOf" }
    ],
    "attributes": [
      { "name": "fused_activation_function", "type": "TFL_AFAttr" }
    ]
  },
  {
    "name": "tfl.leaky_relu",
    "summary": "Leaky Relu operator",
    "description": "Element-wise Leaky ReLU operator\n      x -> x >= 0 ? x : (alpha * x)",
    "inputs": [
      { "name": "input", "type": "TFL_TensorOf" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_TensorOf" }
    ],
    "attributes": [
      { "name": "alpha", "type": "F32Attr" }
    ]
  },
  {
    "name": "tfl.less",
    "summary": "Less operator",
    "description": "Element-wise less operation.",
    "inputs": [
      { "name": "lhs", "type": "TFL_TensorOf" },
      { "name": "rhs", "type": "TFL_TensorOf" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_BoolTensor" }
    ]
  },
  {
    "name": "tfl.less_equal",
    "summary": "Less_equal operator",
    "description": "Element-wise less_equal operation.",
    "inputs": [
      { "name": "lhs", "type": "TFL_TensorOf" },
      { "name": "rhs", "type": "TFL_TensorOf" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_BoolTensor" }
    ]
  },
  {
    "name": "tfl.local_response_normalization",
    "summary": "Local Response Normalization.",
    "description": "The 4-D `input` tensor is treated as a 3-D array of 1-D vectors (along the last\ndimension), and each vector is normalized independently.  Within a given vector,\neach component is divided by the weighted, squared sum of inputs within\n`depth_radius`.  In detail,\n\n    sqr_sum[a, b, c, d] =\n        sum(input[a, b, c, d - depth_radius : d + depth_radius + 1] ** 2)\n    output = input / (bias + alpha * sqr_sum) ** beta\n\nFor details, see [Krizhevsky et al., ImageNet classification with deep\nconvolutional neural networks (NIPS 2012)](http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks).",
    "inputs": [
      { "name": "input", "type": "TFL_FpTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_FpTensor" }
    ],
    "attributes": [
      { "name": "radius", "type": "I32Attr" },
      { "name": "bias", "type": "F32Attr" },
      { "name": "alpha", "type": "F32Attr" },
      { "name": "beta", "type": "F32Attr" }
    ]
  },
  {
    "name": "tfl.log",
    "summary": "Natural logarithm operator",
    "description": "Performs element-wise natural logarithm operation on input.",
    "inputs": [
      { "name": "x", "type": "TFL_TensorOf" }
    ],
    "outputs": [
      { "name": "y", "type": "TFL_TensorOf" }
    ]
  },
  {
    "name": "tfl.log_softmax",
    "summary": "Log softmax operator",
    "description": "Computes element-wise log softmax activations with the following formula\n\n      input - log(reduce_sum(exp(input), dim))",
    "inputs": [
      { "name": "input", "type": "TFL_TensorOf" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_TensorOf" }
    ]
  },
  {
    "name": "tfl.logical_and",
    "summary": "Logical AND operator",
    "description": "Element-wise logical AND operation.",
    "inputs": [
      { "name": "lhs", "type": "TFL_BoolTensor" },
      { "name": "rhs", "type": "TFL_BoolTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_BoolTensor" }
    ]
  },
  {
    "name": "tfl.logical_not",
    "summary": "Logical NOT operator",
    "description": "Element-wise logical NOT operation.",
    "inputs": [
      { "name": "lhs", "type": "TFL_BoolTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_BoolTensor" }
    ]
  },
  {
    "name": "tfl.logical_or",
    "summary": "Logical OR operator",
    "description": "Element-wise logical OR operation.",
    "inputs": [
      { "name": "lhs", "type": "TFL_BoolTensor" },
      { "name": "rhs", "type": "TFL_BoolTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_BoolTensor" }
    ]
  },
  {
    "name": "tfl.logistic",
    "summary": "Logistic operator",
    "description": "Computes element-wise Sigmoid of input",
    "inputs": [
      { "name": "x", "type": "TFL_TensorOf" }
    ],
    "outputs": [
      { "name": "y", "type": "TFL_TensorOf" }
    ]
  },
  {
    "name": "tfl.lstm",
    "summary": "The full lstm operator",
    "description": "Long short-term memory unit (LSTM) recurrent network layer.\nThe default non-peephole implementation is based on:\nhttp://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf\nS. Hochreiter and J. Schmidhuber. 'Long Short-Term Memory'. Neural Computation,\n9(8):1735-1780, 1997.\nThe peephole implementation is based on:\nhttps://research.google.com/pubs/archive/43905.pdf\nHasim Sak, Andrew Senior, and Francoise Beaufays. 'Long short-term memory\nrecurrent neural network architectures for large scale acoustic modeling.'\nINTERSPEECH, 2014.\nThe coupling of input and forget gate (CIFG) is based on:\nhttp://arxiv.org/pdf/1503.04069.pdf\nGreff et al. 'LSTM: A Search Space Odyssey'\nThe layer normalization is based on:\nhttps://arxiv.org/pdf/1607.06450.pdf\nBa et al. 'Layer Normalization'",
    "inputs": [
      { "name": "input", "type": "TFL_TensorOf" },
      { "name": "input_to_input_weights", "type": "TFL_TensorOfOrNone" },
      { "name": "input_to_forget_weights", "type": "TFL_TensorOf" },
      { "name": "input_to_cell_weights", "type": "TFL_TensorOf" },
      { "name": "input_to_output_weights", "type": "TFL_TensorOf" },
      { "name": "recurrent_to_input_weights", "type": "TFL_TensorOfOrNone" },
      { "name": "recurrent_to_forget_weights", "type": "TFL_TensorOf" },
      { "name": "recurrent_to_cell_weights", "type": "TFL_TensorOf" },
      { "name": "recurrent_to_output_weights", "type": "TFL_TensorOf" },
      { "name": "cell_to_input_weights", "type": "TFL_TensorOfOrNone" },
      { "name": "cell_to_forget_weights", "type": "TFL_TensorOfOrNone" },
      { "name": "cell_to_output_weights", "type": "TFL_TensorOfOrNone" },
      { "name": "input_gate_bias", "type": "TFL_TensorOfOrNone" },
      { "name": "forget_gate_bias", "type": "TFL_TensorOf" },
      { "name": "cell_bias", "type": "TFL_TensorOf" },
      { "name": "output_gate_bias", "type": "TFL_TensorOf" },
      { "name": "projection_weights", "type": "TFL_TensorOfOrNone" },
      { "name": "projection_bias", "type": "TFL_TensorOfOrNone" },
      { "name": "input_activation_state", "type": "TFL_StatefulTensor" },
      { "name": "input_cell_state", "type": "TFL_StatefulTensor" },
      { "name": "input_layer_norm_coefficients", "type": "TFL_TensorOfOrNone" },
      { "name": "forget_layer_norm_coefficients", "type": "TFL_TensorOfOrNone" },
      { "name": "cell_layer_norm_coefficients", "type": "TFL_TensorOfOrNone" },
      { "name": "output_layer_norm_coefficients", "type": "TFL_TensorOfOrNone" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ],
    "attributes": [
      { "name": "fused_activation_function", "type": "TFL_AFAttr" },
      { "name": "cell_clip", "type": "ConfinedAttr" },
      { "name": "proj_clip", "type": "ConfinedAttr" },
      { "name": "kernel_type", "type": "ConfinedAttr" },
      { "name": "asymmetric_quantize_inputs", "type": "OptionalAttr" },
      { "name": "input_to_input_intermediate", "type": "OptionalAttr" },
      { "name": "input_to_forget_intermediate", "type": "OptionalAttr" },
      { "name": "input_to_cell_intermediate", "type": "OptionalAttr" },
      { "name": "input_to_output_intermediate", "type": "OptionalAttr" },
      { "name": "effective_hidden_scale_intermediate", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "tfl.matrix_diag",
    "summary": "Returns a tensor with the provided diagonal and everything else padded with zeros.",
    "description": "Given a diagonal, returns a tensor with the diagonal and everything else padded with zeros.\n    Assume diagonal has k dimensions `[I, J, K, ..., N]`, then the output is a tensor of rank `k+1`\n    with dimensions `[I, J, K, ..., N, N]` where:\n       `output[i, j, k, ..., m, n] = 1{m=n} * diagonal[i, j, k, ..., n].`",
    "inputs": [
      { "name": "diagonal", "type": "TFL_TensorOf" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_TensorOf" }
    ]
  },
  {
    "name": "tfl.matrix_set_diag",
    "summary": "Returns a batched matrix tensor with new batched diagonal values.",
    "description": "Given `input` and `diagonal`, this operation returns a tensor with the\nsame shape and values as `input`, except for the main diagonal of the\ninnermost matrices.  These will be overwritten by the values in `diagonal`.",
    "inputs": [
      { "name": "input", "type": "TFL_TensorOf" },
      { "name": "diagonal", "type": "TFL_TensorOf" }
    ],
    "outputs": [
      { "name": "result", "type": "TFL_TensorOf" }
    ]
  },
  {
    "name": "tfl.max_pool_2d",
    "summary": "Max Pool 2D op",
    "description": "Performs max pool 2D on input.\n\n    Inputs:\n      `inputs[0]`: required: the input tensor",
    "inputs": [
      { "name": "input", "type": "TFL_TensorOf" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_TensorOf" }
    ],
    "attributes": [
      { "name": "padding", "type": "TFL_PaddingAttr" },
      { "name": "stride_w", "type": "I32Attr" },
      { "name": "stride_h", "type": "I32Attr" },
      { "name": "filter_width", "type": "I32Attr" },
      { "name": "filter_height", "type": "I32Attr" },
      { "name": "fused_activation_function", "type": "TFL_AFAttr" }
    ]
  },
  {
    "name": "tfl.maximum",
    "summary": "Max operator",
    "description": "Element-wise max operation.",
    "inputs": [
      { "name": "lhs", "type": "TFL_TensorOf" },
      { "name": "rhs", "type": "TFL_TensorOf" }
    ],
    "outputs": [
      { "name": "max", "type": "TFL_TensorOf" }
    ]
  },
  {
    "name": "tfl.mean",
    "summary": "Mean operator",
    "description": "Computes the mean of elements across dimensions of a tensor.\n    Reduces input_tensor along the dimensions given in axis.\n    Unless keepdims is true, the rank of the tensor is reduced by 1 for\n    each entry in axis. If keepdims is true, the reduced dimensions are retained\n    with length 1.",
    "inputs": [
      { "name": "input", "type": "TFL_TensorOf" },
      { "name": "axis", "type": "TFL_TensorOf" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_TensorOf" }
    ],
    "attributes": [
      { "name": "keep_dims", "type": "BoolAttr" }
    ]
  },
  {
    "name": "tfl.minimum",
    "summary": "Min operator",
    "description": "Element-wise min operation.",
    "inputs": [
      { "name": "lhs", "type": "TFL_TensorOf" },
      { "name": "rhs", "type": "TFL_TensorOf" }
    ],
    "outputs": [
      { "name": "min", "type": "TFL_TensorOf" }
    ]
  },
  {
    "name": "tfl.mirror_pad",
    "summary": "MirrorPad Operator. Pads a tensor with mirrored values.",
    "description": "This operation pads a input with mirrored values according to the paddings\n    you specify. paddings is an integer tensor with shape [n, 2],\n    where n is the rank of input.\n    For each dimension D of input, paddings[D, 0] indicates how many values\n    to add before the contents of input in that dimension,\n    and paddings[D, 1] indicates how many values to add after the contents of\n    input in that dimension.\n\n    Both paddings[D, 0] and paddings[D, 1] must be no greater than\n    input.dim_size(D) (or input.dim_size(D) - 1)\n    if copy_border is true (if false, respectively).\n\n    The padded size of each dimension D of the output is:\n\n    paddings(D, 0) + input.dim_size(D) + paddings(D, 1)",
    "inputs": [
      { "name": "input", "type": "TFL_TensorOf" },
      { "name": "pad", "type": "TFL_TensorOf" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_TensorOf" }
    ],
    "attributes": [
      { "name": "mode", "type": "TFL_MirrorPaddingAttr" }
    ]
  },
  {
    "name": "tfl.mul",
    "summary": "Multiplication operator",
    "description": "Element-wise multiplication operation.",
    "inputs": [
      { "name": "lhs", "type": "TFL_TensorOf" },
      { "name": "rhs", "type": "TFL_TensorOf" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_TensorOf" }
    ],
    "attributes": [
      { "name": "fused_activation_function", "type": "TFL_AFAttr" }
    ]
  },
  {
    "name": "tfl.multinomial",
    "summary": "Draws samples from a categorical distribution.",
    "description": "The generated values will have a categorical distribution based on the `logits`\nor unnormalized log-probabilities provided for all classes.",
    "inputs": [
      { "name": "logits", "type": "TFL_FpTensor" },
      { "name": "num_samples", "type": "TFL_I32Tensor" }
    ],
    "outputs": [
      { "name": "out", "type": "TFL_TensorOf" }
    ],
    "attributes": [
      { "name": "seed", "type": "DefaultValuedOptionalAttr" },
      { "name": "seed2", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tfl.neg",
    "summary": "Negation operator",
    "description": "Computes element-wise negation of input",
    "inputs": [
      { "name": "x", "type": "TFL_TensorOf" }
    ],
    "outputs": [
      { "name": "y", "type": "TFL_TensorOf" }
    ]
  },
  {
    "name": "tfl.no_value",
    "summary": "constant representing no value.",
    "description": "No value constant op.",
    "outputs": [
      { "name": "none_val", "type": "NoneType" }
    ],
    "attributes": [
      { "name": "value", "type": "UnitAttr" }
    ]
  },
  {
    "name": "tfl.non_max_suppression_v4",
    "summary": "Greedily selects a subset of bounding boxes in descending order of score,",
    "description": "pruning away boxes that have high intersection-over-union (IOU) overlap\nwith previously selected boxes.  Bounding boxes with score less than\n`score_threshold` are removed.  Bounding boxes are supplied as\n[y1, x1, y2, x2], where (y1, x1) and (y2, x2) are the coordinates of any\ndiagonal pair of box corners and the coordinates can be provided as normalized\n(i.e., lying in the interval [0, 1]) or absolute.  Note that this algorithm\nis agnostic to where the origin is in the coordinate system and more\ngenerally is invariant to orthogonal transformations and translations\nof the coordinate system; thus translating or reflections of the coordinate\nsystem result in the same boxes being selected by the algorithm.\nThe output of this operation is a set of integers indexing into the input\ncollection of bounding boxes representing the selected boxes.  The bounding\nbox coordinates corresponding to the selected indices can then be obtained\nusing the `tf.gather operation`.  For example:\n  selected_indices = tf.image.non_max_suppression_v2(\n      boxes, scores, max_output_size, iou_threshold, score_threshold)\n  selected_boxes = tf.gather(boxes, selected_indices)",
    "inputs": [
      { "name": "boxes", "type": "TFL_FpTensor" },
      { "name": "scores", "type": "TFL_FpTensor" },
      { "name": "max_output_size", "type": "TFL_I32Tensor" },
      { "name": "iou_threshold", "type": "TFL_FpTensor" },
      { "name": "score_threshold", "type": "TFL_FpTensor" }
    ],
    "outputs": [
      { "name": "selected_indices", "type": "TFL_I32Tensor" },
      { "name": "valid_outputs", "type": "TFL_I32Tensor" }
    ]
  },
  {
    "name": "tfl.non_max_suppression_v5",
    "summary": "Greedily selects a subset of bounding boxes in descending order of score,",
    "description": "pruning away boxes that have high intersection-over-union (IOU) overlap\nwith previously selected boxes.  Bounding boxes with score less than\n`score_threshold` are removed.  Bounding boxes are supplied as\n[y1, x1, y2, x2], where (y1, x1) and (y2, x2) are the coordinates of any\ndiagonal pair of box corners and the coordinates can be provided as normalized\n(i.e., lying in the interval [0, 1]) or absolute.  Note that this algorithm\nis agnostic to where the origin is in the coordinate system and more\ngenerally is invariant to orthogonal transformations and translations\nof the coordinate system; thus translating or reflections of the coordinate\nsystem result in the same boxes being selected by the algorithm.\nThe output of this operation is a set of integers indexing into the input\ncollection of bounding boxes representing the selected boxes.  The bounding\nbox coordinates corresponding to the selected indices can then be obtained\nusing the `tf.gather operation`.  For example:\n  selected_indices = tf.image.non_max_suppression_v2(\n      boxes, scores, max_output_size, iou_threshold, score_threshold)\n  selected_boxes = tf.gather(boxes, selected_indices)\nThis op also supports a Soft-NMS (with Gaussian weighting) mode (c.f.\nBodla et al, https://arxiv.org/abs/1704.04503) where boxes reduce the score\nof other overlapping boxes instead of directly causing them to be pruned.\nTo enable this Soft-NMS mode, set the `soft_nms_sigma` parameter to be\nlarger than 0.",
    "inputs": [
      { "name": "boxes", "type": "TFL_FpTensor" },
      { "name": "scores", "type": "TFL_FpTensor" },
      { "name": "max_output_size", "type": "TFL_I32Tensor" },
      { "name": "iou_threshold", "type": "TFL_FpTensor" },
      { "name": "score_threshold", "type": "TFL_FpTensor" },
      { "name": "soft_nms_sigma", "type": "TFL_FpTensor" }
    ],
    "outputs": [
      { "name": "selected_indices", "type": "TFL_I32Tensor" },
      { "name": "selected_scores", "type": "TFL_FpTensor" },
      { "name": "valid_outputs", "type": "TFL_I32Tensor" }
    ]
  },
  {
    "name": "tfl.not_equal",
    "summary": "Not_equal operator",
    "description": "Element-wise not_equal operation.",
    "inputs": [
      { "name": "lhs", "type": "TFL_TensorOf" },
      { "name": "rhs", "type": "TFL_TensorOf" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_BoolTensor" }
    ]
  },
  {
    "name": "tfl.NumericVerify",
    "summary": "Verifies the numericals of the two operands",
    "description": "The NumericVerify op is a debugging op to verify the numericals of the two\n    activations. It is a custom op in TFLite.\n    If log_if_failed is true, the NumericVerify op calculates statistics on\n    differences between float and quantized activations, output\n    logs, set differences to the output tensors, and throws an error if errors\n    above tolerance exist. If log_if_failed = false, then it doesn't care about\n    errors.",
    "inputs": [
      { "name": "input", "type": "TFL_TensorOf" },
      { "name": "ref", "type": "TFL_TensorOf" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_FpTensor" }
    ],
    "attributes": [
      { "name": "tolerance", "type": "DefaultValuedOptionalAttr" },
      { "name": "log_if_failed", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tfl.one_hot",
    "summary": "OneHot operator",
    "description": "Returns a one-hot tensor.The locations represented by indices in `indices`\n    take value `on_value`, while all other locations take value `off_value`.\n\n    If the input `indices` is rank `N`, the output will have rank `N+1`,\n    The new axis is created at dimension `axis` (default: the new axis is\n    appended at the end).",
    "inputs": [
      { "name": "indices", "type": "TFL_TensorOf" },
      { "name": "depth", "type": "TFL_I32Tensor" },
      { "name": "on_value", "type": "TFL_TensorOf" },
      { "name": "off_value", "type": "TFL_TensorOf" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_TensorOf" }
    ],
    "attributes": [
      { "name": "axis", "type": "I32Attr" }
    ]
  },
  {
    "name": "tfl.pack",
    "summary": "Packs a list of tensors along a dimension into one tensor",
    "description": "Packs a list of `values_count` rank-`R` tensors into one rank-`(R+1)`\n    tensor.\n\n    Packs the `values_count` tensors in `values` into a tensor with rank one\n    higher than each tensor in `values`, by packing them along the `axis`\n    dimension.\n\n    Given a list of tensors of shape `(A, B, C)`;\n\n    if `axis == 0` then the `output` tensor will have the shape `(N, A, B, C)`.\n    if `axis == 1` then the `output` tensor will have the shape `(A, N, B, C)`.\n    Etc.\n\n    For example:\n\n    ```\n    # 'x' is [1, 4]\n    # 'y' is [2, 5]\n    # 'z' is [3, 6]\n    pack([x, y, z]) => [[1, 4], [2, 5], [3, 6]]  # Pack along first dim.\n    pack([x, y, z], axis=1) => [[1, 2, 3], [4, 5, 6]]\n    ```\n\n    This is the opposite of `unpack`.",
    "inputs": [
      { "name": "values", "type": "TFL_VariadicTensorOf" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_TensorOf" }
    ],
    "attributes": [
      { "name": "values_count", "type": "ConfinedAttr" },
      { "name": "axis", "type": "I32Attr" }
    ]
  },
  {
    "name": "tfl.pad",
    "summary": "Padding operator",
    "description": "This operation pads a `input` with zeros according to the `paddings` you\n    specify. `paddings` is an integer tensor with shape `[Dn, 2]`, where n is\n    the rank of `input`. For each dimension D of `input`, `paddings[D, 0]`\n    indicates how many zeros to add before the contents of `input` in that\n    dimension, and `paddings[D, 1]` indicates how many zeros to add after the\n    contents of `input` in that dimension.\n\n    The padded size of each dimension D of the output is:\n\n      `paddings(D, 0) + input.dim_size(D) + paddings(D, 1)`\n\n    For example:\n\n    ```\n    # 't' is [[1, 1], [2, 2]]\n    # 'paddings' is [[1, 1], [2, 2]]\n    # rank of 't' is 2\n    pad(t, paddings) ==> [[0, 0, 0, 0, 0, 0]\n                          [0, 0, 1, 1, 0, 0]\n                          [0, 0, 2, 2, 0, 0]\n                          [0, 0, 0, 0, 0, 0]]\n    ```",
    "inputs": [
      { "name": "input", "type": "TFL_TensorOf" },
      { "name": "padding", "type": "TFL_I32OrI64Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_TensorOf" }
    ],
    "category": "Transform"
  },
  {
    "name": "tfl.padv2",
    "summary": "Padding operator v2",
    "description": "This operation pads a `input` according to the `paddings` and\n    `constant_values` you specify. `paddings` is an integer tensor with shape\n    `[Dn, 2]`, where n is the rank of `input`. For each dimension D of `input`,\n    `paddings[D, 0]` indicates how many zeros to add before the contents of\n    `input` in that dimension, and `paddings[D, 1]` indicates how many zeros to\n    add after the contents of `input` in that dimension. `constant_values` is a\n    scalar tensor of the same type as `input` that indicates the value to use\n    for padding `input`.\n\n    The padded size of each dimension D of the output is:\n\n      `paddings(D, 0) + input.dim_size(D) + paddings(D, 1)`\n\n    For example:\n\n    ```\n    # 't' is [[1, 1], [2, 2]]\n    # 'paddings' is [[1, 1], [2, 2]]\n    # rank of 't' is 2\n    pad(t, paddings) ==> [[0, 0, 0, 0, 0, 0]\n                          [0, 0, 1, 1, 0, 0]\n                          [0, 0, 2, 2, 0, 0]\n                          [0, 0, 0, 0, 0, 0]]\n    ```",
    "inputs": [
      { "name": "input", "type": "TFL_TensorOf" },
      { "name": "padding", "type": "TFL_I32OrI64Tensor" },
      { "name": "constant_values", "type": "TFL_TensorOf" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_TensorOf" }
    ]
  },
  {
    "name": "tfl.poly_call",
    "summary": "Poly call",
    "description": "Have multiple function bodies for the same computation. This allows a\n    program compiler/interpreter to choose one of the available options to\n    execute the program based on which one is most suitable for the target\n    backend.\n\n    input:  A list of input tensors whose types are T.\n    output: A list of output tensors whose types are T.\n\n    call:  Multiple regions, each of which encapsulates the same semantic\n           computation but in different forms.",
    "inputs": [
      { "name": "input", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "output", "type": "Variadic" }
    ]
  },
  {
    "name": "tfl.pow",
    "summary": "Power operator",
    "description": "Element-wise power operation.",
    "inputs": [
      { "name": "lhs", "type": "TFL_TensorOf" },
      { "name": "rhs", "type": "TFL_TensorOf" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_TensorOf" }
    ]
  },
  {
    "name": "tfl.prelu",
    "summary": "Parameterized Relu operator",
    "description": "Parameterized Relu operator\n      x -> x >= 0 ? x : (alpha * x)\n    where alpha is a trainable tensor.\n    input and alpha should be the same size as input or be broadcastable.",
    "inputs": [
      { "name": "input", "type": "TFL_TensorOf" },
      { "name": "alpha", "type": "TFL_TensorOf" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_TensorOf" }
    ]
  },
  {
    "name": "tfl.pseudo_const",
    "summary": "Constant pseudo op.",
    "description": "Represents a constant value in TensorFlow Lite dialect. This is not an\n    actual operation and it will be lowered to buffer instead.\n\n    The op is allowed to have all the same type of attributes as tf.Const does\n    (e.g., opaque TF attributes are allowed).",
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ],
    "attributes": [
      { "name": "value", "type": "ElementsAttr" }
    ]
  },
  {
    "name": "tfl.pseudo_qconst",
    "summary": "Quantized constant pseudo op",
    "description": "Represents a quantized constant value in TensorFlow Lite dialect. This is\n    not an actual operation and it will be lowered to buffer instead. The\n    quantization parameters are stored as a type attribute in this constant.",
    "outputs": [
      { "name": "output", "type": "TFL_TensorOf" }
    ],
    "attributes": [
      { "name": "qtype", "type": "TensorTypeAttr" },
      { "name": "value", "type": "ElementsAttr" }
    ]
  },
  {
    "name": "tfl.pseudo_sparse_const",
    "summary": "Sparse constant pseudo op.",
    "description": "Represents a sparse constant value in TensorFlow Lite dialect. This is not\n    an actual operation and it will be lowered to buffer instead.",
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ],
    "attributes": [
      { "name": "value", "type": "ElementsAttr" },
      { "name": "s_param", "type": "SparsityParameterAttr" },
      { "name": "compressed_data", "type": "ElementsAttr" }
    ]
  },
  {
    "name": "tfl.pseudo_sparse_qconst",
    "summary": "Sparse quantized constant pseudo op",
    "description": "Represents a sparse quantized constant value in TensorFlow Lite dialect.\n    This is not an actual operation and it will be lowered to buffer instead.\n    The quantization parameters are stored as a type attribute in this constant.",
    "outputs": [
      { "name": "output", "type": "TFL_TensorOf" }
    ],
    "attributes": [
      { "name": "qtype", "type": "TensorTypeAttr" },
      { "name": "value", "type": "ElementsAttr" },
      { "name": "s_param", "type": "SparsityParameterAttr" },
      { "name": "compressed_data", "type": "ElementsAttr" }
    ]
  },
  {
    "name": "tfl.quantize",
    "summary": "Quantize operator",
    "description": "Converts floating point tensors to quantized integer tensors according to\n    the quantization parameters defined in the type attribute.",
    "inputs": [
      { "name": "input", "type": "TFL_TensorOf" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_TensorOf" }
    ],
    "attributes": [
      { "name": "qtype", "type": "TensorTypeAttr" }
    ]
  },
  {
    "name": "tfl.random_standard_normal",
    "summary": "Outputs random values from a normal distribution.",
    "description": "The generated values will have mean 0 and standard deviation 1.",
    "inputs": [
      { "name": "shape", "type": "TFL_I32Tensor" }
    ],
    "outputs": [
      { "name": "out", "type": "TFL_TensorOf" }
    ],
    "attributes": [
      { "name": "seed", "type": "DefaultValuedOptionalAttr" },
      { "name": "seed2", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tfl.random_uniform",
    "summary": "Outputs random values from a uniform distribution.",
    "description": "The generated values follow a uniform distribution in the range `[0, 1)`. The\nlower bound 0 is included in the range, while the upper bound 1 is excluded.",
    "inputs": [
      { "name": "shape", "type": "TFL_I32Tensor" }
    ],
    "outputs": [
      { "name": "out", "type": "TFL_TensorOf" }
    ],
    "attributes": [
      { "name": "seed", "type": "DefaultValuedOptionalAttr" },
      { "name": "seed2", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tfl.range",
    "summary": "Range operator",
    "description": "Returns a 1D tensor defined by a sequence from `start` to `limit` with\n    a given `delta`.",
    "inputs": [
      { "name": "start", "type": "TFL_TensorOf" },
      { "name": "limit", "type": "TFL_TensorOf" },
      { "name": "delta", "type": "TFL_TensorOf" }
    ],
    "outputs": [
      { "name": "result", "type": "TFL_TensorOf" }
    ]
  },
  {
    "name": "tfl.rank",
    "summary": "Rank operator.",
    "description": "Returns the rank of a tensor.",
    "inputs": [
      { "name": "input", "type": "AnyTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_IntTensor" }
    ]
  },
  {
    "name": "tfl.read_variable",
    "summary": "Reads variable value.",
    "description": "Read variable data identified by 'resource_id'.",
    "inputs": [
      { "name": "resource_id", "type": "TFL_ResourceTensor" }
    ],
    "outputs": [
      { "name": "result", "type": "TFL_TensorOf" }
    ]
  },
  {
    "name": "tfl.real",
    "summary": "Returns the real part of a complex number.",
    "description": "Given a tensor `input` of complex numbers, this operation returns a tensor of\ntype `float` that is the real part of each element in `input`. All elements in\n`input` must be complex numbers of the form \\\\(a + bj\\\\), where *a* is the real\n part returned by this operation and *b* is the imaginary part.",
    "inputs": [
      { "name": "input", "type": "TFL_TensorOf" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_TensorOf" }
    ]
  },
  {
    "name": "tfl.reduce_all",
    "summary": "Computes the \"logical and\" of elements across dimensions of a tensor.",
    "description": "Reduces `input` along the dimensions given in `axis`. Unless\n`keep_dims` is true, the rank of the tensor is reduced by 1 for each entry in\n`axis`. If `keep_dims` is true, the reduced dimensions are\nretained with length 1.",
    "inputs": [
      { "name": "input", "type": "TFL_BoolTensor" },
      { "name": "reduction_indices", "type": "TFL_I32Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_BoolTensor" }
    ],
    "attributes": [
      { "name": "keep_dims", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tfl.reduce_any",
    "summary": "Computes the \"logical or\" of elements across dimensions of a tensor.",
    "description": "Reduces `input` along the dimensions given in `axis`. Unless\n`keep_dims` is true, the rank of the tensor is reduced by 1 for each entry in\n`axis`. If `keep_dims` is true, the reduced dimensions are\nretained with length 1.",
    "inputs": [
      { "name": "input", "type": "TFL_BoolTensor" },
      { "name": "reduction_indices", "type": "TFL_I32Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_BoolTensor" }
    ],
    "attributes": [
      { "name": "keep_dims", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tfl.reduce_max",
    "summary": "Max-reduction operator",
    "description": "Computes the max reduction along the specified axes",
    "inputs": [
      { "name": "input", "type": "TFL_TensorOf" },
      { "name": "axes", "type": "TFL_I32Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_TensorOf" }
    ],
    "attributes": [
      { "name": "keep_dims", "type": "BoolAttr" }
    ]
  },
  {
    "name": "tfl.reduce_min",
    "summary": "Min-reduction operator",
    "description": "Computes the min reduction along the specified axes",
    "inputs": [
      { "name": "input", "type": "TFL_TensorOf" },
      { "name": "axes", "type": "TFL_I32Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_TensorOf" }
    ],
    "attributes": [
      { "name": "keep_dims", "type": "BoolAttr" }
    ]
  },
  {
    "name": "tfl.reduce_prod",
    "summary": "Prod-reduction operator",
    "description": "Computes the product along the specified axes",
    "inputs": [
      { "name": "input", "type": "TFL_TensorOf" },
      { "name": "axes", "type": "TFL_I32Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_TensorOf" }
    ],
    "attributes": [
      { "name": "keep_dims", "type": "BoolAttr" }
    ]
  },
  {
    "name": "tfl.relu",
    "summary": "Relu operator",
    "description": "Element-wise Relu operator\n      x -> max(0, x)",
    "inputs": [
      { "name": "x", "type": "TFL_TensorOf" }
    ],
    "outputs": [
      { "name": "y", "type": "TFL_TensorOf" }
    ],
    "category": "Activation"
  },
  {
    "name": "tfl.relu_0_to_1",
    "summary": "Relu0To1 operator",
    "description": "Element-wise Relu0To1 operator\n      x -> max(0, min(1, x))",
    "inputs": [
      { "name": "x", "type": "TFL_TensorOf" }
    ],
    "outputs": [
      { "name": "y", "type": "TFL_TensorOf" }
    ]
  },
  {
    "name": "tfl.relu_n1_to_1",
    "summary": "Relu1 operator",
    "description": "Element-wise Relu1 operator\n      x -> max(-1, min(1, x))",
    "inputs": [
      { "name": "x", "type": "TFL_TensorOf" }
    ],
    "outputs": [
      { "name": "y", "type": "TFL_TensorOf" }
    ]
  },
  {
    "name": "tfl.relu6",
    "summary": "Relu6 operator",
    "description": "Element-wise Relu6 operator\n      x -> max(0, min(6, x))",
    "inputs": [
      { "name": "x", "type": "TFL_TensorOf" }
    ],
    "outputs": [
      { "name": "y", "type": "TFL_TensorOf" }
    ]
  },
  {
    "name": "tfl.reshape",
    "summary": "Reshape operator",
    "description": "Produces a tensor with the same values but different static shape defined\n    by the output type.",
    "inputs": [
      { "name": "input", "type": "AnyTensor" },
      { "name": "shape", "type": "TFL_I32Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ],
    "category": "Shape"
  },
  {
    "name": "tfl.resize_bilinear",
    "summary": "ResizeBilinear Op",
    "description": "Resize `images` to `size` using bilinear interpolation.",
    "inputs": [
      { "name": "input", "type": "TFL_TensorOf" },
      { "name": "size", "type": "TFL_I32Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_TensorOf" }
    ],
    "attributes": [
      { "name": "align_corners", "type": "BoolAttr" },
      { "name": "half_pixel_centers", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tfl.resize_nearest_neighbor",
    "summary": "ResizeNearestNeighbor Op",
    "description": "Resize `images` to `size` using nearest neighbor interpolation.",
    "inputs": [
      { "name": "input", "type": "TFL_TensorOf" },
      { "name": "size", "type": "TFL_I32Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_TensorOf" }
    ],
    "attributes": [
      { "name": "align_corners", "type": "BoolAttr" },
      { "name": "half_pixel_centers", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tfl.reverse_sequence",
    "summary": "Reverses variable length slices.",
    "description": "This op first slices `input` along the dimension `batch_dim`, and for each\nslice `i`, reverses the first `seq_lengths[i]` elements along\nthe dimension `seq_dim`.\n\nThe elements of `seq_lengths` must obey `seq_lengths[i] <= input.dims[seq_dim]`,\nand `seq_lengths` must be a vector of length `input.dims[batch_dim]`.\n\nThe output slice `i` along dimension `batch_dim` is then given by input\nslice `i`, with the first `seq_lengths[i]` slices along dimension\n`seq_dim` reversed.",
    "inputs": [
      { "name": "input", "type": "TFL_TensorOf" },
      { "name": "seq_lengths", "type": "TFL_I32OrI64Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_TensorOf" }
    ],
    "attributes": [
      { "name": "seq_dim", "type": "ConfinedAttr" },
      { "name": "batch_dim", "type": "ConfinedAttr" }
    ]
  },
  {
    "name": "tfl.reverse_v2",
    "summary": "ReverseV2 Operator",
    "description": "Reverses specific dimensions of a tensor.\n\n    Given a tensor, and a int32/int64 tensor axis representing the set\n    of dimensions of tensor to reverse.\n    This operation reverses each dimension i for\n    which there exists j s.t. axis[j] == i.\n\n    Args:\n      tensor: A Tensor. Must be one of the following types:\n      uint8, int8, int16, int32, int64, float32, bool Up to 8-D.\n\n      axis: A Tensor. Must be one of the following types: int32, int64.\n      with only 1 element which is the axis index.\n      TODO: Add support for multiple elements.",
    "inputs": [
      { "name": "input", "type": "TFL_TensorOf" },
      { "name": "axis", "type": "TFL_I32Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_TensorOf" }
    ]
  },
  {
    "name": "tfl.rfft2d",
    "summary": "2D real-valued fast Fourier transform.",
    "description": "Computes the 2-dimensional discrete Fourier transform of a real-valued signal\nover the inner-most 2 dimensions of `input`.\n\nSince the DFT of a real signal is Hermitian-symmetric, `RFFT2D` only returns the\n`fft_length / 2 + 1` unique components of the FFT for the inner-most dimension\nof `output`: the zero-frequency term, followed by the `fft_length / 2`\npositive-frequency terms.\n\nAlong each axis `RFFT2D` is computed on, if `fft_length` is smaller than the\ncorresponding dimension of `input`, the dimension is cropped. If it is larger,\nthe dimension is padded with zeros.",
    "inputs": [
      { "name": "input", "type": "TFL_FpTensor" },
      { "name": "fft_length", "type": "TFL_I32Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_Complex64Tensor" }
    ]
  },
  {
    "name": "tfl.right_shift",
    "summary": "Right Shift operator",
    "description": "Elementwise computes the bitwise right-shift of `lhs` by `rhs`.",
    "inputs": [
      { "name": "lhs", "type": "TFL_TensorOf" },
      { "name": "rhs", "type": "TFL_TensorOf" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_TensorOf" }
    ]
  },
  {
    "name": "tfl.round",
    "summary": "Round operator",
    "description": "Rounds the values of a tensor to the nearest integer, element-wise.",
    "inputs": [
      { "name": "x", "type": "TFL_FpTensor" }
    ],
    "outputs": [
      { "name": "y", "type": "TFL_FpTensor" }
    ]
  },
  {
    "name": "tfl.rsqrt",
    "summary": "Reciprocal of square root operator",
    "description": "Computes element-wise reverse square root of input",
    "inputs": [
      { "name": "x", "type": "TFL_TensorOf" }
    ],
    "outputs": [
      { "name": "y", "type": "TFL_TensorOf" }
    ]
  },
  {
    "name": "tfl.scatter_nd",
    "summary": "Scatter_nd operator",
    "description": "Scatter `updates` into a new tensor according to `indices`",
    "inputs": [
      { "name": "indices", "type": "TFL_TensorOf" },
      { "name": "updates", "type": "TFL_TensorOf" },
      { "name": "shape", "type": "TFL_1DTensorOf" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_TensorOf" }
    ]
  },
  {
    "name": "tfl.segment_sum",
    "summary": "SegmentSum operator",
    "description": "Computes the sum along segments of a tensor.",
    "inputs": [
      { "name": "input", "type": "TFL_TensorOf" },
      { "name": "segment_ids", "type": "TFL_I32Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_TensorOf" }
    ]
  },
  {
    "name": "tfl.select",
    "summary": "Select operator",
    "description": "Select values of 'x' if the corresponding value of 'condition' is true or\n    the value of 'y' if false. There are valid condition input sizes:\n\n    1. Either the same shape (in which case the select is elementwise), or\n    2. condition must be Rank 1 and match over the first dimension.",
    "inputs": [
      { "name": "condition", "type": "TFL_BoolTensor" },
      { "name": "x", "type": "TFL_TensorOf" },
      { "name": "y", "type": "TFL_TensorOf" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_TensorOf" }
    ]
  },
  {
    "name": "tfl.select_v2",
    "summary": "SelectV2 operator",
    "description": "Select values of 'x' if the corresponding value of 'condition' is true or\n    the value of 'y' if false. There are valid condition input sizes:\n\n    1. Either the same shape (in which case the select is elementwise), or\n    2. Broadcastable shapes between 'condition', 'x' and 'y'.",
    "inputs": [
      { "name": "condition", "type": "TFL_BoolTensor" },
      { "name": "x", "type": "TFL_TensorOf" },
      { "name": "y", "type": "TFL_TensorOf" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_TensorOf" }
    ]
  },
  {
    "name": "tfl.shape",
    "summary": "Shape operator",
    "description": "Returns the shape of a tensor.",
    "inputs": [
      { "name": "input", "type": "AnyTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_TensorOf" }
    ]
  },
  {
    "name": "tfl.sign",
    "summary": "Sign operation",
    "description": "Returns NaN if x is NaN, 0 if x is 0, -1 if x < 0 and 1 if x > 0.",
    "inputs": [
      { "name": "x", "type": "TFL_TensorOf" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_TensorOf" }
    ]
  },
  {
    "name": "tfl.sin",
    "summary": "Sine operator",
    "description": "Computes element-wise Sine of input",
    "inputs": [
      { "name": "x", "type": "TFL_FpTensor" }
    ],
    "outputs": [
      { "name": "y", "type": "TFL_FpTensor" }
    ]
  },
  {
    "name": "tfl.slice",
    "summary": "Return a slice from 'input'.",
    "description": "The output tensor is a tensor with dimensions described by 'size'\nwhose values are extracted from 'input' starting at the offsets in\n'begin'.\n\n`begin` is zero-based; `size` is one-based. If size[i] is -1, all remaining\nelements in dimension i are included in the slice. In other words, this is\nequivalent to setting:\n  size[i] = input.dim_size(i) - begin[i]\n\n*Requirements*:\n  0 <= begin[i] <= begin[i] + size[i] <= Di  for i in [0, n)",
    "inputs": [
      { "name": "input", "type": "TFL_TensorOf" },
      { "name": "begin", "type": "TFL_I32OrI64Tensor" },
      { "name": "size", "type": "TFL_I32OrI64Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_TensorOf" }
    ],
    "category": "Tensor"
  },
  {
    "name": "tfl.softmax",
    "summary": "Softmax operator",
    "description": "Computes element-wise softmax activations with the following formula\n\n      exp(input * beta) / tf.reduce_sum(exp(input * beta), dim)",
    "inputs": [
      { "name": "input", "type": "TFL_TensorOf" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_TensorOf" }
    ],
    "attributes": [
      { "name": "beta", "type": "F32Attr" }
    ],
    "category": "Activation"
  },
  {
    "name": "tfl.space_to_batch_nd",
    "summary": "SpaceToBatchNd operator",
    "description": "This operation reshapes space dimensions into the \"batch\" dimension 0",
    "inputs": [
      { "name": "input", "type": "TFL_TensorOf" },
      { "name": "block_shape", "type": "TFL_I32Tensor" },
      { "name": "paddings", "type": "TFL_I32Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_TensorOf" }
    ]
  },
  {
    "name": "tfl.space_to_depth",
    "summary": "SpaceToDepth operator",
    "description": "Rearranges blocks of spatial data, into depth. More specifically,\n    this op outputs a copy of the input tensor where values from the `height`\n    and `width` dimensions are moved to the `depth` dimension.\n    `block_size` indicates the input block size.",
    "inputs": [
      { "name": "input", "type": "TFL_TensorOf" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_TensorOf" }
    ],
    "attributes": [
      { "name": "block_size", "type": "ConfinedAttr" }
    ]
  },
  {
    "name": "tfl.sparse_to_dense",
    "summary": "Converts a sparse representation into a dense tensor.",
    "description": "Builds an array `dense` with shape `output_shape` such that\n\n```\n# If sparse_indices is scalar\ndense[i] = (i == sparse_indices ? sparse_values : default_value)\n\n# If sparse_indices is a vector, then for each i\ndense[sparse_indices[i]] = sparse_values[i]\n\n# If sparse_indices is an n by d matrix, then for each i in [0, n)\ndense[sparse_indices[i][0], ..., sparse_indices[i][d-1]] = sparse_values[i]\n```\n\nAll other values in `dense` are set to `default_value`.  If `sparse_values` is a\nscalar, all sparse indices are set to this single value.\n\nIndices should be sorted in lexicographic order, and indices must not\ncontain any repeats. If `validate_indices` is true, these properties\nare checked during execution.",
    "inputs": [
      { "name": "sparse_indices", "type": "TFL_I32OrI64Tensor" },
      { "name": "output_shape", "type": "TFL_I32OrI64Tensor" },
      { "name": "sparse_values", "type": "TFL_TensorOf" },
      { "name": "default_value", "type": "TFL_TensorOf" }
    ],
    "outputs": [
      { "name": "dense", "type": "TFL_TensorOf" }
    ]
  },
  {
    "name": "tfl.split",
    "summary": "Splits a tensor into `num_split` tensors along one dimension.",
    "description": "Splits the `value` tensor along `split_dim` into a number of sub-tensors\n    with same shape as the original one, except for `split_dim`. Same as\n    tf.Split.",
    "inputs": [
      { "name": "split_dim", "type": "TFL_TensorOf" },
      { "name": "value", "type": "TFL_TensorOf" }
    ],
    "outputs": [
      { "name": "outputs", "type": "TFL_VariadicTensorOf" }
    ],
    "attributes": [
      { "name": "num_splits", "type": "ConfinedAttr" }
    ]
  },
  {
    "name": "tfl.split_v",
    "summary": "Splits a tensor into `num_split` tensors along one dimension.",
    "description": "Splits the `value` tensor along `split_dim` into a number of sub-tensors\n    with same shape as the original one, except for `split_dim`. The grouping\n    of the resultant sub-tensors is decided by `size-splits`. Same as tf.SplitV.",
    "inputs": [
      { "name": "value", "type": "TFL_TensorOf" },
      { "name": "size_splits", "type": "TFL_1DTensorOf" },
      { "name": "split_dim", "type": "TFL_0DTensorOf" }
    ],
    "outputs": [
      { "name": "outputs", "type": "TFL_VariadicTensorOf" }
    ],
    "attributes": [
      { "name": "num_splits", "type": "ConfinedAttr" }
    ]
  },
  {
    "name": "tfl.sqrt",
    "summary": "Square root operator",
    "description": "Computes element-wise Square root of input",
    "inputs": [
      { "name": "x", "type": "TFL_TensorOf" }
    ],
    "outputs": [
      { "name": "y", "type": "TFL_TensorOf" }
    ]
  },
  {
    "name": "tfl.square",
    "summary": "Square operator",
    "description": "Computes element-wise Square of input",
    "inputs": [
      { "name": "x", "type": "TFL_FpTensor" }
    ],
    "outputs": [
      { "name": "y", "type": "TFL_FpTensor" }
    ]
  },
  {
    "name": "tfl.squared_difference",
    "summary": "Squared difference operator",
    "description": "Element-wise squared difference operation.",
    "inputs": [
      { "name": "lhs", "type": "TFL_TensorOf" },
      { "name": "rhs", "type": "TFL_TensorOf" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_TensorOf" }
    ]
  },
  {
    "name": "tfl.squeeze",
    "summary": "Removes dimensions of size 1 from the shape of a tensor.",
    "description": "Given a tensor `input`, this operation returns a tensor of the same type with\nall dimensions of size 1 removed. If you don't want to remove all size 1\ndimensions, you can remove specific size 1 dimensions by specifying\n`squeeze_dims`.\n\nFor example:\n\n```\n# 't' is a tensor of shape [1, 2, 1, 3, 1, 1]\nshape(squeeze(t)) ==> [2, 3]\n```\n\nOr, to remove specific size 1 dimensions:\n\n```\n# 't' is a tensor of shape [1, 2, 1, 3, 1, 1]\nshape(squeeze(t, [2, 4])) ==> [1, 2, 3, 1]\n```",
    "inputs": [
      { "name": "input", "type": "AnyTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ],
    "attributes": [
      { "name": "squeeze_dims", "type": "ConfinedAttr" }
    ]
  },
  {
    "name": "tfl.strided_slice",
    "summary": "StridedSlice Op",
    "description": "Return a strided slice from `input`.",
    "inputs": [
      { "name": "input", "type": "TFL_TensorOf" },
      { "name": "begin", "type": "TFL_I32Tensor" },
      { "name": "end", "type": "TFL_I32Tensor" },
      { "name": "strides", "type": "TFL_I32Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_TensorOf" }
    ],
    "attributes": [
      { "name": "begin_mask", "type": "I32Attr" },
      { "name": "end_mask", "type": "I32Attr" },
      { "name": "ellipsis_mask", "type": "I32Attr" },
      { "name": "new_axis_mask", "type": "I32Attr" },
      { "name": "shrink_axis_mask", "type": "I32Attr" },
      { "name": "offset", "type": "BoolAttr" }
    ]
  },
  {
    "name": "tfl.sub",
    "summary": "Subtraction operator",
    "description": "Element-wise subtraction operation.",
    "inputs": [
      { "name": "lhs", "type": "TFL_TensorOf" },
      { "name": "rhs", "type": "TFL_TensorOf" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_TensorOf" }
    ],
    "attributes": [
      { "name": "fused_activation_function", "type": "TFL_AFAttr" }
    ]
  },
  {
    "name": "tfl.sum",
    "summary": "Sum operator",
    "description": "Computes the sum reduction along the specified axes",
    "inputs": [
      { "name": "input", "type": "TFL_TensorOf" },
      { "name": "axes", "type": "TFL_I32Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_TensorOf" }
    ],
    "attributes": [
      { "name": "keep_dims", "type": "BoolAttr" }
    ]
  },
  {
    "name": "tfl.svdf",
    "summary": "Single value decomposition filter operator",
    "description": "The SVDF op is a decomposition of a densely connected op into low rank\n    filters.\n    For details: https://research.google.com/pubs/pub43813.html\n                 https://arxiv.org/abs/1812.02802",
    "inputs": [
      { "name": "input", "type": "TFL_TensorOf" },
      { "name": "feature_weights", "type": "TFL_TensorOf" },
      { "name": "time_weights", "type": "TFL_TensorOf" },
      { "name": "input_gate_bias", "type": "TFL_TensorOfOrNone" },
      { "name": "activation_state", "type": "TFL_StatefulTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_TensorOf" }
    ],
    "attributes": [
      { "name": "rank", "type": "ConfinedAttr" },
      { "name": "fused_activation_function", "type": "TFL_AFAttr" },
      { "name": "asymmetric_quantize_inputs", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "tfl.tanh",
    "summary": "Hyperbolic tangent operator",
    "description": "Computes element-wise Hyperbolic tangent of input",
    "inputs": [
      { "name": "input", "type": "TFL_TensorOf" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_TensorOf" }
    ],
    "category": "Activation"
  },
  {
    "name": "tfl.tile",
    "summary": "Tile operator.",
    "description": "Constructs a tensor by tiling a given tensor.\n\n   This operation creates a new tensor by replicating input\n   multiples times. The output tensor's i'th dimension has\n   input.dims(i) * multiples[i] elements, and the values of input\n   are replicated multiples[i] times along the 'i'th dimension.\n   For example, tiling [a b c d] by [2] produces [a b c d a b c d].",
    "inputs": [
      { "name": "input", "type": "TFL_TensorOf" },
      { "name": "multiples", "type": "TFL_I32OrI64Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_TensorOf" }
    ]
  },
  {
    "name": "tfl.topk_v2",
    "summary": "TopK operator",
    "description": "Returns the top `k` largest element along each last dimensional slice of\n    `input` and the indices of values within the last dimension of the input\n    tensor.\n\n    Results are always sorted in the descending order.",
    "inputs": [
      { "name": "input", "type": "TFL_TensorOf" },
      { "name": "k", "type": "TFL_TensorOf" }
    ],
    "outputs": [
      { "name": "values", "type": "TFL_TensorOf" },
      { "name": "indices", "type": "TFL_TensorOf" }
    ]
  },
  {
    "name": "tfl.transpose",
    "summary": "Transpose operator",
    "description": "Returns the Transpose of x",
    "inputs": [
      { "name": "input", "type": "TFL_TensorOf" },
      { "name": "perm", "type": "TFL_TensorOf" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_TensorOf" }
    ],
    "category": "Transform"
  },
  {
    "name": "tfl.transpose_conv",
    "summary": "Transpose convolution operator",
    "description": "Performs transpose convolution operation on input.",
    "inputs": [
      { "name": "output_shape", "type": "TFL_I32Tensor" },
      { "name": "weights", "type": "TFL_TensorOf" },
      { "name": "input", "type": "TFL_TensorOf" },
      { "name": "bias", "type": "TFL_TensorOfOrNone" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_TensorOf" }
    ],
    "attributes": [
      { "name": "padding", "type": "TFL_PaddingAttr" },
      { "name": "stride_h", "type": "ConfinedAttr" },
      { "name": "stride_w", "type": "ConfinedAttr" },
      { "name": "fused_activation_function", "type": "TFL_AFAttr" }
    ]
  },
  {
    "name": "tfl.unidirectional_sequence_lstm",
    "summary": "Unidirectional sequence lstm operator",
    "description": "A recurrent neural network specified by an LSTM cell. This Op supports\n    unrolling the input along the time or batch dimensions, and\n    implements the following operation for\n    each element in the sequence s = 1...sequence_length:\n      outputs[s] = state = activation(LSTMOp(inputs[s]))\n\n    where LSTMOp is LSTM TF Lite Op and the “activation” is the function passed\n    as the “fused_activation_function” argument (if not “NONE”).",
    "inputs": [
      { "name": "input", "type": "TFL_FpTensor" },
      { "name": "input_to_input_weights", "type": "TFL_TensorOfOrNone" },
      { "name": "input_to_forget_weights", "type": "TFL_TensorOf" },
      { "name": "input_to_cell_weights", "type": "TFL_TensorOf" },
      { "name": "input_to_output_weights", "type": "TFL_TensorOf" },
      { "name": "recurrent_to_input_weights", "type": "TFL_TensorOfOrNone" },
      { "name": "recurrent_to_forget_weights", "type": "TFL_TensorOf" },
      { "name": "recurrent_to_cell_weights", "type": "TFL_TensorOf" },
      { "name": "recurrent_to_output_weights", "type": "TFL_TensorOf" },
      { "name": "cell_to_input_weights", "type": "TFL_TensorOfOrNone" },
      { "name": "cell_to_forget_weights", "type": "TFL_TensorOfOrNone" },
      { "name": "cell_to_output_weights", "type": "TFL_TensorOfOrNone" },
      { "name": "input_gate_bias", "type": "TFL_TensorOfOrNone" },
      { "name": "forget_gate_bias", "type": "TFL_FpTensor" },
      { "name": "cell_bias", "type": "TFL_FpTensor" },
      { "name": "output_gate_bias", "type": "TFL_FpTensor" },
      { "name": "projection_weights", "type": "TFL_TensorOfOrNone" },
      { "name": "projection_bias", "type": "TFL_TensorOfOrNone" },
      { "name": "input_activation_state", "type": "TFL_StatefulTensor" },
      { "name": "input_cell_state", "type": "TFL_StatefulTensor" },
      { "name": "input_layer_norm_coefficients", "type": "TFL_TensorOfOrNone" },
      { "name": "forget_layer_norm_coefficients", "type": "TFL_TensorOfOrNone" },
      { "name": "cell_layer_norm_coefficients", "type": "TFL_TensorOfOrNone" },
      { "name": "output_layer_norm_coefficients", "type": "TFL_TensorOfOrNone" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_TensorOf" }
    ],
    "attributes": [
      { "name": "fused_activation_function", "type": "TFL_AFAttr" },
      { "name": "cell_clip", "type": "ConfinedAttr" },
      { "name": "proj_clip", "type": "ConfinedAttr" },
      { "name": "time_major", "type": "BoolAttr" },
      { "name": "asymmetric_quantize_inputs", "type": "OptionalAttr" },
      { "name": "diagonal_recurrent_tensors", "type": "OptionalAttr" },
      { "name": "input_to_input_intermediate", "type": "OptionalAttr" },
      { "name": "input_to_forget_intermediate", "type": "OptionalAttr" },
      { "name": "input_to_cell_intermediate", "type": "OptionalAttr" },
      { "name": "input_to_output_intermediate", "type": "OptionalAttr" },
      { "name": "effective_hidden_scale_intermediate", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "tfl.unidirectional_sequence_rnn",
    "summary": "Unidirectional sequence rnn operator",
    "description": "A recurrent neural network specified by an RNN cell. This Op takes in input\n    in a format {batch_size, seq_len, input_size} or\n    {seq_len, batch_size, input_size} if it's time-majored.\n\n    It implements the following operation for\n    each element in the sequence s = 1...sequence_length:\n      outputs[s] = state = activation(RNNOp(inputs[s]))\n\n    where RNNOp is RNNOp TF Lite Op and the “activation” is the function passed\n    as the “fused_activation_function” argument (if not “NONE”).",
    "inputs": [
      { "name": "input", "type": "TFL_FpTensor" },
      { "name": "input_to_input_weights", "type": "TFL_TensorOf" },
      { "name": "recurrent_to_input_weights", "type": "TFL_TensorOf" },
      { "name": "input_gate_bias", "type": "TFL_FpTensor" },
      { "name": "hidden_state", "type": "TFL_StatefulTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_FpTensor" }
    ],
    "attributes": [
      { "name": "time_major", "type": "BoolAttr" },
      { "name": "fused_activation_function", "type": "TFL_AFAttr" },
      { "name": "asymmetric_quantize_inputs", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "tfl.unique",
    "summary": "Unique Op.",
    "description": "This operation returns a tensor `output` containing all of the unique elements\nof `input` sorted in the same order that they occur in `input`. This operation\nalso returns a tensor `idx` the same size as `x` that contains the index of each\nvalue of `input` in the unique output `output`. In other words:",
    "inputs": [
      { "name": "input", "type": "TFL_TensorOf" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_TensorOf" },
      { "name": "idx", "type": "TFL_I32OrI64Tensor" }
    ]
  },
  {
    "name": "tfl.unpack",
    "summary": "Unpacks a tensor along a dimension into multiple tensors",
    "description": "Unpacks a given dimension of a rank-`R` tensor into `num` rank-`(R-1)` tensors.\n\n    Unpacks `num` tensors from `value` by chipping it along the `axis` dimension.\n    For example, given a tensor of shape `(A, B, C, D)`;\n\n    If `axis == 0` then the i'th tensor in `output` is the slice `value[i, :, :, :]`\n      and each tensor in `output` will have shape `(B, C, D)`. (Note that the\n      dimension unpacked along is gone, unlike `split`).\n\n    If `axis == 1` then the i'th tensor in `output` is the slice `value[:, i, :, :]`\n      and each tensor in `output` will have shape `(A, C, D)`.\n    Etc.\n\n    This is the opposite of `pack`.",
    "inputs": [
      { "name": "input", "type": "TFL_TensorOf" }
    ],
    "outputs": [
      { "name": "outputs", "type": "TFL_VariadicTensorOf" }
    ],
    "attributes": [
      { "name": "num", "type": "ConfinedAttr" },
      { "name": "axis", "type": "I32Attr" }
    ]
  },
  {
    "name": "tfl.unsorted_segment_max",
    "summary": "UnsortedSegmentMax operator",
    "description": "Computes the maximum value along segments of a tensor such that\n    output[i] = max(data[j....]) where segment_ids[j...] = i\n    if the maximum is empty for a given segment ID i,\n    it outputs the smallest possible value for the specific numeric type,\n    output[i] = numeric_limits::lowest().\n    Note the values of segment_ids are always validated to be less than\n    num_segments and an error is thrown for out-of-bound indices.",
    "inputs": [
      { "name": "input", "type": "TFL_TensorOf" },
      { "name": "segment_ids", "type": "TFL_I32Tensor" },
      { "name": "num_segments", "type": "TFL_I32Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_TensorOf" }
    ]
  },
  {
    "name": "tfl.unsorted_segment_min",
    "summary": "UnsortedSegmentMin operator",
    "description": "Computes the minimum value along segments of a tensor such that\n    output[i] = min(data[j....]) where segment_ids[j...] = i\n    if the minimum is empty for a given segment ID i,\n    it outputs the largest possible value for the specific numeric type,\n    output[i] = numeric_limits::max().\n    Note the values of segment_ids are always validated to be less than\n    num_segments and an error is thrown for out-of-bound indices.",
    "inputs": [
      { "name": "input", "type": "TFL_TensorOf" },
      { "name": "segment_ids", "type": "TFL_I32Tensor" },
      { "name": "num_segments", "type": "TFL_I32Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_TensorOf" }
    ]
  },
  {
    "name": "tfl.unsorted_segment_prod",
    "summary": "UnsortedSegmentProd operator",
    "description": "Computes the product along segments of a tensor.",
    "inputs": [
      { "name": "input", "type": "TFL_TensorOf" },
      { "name": "segment_ids", "type": "TFL_I32Tensor" },
      { "name": "num_segments", "type": "TFL_I32Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_TensorOf" }
    ]
  },
  {
    "name": "tfl.unsorted_segment_sum",
    "summary": "UnsortedSegmentSum operator",
    "description": "From a tensor segmentation, computes the `output` resulting from\n    summing together elements mapped to the same segment_id. I.e. `output[i]` is\n    equal to the tensor sum of all elements from the input tensor mapped to\n    segment_id `i`. If no tensors are mapped to a particular included\n    segment_id, the output at that indice will be a zero tensor with the\n    appropriate shape. Note the values of segment_ids are always validated to be\n    less than num_segments and an error is thrown for out-of-bound indices",
    "inputs": [
      { "name": "input", "type": "TFL_TensorOf" },
      { "name": "segment_ids", "type": "TFL_I32Tensor" },
      { "name": "num_segments", "type": "TFL_I32Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_TensorOf" }
    ]
  },
  {
    "name": "tfl.var_handle",
    "summary": "Returns a handle to a variable resource from its name.",
    "description": "Returns a handle for a variable resource from its name.\n    container: the container this variable is placed in.\n    shared_name: the name by which this variable is referred to.",
    "outputs": [
      { "name": "resource_handle", "type": "TFL_ResourceTensor" }
    ],
    "attributes": [
      { "name": "container", "type": "DefaultValuedStrAttr" },
      { "name": "shared_name", "type": "DefaultValuedStrAttr" }
    ]
  },
  {
    "name": "tfl.where",
    "summary": "Returns locations of nonzero / true values in a tensor.",
    "description": "This operation returns the coordinates of true elements in `condition`. The\ncoordinates are returned in a 2-D tensor where the first dimension (rows)\nrepresents the number of true elements, and the second dimension (columns)\nrepresents the coordinates of the true elements. Keep in mind, the shape of\nthe output tensor can vary depending on how many true values there are in\n`condition`. Indices are output in row-major order.",
    "inputs": [
      { "name": "condition", "type": "TFL_TensorOf" }
    ],
    "outputs": [
      { "name": "index", "type": "TFL_I64Tensor" }
    ]
  },
  {
    "name": "tfl.while",
    "summary": "While loop",
    "description": "output = input; while (cond(output)) { output = body(output) }\n\n    While loop where all values are passes through arguments with implicit\n    capture.\n\n    input: A list of input tensors whose types are T.\n    output: A list of output tensors whose types are T.\n    cond: A region that takes 'input' and returns a boolean scalar tensor.\n    body: A region that takes a list of tensors and returns another\n          list of tensors. Both lists have the same types.",
    "inputs": [
      { "name": "input", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "output", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "is_stateless", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tfl.yield",
    "summary": "Yield operation",
    "description": "The \"yield\" operation represents a return operation within the conditional\n    and body of structured control flow (e.g., while), and a terminator for ControlNodeOp.\n    The operation takes a variable number of operands and produces no results.\n    The operand number and types must match the signature of the region that contains the operation."
  },
  {
    "name": "tfl.zeros_like",
    "summary": "ZerosLike operator",
    "description": "Returns a tensor of zeros with the same shape and type as the input tensor.",
    "inputs": [
      { "name": "input", "type": "TFL_TensorOf" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_TensorOf" }
    ]
  },
  {
    "name": "torch.aten.__and__.bool",
    "summary": "Generated op for `aten::__and__.bool : (bool, bool) -> (bool)`",
    "inputs": [
      { "name": "a", "type": "Torch_BoolType" },
      { "name": "b", "type": "Torch_BoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "Torch_BoolType" }
    ]
  },
  {
    "name": "torch.aten.__and__.Scalar",
    "summary": "Generated op for `aten::__and__.Scalar : (Tensor, Scalar) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "other", "type": "AnyTorchScalarType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.__and__.Tensor",
    "summary": "Generated op for `aten::__and__.Tensor : (Tensor, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "other", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.__contains__.int_list",
    "summary": "Generated op for `aten::__contains__.int_list : (int[], int) -> (bool)`",
    "inputs": [
      { "name": "l", "type": "AnyTorchListOfTorchIntType" },
      { "name": "item", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "Torch_BoolType" }
    ]
  },
  {
    "name": "torch.aten.__contains__.str",
    "summary": "Generated op for `aten::__contains__.str : (Dict(str, t), str) -> (bool)`",
    "inputs": [
      { "name": "dict", "type": "Torch_DictType" },
      { "name": "key", "type": "Torch_StringType" }
    ],
    "outputs": [
      { "name": "result", "type": "Torch_BoolType" }
    ]
  },
  {
    "name": "torch.aten.__contains__.str_list",
    "summary": "Generated op for `aten::__contains__.str_list : (str[], str) -> (bool)`",
    "inputs": [
      { "name": "l", "type": "AnyTorchListOfTorchStringType" },
      { "name": "item", "type": "Torch_StringType" }
    ],
    "outputs": [
      { "name": "result", "type": "Torch_BoolType" }
    ]
  },
  {
    "name": "torch.aten.__derive_index",
    "summary": "Generated op for `aten::__derive_index : (int, int, int) -> (int)`",
    "inputs": [
      { "name": "index", "type": "Torch_IntType" },
      { "name": "start", "type": "Torch_IntType" },
      { "name": "step", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "Torch_IntType" }
    ]
  },
  {
    "name": "torch.aten.__getitem__.Dict_str",
    "summary": "Generated op for `aten::__getitem__.Dict_str : (Dict(str, t), str) -> (t)`",
    "inputs": [
      { "name": "self", "type": "Torch_DictType" },
      { "name": "key", "type": "Torch_StringType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchType" }
    ]
  },
  {
    "name": "torch.aten.__getitem__.t",
    "summary": "Generated op for `aten::__getitem__.t : (t[], int) -> (t)`",
    "inputs": [
      { "name": "list", "type": "AnyTorchListType" },
      { "name": "idx", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchType" }
    ]
  },
  {
    "name": "torch.aten.__interpolate.size_list_scale_list",
    "summary": "Generated op for `aten::__interpolate.size_list_scale_list : (Tensor, int[]?, float[]?, str, bool?, bool?, bool) -> (Tensor)`",
    "inputs": [
      { "name": "input", "type": "AnyTorchTensorType" },
      { "name": "size", "type": "AnyTorchOptionalListOfTorchIntType" },
      { "name": "scale_factor", "type": "AnyTorchOptionalListOfTorchFloatType" },
      { "name": "mode", "type": "Torch_StringType" },
      { "name": "align_corners", "type": "AnyTorchOptionalBoolType" },
      { "name": "recompute_scale_factor", "type": "AnyTorchOptionalBoolType" },
      { "name": "antialias", "type": "Torch_BoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.__is__",
    "summary": "Generated op for `aten::__is__ : (t1, t2) -> (bool)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchType" },
      { "name": "obj", "type": "AnyTorchType" }
    ],
    "outputs": [
      { "name": "result", "type": "Torch_BoolType" }
    ]
  },
  {
    "name": "torch.aten.__isnot__",
    "summary": "Generated op for `aten::__isnot__ : (t1, t2) -> (bool)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchType" },
      { "name": "obj", "type": "AnyTorchType" }
    ],
    "outputs": [
      { "name": "result", "type": "Torch_BoolType" }
    ]
  },
  {
    "name": "torch.aten.__lshift__.Scalar",
    "summary": "Generated op for `aten::__lshift__.Scalar : (Tensor, Scalar) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "other", "type": "AnyTorchScalarType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.__not__",
    "summary": "Generated op for `aten::__not__ : (bool) -> (bool)`",
    "inputs": [
      { "name": "self", "type": "Torch_BoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "Torch_BoolType" }
    ]
  },
  {
    "name": "torch.aten.__or__.bool",
    "summary": "Generated op for `aten::__or__.bool : (bool, bool) -> (bool)`",
    "inputs": [
      { "name": "a", "type": "Torch_BoolType" },
      { "name": "b", "type": "Torch_BoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "Torch_BoolType" }
    ]
  },
  {
    "name": "torch.aten.__or__.Tensor",
    "summary": "Generated op for `aten::__or__.Tensor : (Tensor, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "other", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.__range_length",
    "summary": "Generated op for `aten::__range_length : (int, int, int) -> (int)`",
    "inputs": [
      { "name": "lo", "type": "Torch_IntType" },
      { "name": "hi", "type": "Torch_IntType" },
      { "name": "step", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "Torch_IntType" }
    ]
  },
  {
    "name": "torch.aten.__rshift__.Scalar",
    "summary": "Generated op for `aten::__rshift__.Scalar : (Tensor, Scalar) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "other", "type": "AnyTorchScalarType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten._adaptive_avg_pool2d",
    "summary": "Generated op for `aten::_adaptive_avg_pool2d : (Tensor, int[]) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "output_size", "type": "AnyTorchListOfTorchIntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten._adaptive_avg_pool2d_backward",
    "summary": "Generated op for `aten::_adaptive_avg_pool2d_backward : (Tensor, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "grad_output", "type": "AnyTorchTensorType" },
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten._adaptive_avg_pool3d",
    "summary": "Generated op for `aten::_adaptive_avg_pool3d : (Tensor, int[]) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "output_size", "type": "AnyTorchListOfTorchIntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten._adaptive_avg_pool3d_backward",
    "summary": "Generated op for `aten::_adaptive_avg_pool3d_backward : (Tensor, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "grad_output", "type": "AnyTorchTensorType" },
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten._assert_scalar",
    "summary": "Generated op for `aten::_assert_scalar : (Scalar, str) -> ()`",
    "inputs": [
      { "name": "self", "type": "AnyTorchScalarType" },
      { "name": "assert_msg", "type": "Torch_StringType" }
    ]
  },
  {
    "name": "torch.aten._assert_tensor_metadata",
    "summary": "Generated op for `aten::_assert_tensor_metadata : (Tensor, int[]?, int[]?, int?, Device?, int?) -> ()`",
    "inputs": [
      { "name": "a", "type": "AnyTorchTensorType" },
      { "name": "size", "type": "AnyTorchOptionalListOfTorchIntType" },
      { "name": "stride", "type": "AnyTorchOptionalListOfTorchIntType" },
      { "name": "dtype", "type": "AnyTorchOptionalIntType" },
      { "name": "device", "type": "AnyTorchOptionalDeviceType" },
      { "name": "layout", "type": "AnyTorchOptionalIntType" }
    ]
  },
  {
    "name": "torch.aten._cast_Float",
    "summary": "Generated op for `aten::_cast_Float : (Tensor, bool) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "non_blocking", "type": "Torch_BoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten._cast_Long",
    "summary": "Generated op for `aten::_cast_Long : (Tensor, bool) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "non_blocking", "type": "Torch_BoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten._convolution",
    "summary": "Generated op for `aten::_convolution : (Tensor, Tensor, Tensor?, int[], int[], int[], bool, int[], int, bool, bool, bool, bool) -> (Tensor)`",
    "inputs": [
      { "name": "input", "type": "AnyTorchTensorType" },
      { "name": "weight", "type": "AnyTorchTensorType" },
      { "name": "bias", "type": "AnyTorchOptionalTensorType" },
      { "name": "stride", "type": "AnyTorchListOfTorchIntType" },
      { "name": "padding", "type": "AnyTorchListOfTorchIntType" },
      { "name": "dilation", "type": "AnyTorchListOfTorchIntType" },
      { "name": "transposed", "type": "Torch_BoolType" },
      { "name": "output_padding", "type": "AnyTorchListOfTorchIntType" },
      { "name": "groups", "type": "Torch_IntType" },
      { "name": "benchmark", "type": "Torch_BoolType" },
      { "name": "deterministic", "type": "Torch_BoolType" },
      { "name": "cudnn_enabled", "type": "Torch_BoolType" },
      { "name": "allow_tf32", "type": "Torch_BoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten._convolution.deprecated",
    "summary": "Generated op for `aten::_convolution.deprecated : (Tensor, Tensor, Tensor?, int[], int[], int[], bool, int[], int, bool, bool, bool) -> (Tensor)`",
    "inputs": [
      { "name": "input", "type": "AnyTorchTensorType" },
      { "name": "weight", "type": "AnyTorchTensorType" },
      { "name": "bias", "type": "AnyTorchOptionalTensorType" },
      { "name": "stride", "type": "AnyTorchListOfTorchIntType" },
      { "name": "padding", "type": "AnyTorchListOfTorchIntType" },
      { "name": "dilation", "type": "AnyTorchListOfTorchIntType" },
      { "name": "transposed", "type": "Torch_BoolType" },
      { "name": "output_padding", "type": "AnyTorchListOfTorchIntType" },
      { "name": "groups", "type": "Torch_IntType" },
      { "name": "benchmark", "type": "Torch_BoolType" },
      { "name": "deterministic", "type": "Torch_BoolType" },
      { "name": "cudnn_enabled", "type": "Torch_BoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten._embedding_bag",
    "summary": "Generated op for `aten::_embedding_bag : (Tensor, Tensor, Tensor, bool, int, bool, Tensor?, bool, int) -> (Tensor, Tensor, Tensor, Tensor)`",
    "inputs": [
      { "name": "weight", "type": "AnyTorchTensorType" },
      { "name": "indices", "type": "AnyTorchTensorType" },
      { "name": "offsets", "type": "AnyTorchTensorType" },
      { "name": "scale_grad_by_freq", "type": "Torch_BoolType" },
      { "name": "mode", "type": "Torch_IntType" },
      { "name": "sparse", "type": "Torch_BoolType" },
      { "name": "per_sample_weights", "type": "AnyTorchOptionalTensorType" },
      { "name": "include_last_offset", "type": "Torch_BoolType" },
      { "name": "padding_idx", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result0", "type": "AnyTorchOptionalTensorType" },
      { "name": "result1", "type": "AnyTorchOptionalTensorType" },
      { "name": "result2", "type": "AnyTorchOptionalTensorType" },
      { "name": "result3", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten._fake_quantize_per_tensor_affine_cachemask_tensor_qparams",
    "summary": "Generated op for `aten::_fake_quantize_per_tensor_affine_cachemask_tensor_qparams : (Tensor, Tensor, Tensor, Tensor, int, int) -> (Tensor, Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "scale", "type": "AnyTorchTensorType" },
      { "name": "zero_point", "type": "AnyTorchTensorType" },
      { "name": "fake_quant_enabled", "type": "AnyTorchTensorType" },
      { "name": "quant_min", "type": "Torch_IntType" },
      { "name": "quant_max", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTorchOptionalTensorType" },
      { "name": "mask", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten._index_put_impl",
    "summary": "Generated op for `aten::_index_put_impl : (Tensor, Tensor?[], Tensor, bool, bool) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "indices", "type": "AnyTorchListOfOptionalTensorType" },
      { "name": "values", "type": "AnyTorchTensorType" },
      { "name": "accumulate", "type": "Torch_BoolType" },
      { "name": "unsafe", "type": "Torch_BoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten._index_put_impl_",
    "summary": "Generated op for `aten::_index_put_impl_ : (Tensor, Tensor?[], Tensor, bool, bool) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" },
      { "name": "indices", "type": "AnyTorchListOfOptionalNonValueTensorType" },
      { "name": "values", "type": "Torch_NonValueTensorType" },
      { "name": "accumulate", "type": "Torch_BoolType" },
      { "name": "unsafe", "type": "Torch_BoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten._int_mm",
    "summary": "Generated op for `aten::_int_mm : (Tensor, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "mat2", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten._linalg_det",
    "summary": "Generated op for `aten::_linalg_det : (Tensor) -> (Tensor, Tensor, Tensor)`",
    "inputs": [
      { "name": "A", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" },
      { "name": "LU", "type": "AnyTorchOptionalTensorType" },
      { "name": "pivots", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten._log_softmax",
    "summary": "Generated op for `aten::_log_softmax : (Tensor, int, bool) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "dim", "type": "Torch_IntType" },
      { "name": "half_to_float", "type": "Torch_BoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten._log_softmax_backward_data",
    "summary": "Generated op for `aten::_log_softmax_backward_data : (Tensor, Tensor, int, int) -> (Tensor)`",
    "inputs": [
      { "name": "grad_output", "type": "AnyTorchTensorType" },
      { "name": "output", "type": "AnyTorchTensorType" },
      { "name": "dim", "type": "Torch_IntType" },
      { "name": "input_dtype", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten._make_per_channel_quantized_tensor",
    "summary": "Generated op for `aten::_make_per_channel_quantized_tensor : (Tensor, Tensor, Tensor, int) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "scale", "type": "AnyTorchTensorType" },
      { "name": "zero_point", "type": "AnyTorchTensorType" },
      { "name": "axis", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten._make_per_tensor_quantized_tensor",
    "summary": "Generated op for `aten::_make_per_tensor_quantized_tensor : (Tensor, float, int) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "scale", "type": "Torch_FloatType" },
      { "name": "zero_point", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten._reshape_alias",
    "summary": "Generated op for `aten::_reshape_alias : (Tensor, int[], int[]) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "size", "type": "AnyTorchListOfTorchIntType" },
      { "name": "stride", "type": "AnyTorchListOfTorchIntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten._reshape_alias_copy",
    "summary": "Generated op for `aten::_reshape_alias_copy : (Tensor, int[], int[]) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "size", "type": "AnyTorchListOfTorchIntType" },
      { "name": "stride", "type": "AnyTorchListOfTorchIntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten._safe_softmax",
    "summary": "Generated op for `aten::_safe_softmax : (Tensor, int, int?) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "dim", "type": "Torch_IntType" },
      { "name": "dtype", "type": "AnyTorchOptionalIntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten._set_item.str",
    "summary": "Generated op for `aten::_set_item.str : (Dict(str, t), str, t) -> ()`",
    "inputs": [
      { "name": "l", "type": "Torch_DictType" },
      { "name": "idx", "type": "Torch_StringType" },
      { "name": "v", "type": "AnyTorchType" }
    ]
  },
  {
    "name": "torch.aten._set_item.t",
    "summary": "Generated op for `aten::_set_item.t : (t[], int, t) -> (t[])`",
    "inputs": [
      { "name": "l", "type": "AnyTorchListType" },
      { "name": "idx", "type": "Torch_IntType" },
      { "name": "el", "type": "AnyTorchType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchListType" }
    ]
  },
  {
    "name": "torch.aten._shape_as_tensor",
    "summary": "Generated op for `aten::_shape_as_tensor : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten._softmax",
    "summary": "Generated op for `aten::_softmax : (Tensor, int, bool) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "dim", "type": "Torch_IntType" },
      { "name": "half_to_float", "type": "Torch_BoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten._softmax_backward_data",
    "summary": "Generated op for `aten::_softmax_backward_data : (Tensor, Tensor, int, int) -> (Tensor)`",
    "inputs": [
      { "name": "grad_output", "type": "AnyTorchTensorType" },
      { "name": "output", "type": "AnyTorchTensorType" },
      { "name": "dim", "type": "Torch_IntType" },
      { "name": "input_dtype", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten._to_copy",
    "summary": "Generated op for `aten::_to_copy : (Tensor, int?, int?, Device?, bool?, bool, int?) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "dtype", "type": "AnyTorchOptionalIntType" },
      { "name": "layout", "type": "AnyTorchOptionalIntType" },
      { "name": "device", "type": "AnyTorchOptionalDeviceType" },
      { "name": "pin_memory", "type": "AnyTorchOptionalBoolType" },
      { "name": "non_blocking", "type": "Torch_BoolType" },
      { "name": "memory_format", "type": "AnyTorchOptionalIntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten._trilinear",
    "summary": "Generated op for `aten::_trilinear : (Tensor, Tensor, Tensor, int[], int[], int[], int[], int) -> (Tensor)`",
    "inputs": [
      { "name": "i1", "type": "AnyTorchTensorType" },
      { "name": "i2", "type": "AnyTorchTensorType" },
      { "name": "i3", "type": "AnyTorchTensorType" },
      { "name": "expand1", "type": "AnyTorchListOfTorchIntType" },
      { "name": "expand2", "type": "AnyTorchListOfTorchIntType" },
      { "name": "expand3", "type": "AnyTorchListOfTorchIntType" },
      { "name": "sumdim", "type": "AnyTorchListOfTorchIntType" },
      { "name": "unroll_dim", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten._unsafe_index_put.hacked_twin",
    "summary": "Generated op for `aten::_unsafe_index_put.hacked_twin : (Tensor, Tensor[], Tensor, bool) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "indices", "type": "AnyTorchListOfTensorType" },
      { "name": "values", "type": "AnyTorchTensorType" },
      { "name": "accumulate", "type": "Torch_BoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten._unsafe_view",
    "summary": "Generated op for `aten::_unsafe_view : (Tensor, int[]) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "size", "type": "AnyTorchListOfTorchIntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten._weight_norm_interface",
    "summary": "Generated op for `aten::_weight_norm_interface : (Tensor, Tensor, int) -> (Tensor, Tensor)`",
    "inputs": [
      { "name": "v", "type": "AnyTorchTensorType" },
      { "name": "g", "type": "AnyTorchTensorType" },
      { "name": "dim", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result0", "type": "AnyTorchOptionalTensorType" },
      { "name": "result1", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.abs",
    "summary": "Generated op for `aten::abs : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.abs_",
    "summary": "Generated op for `aten::abs_ : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.acos",
    "summary": "Generated op for `aten::acos : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.acos_",
    "summary": "Generated op for `aten::acos_ : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.acosh",
    "summary": "Generated op for `aten::acosh : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.acosh_",
    "summary": "Generated op for `aten::acosh_ : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.adaptive_avg_pool1d",
    "summary": "Generated op for `aten::adaptive_avg_pool1d : (Tensor, int[]) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "output_size", "type": "AnyTorchListOfTorchIntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.adaptive_avg_pool2d",
    "summary": "Generated op for `aten::adaptive_avg_pool2d : (Tensor, int[]) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "output_size", "type": "AnyTorchListOfTorchIntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.adaptive_avg_pool3d",
    "summary": "Generated op for `aten::adaptive_avg_pool3d : (Tensor, int[]) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "output_size", "type": "AnyTorchListOfTorchIntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.adaptive_max_pool1d",
    "summary": "Generated op for `aten::adaptive_max_pool1d : (Tensor, int[]) -> (Tensor, Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "output_size", "type": "AnyTorchListOfTorchIntType" }
    ],
    "outputs": [
      { "name": "result0", "type": "AnyTorchOptionalTensorType" },
      { "name": "result1", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.adaptive_max_pool2d",
    "summary": "Generated op for `aten::adaptive_max_pool2d : (Tensor, int[]) -> (Tensor, Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "output_size", "type": "AnyTorchListOfTorchIntType" }
    ],
    "outputs": [
      { "name": "result0", "type": "AnyTorchOptionalTensorType" },
      { "name": "result1", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.adaptive_max_pool3d",
    "summary": "Generated op for `aten::adaptive_max_pool3d : (Tensor, int[]) -> (Tensor, Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "output_size", "type": "AnyTorchListOfTorchIntType" }
    ],
    "outputs": [
      { "name": "result0", "type": "AnyTorchOptionalTensorType" },
      { "name": "result1", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.add",
    "summary": "Generated op for `aten::add : (Scalar, Scalar) -> (Scalar)`",
    "inputs": [
      { "name": "a", "type": "AnyTorchScalarType" },
      { "name": "b", "type": "AnyTorchScalarType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchScalarType" }
    ]
  },
  {
    "name": "torch.aten.add_.Scalar",
    "summary": "Generated op for `aten::add_.Scalar : (Tensor, Scalar, Scalar) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" },
      { "name": "other", "type": "AnyTorchScalarType" },
      { "name": "alpha", "type": "AnyTorchScalarType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.add_.Tensor",
    "summary": "Generated op for `aten::add_.Tensor : (Tensor, Tensor, Scalar) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" },
      { "name": "other", "type": "Torch_NonValueTensorType" },
      { "name": "alpha", "type": "AnyTorchScalarType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.add.float_int",
    "summary": "Generated op for `aten::add.float_int : (float, int) -> (float)`",
    "inputs": [
      { "name": "a", "type": "Torch_FloatType" },
      { "name": "b", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "Torch_FloatType" }
    ]
  },
  {
    "name": "torch.aten.add.int",
    "summary": "Generated op for `aten::add.int : (int, int) -> (int)`",
    "inputs": [
      { "name": "a", "type": "Torch_IntType" },
      { "name": "b", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "Torch_IntType" }
    ]
  },
  {
    "name": "torch.aten.add.Scalar",
    "summary": "Generated op for `aten::add.Scalar : (Tensor, Scalar, Scalar) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "other", "type": "AnyTorchScalarType" },
      { "name": "alpha", "type": "AnyTorchScalarType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.add.str",
    "summary": "Generated op for `aten::add.str : (str, str) -> (str)`",
    "inputs": [
      { "name": "a", "type": "Torch_StringType" },
      { "name": "b", "type": "Torch_StringType" }
    ],
    "outputs": [
      { "name": "result", "type": "Torch_StringType" }
    ]
  },
  {
    "name": "torch.aten.add.t",
    "summary": "Generated op for `aten::add.t : (t[], t[]) -> (t[])`",
    "inputs": [
      { "name": "a", "type": "AnyTorchListType" },
      { "name": "b", "type": "AnyTorchListType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchListType" }
    ]
  },
  {
    "name": "torch.aten.add.Tensor",
    "summary": "Generated op for `aten::add.Tensor : (Tensor, Tensor, Scalar) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "other", "type": "AnyTorchTensorType" },
      { "name": "alpha", "type": "AnyTorchScalarType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.addcdiv",
    "summary": "Generated op for `aten::addcdiv : (Tensor, Tensor, Tensor, Scalar) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "tensor1", "type": "AnyTorchTensorType" },
      { "name": "tensor2", "type": "AnyTorchTensorType" },
      { "name": "value", "type": "AnyTorchScalarType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.addcdiv_",
    "summary": "Generated op for `aten::addcdiv_ : (Tensor, Tensor, Tensor, Scalar) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" },
      { "name": "tensor1", "type": "Torch_NonValueTensorType" },
      { "name": "tensor2", "type": "Torch_NonValueTensorType" },
      { "name": "value", "type": "AnyTorchScalarType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.addcmul",
    "summary": "Generated op for `aten::addcmul : (Tensor, Tensor, Tensor, Scalar) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "tensor1", "type": "AnyTorchTensorType" },
      { "name": "tensor2", "type": "AnyTorchTensorType" },
      { "name": "value", "type": "AnyTorchScalarType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.addcmul_",
    "summary": "Generated op for `aten::addcmul_ : (Tensor, Tensor, Tensor, Scalar) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" },
      { "name": "tensor1", "type": "Torch_NonValueTensorType" },
      { "name": "tensor2", "type": "Torch_NonValueTensorType" },
      { "name": "value", "type": "AnyTorchScalarType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.addmm",
    "summary": "Generated op for `aten::addmm : (Tensor, Tensor, Tensor, Scalar, Scalar) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "mat1", "type": "AnyTorchTensorType" },
      { "name": "mat2", "type": "AnyTorchTensorType" },
      { "name": "beta", "type": "AnyTorchScalarType" },
      { "name": "alpha", "type": "AnyTorchScalarType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.alias",
    "summary": "Generated op for `aten::alias : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.alias_copy",
    "summary": "Generated op for `aten::alias_copy : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.all",
    "summary": "Generated op for `aten::all : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.all.bool",
    "summary": "Generated op for `aten::all.bool : (bool[]) -> (bool)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchListOfTorchBoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "Torch_BoolType" }
    ]
  },
  {
    "name": "torch.aten.all.dim",
    "summary": "Generated op for `aten::all.dim : (Tensor, int, bool) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "dim", "type": "Torch_IntType" },
      { "name": "keepdim", "type": "Torch_BoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.amax",
    "summary": "Generated op for `aten::amax : (Tensor, int[], bool) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "dim", "type": "AnyTorchListOfTorchIntType" },
      { "name": "keepdim", "type": "Torch_BoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.amin",
    "summary": "Generated op for `aten::amin : (Tensor, int[], bool) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "dim", "type": "AnyTorchListOfTorchIntType" },
      { "name": "keepdim", "type": "Torch_BoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.aminmax",
    "summary": "Generated op for `aten::aminmax : (Tensor, int?, bool) -> (Tensor, Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "dim", "type": "AnyTorchOptionalIntType" },
      { "name": "keepdim", "type": "Torch_BoolType" }
    ],
    "outputs": [
      { "name": "min", "type": "AnyTorchOptionalTensorType" },
      { "name": "max", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.any",
    "summary": "Generated op for `aten::any : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.any.bool",
    "summary": "Generated op for `aten::any.bool : (bool[]) -> (bool)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchListOfTorchBoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "Torch_BoolType" }
    ]
  },
  {
    "name": "torch.aten.any.dim",
    "summary": "Generated op for `aten::any.dim : (Tensor, int, bool) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "dim", "type": "Torch_IntType" },
      { "name": "keepdim", "type": "Torch_BoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.any.dims",
    "summary": "Generated op for `aten::any.dims : (Tensor, int[]?, bool) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "dim", "type": "AnyTorchOptionalListOfTorchIntType" },
      { "name": "keepdim", "type": "Torch_BoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.append.t",
    "summary": "Generated op for `aten::append.t : (t[], t) -> (t[])`",
    "inputs": [
      { "name": "self", "type": "AnyTorchListType" },
      { "name": "el", "type": "AnyTorchType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchListType" }
    ]
  },
  {
    "name": "torch.aten.arange",
    "summary": "Generated op for `aten::arange : (Scalar, int?, int?, Device?, bool?) -> (Tensor)`",
    "inputs": [
      { "name": "end", "type": "AnyTorchScalarType" },
      { "name": "dtype", "type": "AnyTorchOptionalIntType" },
      { "name": "layout", "type": "AnyTorchOptionalIntType" },
      { "name": "device", "type": "AnyTorchOptionalDeviceType" },
      { "name": "pin_memory", "type": "AnyTorchOptionalBoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.arange.start",
    "summary": "Generated op for `aten::arange.start : (Scalar, Scalar, int?, int?, Device?, bool?) -> (Tensor)`",
    "inputs": [
      { "name": "start", "type": "AnyTorchScalarType" },
      { "name": "end", "type": "AnyTorchScalarType" },
      { "name": "dtype", "type": "AnyTorchOptionalIntType" },
      { "name": "layout", "type": "AnyTorchOptionalIntType" },
      { "name": "device", "type": "AnyTorchOptionalDeviceType" },
      { "name": "pin_memory", "type": "AnyTorchOptionalBoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.arange.start_out",
    "summary": "Generated op for `aten::arange.start_out : (Scalar, Scalar, Scalar, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "start", "type": "AnyTorchScalarType" },
      { "name": "end", "type": "AnyTorchScalarType" },
      { "name": "step", "type": "AnyTorchScalarType" },
      { "name": "out", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.arange.start_step",
    "summary": "Generated op for `aten::arange.start_step : (Scalar, Scalar, Scalar, int?, int?, Device?, bool?) -> (Tensor)`",
    "inputs": [
      { "name": "start", "type": "AnyTorchScalarType" },
      { "name": "end", "type": "AnyTorchScalarType" },
      { "name": "step", "type": "AnyTorchScalarType" },
      { "name": "dtype", "type": "AnyTorchOptionalIntType" },
      { "name": "layout", "type": "AnyTorchOptionalIntType" },
      { "name": "device", "type": "AnyTorchOptionalDeviceType" },
      { "name": "pin_memory", "type": "AnyTorchOptionalBoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.argmax",
    "summary": "Generated op for `aten::argmax : (Tensor, int?, bool) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "dim", "type": "AnyTorchOptionalIntType" },
      { "name": "keepdim", "type": "Torch_BoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.argmin",
    "summary": "Generated op for `aten::argmin : (Tensor, int?, bool) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "dim", "type": "AnyTorchOptionalIntType" },
      { "name": "keepdim", "type": "Torch_BoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.argsort",
    "summary": "Generated op for `aten::argsort : (Tensor, int, bool) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "dim", "type": "Torch_IntType" },
      { "name": "descending", "type": "Torch_BoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.as_strided",
    "summary": "Generated op for `aten::as_strided : (Tensor, int[], int[], int?) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "size", "type": "AnyTorchListOfTorchIntType" },
      { "name": "stride", "type": "AnyTorchListOfTorchIntType" },
      { "name": "storage_offset", "type": "AnyTorchOptionalIntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.as_strided_copy",
    "summary": "Generated op for `aten::as_strided_copy : (Tensor, int[], int[], int?) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "size", "type": "AnyTorchListOfTorchIntType" },
      { "name": "stride", "type": "AnyTorchListOfTorchIntType" },
      { "name": "storage_offset", "type": "AnyTorchOptionalIntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.as_strided_scatter",
    "summary": "Generated op for `aten::as_strided_scatter : (Tensor, Tensor, int[], int[], int?) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "src", "type": "AnyTorchTensorType" },
      { "name": "size", "type": "AnyTorchListOfTorchIntType" },
      { "name": "stride", "type": "AnyTorchListOfTorchIntType" },
      { "name": "storage_offset", "type": "AnyTorchOptionalIntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.asin",
    "summary": "Generated op for `aten::asin : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.asin_",
    "summary": "Generated op for `aten::asin_ : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.asinh",
    "summary": "Generated op for `aten::asinh : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.asinh_",
    "summary": "Generated op for `aten::asinh_ : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.atan",
    "summary": "Generated op for `aten::atan : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.atan_",
    "summary": "Generated op for `aten::atan_ : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.atan2",
    "summary": "Generated op for `aten::atan2 : (Tensor, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "other", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.atan2_",
    "summary": "Generated op for `aten::atan2_ : (Tensor, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" },
      { "name": "other", "type": "Torch_NonValueTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.atanh",
    "summary": "Generated op for `aten::atanh : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.atanh_",
    "summary": "Generated op for `aten::atanh_ : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.atleast_1d",
    "summary": "Generated op for `aten::atleast_1d : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.atleast_2d",
    "summary": "Generated op for `aten::atleast_2d : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.avg_pool1d",
    "summary": "Generated op for `aten::avg_pool1d : (Tensor, int[], int[], int[], bool, bool) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "kernel_size", "type": "AnyTorchListOfTorchIntType" },
      { "name": "stride", "type": "AnyTorchListOfTorchIntType" },
      { "name": "padding", "type": "AnyTorchListOfTorchIntType" },
      { "name": "ceil_mode", "type": "Torch_BoolType" },
      { "name": "count_include_pad", "type": "Torch_BoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.avg_pool2d",
    "summary": "Generated op for `aten::avg_pool2d : (Tensor, int[], int[], int[], bool, bool, int?) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "kernel_size", "type": "AnyTorchListOfTorchIntType" },
      { "name": "stride", "type": "AnyTorchListOfTorchIntType" },
      { "name": "padding", "type": "AnyTorchListOfTorchIntType" },
      { "name": "ceil_mode", "type": "Torch_BoolType" },
      { "name": "count_include_pad", "type": "Torch_BoolType" },
      { "name": "divisor_override", "type": "AnyTorchOptionalIntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.avg_pool2d_backward",
    "summary": "Generated op for `aten::avg_pool2d_backward : (Tensor, Tensor, int[], int[], int[], bool, bool, int?) -> (Tensor)`",
    "inputs": [
      { "name": "grad_output", "type": "AnyTorchTensorType" },
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "kernel_size", "type": "AnyTorchListOfTorchIntType" },
      { "name": "stride", "type": "AnyTorchListOfTorchIntType" },
      { "name": "padding", "type": "AnyTorchListOfTorchIntType" },
      { "name": "ceil_mode", "type": "Torch_BoolType" },
      { "name": "count_include_pad", "type": "Torch_BoolType" },
      { "name": "divisor_override", "type": "AnyTorchOptionalIntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.avg_pool3d",
    "summary": "Generated op for `aten::avg_pool3d : (Tensor, int[], int[], int[], bool, bool, int?) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "kernel_size", "type": "AnyTorchListOfTorchIntType" },
      { "name": "stride", "type": "AnyTorchListOfTorchIntType" },
      { "name": "padding", "type": "AnyTorchListOfTorchIntType" },
      { "name": "ceil_mode", "type": "Torch_BoolType" },
      { "name": "count_include_pad", "type": "Torch_BoolType" },
      { "name": "divisor_override", "type": "AnyTorchOptionalIntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.avg_pool3d_backward",
    "summary": "Generated op for `aten::avg_pool3d_backward : (Tensor, Tensor, int[], int[], int[], bool, bool, int?) -> (Tensor)`",
    "inputs": [
      { "name": "grad_output", "type": "AnyTorchTensorType" },
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "kernel_size", "type": "AnyTorchListOfTorchIntType" },
      { "name": "stride", "type": "AnyTorchListOfTorchIntType" },
      { "name": "padding", "type": "AnyTorchListOfTorchIntType" },
      { "name": "ceil_mode", "type": "Torch_BoolType" },
      { "name": "count_include_pad", "type": "Torch_BoolType" },
      { "name": "divisor_override", "type": "AnyTorchOptionalIntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.baddbmm",
    "summary": "Generated op for `aten::baddbmm : (Tensor, Tensor, Tensor, Scalar, Scalar) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "batch1", "type": "AnyTorchTensorType" },
      { "name": "batch2", "type": "AnyTorchTensorType" },
      { "name": "beta", "type": "AnyTorchScalarType" },
      { "name": "alpha", "type": "AnyTorchScalarType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.baddbmm_",
    "summary": "Generated op for `aten::baddbmm_ : (Tensor, Tensor, Tensor, Scalar, Scalar) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" },
      { "name": "batch1", "type": "Torch_NonValueTensorType" },
      { "name": "batch2", "type": "Torch_NonValueTensorType" },
      { "name": "beta", "type": "AnyTorchScalarType" },
      { "name": "alpha", "type": "AnyTorchScalarType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.batch_norm",
    "summary": "Generated op for `aten::batch_norm : (Tensor, Tensor?, Tensor?, Tensor?, Tensor?, bool, float, float, bool) -> (Tensor)`",
    "inputs": [
      { "name": "input", "type": "AnyTorchTensorType" },
      { "name": "weight", "type": "AnyTorchOptionalTensorType" },
      { "name": "bias", "type": "AnyTorchOptionalTensorType" },
      { "name": "running_mean", "type": "AnyTorchOptionalTensorType" },
      { "name": "running_var", "type": "AnyTorchOptionalTensorType" },
      { "name": "training", "type": "Torch_BoolType" },
      { "name": "momentum", "type": "Torch_FloatType" },
      { "name": "eps", "type": "Torch_FloatType" },
      { "name": "cudnn_enabled", "type": "Torch_BoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.bernoulli",
    "summary": "Generated op for `aten::bernoulli : (Tensor, Generator?) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "generator", "type": "AnyTorchOptionalGeneratorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.bernoulli_.float",
    "summary": "Generated op for `aten::bernoulli_.float : (Tensor, float, Generator?) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "p", "type": "Torch_FloatType" },
      { "name": "generator", "type": "AnyTorchOptionalGeneratorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.bernoulli_.Tensor",
    "summary": "Generated op for `aten::bernoulli_.Tensor : (Tensor, Tensor, Generator?) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" },
      { "name": "p", "type": "Torch_NonValueTensorType" },
      { "name": "generator", "type": "AnyTorchOptionalGeneratorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.bernoulli.p",
    "summary": "Generated op for `aten::bernoulli.p : (Tensor, float, Generator?) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "p", "type": "Torch_FloatType" },
      { "name": "generator", "type": "AnyTorchOptionalGeneratorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.bernoulli.Tensor",
    "summary": "Generated op for `aten::bernoulli.Tensor : (Tensor, Tensor, Generator?) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "p", "type": "AnyTorchTensorType" },
      { "name": "generator", "type": "AnyTorchOptionalGeneratorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.binary_cross_entropy",
    "summary": "Generated op for `aten::binary_cross_entropy : (Tensor, Tensor, Tensor?, int) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "target", "type": "AnyTorchTensorType" },
      { "name": "weight", "type": "AnyTorchOptionalTensorType" },
      { "name": "reduction", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.binary_cross_entropy_backward",
    "summary": "Generated op for `aten::binary_cross_entropy_backward : (Tensor, Tensor, Tensor, Tensor?, int) -> (Tensor)`",
    "inputs": [
      { "name": "grad_output", "type": "AnyTorchTensorType" },
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "target", "type": "AnyTorchTensorType" },
      { "name": "weight", "type": "AnyTorchOptionalTensorType" },
      { "name": "reduction", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.binary_cross_entropy_with_logits",
    "summary": "Generated op for `aten::binary_cross_entropy_with_logits : (Tensor, Tensor, Tensor?, Tensor?, int) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "target", "type": "AnyTorchTensorType" },
      { "name": "weight", "type": "AnyTorchOptionalTensorType" },
      { "name": "pos_weight", "type": "AnyTorchOptionalTensorType" },
      { "name": "reduction", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.bincount",
    "summary": "Generated op for `aten::bincount : (Tensor, Tensor?, int) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "weights", "type": "AnyTorchOptionalTensorType" },
      { "name": "minlength", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.bitwise_and_.Scalar",
    "summary": "Generated op for `aten::bitwise_and_.Scalar : (Tensor, Scalar) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" },
      { "name": "other", "type": "AnyTorchScalarType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.bitwise_and_.Tensor",
    "summary": "Generated op for `aten::bitwise_and_.Tensor : (Tensor, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" },
      { "name": "other", "type": "Torch_NonValueTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.bitwise_and.Scalar",
    "summary": "Generated op for `aten::bitwise_and.Scalar : (Tensor, Scalar) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "other", "type": "AnyTorchScalarType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.bitwise_and.Tensor",
    "summary": "Generated op for `aten::bitwise_and.Tensor : (Tensor, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "other", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.bitwise_left_shift_.Tensor",
    "summary": "Generated op for `aten::bitwise_left_shift_.Tensor : (Tensor, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" },
      { "name": "other", "type": "Torch_NonValueTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.bitwise_left_shift.Tensor",
    "summary": "Generated op for `aten::bitwise_left_shift.Tensor : (Tensor, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "other", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.bitwise_not",
    "summary": "Generated op for `aten::bitwise_not : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.bitwise_not_",
    "summary": "Generated op for `aten::bitwise_not_ : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.bitwise_or_.Tensor",
    "summary": "Generated op for `aten::bitwise_or_.Tensor : (Tensor, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" },
      { "name": "other", "type": "Torch_NonValueTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.bitwise_or.Tensor",
    "summary": "Generated op for `aten::bitwise_or.Tensor : (Tensor, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "other", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.bitwise_right_shift_.Tensor",
    "summary": "Generated op for `aten::bitwise_right_shift_.Tensor : (Tensor, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" },
      { "name": "other", "type": "Torch_NonValueTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.bitwise_right_shift.Tensor",
    "summary": "Generated op for `aten::bitwise_right_shift.Tensor : (Tensor, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "other", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.bitwise_xor_.Tensor",
    "summary": "Generated op for `aten::bitwise_xor_.Tensor : (Tensor, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" },
      { "name": "other", "type": "Torch_NonValueTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.bitwise_xor.Tensor",
    "summary": "Generated op for `aten::bitwise_xor.Tensor : (Tensor, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "other", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.bmm",
    "summary": "Generated op for `aten::bmm : (Tensor, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "mat2", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.Bool.float",
    "summary": "Generated op for `aten::Bool.float : (float) -> (bool)`",
    "inputs": [
      { "name": "a", "type": "Torch_FloatType" }
    ],
    "outputs": [
      { "name": "result", "type": "Torch_BoolType" }
    ]
  },
  {
    "name": "torch.aten.Bool.int",
    "summary": "Generated op for `aten::Bool.int : (int) -> (bool)`",
    "inputs": [
      { "name": "a", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "Torch_BoolType" }
    ]
  },
  {
    "name": "torch.aten.Bool.Tensor",
    "summary": "Generated op for `aten::Bool.Tensor : (Tensor) -> (bool)`",
    "inputs": [
      { "name": "a", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "Torch_BoolType" }
    ]
  },
  {
    "name": "torch.aten.broadcast_tensors",
    "summary": "Generated op for `aten::broadcast_tensors : (Tensor[]) -> (Tensor[])`",
    "inputs": [
      { "name": "tensors", "type": "AnyTorchListOfTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchListOfTensorType" }
    ]
  },
  {
    "name": "torch.aten.broadcast_to",
    "summary": "Generated op for `aten::broadcast_to : (Tensor, int[]) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "size", "type": "AnyTorchListOfTorchIntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.bucketize.Tensor",
    "summary": "Generated op for `aten::bucketize.Tensor : (Tensor, Tensor, bool, bool) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "boundaries", "type": "AnyTorchTensorType" },
      { "name": "out_int32", "type": "Torch_BoolType" },
      { "name": "right", "type": "Torch_BoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.cat",
    "summary": "Generated op for `aten::cat : (Tensor[], int) -> (Tensor)`",
    "inputs": [
      { "name": "tensors", "type": "AnyTorchListOfTensorType" },
      { "name": "dim", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.ceil",
    "summary": "Generated op for `aten::ceil : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.ceil_",
    "summary": "Generated op for `aten::ceil_ : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.ceil.float",
    "summary": "Generated op for `aten::ceil.float : (float) -> (int)`",
    "inputs": [
      { "name": "a", "type": "Torch_FloatType" }
    ],
    "outputs": [
      { "name": "result", "type": "Torch_IntType" }
    ]
  },
  {
    "name": "torch.aten.ceil.Scalar",
    "summary": "Generated op for `aten::ceil.Scalar : (Scalar) -> (Scalar)`",
    "inputs": [
      { "name": "a", "type": "AnyTorchScalarType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchScalarType" }
    ]
  },
  {
    "name": "torch.aten.celu",
    "summary": "Generated op for `aten::celu : (Tensor, Scalar) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "alpha", "type": "AnyTorchScalarType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.celu_",
    "summary": "Generated op for `aten::celu_ : (Tensor, Scalar) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" },
      { "name": "alpha", "type": "AnyTorchScalarType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.channel_shuffle",
    "summary": "Generated op for `aten::channel_shuffle : (Tensor, int) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "groups", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.chunk",
    "summary": "Generated op for `aten::chunk : (Tensor, int, int) -> (Tensor[])`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "chunks", "type": "Torch_IntType" },
      { "name": "dim", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchListOfTensorType" }
    ]
  },
  {
    "name": "torch.aten.clamp",
    "summary": "Generated op for `aten::clamp : (Tensor, Scalar?, Scalar?) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "min", "type": "AnyTorchOptionalScalarType" },
      { "name": "max", "type": "AnyTorchOptionalScalarType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.clamp_",
    "summary": "Generated op for `aten::clamp_ : (Tensor, Scalar?, Scalar?) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" },
      { "name": "min", "type": "AnyTorchOptionalScalarType" },
      { "name": "max", "type": "AnyTorchOptionalScalarType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.clamp_.Tensor",
    "summary": "Generated op for `aten::clamp_.Tensor : (Tensor, Tensor?, Tensor?) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" },
      { "name": "min", "type": "AnyTorchOptionalNonValueTensorType" },
      { "name": "max", "type": "AnyTorchOptionalNonValueTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.clamp_max",
    "summary": "Generated op for `aten::clamp_max : (Tensor, Scalar) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "max", "type": "AnyTorchScalarType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.clamp_max_",
    "summary": "Generated op for `aten::clamp_max_ : (Tensor, Scalar) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" },
      { "name": "max", "type": "AnyTorchScalarType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.clamp_max_.Tensor",
    "summary": "Generated op for `aten::clamp_max_.Tensor : (Tensor, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" },
      { "name": "max", "type": "Torch_NonValueTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.clamp_max.Tensor",
    "summary": "Generated op for `aten::clamp_max.Tensor : (Tensor, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "max", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.clamp_min",
    "summary": "Generated op for `aten::clamp_min : (Tensor, Scalar) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "min", "type": "AnyTorchScalarType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.clamp_min_",
    "summary": "Generated op for `aten::clamp_min_ : (Tensor, Scalar) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" },
      { "name": "min", "type": "AnyTorchScalarType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.clamp_min_.Tensor",
    "summary": "Generated op for `aten::clamp_min_.Tensor : (Tensor, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" },
      { "name": "min", "type": "Torch_NonValueTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.clamp_min.Tensor",
    "summary": "Generated op for `aten::clamp_min.Tensor : (Tensor, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "min", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.clamp.Tensor",
    "summary": "Generated op for `aten::clamp.Tensor : (Tensor, Tensor?, Tensor?) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "min", "type": "AnyTorchOptionalTensorType" },
      { "name": "max", "type": "AnyTorchOptionalTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.clone",
    "summary": "Generated op for `aten::clone : (Tensor, int?) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "memory_format", "type": "AnyTorchOptionalIntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.col2im",
    "summary": "Generated op for `aten::col2im : (Tensor, int[], int[], int[], int[], int[]) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "output_size", "type": "AnyTorchListOfTorchIntType" },
      { "name": "kernel_size", "type": "AnyTorchListOfTorchIntType" },
      { "name": "dilation", "type": "AnyTorchListOfTorchIntType" },
      { "name": "padding", "type": "AnyTorchListOfTorchIntType" },
      { "name": "stride", "type": "AnyTorchListOfTorchIntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.column_stack",
    "summary": "Generated op for `aten::column_stack : (Tensor[]) -> (Tensor)`",
    "inputs": [
      { "name": "tensors", "type": "AnyTorchListOfTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.complex",
    "summary": "Generated op for `aten::complex : (Tensor, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "real", "type": "AnyTorchTensorType" },
      { "name": "imag", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.constant_pad_nd",
    "summary": "Generated op for `aten::constant_pad_nd : (Tensor, int[], Scalar) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "pad", "type": "AnyTorchListOfTorchIntType" },
      { "name": "value", "type": "AnyTorchScalarType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.contiguous",
    "summary": "Generated op for `aten::contiguous : (Tensor, int) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "memory_format", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.conv_tbc",
    "summary": "Generated op for `aten::conv_tbc : (Tensor, Tensor, Tensor, int) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "weight", "type": "AnyTorchTensorType" },
      { "name": "bias", "type": "AnyTorchTensorType" },
      { "name": "pad", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.conv_tbc_backward",
    "summary": "Generated op for `aten::conv_tbc_backward : (Tensor, Tensor, Tensor, Tensor, int) -> (Tensor, Tensor, Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "input", "type": "AnyTorchTensorType" },
      { "name": "weight", "type": "AnyTorchTensorType" },
      { "name": "bias", "type": "AnyTorchTensorType" },
      { "name": "pad", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result0", "type": "AnyTorchOptionalTensorType" },
      { "name": "result1", "type": "AnyTorchOptionalTensorType" },
      { "name": "result2", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.conv_transpose1d",
    "summary": "Generated op for `aten::conv_transpose1d : (Tensor, Tensor, Tensor?, int[], int[], int[], int, int[]) -> (Tensor)`",
    "inputs": [
      { "name": "input", "type": "AnyTorchTensorType" },
      { "name": "weight", "type": "AnyTorchTensorType" },
      { "name": "bias", "type": "AnyTorchOptionalTensorType" },
      { "name": "stride", "type": "AnyTorchListOfTorchIntType" },
      { "name": "padding", "type": "AnyTorchListOfTorchIntType" },
      { "name": "output_padding", "type": "AnyTorchListOfTorchIntType" },
      { "name": "groups", "type": "Torch_IntType" },
      { "name": "dilation", "type": "AnyTorchListOfTorchIntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.conv_transpose2d.input",
    "summary": "Generated op for `aten::conv_transpose2d.input : (Tensor, Tensor, Tensor?, int[], int[], int[], int, int[]) -> (Tensor)`",
    "inputs": [
      { "name": "input", "type": "AnyTorchTensorType" },
      { "name": "weight", "type": "AnyTorchTensorType" },
      { "name": "bias", "type": "AnyTorchOptionalTensorType" },
      { "name": "stride", "type": "AnyTorchListOfTorchIntType" },
      { "name": "padding", "type": "AnyTorchListOfTorchIntType" },
      { "name": "output_padding", "type": "AnyTorchListOfTorchIntType" },
      { "name": "groups", "type": "Torch_IntType" },
      { "name": "dilation", "type": "AnyTorchListOfTorchIntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.conv_transpose3d.input",
    "summary": "Generated op for `aten::conv_transpose3d.input : (Tensor, Tensor, Tensor?, int[], int[], int[], int, int[]) -> (Tensor)`",
    "inputs": [
      { "name": "input", "type": "AnyTorchTensorType" },
      { "name": "weight", "type": "AnyTorchTensorType" },
      { "name": "bias", "type": "AnyTorchOptionalTensorType" },
      { "name": "stride", "type": "AnyTorchListOfTorchIntType" },
      { "name": "padding", "type": "AnyTorchListOfTorchIntType" },
      { "name": "output_padding", "type": "AnyTorchListOfTorchIntType" },
      { "name": "groups", "type": "Torch_IntType" },
      { "name": "dilation", "type": "AnyTorchListOfTorchIntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.conv1d",
    "summary": "Generated op for `aten::conv1d : (Tensor, Tensor, Tensor?, int[], int[], int[], int) -> (Tensor)`",
    "inputs": [
      { "name": "input", "type": "AnyTorchTensorType" },
      { "name": "weight", "type": "AnyTorchTensorType" },
      { "name": "bias", "type": "AnyTorchOptionalTensorType" },
      { "name": "stride", "type": "AnyTorchListOfTorchIntType" },
      { "name": "padding", "type": "AnyTorchListOfTorchIntType" },
      { "name": "dilation", "type": "AnyTorchListOfTorchIntType" },
      { "name": "groups", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.conv1d.padding",
    "summary": "Generated op for `aten::conv1d.padding : (Tensor, Tensor, Tensor?, int[], str, int[], int) -> (Tensor)`",
    "inputs": [
      { "name": "input", "type": "AnyTorchTensorType" },
      { "name": "weight", "type": "AnyTorchTensorType" },
      { "name": "bias", "type": "AnyTorchOptionalTensorType" },
      { "name": "stride", "type": "AnyTorchListOfTorchIntType" },
      { "name": "padding", "type": "Torch_StringType" },
      { "name": "dilation", "type": "AnyTorchListOfTorchIntType" },
      { "name": "groups", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.conv2d",
    "summary": "Generated op for `aten::conv2d : (Tensor, Tensor, Tensor?, int[], int[], int[], int) -> (Tensor)`",
    "inputs": [
      { "name": "input", "type": "AnyTorchTensorType" },
      { "name": "weight", "type": "AnyTorchTensorType" },
      { "name": "bias", "type": "AnyTorchOptionalTensorType" },
      { "name": "stride", "type": "AnyTorchListOfTorchIntType" },
      { "name": "padding", "type": "AnyTorchListOfTorchIntType" },
      { "name": "dilation", "type": "AnyTorchListOfTorchIntType" },
      { "name": "groups", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.conv2d.padding",
    "summary": "Generated op for `aten::conv2d.padding : (Tensor, Tensor, Tensor?, int[], str, int[], int) -> (Tensor)`",
    "inputs": [
      { "name": "input", "type": "AnyTorchTensorType" },
      { "name": "weight", "type": "AnyTorchTensorType" },
      { "name": "bias", "type": "AnyTorchOptionalTensorType" },
      { "name": "stride", "type": "AnyTorchListOfTorchIntType" },
      { "name": "padding", "type": "Torch_StringType" },
      { "name": "dilation", "type": "AnyTorchListOfTorchIntType" },
      { "name": "groups", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.conv3d",
    "summary": "Generated op for `aten::conv3d : (Tensor, Tensor, Tensor?, int[], int[], int[], int) -> (Tensor)`",
    "inputs": [
      { "name": "input", "type": "AnyTorchTensorType" },
      { "name": "weight", "type": "AnyTorchTensorType" },
      { "name": "bias", "type": "AnyTorchOptionalTensorType" },
      { "name": "stride", "type": "AnyTorchListOfTorchIntType" },
      { "name": "padding", "type": "AnyTorchListOfTorchIntType" },
      { "name": "dilation", "type": "AnyTorchListOfTorchIntType" },
      { "name": "groups", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.conv3d.padding",
    "summary": "Generated op for `aten::conv3d.padding : (Tensor, Tensor, Tensor?, int[], str, int[], int) -> (Tensor)`",
    "inputs": [
      { "name": "input", "type": "AnyTorchTensorType" },
      { "name": "weight", "type": "AnyTorchTensorType" },
      { "name": "bias", "type": "AnyTorchOptionalTensorType" },
      { "name": "stride", "type": "AnyTorchListOfTorchIntType" },
      { "name": "padding", "type": "Torch_StringType" },
      { "name": "dilation", "type": "AnyTorchListOfTorchIntType" },
      { "name": "groups", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.convolution",
    "summary": "Generated op for `aten::convolution : (Tensor, Tensor, Tensor?, int[], int[], int[], bool, int[], int) -> (Tensor)`",
    "inputs": [
      { "name": "input", "type": "AnyTorchTensorType" },
      { "name": "weight", "type": "AnyTorchTensorType" },
      { "name": "bias", "type": "AnyTorchOptionalTensorType" },
      { "name": "stride", "type": "AnyTorchListOfTorchIntType" },
      { "name": "padding", "type": "AnyTorchListOfTorchIntType" },
      { "name": "dilation", "type": "AnyTorchListOfTorchIntType" },
      { "name": "transposed", "type": "Torch_BoolType" },
      { "name": "output_padding", "type": "AnyTorchListOfTorchIntType" },
      { "name": "groups", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.convolution_backward",
    "summary": "Generated op for `aten::convolution_backward : (Tensor, Tensor, Tensor, int[]?, int[], int[], int[], bool, int[], int, bool[]) -> (Tensor, Tensor, Tensor)`",
    "inputs": [
      { "name": "grad_output", "type": "AnyTorchTensorType" },
      { "name": "input", "type": "AnyTorchTensorType" },
      { "name": "weight", "type": "AnyTorchTensorType" },
      { "name": "bias_sizes", "type": "AnyTorchOptionalListOfTorchIntType" },
      { "name": "stride", "type": "AnyTorchListOfTorchIntType" },
      { "name": "padding", "type": "AnyTorchListOfTorchIntType" },
      { "name": "dilation", "type": "AnyTorchListOfTorchIntType" },
      { "name": "transposed", "type": "Torch_BoolType" },
      { "name": "output_padding", "type": "AnyTorchListOfTorchIntType" },
      { "name": "groups", "type": "Torch_IntType" },
      { "name": "output_mask", "type": "AnyTorchListOfTorchBoolType" }
    ],
    "outputs": [
      { "name": "result0", "type": "AnyTorchOptionalTensorType" },
      { "name": "result1", "type": "AnyTorchOptionalTensorType" },
      { "name": "result2", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.copy",
    "summary": "Generated op for `aten::copy : (Tensor, Tensor, bool) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "src", "type": "AnyTorchTensorType" },
      { "name": "non_blocking", "type": "Torch_BoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.copy_",
    "summary": "Generated op for `aten::copy_ : (Tensor, Tensor, bool) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" },
      { "name": "src", "type": "Torch_NonValueTensorType" },
      { "name": "non_blocking", "type": "Torch_BoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.copysign_.Tensor",
    "summary": "Generated op for `aten::copysign_.Tensor : (Tensor, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" },
      { "name": "other", "type": "Torch_NonValueTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.copysign.Tensor",
    "summary": "Generated op for `aten::copysign.Tensor : (Tensor, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "other", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.cos",
    "summary": "Generated op for `aten::cos : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.cos_",
    "summary": "Generated op for `aten::cos_ : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.cosh",
    "summary": "Generated op for `aten::cosh : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.cosh_",
    "summary": "Generated op for `aten::cosh_ : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.cosine_embedding_loss",
    "summary": "Generated op for `aten::cosine_embedding_loss : (Tensor, Tensor, Tensor, float, int) -> (Tensor)`",
    "inputs": [
      { "name": "input1", "type": "AnyTorchTensorType" },
      { "name": "input2", "type": "AnyTorchTensorType" },
      { "name": "target", "type": "AnyTorchTensorType" },
      { "name": "margin", "type": "Torch_FloatType" },
      { "name": "reduction", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.cosine_similarity",
    "summary": "Generated op for `aten::cosine_similarity : (Tensor, Tensor, int, float) -> (Tensor)`",
    "inputs": [
      { "name": "x1", "type": "AnyTorchTensorType" },
      { "name": "x2", "type": "AnyTorchTensorType" },
      { "name": "dim", "type": "Torch_IntType" },
      { "name": "eps", "type": "Torch_FloatType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.count_nonzero",
    "summary": "Generated op for `aten::count_nonzero : (Tensor, int?) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "dim", "type": "AnyTorchOptionalIntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.count_nonzero.dim_IntList",
    "summary": "Generated op for `aten::count_nonzero.dim_IntList : (Tensor, int[]) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "dim", "type": "AnyTorchListOfTorchIntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.cpu",
    "summary": "Generated op for `aten::cpu : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.cross_entropy_loss",
    "summary": "Generated op for `aten::cross_entropy_loss : (Tensor, Tensor, Tensor?, int, int, float) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "target", "type": "AnyTorchTensorType" },
      { "name": "weight", "type": "AnyTorchOptionalTensorType" },
      { "name": "reduction", "type": "Torch_IntType" },
      { "name": "ignore_index", "type": "Torch_IntType" },
      { "name": "label_smoothing", "type": "Torch_FloatType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.cuda",
    "summary": "Generated op for `aten::cuda : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.cumprod",
    "summary": "Generated op for `aten::cumprod : (Tensor, int, int?) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "dim", "type": "Torch_IntType" },
      { "name": "dtype", "type": "AnyTorchOptionalIntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.cumsum",
    "summary": "Generated op for `aten::cumsum : (Tensor, int, int?) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "dim", "type": "Torch_IntType" },
      { "name": "dtype", "type": "AnyTorchOptionalIntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.deg2rad",
    "summary": "Generated op for `aten::deg2rad : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.Delete.Dict_str",
    "summary": "Generated op for `aten::Delete.Dict_str : (Dict(str, t), str) -> ()`",
    "inputs": [
      { "name": "self", "type": "Torch_DictType" },
      { "name": "key", "type": "Torch_StringType" }
    ]
  },
  {
    "name": "torch.aten.dequantize.self",
    "summary": "Generated op for `aten::dequantize.self : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.dequantize.tensor",
    "summary": "Generated op for `aten::dequantize.tensor : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "qtensor", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.detach",
    "summary": "Generated op for `aten::detach : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.detach_copy",
    "summary": "Generated op for `aten::detach_copy : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.device.with_index",
    "summary": "Generated op for `aten::device.with_index : (str, int) -> (Device)`",
    "inputs": [
      { "name": "type", "type": "Torch_StringType" },
      { "name": "index", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "Torch_DeviceType" }
    ]
  },
  {
    "name": "torch.aten.diag_embed",
    "summary": "Generated op for `aten::diag_embed : (Tensor, int, int, int) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "offset", "type": "Torch_IntType" },
      { "name": "dim1", "type": "Torch_IntType" },
      { "name": "dim2", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.diagonal",
    "summary": "Generated op for `aten::diagonal : (Tensor, int, int, int) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "offset", "type": "Torch_IntType" },
      { "name": "dim1", "type": "Torch_IntType" },
      { "name": "dim2", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.diagonal_copy",
    "summary": "Generated op for `aten::diagonal_copy : (Tensor, int, int, int) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "offset", "type": "Torch_IntType" },
      { "name": "dim1", "type": "Torch_IntType" },
      { "name": "dim2", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.diagonal_scatter",
    "summary": "Generated op for `aten::diagonal_scatter : (Tensor, Tensor, int, int, int) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "src", "type": "AnyTorchTensorType" },
      { "name": "offset", "type": "Torch_IntType" },
      { "name": "dim1", "type": "Torch_IntType" },
      { "name": "dim2", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.dim",
    "summary": "Generated op for `aten::dim : (Tensor) -> (int)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "Torch_IntType" }
    ]
  },
  {
    "name": "torch.aten.div",
    "summary": "Generated op for `aten::div : (Scalar, Scalar) -> (float)`",
    "inputs": [
      { "name": "a", "type": "AnyTorchScalarType" },
      { "name": "b", "type": "AnyTorchScalarType" }
    ],
    "outputs": [
      { "name": "result", "type": "Torch_FloatType" }
    ]
  },
  {
    "name": "torch.aten.div_.Scalar",
    "summary": "Generated op for `aten::div_.Scalar : (Tensor, Scalar) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" },
      { "name": "other", "type": "AnyTorchScalarType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.div_.Scalar_mode",
    "summary": "Generated op for `aten::div_.Scalar_mode : (Tensor, Scalar, str?) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" },
      { "name": "other", "type": "AnyTorchScalarType" },
      { "name": "rounding_mode", "type": "AnyTorchOptionalStringType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.div_.Tensor",
    "summary": "Generated op for `aten::div_.Tensor : (Tensor, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" },
      { "name": "other", "type": "Torch_NonValueTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.div_.Tensor_mode",
    "summary": "Generated op for `aten::div_.Tensor_mode : (Tensor, Tensor, str?) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" },
      { "name": "other", "type": "Torch_NonValueTensorType" },
      { "name": "rounding_mode", "type": "AnyTorchOptionalStringType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.div.float",
    "summary": "Generated op for `aten::div.float : (float, float) -> (float)`",
    "inputs": [
      { "name": "a", "type": "Torch_FloatType" },
      { "name": "b", "type": "Torch_FloatType" }
    ],
    "outputs": [
      { "name": "result", "type": "Torch_FloatType" }
    ]
  },
  {
    "name": "torch.aten.div.int",
    "summary": "Generated op for `aten::div.int : (int, int) -> (float)`",
    "inputs": [
      { "name": "a", "type": "Torch_IntType" },
      { "name": "b", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "Torch_FloatType" }
    ]
  },
  {
    "name": "torch.aten.div.Scalar",
    "summary": "Generated op for `aten::div.Scalar : (Tensor, Scalar) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "other", "type": "AnyTorchScalarType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.div.Scalar_mode",
    "summary": "Generated op for `aten::div.Scalar_mode : (Tensor, Scalar, str?) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "other", "type": "AnyTorchScalarType" },
      { "name": "rounding_mode", "type": "AnyTorchOptionalStringType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.div.Tensor",
    "summary": "Generated op for `aten::div.Tensor : (Tensor, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "other", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.div.Tensor_mode",
    "summary": "Generated op for `aten::div.Tensor_mode : (Tensor, Tensor, str?) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "other", "type": "AnyTorchTensorType" },
      { "name": "rounding_mode", "type": "AnyTorchOptionalStringType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.dot",
    "summary": "Generated op for `aten::dot : (Tensor, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "tensor", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.dropout",
    "summary": "Generated op for `aten::dropout : (Tensor, float, bool) -> (Tensor)`",
    "inputs": [
      { "name": "input", "type": "AnyTorchTensorType" },
      { "name": "p", "type": "Torch_FloatType" },
      { "name": "train", "type": "Torch_BoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.dropout_",
    "summary": "Generated op for `aten::dropout_ : (Tensor, float, bool) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" },
      { "name": "p", "type": "Torch_FloatType" },
      { "name": "train", "type": "Torch_BoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.einsum",
    "summary": "Generated op for `aten::einsum : (str, Tensor[], int[]?) -> (Tensor)`",
    "inputs": [
      { "name": "equation", "type": "Torch_StringType" },
      { "name": "tensors", "type": "AnyTorchListOfTensorType" },
      { "name": "path", "type": "AnyTorchOptionalListOfTorchIntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.elu",
    "summary": "Generated op for `aten::elu : (Tensor, Scalar, Scalar, Scalar) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "alpha", "type": "AnyTorchScalarType" },
      { "name": "scale", "type": "AnyTorchScalarType" },
      { "name": "input_scale", "type": "AnyTorchScalarType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.elu_",
    "summary": "Generated op for `aten::elu_ : (Tensor, Scalar, Scalar, Scalar) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" },
      { "name": "alpha", "type": "AnyTorchScalarType" },
      { "name": "scale", "type": "AnyTorchScalarType" },
      { "name": "input_scale", "type": "AnyTorchScalarType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.elu_backward",
    "summary": "Generated op for `aten::elu_backward : (Tensor, Scalar, Scalar, Scalar, bool, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "grad_output", "type": "AnyTorchTensorType" },
      { "name": "alpha", "type": "AnyTorchScalarType" },
      { "name": "scale", "type": "AnyTorchScalarType" },
      { "name": "input_scale", "type": "AnyTorchScalarType" },
      { "name": "is_result", "type": "Torch_BoolType" },
      { "name": "self_or_result", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.embedding",
    "summary": "Generated op for `aten::embedding : (Tensor, Tensor, int, bool, bool) -> (Tensor)`",
    "inputs": [
      { "name": "weight", "type": "AnyTorchTensorType" },
      { "name": "indices", "type": "AnyTorchTensorType" },
      { "name": "padding_idx", "type": "Torch_IntType" },
      { "name": "scale_grad_by_freq", "type": "Torch_BoolType" },
      { "name": "sparse", "type": "Torch_BoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.embedding_bag.padding_idx",
    "summary": "Generated op for `aten::embedding_bag.padding_idx : (Tensor, Tensor, Tensor, bool, int, bool, Tensor?, bool, int?) -> (Tensor, Tensor, Tensor, Tensor)`",
    "inputs": [
      { "name": "weight", "type": "AnyTorchTensorType" },
      { "name": "indices", "type": "AnyTorchTensorType" },
      { "name": "offsets", "type": "AnyTorchTensorType" },
      { "name": "scale_grad_by_freq", "type": "Torch_BoolType" },
      { "name": "mode", "type": "Torch_IntType" },
      { "name": "sparse", "type": "Torch_BoolType" },
      { "name": "per_sample_weights", "type": "AnyTorchOptionalTensorType" },
      { "name": "include_last_offset", "type": "Torch_BoolType" },
      { "name": "padding_idx", "type": "AnyTorchOptionalIntType" }
    ],
    "outputs": [
      { "name": "result0", "type": "AnyTorchOptionalTensorType" },
      { "name": "result1", "type": "AnyTorchOptionalTensorType" },
      { "name": "result2", "type": "AnyTorchOptionalTensorType" },
      { "name": "result3", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.embedding_dense_backward",
    "summary": "Generated op for `aten::embedding_dense_backward : (Tensor, Tensor, int, int, bool) -> (Tensor)`",
    "inputs": [
      { "name": "grad_output", "type": "AnyTorchTensorType" },
      { "name": "indices", "type": "AnyTorchTensorType" },
      { "name": "num_weights", "type": "Torch_IntType" },
      { "name": "padding_idx", "type": "Torch_IntType" },
      { "name": "scale_grad_by_freq", "type": "Torch_BoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.empty_like",
    "summary": "Generated op for `aten::empty_like : (Tensor, int?, int?, Device?, bool?, int?) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "dtype", "type": "AnyTorchOptionalIntType" },
      { "name": "layout", "type": "AnyTorchOptionalIntType" },
      { "name": "device", "type": "AnyTorchOptionalDeviceType" },
      { "name": "pin_memory", "type": "AnyTorchOptionalBoolType" },
      { "name": "memory_format", "type": "AnyTorchOptionalIntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.empty_strided",
    "summary": "Generated op for `aten::empty_strided : (int[], int[], int?, int?, Device?, bool?) -> (Tensor)`",
    "inputs": [
      { "name": "size", "type": "AnyTorchListOfTorchIntType" },
      { "name": "stride", "type": "AnyTorchListOfTorchIntType" },
      { "name": "dtype", "type": "AnyTorchOptionalIntType" },
      { "name": "layout", "type": "AnyTorchOptionalIntType" },
      { "name": "device", "type": "AnyTorchOptionalDeviceType" },
      { "name": "pin_memory", "type": "AnyTorchOptionalBoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.empty.memory_format",
    "summary": "Generated op for `aten::empty.memory_format : (int[], int?, int?, Device?, bool?, int?) -> (Tensor)`",
    "inputs": [
      { "name": "size", "type": "AnyTorchListOfTorchIntType" },
      { "name": "dtype", "type": "AnyTorchOptionalIntType" },
      { "name": "layout", "type": "AnyTorchOptionalIntType" },
      { "name": "device", "type": "AnyTorchOptionalDeviceType" },
      { "name": "pin_memory", "type": "AnyTorchOptionalBoolType" },
      { "name": "memory_format", "type": "AnyTorchOptionalIntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.eq_.Scalar",
    "summary": "Generated op for `aten::eq_.Scalar : (Tensor, Scalar) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" },
      { "name": "other", "type": "AnyTorchScalarType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.eq_.Tensor",
    "summary": "Generated op for `aten::eq_.Tensor : (Tensor, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" },
      { "name": "other", "type": "Torch_NonValueTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.eq.bool",
    "summary": "Generated op for `aten::eq.bool : (bool, bool) -> (bool)`",
    "inputs": [
      { "name": "a", "type": "Torch_BoolType" },
      { "name": "b", "type": "Torch_BoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "Torch_BoolType" }
    ]
  },
  {
    "name": "torch.aten.eq.device",
    "summary": "Generated op for `aten::eq.device : (Device, Device) -> (bool)`",
    "inputs": [
      { "name": "a", "type": "Torch_DeviceType" },
      { "name": "b", "type": "Torch_DeviceType" }
    ],
    "outputs": [
      { "name": "result", "type": "Torch_BoolType" }
    ]
  },
  {
    "name": "torch.aten.eq.float",
    "summary": "Generated op for `aten::eq.float : (float, float) -> (bool)`",
    "inputs": [
      { "name": "a", "type": "Torch_FloatType" },
      { "name": "b", "type": "Torch_FloatType" }
    ],
    "outputs": [
      { "name": "result", "type": "Torch_BoolType" }
    ]
  },
  {
    "name": "torch.aten.eq.int",
    "summary": "Generated op for `aten::eq.int : (int, int) -> (bool)`",
    "inputs": [
      { "name": "a", "type": "Torch_IntType" },
      { "name": "b", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "Torch_BoolType" }
    ]
  },
  {
    "name": "torch.aten.eq.int_list",
    "summary": "Generated op for `aten::eq.int_list : (int[], int[]) -> (bool)`",
    "inputs": [
      { "name": "a", "type": "AnyTorchListOfTorchIntType" },
      { "name": "b", "type": "AnyTorchListOfTorchIntType" }
    ],
    "outputs": [
      { "name": "result", "type": "Torch_BoolType" }
    ]
  },
  {
    "name": "torch.aten.eq.Scalar",
    "summary": "Generated op for `aten::eq.Scalar : (Tensor, Scalar) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "other", "type": "AnyTorchScalarType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.eq.str",
    "summary": "Generated op for `aten::eq.str : (str, str) -> (bool)`",
    "inputs": [
      { "name": "a", "type": "Torch_StringType" },
      { "name": "b", "type": "Torch_StringType" }
    ],
    "outputs": [
      { "name": "result", "type": "Torch_BoolType" }
    ]
  },
  {
    "name": "torch.aten.eq.Tensor",
    "summary": "Generated op for `aten::eq.Tensor : (Tensor, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "other", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.erf",
    "summary": "Generated op for `aten::erf : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.erf_",
    "summary": "Generated op for `aten::erf_ : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.erfinv",
    "summary": "Generated op for `aten::erfinv : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.erfinv_",
    "summary": "Generated op for `aten::erfinv_ : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.exp",
    "summary": "Generated op for `aten::exp : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.exp_",
    "summary": "Generated op for `aten::exp_ : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.exp2",
    "summary": "Generated op for `aten::exp2 : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.exp2_",
    "summary": "Generated op for `aten::exp2_ : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.expand",
    "summary": "Generated op for `aten::expand : (Tensor, int[], bool) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "size", "type": "AnyTorchListOfTorchIntType" },
      { "name": "implicit", "type": "Torch_BoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.expand_as",
    "summary": "Generated op for `aten::expand_as : (Tensor, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "other", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.expand_copy",
    "summary": "Generated op for `aten::expand_copy : (Tensor, int[], bool) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "size", "type": "AnyTorchListOfTorchIntType" },
      { "name": "implicit", "type": "Torch_BoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.expm1",
    "summary": "Generated op for `aten::expm1 : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.expm1_",
    "summary": "Generated op for `aten::expm1_ : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.exponential",
    "summary": "Generated op for `aten::exponential : (Tensor, float, Generator?) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "lambd", "type": "Torch_FloatType" },
      { "name": "generator", "type": "AnyTorchOptionalGeneratorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.eye",
    "summary": "Generated op for `aten::eye : (int, int?, int?, Device?, bool?) -> (Tensor)`",
    "inputs": [
      { "name": "n", "type": "Torch_IntType" },
      { "name": "dtype", "type": "AnyTorchOptionalIntType" },
      { "name": "layout", "type": "AnyTorchOptionalIntType" },
      { "name": "device", "type": "AnyTorchOptionalDeviceType" },
      { "name": "pin_memory", "type": "AnyTorchOptionalBoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.eye.m",
    "summary": "Generated op for `aten::eye.m : (int, int, int?, int?, Device?, bool?) -> (Tensor)`",
    "inputs": [
      { "name": "n", "type": "Torch_IntType" },
      { "name": "m", "type": "Torch_IntType" },
      { "name": "dtype", "type": "AnyTorchOptionalIntType" },
      { "name": "layout", "type": "AnyTorchOptionalIntType" },
      { "name": "device", "type": "AnyTorchOptionalDeviceType" },
      { "name": "pin_memory", "type": "AnyTorchOptionalBoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.fake_quantize_per_channel_affine",
    "summary": "Generated op for `aten::fake_quantize_per_channel_affine : (Tensor, Tensor, Tensor, int, int, int) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "scale", "type": "AnyTorchTensorType" },
      { "name": "zero_point", "type": "AnyTorchTensorType" },
      { "name": "axis", "type": "Torch_IntType" },
      { "name": "quant_min", "type": "Torch_IntType" },
      { "name": "quant_max", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.fake_quantize_per_channel_affine_cachemask",
    "summary": "Generated op for `aten::fake_quantize_per_channel_affine_cachemask : (Tensor, Tensor, Tensor, int, int, int) -> (Tensor, Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "scale", "type": "AnyTorchTensorType" },
      { "name": "zero_point", "type": "AnyTorchTensorType" },
      { "name": "axis", "type": "Torch_IntType" },
      { "name": "quant_min", "type": "Torch_IntType" },
      { "name": "quant_max", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTorchOptionalTensorType" },
      { "name": "mask", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.fake_quantize_per_tensor_affine",
    "summary": "Generated op for `aten::fake_quantize_per_tensor_affine : (Tensor, float, int, int, int) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "scale", "type": "Torch_FloatType" },
      { "name": "zero_point", "type": "Torch_IntType" },
      { "name": "quant_min", "type": "Torch_IntType" },
      { "name": "quant_max", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.fake_quantize_per_tensor_affine_cachemask",
    "summary": "Generated op for `aten::fake_quantize_per_tensor_affine_cachemask : (Tensor, float, int, int, int) -> (Tensor, Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "scale", "type": "Torch_FloatType" },
      { "name": "zero_point", "type": "Torch_IntType" },
      { "name": "quant_min", "type": "Torch_IntType" },
      { "name": "quant_max", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTorchOptionalTensorType" },
      { "name": "mask", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.fake_quantize_per_tensor_affine.tensor_qparams",
    "summary": "Generated op for `aten::fake_quantize_per_tensor_affine.tensor_qparams : (Tensor, Tensor, Tensor, int, int) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "scale", "type": "AnyTorchTensorType" },
      { "name": "zero_point", "type": "AnyTorchTensorType" },
      { "name": "quant_min", "type": "Torch_IntType" },
      { "name": "quant_max", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.fft_fft",
    "summary": "Generated op for `aten::fft_fft : (Tensor, int?, int, str?) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "n", "type": "AnyTorchOptionalIntType" },
      { "name": "dim", "type": "Torch_IntType" },
      { "name": "norm", "type": "AnyTorchOptionalStringType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.fft_ifft",
    "summary": "Generated op for `aten::fft_ifft : (Tensor, int?, int, str?) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "n", "type": "AnyTorchOptionalIntType" },
      { "name": "dim", "type": "Torch_IntType" },
      { "name": "norm", "type": "AnyTorchOptionalStringType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.fft_rfft",
    "summary": "Generated op for `aten::fft_rfft : (Tensor, int?, int, str?) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "n", "type": "AnyTorchOptionalIntType" },
      { "name": "dim", "type": "Torch_IntType" },
      { "name": "norm", "type": "AnyTorchOptionalStringType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.fill_.Scalar",
    "summary": "Generated op for `aten::fill_.Scalar : (Tensor, Scalar) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" },
      { "name": "value", "type": "AnyTorchScalarType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.fill_.Tensor",
    "summary": "Generated op for `aten::fill_.Tensor : (Tensor, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" },
      { "name": "value", "type": "Torch_NonValueTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.fill.Scalar",
    "summary": "Generated op for `aten::fill.Scalar : (Tensor, Scalar) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "value", "type": "AnyTorchScalarType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.fill.Tensor",
    "summary": "Generated op for `aten::fill.Tensor : (Tensor, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "value", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.fix",
    "summary": "Generated op for `aten::fix : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.fix_",
    "summary": "Generated op for `aten::fix_ : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.flatten.using_ints",
    "summary": "Generated op for `aten::flatten.using_ints : (Tensor, int, int) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "start_dim", "type": "Torch_IntType" },
      { "name": "end_dim", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.flip",
    "summary": "Generated op for `aten::flip : (Tensor, int[]) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "dims", "type": "AnyTorchListOfTorchIntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.fliplr",
    "summary": "Generated op for `aten::fliplr : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.flipud",
    "summary": "Generated op for `aten::flipud : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.float_power.Tensor_Tensor",
    "summary": "Generated op for `aten::float_power.Tensor_Tensor : (Tensor, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "exponent", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.Float.Scalar",
    "summary": "Generated op for `aten::Float.Scalar : (Scalar) -> (float)`",
    "inputs": [
      { "name": "a", "type": "AnyTorchScalarType" }
    ],
    "outputs": [
      { "name": "result", "type": "Torch_FloatType" }
    ]
  },
  {
    "name": "torch.aten.Float.str",
    "summary": "Generated op for `aten::Float.str : (str) -> (float)`",
    "inputs": [
      { "name": "a", "type": "Torch_StringType" }
    ],
    "outputs": [
      { "name": "result", "type": "Torch_FloatType" }
    ]
  },
  {
    "name": "torch.aten.Float.Tensor",
    "summary": "Generated op for `aten::Float.Tensor : (Tensor) -> (float)`",
    "inputs": [
      { "name": "a", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "Torch_FloatType" }
    ]
  },
  {
    "name": "torch.aten.FloatImplicit",
    "summary": "Generated op for `aten::FloatImplicit : (Tensor) -> (float)`",
    "inputs": [
      { "name": "a", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "Torch_FloatType" }
    ]
  },
  {
    "name": "torch.aten.floor",
    "summary": "Generated op for `aten::floor : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.floor_",
    "summary": "Generated op for `aten::floor_ : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.floor_divide",
    "summary": "Generated op for `aten::floor_divide : (Tensor, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "other", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.floor_divide.Scalar",
    "summary": "Generated op for `aten::floor_divide.Scalar : (Tensor, Scalar) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "other", "type": "AnyTorchScalarType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.floordiv.int",
    "summary": "Generated op for `aten::floordiv.int : (int, int) -> (int)`",
    "inputs": [
      { "name": "a", "type": "Torch_IntType" },
      { "name": "b", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "Torch_IntType" }
    ]
  },
  {
    "name": "torch.aten.fmax",
    "summary": "Generated op for `aten::fmax : (Tensor, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "other", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.fmin",
    "summary": "Generated op for `aten::fmin : (Tensor, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "other", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.fmod_.Scalar",
    "summary": "Generated op for `aten::fmod_.Scalar : (Tensor, Scalar) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" },
      { "name": "other", "type": "AnyTorchScalarType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.fmod.Scalar",
    "summary": "Generated op for `aten::fmod.Scalar : (Tensor, Scalar) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "other", "type": "AnyTorchScalarType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.fmod.Tensor",
    "summary": "Generated op for `aten::fmod.Tensor : (Tensor, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "other", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.format",
    "summary": "Generated op for `aten::format : (...) -> (str)`",
    "inputs": [
      { "name": "operands", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "result", "type": "Torch_StringType" }
    ],
    "assemblyFormat": "`(` $operands `)` attr-dict `:` qualified(type($operands)) `->` qualified(type($result))"
  },
  {
    "name": "torch.aten.frac",
    "summary": "Generated op for `aten::frac : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.frac_",
    "summary": "Generated op for `aten::frac_ : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.frobenius_norm.dim",
    "summary": "Generated op for `aten::frobenius_norm.dim : (Tensor, int[], bool) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "dim", "type": "AnyTorchListOfTorchIntType" },
      { "name": "keepdim", "type": "Torch_BoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.full",
    "summary": "Generated op for `aten::full : (int[], Scalar, int?, int?, Device?, bool?) -> (Tensor)`",
    "inputs": [
      { "name": "size", "type": "AnyTorchListOfTorchIntType" },
      { "name": "fill_value", "type": "AnyTorchScalarType" },
      { "name": "dtype", "type": "AnyTorchOptionalIntType" },
      { "name": "layout", "type": "AnyTorchOptionalIntType" },
      { "name": "device", "type": "AnyTorchOptionalDeviceType" },
      { "name": "pin_memory", "type": "AnyTorchOptionalBoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.full_like",
    "summary": "Generated op for `aten::full_like : (Tensor, Scalar, int?, int?, Device?, bool?, int?) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "fill_value", "type": "AnyTorchScalarType" },
      { "name": "dtype", "type": "AnyTorchOptionalIntType" },
      { "name": "layout", "type": "AnyTorchOptionalIntType" },
      { "name": "device", "type": "AnyTorchOptionalDeviceType" },
      { "name": "pin_memory", "type": "AnyTorchOptionalBoolType" },
      { "name": "memory_format", "type": "AnyTorchOptionalIntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.gather",
    "summary": "Generated op for `aten::gather : (Tensor, int, Tensor, bool) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "dim", "type": "Torch_IntType" },
      { "name": "index", "type": "AnyTorchTensorType" },
      { "name": "sparse_grad", "type": "Torch_BoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.ge_.Scalar",
    "summary": "Generated op for `aten::ge_.Scalar : (Tensor, Scalar) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" },
      { "name": "other", "type": "AnyTorchScalarType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.ge_.Tensor",
    "summary": "Generated op for `aten::ge_.Tensor : (Tensor, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" },
      { "name": "other", "type": "Torch_NonValueTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.ge.float",
    "summary": "Generated op for `aten::ge.float : (float, float) -> (bool)`",
    "inputs": [
      { "name": "a", "type": "Torch_FloatType" },
      { "name": "b", "type": "Torch_FloatType" }
    ],
    "outputs": [
      { "name": "result", "type": "Torch_BoolType" }
    ]
  },
  {
    "name": "torch.aten.ge.float_int",
    "summary": "Generated op for `aten::ge.float_int : (float, int) -> (bool)`",
    "inputs": [
      { "name": "a", "type": "Torch_FloatType" },
      { "name": "b", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "Torch_BoolType" }
    ]
  },
  {
    "name": "torch.aten.ge.int",
    "summary": "Generated op for `aten::ge.int : (int, int) -> (bool)`",
    "inputs": [
      { "name": "a", "type": "Torch_IntType" },
      { "name": "b", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "Torch_BoolType" }
    ]
  },
  {
    "name": "torch.aten.ge.Scalar",
    "summary": "Generated op for `aten::ge.Scalar : (Tensor, Scalar) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "other", "type": "AnyTorchScalarType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.ge.Tensor",
    "summary": "Generated op for `aten::ge.Tensor : (Tensor, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "other", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.gelu",
    "summary": "Generated op for `aten::gelu : (Tensor, str) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "approximate", "type": "Torch_StringType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.gelu_backward",
    "summary": "Generated op for `aten::gelu_backward : (Tensor, Tensor, str) -> (Tensor)`",
    "inputs": [
      { "name": "grad_output", "type": "AnyTorchTensorType" },
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "approximate", "type": "Torch_StringType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.get.default_str",
    "summary": "Generated op for `aten::get.default_str : (Dict(str, t), str, t) -> (t)`",
    "inputs": [
      { "name": "self", "type": "Torch_DictType" },
      { "name": "key", "type": "Torch_StringType" },
      { "name": "default_value", "type": "AnyTorchType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchType" }
    ]
  },
  {
    "name": "torch.aten.glu",
    "summary": "Generated op for `aten::glu : (Tensor, int) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "dim", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.grid_sampler",
    "summary": "Generated op for `aten::grid_sampler : (Tensor, Tensor, int, int, bool) -> (Tensor)`",
    "inputs": [
      { "name": "input", "type": "AnyTorchTensorType" },
      { "name": "grid", "type": "AnyTorchTensorType" },
      { "name": "interpolation_mode", "type": "Torch_IntType" },
      { "name": "padding_mode", "type": "Torch_IntType" },
      { "name": "align_corners", "type": "Torch_BoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.group_norm",
    "summary": "Generated op for `aten::group_norm : (Tensor, int, Tensor?, Tensor?, float, bool) -> (Tensor)`",
    "inputs": [
      { "name": "input", "type": "AnyTorchTensorType" },
      { "name": "num_groups", "type": "Torch_IntType" },
      { "name": "weight", "type": "AnyTorchOptionalTensorType" },
      { "name": "bias", "type": "AnyTorchOptionalTensorType" },
      { "name": "eps", "type": "Torch_FloatType" },
      { "name": "cudnn_enabled", "type": "Torch_BoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.gt_.Scalar",
    "summary": "Generated op for `aten::gt_.Scalar : (Tensor, Scalar) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" },
      { "name": "other", "type": "AnyTorchScalarType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.gt_.Tensor",
    "summary": "Generated op for `aten::gt_.Tensor : (Tensor, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" },
      { "name": "other", "type": "Torch_NonValueTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.gt.float",
    "summary": "Generated op for `aten::gt.float : (float, float) -> (bool)`",
    "inputs": [
      { "name": "a", "type": "Torch_FloatType" },
      { "name": "b", "type": "Torch_FloatType" }
    ],
    "outputs": [
      { "name": "result", "type": "Torch_BoolType" }
    ]
  },
  {
    "name": "torch.aten.gt.float_int",
    "summary": "Generated op for `aten::gt.float_int : (float, int) -> (bool)`",
    "inputs": [
      { "name": "a", "type": "Torch_FloatType" },
      { "name": "b", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "Torch_BoolType" }
    ]
  },
  {
    "name": "torch.aten.gt.int",
    "summary": "Generated op for `aten::gt.int : (int, int) -> (bool)`",
    "inputs": [
      { "name": "a", "type": "Torch_IntType" },
      { "name": "b", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "Torch_BoolType" }
    ]
  },
  {
    "name": "torch.aten.gt.Scalar",
    "summary": "Generated op for `aten::gt.Scalar : (Tensor, Scalar) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "other", "type": "AnyTorchScalarType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.gt.Tensor",
    "summary": "Generated op for `aten::gt.Tensor : (Tensor, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "other", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.hann_window.periodic",
    "summary": "Generated op for `aten::hann_window.periodic : (int, bool, int?, int?, Device?, bool?) -> (Tensor)`",
    "inputs": [
      { "name": "window_length", "type": "Torch_IntType" },
      { "name": "periodic", "type": "Torch_BoolType" },
      { "name": "dtype", "type": "AnyTorchOptionalIntType" },
      { "name": "layout", "type": "AnyTorchOptionalIntType" },
      { "name": "device", "type": "AnyTorchOptionalDeviceType" },
      { "name": "pin_memory", "type": "AnyTorchOptionalBoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.hardshrink",
    "summary": "Generated op for `aten::hardshrink : (Tensor, Scalar) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "lambd", "type": "AnyTorchScalarType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.hardsigmoid",
    "summary": "Generated op for `aten::hardsigmoid : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.hardsigmoid_",
    "summary": "Generated op for `aten::hardsigmoid_ : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.hardswish",
    "summary": "Generated op for `aten::hardswish : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.hardswish_",
    "summary": "Generated op for `aten::hardswish_ : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.hardtanh",
    "summary": "Generated op for `aten::hardtanh : (Tensor, Scalar, Scalar) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "min_val", "type": "AnyTorchScalarType" },
      { "name": "max_val", "type": "AnyTorchScalarType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.hardtanh_",
    "summary": "Generated op for `aten::hardtanh_ : (Tensor, Scalar, Scalar) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" },
      { "name": "min_val", "type": "AnyTorchScalarType" },
      { "name": "max_val", "type": "AnyTorchScalarType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.hardtanh_backward",
    "summary": "Generated op for `aten::hardtanh_backward : (Tensor, Tensor, Scalar, Scalar) -> (Tensor)`",
    "inputs": [
      { "name": "grad_output", "type": "AnyTorchTensorType" },
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "min_val", "type": "AnyTorchScalarType" },
      { "name": "max_val", "type": "AnyTorchScalarType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.heaviside",
    "summary": "Generated op for `aten::heaviside : (Tensor, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "values", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.heaviside_",
    "summary": "Generated op for `aten::heaviside_ : (Tensor, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" },
      { "name": "values", "type": "Torch_NonValueTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.hstack",
    "summary": "Generated op for `aten::hstack : (Tensor[]) -> (Tensor)`",
    "inputs": [
      { "name": "tensors", "type": "AnyTorchListOfTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.im2col",
    "summary": "Generated op for `aten::im2col : (Tensor, int[], int[], int[], int[]) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "kernel_size", "type": "AnyTorchListOfTorchIntType" },
      { "name": "dilation", "type": "AnyTorchListOfTorchIntType" },
      { "name": "padding", "type": "AnyTorchListOfTorchIntType" },
      { "name": "stride", "type": "AnyTorchListOfTorchIntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.imag",
    "summary": "Generated op for `aten::imag : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.index_put",
    "summary": "Generated op for `aten::index_put : (Tensor, Tensor?[], Tensor, bool) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "indices", "type": "AnyTorchListOfOptionalTensorType" },
      { "name": "values", "type": "AnyTorchTensorType" },
      { "name": "accumulate", "type": "Torch_BoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.index_put_",
    "summary": "Generated op for `aten::index_put_ : (Tensor, Tensor?[], Tensor, bool) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" },
      { "name": "indices", "type": "AnyTorchListOfOptionalNonValueTensorType" },
      { "name": "values", "type": "Torch_NonValueTensorType" },
      { "name": "accumulate", "type": "Torch_BoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.index_put_.hacked_twin",
    "summary": "Generated op for `aten::index_put_.hacked_twin : (Tensor, Tensor[], Tensor, bool) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" },
      { "name": "indices", "type": "AnyTorchListOfNonValueTensorType" },
      { "name": "values", "type": "Torch_NonValueTensorType" },
      { "name": "accumulate", "type": "Torch_BoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.index_put.hacked_twin",
    "summary": "Generated op for `aten::index_put.hacked_twin : (Tensor, Tensor[], Tensor, bool) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "indices", "type": "AnyTorchListOfTensorType" },
      { "name": "values", "type": "AnyTorchTensorType" },
      { "name": "accumulate", "type": "Torch_BoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.index_select",
    "summary": "Generated op for `aten::index_select : (Tensor, int, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "dim", "type": "Torch_IntType" },
      { "name": "index", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.index.Tensor",
    "summary": "Generated op for `aten::index.Tensor : (Tensor, Tensor?[]) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "indices", "type": "AnyTorchListOfOptionalTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.index.Tensor_hacked_twin",
    "summary": "Generated op for `aten::index.Tensor_hacked_twin : (Tensor, Tensor[]) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "indices", "type": "AnyTorchListOfTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.insert.t",
    "summary": "Generated op for `aten::insert.t : (t[], int, t) -> ()`",
    "inputs": [
      { "name": "self", "type": "AnyTorchListType" },
      { "name": "idx", "type": "Torch_IntType" },
      { "name": "el", "type": "AnyTorchType" }
    ]
  },
  {
    "name": "torch.aten.instance_norm",
    "summary": "Generated op for `aten::instance_norm : (Tensor, Tensor?, Tensor?, Tensor?, Tensor?, bool, float, float, bool) -> (Tensor)`",
    "inputs": [
      { "name": "input", "type": "AnyTorchTensorType" },
      { "name": "weight", "type": "AnyTorchOptionalTensorType" },
      { "name": "bias", "type": "AnyTorchOptionalTensorType" },
      { "name": "running_mean", "type": "AnyTorchOptionalTensorType" },
      { "name": "running_var", "type": "AnyTorchOptionalTensorType" },
      { "name": "use_input_stats", "type": "Torch_BoolType" },
      { "name": "momentum", "type": "Torch_FloatType" },
      { "name": "eps", "type": "Torch_FloatType" },
      { "name": "cudnn_enabled", "type": "Torch_BoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.int_repr",
    "summary": "Generated op for `aten::int_repr : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.Int.bool",
    "summary": "Generated op for `aten::Int.bool : (bool) -> (int)`",
    "inputs": [
      { "name": "a", "type": "Torch_BoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "Torch_IntType" }
    ]
  },
  {
    "name": "torch.aten.Int.float",
    "summary": "Generated op for `aten::Int.float : (float) -> (int)`",
    "inputs": [
      { "name": "a", "type": "Torch_FloatType" }
    ],
    "outputs": [
      { "name": "result", "type": "Torch_IntType" }
    ]
  },
  {
    "name": "torch.aten.Int.Scalar",
    "summary": "Generated op for `aten::Int.Scalar : (Scalar) -> (int)`",
    "inputs": [
      { "name": "a", "type": "AnyTorchScalarType" }
    ],
    "outputs": [
      { "name": "result", "type": "Torch_IntType" }
    ]
  },
  {
    "name": "torch.aten.Int.Tensor",
    "summary": "Generated op for `aten::Int.Tensor : (Tensor) -> (int)`",
    "inputs": [
      { "name": "a", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "Torch_IntType" }
    ]
  },
  {
    "name": "torch.aten.IntImplicit",
    "summary": "Generated op for `aten::IntImplicit : (Tensor) -> (int)`",
    "inputs": [
      { "name": "a", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "Torch_IntType" }
    ]
  },
  {
    "name": "torch.aten.is_floating_point",
    "summary": "Generated op for `aten::is_floating_point : (Tensor) -> (bool)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "Torch_BoolType" }
    ]
  },
  {
    "name": "torch.aten.isclose",
    "summary": "Generated op for `aten::isclose : (Tensor, Tensor, float, float, bool) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "other", "type": "AnyTorchTensorType" },
      { "name": "rtol", "type": "Torch_FloatType" },
      { "name": "atol", "type": "Torch_FloatType" },
      { "name": "equal_nan", "type": "Torch_BoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.isfinite",
    "summary": "Generated op for `aten::isfinite : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.isinf",
    "summary": "Generated op for `aten::isinf : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.isnan",
    "summary": "Generated op for `aten::isnan : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.isneginf",
    "summary": "Generated op for `aten::isneginf : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.isposinf",
    "summary": "Generated op for `aten::isposinf : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.item",
    "summary": "Generated op for `aten::item : (Tensor) -> (Scalar)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchScalarType" }
    ]
  },
  {
    "name": "torch.aten.join",
    "summary": "Generated op for `aten::join : (str, str[]) -> (str)`",
    "inputs": [
      { "name": "self", "type": "Torch_StringType" },
      { "name": "values", "type": "AnyTorchListOfTorchStringType" }
    ],
    "outputs": [
      { "name": "result", "type": "Torch_StringType" }
    ]
  },
  {
    "name": "torch.aten.keys.str",
    "summary": "Generated op for `aten::keys.str : (Dict(str, t)) -> (str[])`",
    "inputs": [
      { "name": "self", "type": "Torch_DictType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchListOfTorchStringType" }
    ]
  },
  {
    "name": "torch.aten.kl_div",
    "summary": "Generated op for `aten::kl_div : (Tensor, Tensor, int, bool) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "target", "type": "AnyTorchTensorType" },
      { "name": "reduction", "type": "Torch_IntType" },
      { "name": "log_target", "type": "Torch_BoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.kthvalue",
    "summary": "Generated op for `aten::kthvalue : (Tensor, int, int, bool) -> (Tensor, Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "k", "type": "Torch_IntType" },
      { "name": "dim", "type": "Torch_IntType" },
      { "name": "keepdim", "type": "Torch_BoolType" }
    ],
    "outputs": [
      { "name": "values", "type": "AnyTorchOptionalTensorType" },
      { "name": "indices", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.l1_loss",
    "summary": "Generated op for `aten::l1_loss : (Tensor, Tensor, int) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "target", "type": "AnyTorchTensorType" },
      { "name": "reduction", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.layer_norm",
    "summary": "Generated op for `aten::layer_norm : (Tensor, int[], Tensor?, Tensor?, float, bool) -> (Tensor)`",
    "inputs": [
      { "name": "input", "type": "AnyTorchTensorType" },
      { "name": "normalized_shape", "type": "AnyTorchListOfTorchIntType" },
      { "name": "weight", "type": "AnyTorchOptionalTensorType" },
      { "name": "bias", "type": "AnyTorchOptionalTensorType" },
      { "name": "eps", "type": "Torch_FloatType" },
      { "name": "cudnn_enable", "type": "Torch_BoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.ldexp.Tensor",
    "summary": "Generated op for `aten::ldexp.Tensor : (Tensor, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "other", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.le_.Scalar",
    "summary": "Generated op for `aten::le_.Scalar : (Tensor, Scalar) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" },
      { "name": "other", "type": "AnyTorchScalarType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.le_.Tensor",
    "summary": "Generated op for `aten::le_.Tensor : (Tensor, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" },
      { "name": "other", "type": "Torch_NonValueTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.le.int",
    "summary": "Generated op for `aten::le.int : (int, int) -> (bool)`",
    "inputs": [
      { "name": "a", "type": "Torch_IntType" },
      { "name": "b", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "Torch_BoolType" }
    ]
  },
  {
    "name": "torch.aten.le.Scalar",
    "summary": "Generated op for `aten::le.Scalar : (Tensor, Scalar) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "other", "type": "AnyTorchScalarType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.le.Tensor",
    "summary": "Generated op for `aten::le.Tensor : (Tensor, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "other", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.leaky_relu",
    "summary": "Generated op for `aten::leaky_relu : (Tensor, Scalar) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "negative_slope", "type": "AnyTorchScalarType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.leaky_relu_",
    "summary": "Generated op for `aten::leaky_relu_ : (Tensor, Scalar) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" },
      { "name": "negative_slope", "type": "AnyTorchScalarType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.leaky_relu_backward",
    "summary": "Generated op for `aten::leaky_relu_backward : (Tensor, Tensor, Scalar, bool) -> (Tensor)`",
    "inputs": [
      { "name": "grad_output", "type": "AnyTorchTensorType" },
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "negative_slope", "type": "AnyTorchScalarType" },
      { "name": "self_is_result", "type": "Torch_BoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.len.str",
    "summary": "Generated op for `aten::len.str : (str) -> (int)`",
    "inputs": [
      { "name": "s", "type": "Torch_StringType" }
    ],
    "outputs": [
      { "name": "result", "type": "Torch_IntType" }
    ]
  },
  {
    "name": "torch.aten.len.t",
    "summary": "Generated op for `aten::len.t : (t[]) -> (int)`",
    "inputs": [
      { "name": "a", "type": "AnyTorchListType" }
    ],
    "outputs": [
      { "name": "result", "type": "Torch_IntType" }
    ]
  },
  {
    "name": "torch.aten.len.Tensor",
    "summary": "Generated op for `aten::len.Tensor : (Tensor) -> (int)`",
    "inputs": [
      { "name": "t", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "Torch_IntType" }
    ]
  },
  {
    "name": "torch.aten.lerp_.Scalar",
    "summary": "Generated op for `aten::lerp_.Scalar : (Tensor, Tensor, Scalar) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" },
      { "name": "end", "type": "Torch_NonValueTensorType" },
      { "name": "weight", "type": "AnyTorchScalarType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.lerp_.Tensor",
    "summary": "Generated op for `aten::lerp_.Tensor : (Tensor, Tensor, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" },
      { "name": "end", "type": "Torch_NonValueTensorType" },
      { "name": "weight", "type": "Torch_NonValueTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.lerp.Scalar",
    "summary": "Generated op for `aten::lerp.Scalar : (Tensor, Tensor, Scalar) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "end", "type": "AnyTorchTensorType" },
      { "name": "weight", "type": "AnyTorchScalarType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.lerp.Tensor",
    "summary": "Generated op for `aten::lerp.Tensor : (Tensor, Tensor, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "end", "type": "AnyTorchTensorType" },
      { "name": "weight", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.lift_fresh_copy",
    "summary": "Generated op for `aten::lift_fresh_copy : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.linalg_cross",
    "summary": "Generated op for `aten::linalg_cross : (Tensor, Tensor, int) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "other", "type": "AnyTorchTensorType" },
      { "name": "dim", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.linalg_det",
    "summary": "Generated op for `aten::linalg_det : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "A", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.linalg_norm",
    "summary": "Generated op for `aten::linalg_norm : (Tensor, Scalar?, int[]?, bool, int?) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "ord", "type": "AnyTorchOptionalScalarType" },
      { "name": "dim", "type": "AnyTorchOptionalListOfTorchIntType" },
      { "name": "keepdim", "type": "Torch_BoolType" },
      { "name": "dtype", "type": "AnyTorchOptionalIntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.linalg_qr",
    "summary": "Generated op for `aten::linalg_qr : (Tensor, str) -> (Tensor, Tensor)`",
    "inputs": [
      { "name": "A", "type": "AnyTorchTensorType" },
      { "name": "mode", "type": "Torch_StringType" }
    ],
    "outputs": [
      { "name": "Q", "type": "AnyTorchOptionalTensorType" },
      { "name": "R", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.linalg_slogdet",
    "summary": "Generated op for `aten::linalg_slogdet : (Tensor) -> (Tensor, Tensor)`",
    "inputs": [
      { "name": "A", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "sign", "type": "AnyTorchOptionalTensorType" },
      { "name": "logabsdet", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.linalg_vector_norm",
    "summary": "Generated op for `aten::linalg_vector_norm : (Tensor, Scalar, int[]?, bool, int?) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "ord", "type": "AnyTorchScalarType" },
      { "name": "dim", "type": "AnyTorchOptionalListOfTorchIntType" },
      { "name": "keepdim", "type": "Torch_BoolType" },
      { "name": "dtype", "type": "AnyTorchOptionalIntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.linear",
    "summary": "Generated op for `aten::linear : (Tensor, Tensor, Tensor?) -> (Tensor)`",
    "inputs": [
      { "name": "input", "type": "AnyTorchTensorType" },
      { "name": "weight", "type": "AnyTorchTensorType" },
      { "name": "bias", "type": "AnyTorchOptionalTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.linspace",
    "summary": "Generated op for `aten::linspace : (Scalar, Scalar, int, int?, int?, Device?, bool?) -> (Tensor)`",
    "inputs": [
      { "name": "start", "type": "AnyTorchScalarType" },
      { "name": "end", "type": "AnyTorchScalarType" },
      { "name": "steps", "type": "Torch_IntType" },
      { "name": "dtype", "type": "AnyTorchOptionalIntType" },
      { "name": "layout", "type": "AnyTorchOptionalIntType" },
      { "name": "device", "type": "AnyTorchOptionalDeviceType" },
      { "name": "pin_memory", "type": "AnyTorchOptionalBoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.list.t",
    "summary": "Generated op for `aten::list.t : (t[]) -> (t[])`",
    "inputs": [
      { "name": "l", "type": "AnyTorchListType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchListType" }
    ]
  },
  {
    "name": "torch.aten.log",
    "summary": "Generated op for `aten::log : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.log_",
    "summary": "Generated op for `aten::log_ : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.log_sigmoid",
    "summary": "Generated op for `aten::log_sigmoid : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.log_sigmoid_backward",
    "summary": "Generated op for `aten::log_sigmoid_backward : (Tensor, Tensor, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "grad_output", "type": "AnyTorchTensorType" },
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "buffer", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.log_sigmoid_forward",
    "summary": "Generated op for `aten::log_sigmoid_forward : (Tensor) -> (Tensor, Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTorchOptionalTensorType" },
      { "name": "buffer", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.log_softmax.int",
    "summary": "Generated op for `aten::log_softmax.int : (Tensor, int, int?) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "dim", "type": "Torch_IntType" },
      { "name": "dtype", "type": "AnyTorchOptionalIntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.log.int",
    "summary": "Generated op for `aten::log.int : (int) -> (float)`",
    "inputs": [
      { "name": "a", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "Torch_FloatType" }
    ]
  },
  {
    "name": "torch.aten.log10",
    "summary": "Generated op for `aten::log10 : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.log10_",
    "summary": "Generated op for `aten::log10_ : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.log1p",
    "summary": "Generated op for `aten::log1p : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.log1p_",
    "summary": "Generated op for `aten::log1p_ : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.log2",
    "summary": "Generated op for `aten::log2 : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.log2_",
    "summary": "Generated op for `aten::log2_ : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.logaddexp",
    "summary": "Generated op for `aten::logaddexp : (Tensor, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "other", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.logaddexp2",
    "summary": "Generated op for `aten::logaddexp2 : (Tensor, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "other", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.logcumsumexp",
    "summary": "Generated op for `aten::logcumsumexp : (Tensor, int) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "dim", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.logical_and",
    "summary": "Generated op for `aten::logical_and : (Tensor, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "other", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.logical_and_",
    "summary": "Generated op for `aten::logical_and_ : (Tensor, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" },
      { "name": "other", "type": "Torch_NonValueTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.logical_not",
    "summary": "Generated op for `aten::logical_not : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.logical_not_",
    "summary": "Generated op for `aten::logical_not_ : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.logical_or",
    "summary": "Generated op for `aten::logical_or : (Tensor, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "other", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.logical_or_",
    "summary": "Generated op for `aten::logical_or_ : (Tensor, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" },
      { "name": "other", "type": "Torch_NonValueTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.logical_xor",
    "summary": "Generated op for `aten::logical_xor : (Tensor, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "other", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.logical_xor_",
    "summary": "Generated op for `aten::logical_xor_ : (Tensor, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" },
      { "name": "other", "type": "Torch_NonValueTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.logit",
    "summary": "Generated op for `aten::logit : (Tensor, float?) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "eps", "type": "AnyTorchOptionalFloatType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.logit_",
    "summary": "Generated op for `aten::logit_ : (Tensor, float?) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" },
      { "name": "eps", "type": "AnyTorchOptionalFloatType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.logsumexp",
    "summary": "Generated op for `aten::logsumexp : (Tensor, int[], bool) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "dim", "type": "AnyTorchListOfTorchIntType" },
      { "name": "keepdim", "type": "Torch_BoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.lt_.Scalar",
    "summary": "Generated op for `aten::lt_.Scalar : (Tensor, Scalar) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" },
      { "name": "other", "type": "AnyTorchScalarType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.lt_.Tensor",
    "summary": "Generated op for `aten::lt_.Tensor : (Tensor, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" },
      { "name": "other", "type": "Torch_NonValueTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.lt.float",
    "summary": "Generated op for `aten::lt.float : (float, float) -> (bool)`",
    "inputs": [
      { "name": "a", "type": "Torch_FloatType" },
      { "name": "b", "type": "Torch_FloatType" }
    ],
    "outputs": [
      { "name": "result", "type": "Torch_BoolType" }
    ]
  },
  {
    "name": "torch.aten.lt.float_int",
    "summary": "Generated op for `aten::lt.float_int : (float, int) -> (bool)`",
    "inputs": [
      { "name": "a", "type": "Torch_FloatType" },
      { "name": "b", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "Torch_BoolType" }
    ]
  },
  {
    "name": "torch.aten.lt.int",
    "summary": "Generated op for `aten::lt.int : (int, int) -> (bool)`",
    "inputs": [
      { "name": "a", "type": "Torch_IntType" },
      { "name": "b", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "Torch_BoolType" }
    ]
  },
  {
    "name": "torch.aten.lt.Scalar",
    "summary": "Generated op for `aten::lt.Scalar : (Tensor, Scalar) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "other", "type": "AnyTorchScalarType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.lt.Tensor",
    "summary": "Generated op for `aten::lt.Tensor : (Tensor, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "other", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.masked_fill_.Scalar",
    "summary": "Generated op for `aten::masked_fill_.Scalar : (Tensor, Tensor, Scalar) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" },
      { "name": "mask", "type": "Torch_NonValueTensorType" },
      { "name": "value", "type": "AnyTorchScalarType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.masked_fill_.Tensor",
    "summary": "Generated op for `aten::masked_fill_.Tensor : (Tensor, Tensor, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" },
      { "name": "mask", "type": "Torch_NonValueTensorType" },
      { "name": "value", "type": "Torch_NonValueTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.masked_fill.Scalar",
    "summary": "Generated op for `aten::masked_fill.Scalar : (Tensor, Tensor, Scalar) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "mask", "type": "AnyTorchTensorType" },
      { "name": "value", "type": "AnyTorchScalarType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.masked_fill.Tensor",
    "summary": "Generated op for `aten::masked_fill.Tensor : (Tensor, Tensor, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "mask", "type": "AnyTorchTensorType" },
      { "name": "value", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.masked_scatter",
    "summary": "Generated op for `aten::masked_scatter : (Tensor, Tensor, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "mask", "type": "AnyTorchTensorType" },
      { "name": "source", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.masked_scatter_",
    "summary": "Generated op for `aten::masked_scatter_ : (Tensor, Tensor, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" },
      { "name": "mask", "type": "Torch_NonValueTensorType" },
      { "name": "source", "type": "Torch_NonValueTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.masked_select",
    "summary": "Generated op for `aten::masked_select : (Tensor, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "mask", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.matmul",
    "summary": "Generated op for `aten::matmul : (Tensor, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "other", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.max",
    "summary": "Generated op for `aten::max : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.max_pool1d",
    "summary": "Generated op for `aten::max_pool1d : (Tensor, int[], int[], int[], int[], bool) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "kernel_size", "type": "AnyTorchListOfTorchIntType" },
      { "name": "stride", "type": "AnyTorchListOfTorchIntType" },
      { "name": "padding", "type": "AnyTorchListOfTorchIntType" },
      { "name": "dilation", "type": "AnyTorchListOfTorchIntType" },
      { "name": "ceil_mode", "type": "Torch_BoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.max_pool1d_with_indices",
    "summary": "Generated op for `aten::max_pool1d_with_indices : (Tensor, int[], int[], int[], int[], bool) -> (Tensor, Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "kernel_size", "type": "AnyTorchListOfTorchIntType" },
      { "name": "stride", "type": "AnyTorchListOfTorchIntType" },
      { "name": "padding", "type": "AnyTorchListOfTorchIntType" },
      { "name": "dilation", "type": "AnyTorchListOfTorchIntType" },
      { "name": "ceil_mode", "type": "Torch_BoolType" }
    ],
    "outputs": [
      { "name": "result0", "type": "AnyTorchOptionalTensorType" },
      { "name": "result1", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.max_pool2d",
    "summary": "Generated op for `aten::max_pool2d : (Tensor, int[], int[], int[], int[], bool) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "kernel_size", "type": "AnyTorchListOfTorchIntType" },
      { "name": "stride", "type": "AnyTorchListOfTorchIntType" },
      { "name": "padding", "type": "AnyTorchListOfTorchIntType" },
      { "name": "dilation", "type": "AnyTorchListOfTorchIntType" },
      { "name": "ceil_mode", "type": "Torch_BoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.max_pool2d_with_indices",
    "summary": "Generated op for `aten::max_pool2d_with_indices : (Tensor, int[], int[], int[], int[], bool) -> (Tensor, Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "kernel_size", "type": "AnyTorchListOfTorchIntType" },
      { "name": "stride", "type": "AnyTorchListOfTorchIntType" },
      { "name": "padding", "type": "AnyTorchListOfTorchIntType" },
      { "name": "dilation", "type": "AnyTorchListOfTorchIntType" },
      { "name": "ceil_mode", "type": "Torch_BoolType" }
    ],
    "outputs": [
      { "name": "result0", "type": "AnyTorchOptionalTensorType" },
      { "name": "result1", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.max_pool2d_with_indices_backward",
    "summary": "Generated op for `aten::max_pool2d_with_indices_backward : (Tensor, Tensor, int[], int[], int[], int[], bool, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "grad_output", "type": "AnyTorchTensorType" },
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "kernel_size", "type": "AnyTorchListOfTorchIntType" },
      { "name": "stride", "type": "AnyTorchListOfTorchIntType" },
      { "name": "padding", "type": "AnyTorchListOfTorchIntType" },
      { "name": "dilation", "type": "AnyTorchListOfTorchIntType" },
      { "name": "ceil_mode", "type": "Torch_BoolType" },
      { "name": "indices", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.max_pool3d",
    "summary": "Generated op for `aten::max_pool3d : (Tensor, int[], int[], int[], int[], bool) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "kernel_size", "type": "AnyTorchListOfTorchIntType" },
      { "name": "stride", "type": "AnyTorchListOfTorchIntType" },
      { "name": "padding", "type": "AnyTorchListOfTorchIntType" },
      { "name": "dilation", "type": "AnyTorchListOfTorchIntType" },
      { "name": "ceil_mode", "type": "Torch_BoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.max_pool3d_with_indices",
    "summary": "Generated op for `aten::max_pool3d_with_indices : (Tensor, int[], int[], int[], int[], bool) -> (Tensor, Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "kernel_size", "type": "AnyTorchListOfTorchIntType" },
      { "name": "stride", "type": "AnyTorchListOfTorchIntType" },
      { "name": "padding", "type": "AnyTorchListOfTorchIntType" },
      { "name": "dilation", "type": "AnyTorchListOfTorchIntType" },
      { "name": "ceil_mode", "type": "Torch_BoolType" }
    ],
    "outputs": [
      { "name": "result0", "type": "AnyTorchOptionalTensorType" },
      { "name": "result1", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.max_pool3d_with_indices_backward",
    "summary": "Generated op for `aten::max_pool3d_with_indices_backward : (Tensor, Tensor, int[], int[], int[], int[], bool, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "grad_output", "type": "AnyTorchTensorType" },
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "kernel_size", "type": "AnyTorchListOfTorchIntType" },
      { "name": "stride", "type": "AnyTorchListOfTorchIntType" },
      { "name": "padding", "type": "AnyTorchListOfTorchIntType" },
      { "name": "dilation", "type": "AnyTorchListOfTorchIntType" },
      { "name": "ceil_mode", "type": "Torch_BoolType" },
      { "name": "indices", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.max_unpool2d",
    "summary": "Generated op for `aten::max_unpool2d : (Tensor, Tensor, int[]) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "indices", "type": "AnyTorchTensorType" },
      { "name": "output_size", "type": "AnyTorchListOfTorchIntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.max_unpool3d",
    "summary": "Generated op for `aten::max_unpool3d : (Tensor, Tensor, int[], int[], int[]) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "indices", "type": "AnyTorchTensorType" },
      { "name": "output_size", "type": "AnyTorchListOfTorchIntType" },
      { "name": "stride", "type": "AnyTorchListOfTorchIntType" },
      { "name": "padding", "type": "AnyTorchListOfTorchIntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.max.dim",
    "summary": "Generated op for `aten::max.dim : (Tensor, int, bool) -> (Tensor, Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "dim", "type": "Torch_IntType" },
      { "name": "keepdim", "type": "Torch_BoolType" }
    ],
    "outputs": [
      { "name": "values", "type": "AnyTorchOptionalTensorType" },
      { "name": "indices", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.max.other",
    "summary": "Generated op for `aten::max.other : (Tensor, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "other", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.maximum",
    "summary": "Generated op for `aten::maximum : (Tensor, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "other", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.mean",
    "summary": "Generated op for `aten::mean : (Tensor, int?) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "dtype", "type": "AnyTorchOptionalIntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.mean.dim",
    "summary": "Generated op for `aten::mean.dim : (Tensor, int[]?, bool, int?) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "dim", "type": "AnyTorchOptionalListOfTorchIntType" },
      { "name": "keepdim", "type": "Torch_BoolType" },
      { "name": "dtype", "type": "AnyTorchOptionalIntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.meshgrid",
    "summary": "Generated op for `aten::meshgrid : (Tensor[]) -> (Tensor[])`",
    "inputs": [
      { "name": "tensors", "type": "AnyTorchListOfTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchListOfTensorType" }
    ]
  },
  {
    "name": "torch.aten.meshgrid.indexing",
    "summary": "Generated op for `aten::meshgrid.indexing : (Tensor[], str) -> (Tensor[])`",
    "inputs": [
      { "name": "tensors", "type": "AnyTorchListOfTensorType" },
      { "name": "indexing", "type": "Torch_StringType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchListOfTensorType" }
    ]
  },
  {
    "name": "torch.aten.min",
    "summary": "Generated op for `aten::min : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.min.dim",
    "summary": "Generated op for `aten::min.dim : (Tensor, int, bool) -> (Tensor, Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "dim", "type": "Torch_IntType" },
      { "name": "keepdim", "type": "Torch_BoolType" }
    ],
    "outputs": [
      { "name": "values", "type": "AnyTorchOptionalTensorType" },
      { "name": "indices", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.min.other",
    "summary": "Generated op for `aten::min.other : (Tensor, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "other", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.minimum",
    "summary": "Generated op for `aten::minimum : (Tensor, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "other", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.mish",
    "summary": "Generated op for `aten::mish : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.mm",
    "summary": "Generated op for `aten::mm : (Tensor, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "mat2", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.movedim.int",
    "summary": "Generated op for `aten::movedim.int : (Tensor, int, int) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "source", "type": "Torch_IntType" },
      { "name": "destination", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.mse_loss",
    "summary": "Generated op for `aten::mse_loss : (Tensor, Tensor, int) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "target", "type": "AnyTorchTensorType" },
      { "name": "reduction", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.mse_loss_backward",
    "summary": "Generated op for `aten::mse_loss_backward : (Tensor, Tensor, Tensor, int) -> (Tensor)`",
    "inputs": [
      { "name": "grad_output", "type": "AnyTorchTensorType" },
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "target", "type": "AnyTorchTensorType" },
      { "name": "reduction", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.mul",
    "summary": "Generated op for `aten::mul : (Scalar, Scalar) -> (Scalar)`",
    "inputs": [
      { "name": "a", "type": "AnyTorchScalarType" },
      { "name": "b", "type": "AnyTorchScalarType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchScalarType" }
    ]
  },
  {
    "name": "torch.aten.mul_.Scalar",
    "summary": "Generated op for `aten::mul_.Scalar : (Tensor, Scalar) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" },
      { "name": "other", "type": "AnyTorchScalarType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.mul_.Tensor",
    "summary": "Generated op for `aten::mul_.Tensor : (Tensor, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" },
      { "name": "other", "type": "Torch_NonValueTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.mul.float",
    "summary": "Generated op for `aten::mul.float : (float, float) -> (float)`",
    "inputs": [
      { "name": "a", "type": "Torch_FloatType" },
      { "name": "b", "type": "Torch_FloatType" }
    ],
    "outputs": [
      { "name": "result", "type": "Torch_FloatType" }
    ]
  },
  {
    "name": "torch.aten.mul.float_int",
    "summary": "Generated op for `aten::mul.float_int : (float, int) -> (float)`",
    "inputs": [
      { "name": "a", "type": "Torch_FloatType" },
      { "name": "b", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "Torch_FloatType" }
    ]
  },
  {
    "name": "torch.aten.mul.int",
    "summary": "Generated op for `aten::mul.int : (int, int) -> (int)`",
    "inputs": [
      { "name": "a", "type": "Torch_IntType" },
      { "name": "b", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "Torch_IntType" }
    ]
  },
  {
    "name": "torch.aten.mul.int_float",
    "summary": "Generated op for `aten::mul.int_float : (int, float) -> (float)`",
    "inputs": [
      { "name": "a", "type": "Torch_IntType" },
      { "name": "b", "type": "Torch_FloatType" }
    ],
    "outputs": [
      { "name": "result", "type": "Torch_FloatType" }
    ]
  },
  {
    "name": "torch.aten.mul.left_t",
    "summary": "Generated op for `aten::mul.left_t : (t[], int) -> (t[])`",
    "inputs": [
      { "name": "l", "type": "AnyTorchListType" },
      { "name": "n", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchListType" }
    ]
  },
  {
    "name": "torch.aten.mul.Scalar",
    "summary": "Generated op for `aten::mul.Scalar : (Tensor, Scalar) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "other", "type": "AnyTorchScalarType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.mul.Tensor",
    "summary": "Generated op for `aten::mul.Tensor : (Tensor, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "other", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.multinomial",
    "summary": "Generated op for `aten::multinomial : (Tensor, int, bool, Generator?) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "num_samples", "type": "Torch_IntType" },
      { "name": "replacement", "type": "Torch_BoolType" },
      { "name": "generator", "type": "AnyTorchOptionalGeneratorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.mv",
    "summary": "Generated op for `aten::mv : (Tensor, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "vec", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.nan_to_num",
    "summary": "Generated op for `aten::nan_to_num : (Tensor, float?, float?, float?) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "nan", "type": "AnyTorchOptionalFloatType" },
      { "name": "posinf", "type": "AnyTorchOptionalFloatType" },
      { "name": "neginf", "type": "AnyTorchOptionalFloatType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.narrow",
    "summary": "Generated op for `aten::narrow : (Tensor, int, int, int) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "dim", "type": "Torch_IntType" },
      { "name": "start", "type": "Torch_IntType" },
      { "name": "length", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.narrow.Tensor",
    "summary": "Generated op for `aten::narrow.Tensor : (Tensor, int, Tensor, int) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "dim", "type": "Torch_IntType" },
      { "name": "start", "type": "AnyTorchTensorType" },
      { "name": "length", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.native_batch_norm",
    "summary": "Generated op for `aten::native_batch_norm : (Tensor, Tensor?, Tensor?, Tensor?, Tensor?, bool, float, float) -> (Tensor, Tensor, Tensor)`",
    "inputs": [
      { "name": "input", "type": "AnyTorchTensorType" },
      { "name": "weight", "type": "AnyTorchOptionalTensorType" },
      { "name": "bias", "type": "AnyTorchOptionalTensorType" },
      { "name": "running_mean", "type": "AnyTorchOptionalTensorType" },
      { "name": "running_var", "type": "AnyTorchOptionalTensorType" },
      { "name": "training", "type": "Torch_BoolType" },
      { "name": "momentum", "type": "Torch_FloatType" },
      { "name": "eps", "type": "Torch_FloatType" }
    ],
    "outputs": [
      { "name": "result0", "type": "AnyTorchOptionalTensorType" },
      { "name": "result1", "type": "AnyTorchOptionalTensorType" },
      { "name": "result2", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.native_batch_norm_backward",
    "summary": "Generated op for `aten::native_batch_norm_backward : (Tensor, Tensor, Tensor?, Tensor?, Tensor?, Tensor?, Tensor?, bool, float, bool[]) -> (Tensor, Tensor, Tensor)`",
    "inputs": [
      { "name": "grad_out", "type": "AnyTorchTensorType" },
      { "name": "input", "type": "AnyTorchTensorType" },
      { "name": "weight", "type": "AnyTorchOptionalTensorType" },
      { "name": "running_mean", "type": "AnyTorchOptionalTensorType" },
      { "name": "running_var", "type": "AnyTorchOptionalTensorType" },
      { "name": "save_mean", "type": "AnyTorchOptionalTensorType" },
      { "name": "save_invstd", "type": "AnyTorchOptionalTensorType" },
      { "name": "train", "type": "Torch_BoolType" },
      { "name": "eps", "type": "Torch_FloatType" },
      { "name": "output_mask", "type": "AnyTorchListOfTorchBoolType" }
    ],
    "outputs": [
      { "name": "result0", "type": "AnyTorchOptionalTensorType" },
      { "name": "result1", "type": "AnyTorchOptionalTensorType" },
      { "name": "result2", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.native_dropout",
    "summary": "Generated op for `aten::native_dropout : (Tensor, float, bool?) -> (Tensor, Tensor)`",
    "inputs": [
      { "name": "input", "type": "AnyTorchTensorType" },
      { "name": "p", "type": "Torch_FloatType" },
      { "name": "train", "type": "AnyTorchOptionalBoolType" }
    ],
    "outputs": [
      { "name": "result0", "type": "AnyTorchOptionalTensorType" },
      { "name": "result1", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.native_dropout_backward",
    "summary": "Generated op for `aten::native_dropout_backward : (Tensor, Tensor, float) -> (Tensor)`",
    "inputs": [
      { "name": "grad_output", "type": "AnyTorchTensorType" },
      { "name": "mask", "type": "AnyTorchTensorType" },
      { "name": "scale", "type": "Torch_FloatType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.native_group_norm",
    "summary": "Generated op for `aten::native_group_norm : (Tensor, Tensor?, Tensor?, int, int, int, int, float) -> (Tensor, Tensor, Tensor)`",
    "inputs": [
      { "name": "input", "type": "AnyTorchTensorType" },
      { "name": "weight", "type": "AnyTorchOptionalTensorType" },
      { "name": "bias", "type": "AnyTorchOptionalTensorType" },
      { "name": "N", "type": "Torch_IntType" },
      { "name": "C", "type": "Torch_IntType" },
      { "name": "HxW", "type": "Torch_IntType" },
      { "name": "group", "type": "Torch_IntType" },
      { "name": "eps", "type": "Torch_FloatType" }
    ],
    "outputs": [
      { "name": "result0", "type": "AnyTorchOptionalTensorType" },
      { "name": "result1", "type": "AnyTorchOptionalTensorType" },
      { "name": "result2", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.native_group_norm_backward",
    "summary": "Generated op for `aten::native_group_norm_backward : (Tensor, Tensor, Tensor, Tensor, Tensor?, int, int, int, int, bool[]) -> (Tensor, Tensor, Tensor)`",
    "inputs": [
      { "name": "grad_out", "type": "AnyTorchTensorType" },
      { "name": "input", "type": "AnyTorchTensorType" },
      { "name": "mean", "type": "AnyTorchTensorType" },
      { "name": "rstd", "type": "AnyTorchTensorType" },
      { "name": "weight", "type": "AnyTorchOptionalTensorType" },
      { "name": "N", "type": "Torch_IntType" },
      { "name": "C", "type": "Torch_IntType" },
      { "name": "HxW", "type": "Torch_IntType" },
      { "name": "group", "type": "Torch_IntType" },
      { "name": "output_mask", "type": "AnyTorchListOfTorchBoolType" }
    ],
    "outputs": [
      { "name": "result0", "type": "AnyTorchOptionalTensorType" },
      { "name": "result1", "type": "AnyTorchOptionalTensorType" },
      { "name": "result2", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.native_layer_norm",
    "summary": "Generated op for `aten::native_layer_norm : (Tensor, int[], Tensor?, Tensor?, float) -> (Tensor, Tensor, Tensor)`",
    "inputs": [
      { "name": "input", "type": "AnyTorchTensorType" },
      { "name": "normalized_shape", "type": "AnyTorchListOfTorchIntType" },
      { "name": "weight", "type": "AnyTorchOptionalTensorType" },
      { "name": "bias", "type": "AnyTorchOptionalTensorType" },
      { "name": "eps", "type": "Torch_FloatType" }
    ],
    "outputs": [
      { "name": "result0", "type": "AnyTorchOptionalTensorType" },
      { "name": "result1", "type": "AnyTorchOptionalTensorType" },
      { "name": "result2", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.native_layer_norm_backward",
    "summary": "Generated op for `aten::native_layer_norm_backward : (Tensor, Tensor, int[], Tensor, Tensor, Tensor?, Tensor?, bool[]) -> (Tensor, Tensor, Tensor)`",
    "inputs": [
      { "name": "grad_out", "type": "AnyTorchTensorType" },
      { "name": "input", "type": "AnyTorchTensorType" },
      { "name": "normalized_shape", "type": "AnyTorchListOfTorchIntType" },
      { "name": "mean", "type": "AnyTorchTensorType" },
      { "name": "rstd", "type": "AnyTorchTensorType" },
      { "name": "weight", "type": "AnyTorchOptionalTensorType" },
      { "name": "bias", "type": "AnyTorchOptionalTensorType" },
      { "name": "output_mask", "type": "AnyTorchListOfTorchBoolType" }
    ],
    "outputs": [
      { "name": "result0", "type": "AnyTorchOptionalTensorType" },
      { "name": "result1", "type": "AnyTorchOptionalTensorType" },
      { "name": "result2", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.ne_.Scalar",
    "summary": "Generated op for `aten::ne_.Scalar : (Tensor, Scalar) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" },
      { "name": "other", "type": "AnyTorchScalarType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.ne_.Tensor",
    "summary": "Generated op for `aten::ne_.Tensor : (Tensor, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" },
      { "name": "other", "type": "Torch_NonValueTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.ne.bool",
    "summary": "Generated op for `aten::ne.bool : (bool, bool) -> (bool)`",
    "inputs": [
      { "name": "a", "type": "Torch_BoolType" },
      { "name": "b", "type": "Torch_BoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "Torch_BoolType" }
    ]
  },
  {
    "name": "torch.aten.ne.float_int",
    "summary": "Generated op for `aten::ne.float_int : (float, int) -> (bool)`",
    "inputs": [
      { "name": "a", "type": "Torch_FloatType" },
      { "name": "b", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "Torch_BoolType" }
    ]
  },
  {
    "name": "torch.aten.ne.int",
    "summary": "Generated op for `aten::ne.int : (int, int) -> (bool)`",
    "inputs": [
      { "name": "a", "type": "Torch_IntType" },
      { "name": "b", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "Torch_BoolType" }
    ]
  },
  {
    "name": "torch.aten.ne.int_list",
    "summary": "Generated op for `aten::ne.int_list : (int[], int[]) -> (bool)`",
    "inputs": [
      { "name": "a", "type": "AnyTorchListOfTorchIntType" },
      { "name": "b", "type": "AnyTorchListOfTorchIntType" }
    ],
    "outputs": [
      { "name": "result", "type": "Torch_BoolType" }
    ]
  },
  {
    "name": "torch.aten.ne.Scalar",
    "summary": "Generated op for `aten::ne.Scalar : (Tensor, Scalar) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "other", "type": "AnyTorchScalarType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.ne.str",
    "summary": "Generated op for `aten::ne.str : (str, str) -> (bool)`",
    "inputs": [
      { "name": "a", "type": "Torch_StringType" },
      { "name": "b", "type": "Torch_StringType" }
    ],
    "outputs": [
      { "name": "result", "type": "Torch_BoolType" }
    ]
  },
  {
    "name": "torch.aten.ne.Tensor",
    "summary": "Generated op for `aten::ne.Tensor : (Tensor, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "other", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.neg",
    "summary": "Generated op for `aten::neg : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.neg_",
    "summary": "Generated op for `aten::neg_ : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.neg.float",
    "summary": "Generated op for `aten::neg.float : (float) -> (float)`",
    "inputs": [
      { "name": "a", "type": "Torch_FloatType" }
    ],
    "outputs": [
      { "name": "result", "type": "Torch_FloatType" }
    ]
  },
  {
    "name": "torch.aten.neg.int",
    "summary": "Generated op for `aten::neg.int : (int) -> (int)`",
    "inputs": [
      { "name": "a", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "Torch_IntType" }
    ]
  },
  {
    "name": "torch.aten.new_empty",
    "summary": "Generated op for `aten::new_empty : (Tensor, int[], int?, int?, Device?, bool?) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "size", "type": "AnyTorchListOfTorchIntType" },
      { "name": "dtype", "type": "AnyTorchOptionalIntType" },
      { "name": "layout", "type": "AnyTorchOptionalIntType" },
      { "name": "device", "type": "AnyTorchOptionalDeviceType" },
      { "name": "pin_memory", "type": "AnyTorchOptionalBoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.new_empty_strided",
    "summary": "Generated op for `aten::new_empty_strided : (Tensor, int[], int[], int?, int?, Device?, bool?) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "size", "type": "AnyTorchListOfTorchIntType" },
      { "name": "stride", "type": "AnyTorchListOfTorchIntType" },
      { "name": "dtype", "type": "AnyTorchOptionalIntType" },
      { "name": "layout", "type": "AnyTorchOptionalIntType" },
      { "name": "device", "type": "AnyTorchOptionalDeviceType" },
      { "name": "pin_memory", "type": "AnyTorchOptionalBoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.new_full",
    "summary": "Generated op for `aten::new_full : (Tensor, int[], Scalar, int?, int?, Device?, bool?) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "size", "type": "AnyTorchListOfTorchIntType" },
      { "name": "fill_value", "type": "AnyTorchScalarType" },
      { "name": "dtype", "type": "AnyTorchOptionalIntType" },
      { "name": "layout", "type": "AnyTorchOptionalIntType" },
      { "name": "device", "type": "AnyTorchOptionalDeviceType" },
      { "name": "pin_memory", "type": "AnyTorchOptionalBoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.new_ones",
    "summary": "Generated op for `aten::new_ones : (Tensor, int[], int?, int?, Device?, bool?) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "size", "type": "AnyTorchListOfTorchIntType" },
      { "name": "dtype", "type": "AnyTorchOptionalIntType" },
      { "name": "layout", "type": "AnyTorchOptionalIntType" },
      { "name": "device", "type": "AnyTorchOptionalDeviceType" },
      { "name": "pin_memory", "type": "AnyTorchOptionalBoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.new_zeros",
    "summary": "Generated op for `aten::new_zeros : (Tensor, int[], int?, int?, Device?, bool?) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "size", "type": "AnyTorchListOfTorchIntType" },
      { "name": "dtype", "type": "AnyTorchOptionalIntType" },
      { "name": "layout", "type": "AnyTorchOptionalIntType" },
      { "name": "device", "type": "AnyTorchOptionalDeviceType" },
      { "name": "pin_memory", "type": "AnyTorchOptionalBoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.nll_loss_backward",
    "summary": "Generated op for `aten::nll_loss_backward : (Tensor, Tensor, Tensor, Tensor?, int, int, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "grad_output", "type": "AnyTorchTensorType" },
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "target", "type": "AnyTorchTensorType" },
      { "name": "weight", "type": "AnyTorchOptionalTensorType" },
      { "name": "reduction", "type": "Torch_IntType" },
      { "name": "ignore_index", "type": "Torch_IntType" },
      { "name": "total_weight", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.nll_loss_forward",
    "summary": "Generated op for `aten::nll_loss_forward : (Tensor, Tensor, Tensor?, int, int) -> (Tensor, Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "target", "type": "AnyTorchTensorType" },
      { "name": "weight", "type": "AnyTorchOptionalTensorType" },
      { "name": "reduction", "type": "Torch_IntType" },
      { "name": "ignore_index", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTorchOptionalTensorType" },
      { "name": "total_weight", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.nll_loss2d_backward",
    "summary": "Generated op for `aten::nll_loss2d_backward : (Tensor, Tensor, Tensor, Tensor?, int, int, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "grad_output", "type": "AnyTorchTensorType" },
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "target", "type": "AnyTorchTensorType" },
      { "name": "weight", "type": "AnyTorchOptionalTensorType" },
      { "name": "reduction", "type": "Torch_IntType" },
      { "name": "ignore_index", "type": "Torch_IntType" },
      { "name": "total_weight", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.nll_loss2d_forward",
    "summary": "Generated op for `aten::nll_loss2d_forward : (Tensor, Tensor, Tensor?, int, int) -> (Tensor, Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "target", "type": "AnyTorchTensorType" },
      { "name": "weight", "type": "AnyTorchOptionalTensorType" },
      { "name": "reduction", "type": "Torch_IntType" },
      { "name": "ignore_index", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTorchOptionalTensorType" },
      { "name": "total_weight", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.nonzero",
    "summary": "Generated op for `aten::nonzero : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.nonzero_numpy",
    "summary": "Generated op for `aten::nonzero_numpy : (Tensor) -> (Tensor[])`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchListOfTensorType" }
    ]
  },
  {
    "name": "torch.aten.nonzero_static",
    "summary": "Generated op for `aten::nonzero_static : (Tensor, int, int) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "size", "type": "Torch_IntType" },
      { "name": "fill_value", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.norm.Scalar",
    "summary": "Generated op for `aten::norm.Scalar : (Tensor, Scalar) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "p", "type": "AnyTorchScalarType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.norm.ScalarOpt_dim",
    "summary": "Generated op for `aten::norm.ScalarOpt_dim : (Tensor, Scalar?, int[], bool) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "p", "type": "AnyTorchOptionalScalarType" },
      { "name": "dim", "type": "AnyTorchListOfTorchIntType" },
      { "name": "keepdim", "type": "Torch_BoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.normal_functional",
    "summary": "Generated op for `aten::normal_functional : (Tensor, float, float, Generator?) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "mean", "type": "Torch_FloatType" },
      { "name": "std", "type": "Torch_FloatType" },
      { "name": "generator", "type": "AnyTorchOptionalGeneratorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.numel",
    "summary": "Generated op for `aten::numel : (Tensor) -> (int)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "Torch_IntType" }
    ]
  },
  {
    "name": "torch.aten.numpy_T",
    "summary": "Generated op for `aten::numpy_T : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.one_hot",
    "summary": "Generated op for `aten::one_hot : (Tensor, int) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "num_classes", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.ones",
    "summary": "Generated op for `aten::ones : (int[], int?, int?, Device?, bool?) -> (Tensor)`",
    "inputs": [
      { "name": "size", "type": "AnyTorchListOfTorchIntType" },
      { "name": "dtype", "type": "AnyTorchOptionalIntType" },
      { "name": "layout", "type": "AnyTorchOptionalIntType" },
      { "name": "device", "type": "AnyTorchOptionalDeviceType" },
      { "name": "pin_memory", "type": "AnyTorchOptionalBoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.ones_like",
    "summary": "Generated op for `aten::ones_like : (Tensor, int?, int?, Device?, bool?, int?) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "dtype", "type": "AnyTorchOptionalIntType" },
      { "name": "layout", "type": "AnyTorchOptionalIntType" },
      { "name": "device", "type": "AnyTorchOptionalDeviceType" },
      { "name": "pin_memory", "type": "AnyTorchOptionalBoolType" },
      { "name": "memory_format", "type": "AnyTorchOptionalIntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.outer",
    "summary": "Generated op for `aten::outer : (Tensor, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "vec2", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.pad",
    "summary": "Generated op for `aten::pad : (Tensor, int[], str, float?) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "pad", "type": "AnyTorchListOfTorchIntType" },
      { "name": "mode", "type": "Torch_StringType" },
      { "name": "value", "type": "AnyTorchOptionalFloatType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.permute",
    "summary": "Generated op for `aten::permute : (Tensor, int[]) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "dims", "type": "AnyTorchListOfTorchIntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.permute_copy",
    "summary": "Generated op for `aten::permute_copy : (Tensor, int[]) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "dims", "type": "AnyTorchListOfTorchIntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.pixel_shuffle",
    "summary": "Generated op for `aten::pixel_shuffle : (Tensor, int) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "upscale_factor", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.pixel_unshuffle",
    "summary": "Generated op for `aten::pixel_unshuffle : (Tensor, int) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "downscale_factor", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.poisson_nll_loss",
    "summary": "Generated op for `aten::poisson_nll_loss : (Tensor, Tensor, bool, bool, float, int) -> (Tensor)`",
    "inputs": [
      { "name": "input", "type": "AnyTorchTensorType" },
      { "name": "target", "type": "AnyTorchTensorType" },
      { "name": "log_input", "type": "Torch_BoolType" },
      { "name": "full", "type": "Torch_BoolType" },
      { "name": "eps", "type": "Torch_FloatType" },
      { "name": "reduction", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.polar",
    "summary": "Generated op for `aten::polar : (Tensor, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "abs", "type": "AnyTorchTensorType" },
      { "name": "angle", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.pow.int_float",
    "summary": "Generated op for `aten::pow.int_float : (int, float) -> (float)`",
    "inputs": [
      { "name": "a", "type": "Torch_IntType" },
      { "name": "b", "type": "Torch_FloatType" }
    ],
    "outputs": [
      { "name": "result", "type": "Torch_FloatType" }
    ]
  },
  {
    "name": "torch.aten.pow.Scalar",
    "summary": "Generated op for `aten::pow.Scalar : (Scalar, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchScalarType" },
      { "name": "exponent", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.pow.Tensor_Scalar",
    "summary": "Generated op for `aten::pow.Tensor_Scalar : (Tensor, Scalar) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "exponent", "type": "AnyTorchScalarType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.pow.Tensor_Tensor",
    "summary": "Generated op for `aten::pow.Tensor_Tensor : (Tensor, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "exponent", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.prelu",
    "summary": "Generated op for `aten::prelu : (Tensor, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "weight", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.prod",
    "summary": "Generated op for `aten::prod : (Tensor, int?) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "dtype", "type": "AnyTorchOptionalIntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.prod.dim_int",
    "summary": "Generated op for `aten::prod.dim_int : (Tensor, int, bool, int?) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "dim", "type": "Torch_IntType" },
      { "name": "keepdim", "type": "Torch_BoolType" },
      { "name": "dtype", "type": "AnyTorchOptionalIntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.quantize_per_channel",
    "summary": "Generated op for `aten::quantize_per_channel : (Tensor, Tensor, Tensor, int, int) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "scales", "type": "AnyTorchTensorType" },
      { "name": "zero_points", "type": "AnyTorchTensorType" },
      { "name": "axis", "type": "Torch_IntType" },
      { "name": "dtype", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.quantize_per_tensor",
    "summary": "Generated op for `aten::quantize_per_tensor : (Tensor, float, int, int) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "scale", "type": "Torch_FloatType" },
      { "name": "zero_point", "type": "Torch_IntType" },
      { "name": "dtype", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.rad2deg",
    "summary": "Generated op for `aten::rad2deg : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.rand",
    "summary": "Generated op for `aten::rand : (int[], int?, int?, Device?, bool?) -> (Tensor)`",
    "inputs": [
      { "name": "size", "type": "AnyTorchListOfTorchIntType" },
      { "name": "dtype", "type": "AnyTorchOptionalIntType" },
      { "name": "layout", "type": "AnyTorchOptionalIntType" },
      { "name": "device", "type": "AnyTorchOptionalDeviceType" },
      { "name": "pin_memory", "type": "AnyTorchOptionalBoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.rand_like",
    "summary": "Generated op for `aten::rand_like : (Tensor, int?, int?, Device?, bool?, int?) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "dtype", "type": "AnyTorchOptionalIntType" },
      { "name": "layout", "type": "AnyTorchOptionalIntType" },
      { "name": "device", "type": "AnyTorchOptionalDeviceType" },
      { "name": "pin_memory", "type": "AnyTorchOptionalBoolType" },
      { "name": "memory_format", "type": "AnyTorchOptionalIntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.randint",
    "summary": "Generated op for `aten::randint : (int, int[], int?, int?, Device?, bool?) -> (Tensor)`",
    "inputs": [
      { "name": "high", "type": "Torch_IntType" },
      { "name": "size", "type": "AnyTorchListOfTorchIntType" },
      { "name": "dtype", "type": "AnyTorchOptionalIntType" },
      { "name": "layout", "type": "AnyTorchOptionalIntType" },
      { "name": "device", "type": "AnyTorchOptionalDeviceType" },
      { "name": "pin_memory", "type": "AnyTorchOptionalBoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.randint.low",
    "summary": "Generated op for `aten::randint.low : (int, int, int[], int?, int?, Device?, bool?) -> (Tensor)`",
    "inputs": [
      { "name": "low", "type": "Torch_IntType" },
      { "name": "high", "type": "Torch_IntType" },
      { "name": "size", "type": "AnyTorchListOfTorchIntType" },
      { "name": "dtype", "type": "AnyTorchOptionalIntType" },
      { "name": "layout", "type": "AnyTorchOptionalIntType" },
      { "name": "device", "type": "AnyTorchOptionalDeviceType" },
      { "name": "pin_memory", "type": "AnyTorchOptionalBoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.randn",
    "summary": "Generated op for `aten::randn : (int[], int?, int?, Device?, bool?) -> (Tensor)`",
    "inputs": [
      { "name": "size", "type": "AnyTorchListOfTorchIntType" },
      { "name": "dtype", "type": "AnyTorchOptionalIntType" },
      { "name": "layout", "type": "AnyTorchOptionalIntType" },
      { "name": "device", "type": "AnyTorchOptionalDeviceType" },
      { "name": "pin_memory", "type": "AnyTorchOptionalBoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.randn_like",
    "summary": "Generated op for `aten::randn_like : (Tensor, int?, int?, Device?, bool?, int?) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "dtype", "type": "AnyTorchOptionalIntType" },
      { "name": "layout", "type": "AnyTorchOptionalIntType" },
      { "name": "device", "type": "AnyTorchOptionalDeviceType" },
      { "name": "pin_memory", "type": "AnyTorchOptionalBoolType" },
      { "name": "memory_format", "type": "AnyTorchOptionalIntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.randn.generator",
    "summary": "Generated op for `aten::randn.generator : (int[], Generator?, int?, int?, Device?, bool?) -> (Tensor)`",
    "inputs": [
      { "name": "size", "type": "AnyTorchListOfTorchIntType" },
      { "name": "generator", "type": "AnyTorchOptionalGeneratorType" },
      { "name": "dtype", "type": "AnyTorchOptionalIntType" },
      { "name": "layout", "type": "AnyTorchOptionalIntType" },
      { "name": "device", "type": "AnyTorchOptionalDeviceType" },
      { "name": "pin_memory", "type": "AnyTorchOptionalBoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.random",
    "summary": "Generated op for `aten::random : (Tensor, Generator?) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "generator", "type": "AnyTorchOptionalGeneratorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.random.from",
    "summary": "Generated op for `aten::random.from : (Tensor, int, int?, Generator?) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "from", "type": "Torch_IntType" },
      { "name": "to", "type": "AnyTorchOptionalIntType" },
      { "name": "generator", "type": "AnyTorchOptionalGeneratorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.real",
    "summary": "Generated op for `aten::real : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.reciprocal",
    "summary": "Generated op for `aten::reciprocal : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.reciprocal_",
    "summary": "Generated op for `aten::reciprocal_ : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.reflection_pad1d",
    "summary": "Generated op for `aten::reflection_pad1d : (Tensor, int[]) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "padding", "type": "AnyTorchListOfTorchIntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.reflection_pad2d",
    "summary": "Generated op for `aten::reflection_pad2d : (Tensor, int[]) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "padding", "type": "AnyTorchListOfTorchIntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.reflection_pad3d",
    "summary": "Generated op for `aten::reflection_pad3d : (Tensor, int[]) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "padding", "type": "AnyTorchListOfTorchIntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.relu",
    "summary": "Generated op for `aten::relu : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.relu_",
    "summary": "Generated op for `aten::relu_ : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.relu6",
    "summary": "Generated op for `aten::relu6 : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.relu6_",
    "summary": "Generated op for `aten::relu6_ : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.remainder.int",
    "summary": "Generated op for `aten::remainder.int : (int, int) -> (int)`",
    "inputs": [
      { "name": "a", "type": "Torch_IntType" },
      { "name": "b", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "Torch_IntType" }
    ]
  },
  {
    "name": "torch.aten.remainder.Scalar",
    "summary": "Generated op for `aten::remainder.Scalar : (Tensor, Scalar) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "other", "type": "AnyTorchScalarType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.remainder.Tensor",
    "summary": "Generated op for `aten::remainder.Tensor : (Tensor, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "other", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.renorm",
    "summary": "Generated op for `aten::renorm : (Tensor, Scalar, int, Scalar) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "p", "type": "AnyTorchScalarType" },
      { "name": "dim", "type": "Torch_IntType" },
      { "name": "maxnorm", "type": "AnyTorchScalarType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.repeat",
    "summary": "Generated op for `aten::repeat : (Tensor, int[]) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "repeats", "type": "AnyTorchListOfTorchIntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.repeat_interleave.self_int",
    "summary": "Generated op for `aten::repeat_interleave.self_int : (Tensor, int, int?, int?) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "repeats", "type": "Torch_IntType" },
      { "name": "dim", "type": "AnyTorchOptionalIntType" },
      { "name": "output_size", "type": "AnyTorchOptionalIntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.replication_pad1d",
    "summary": "Generated op for `aten::replication_pad1d : (Tensor, int[]) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "padding", "type": "AnyTorchListOfTorchIntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.replication_pad2d",
    "summary": "Generated op for `aten::replication_pad2d : (Tensor, int[]) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "padding", "type": "AnyTorchListOfTorchIntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.replication_pad3d",
    "summary": "Generated op for `aten::replication_pad3d : (Tensor, int[]) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "padding", "type": "AnyTorchListOfTorchIntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.reshape",
    "summary": "Generated op for `aten::reshape : (Tensor, int[]) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "shape", "type": "AnyTorchListOfTorchIntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.reshape_as",
    "summary": "Generated op for `aten::reshape_as : (Tensor, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "other", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.resize",
    "summary": "Generated op for `aten::resize : (Tensor, int[], int?) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "size", "type": "AnyTorchListOfTorchIntType" },
      { "name": "memory_format", "type": "AnyTorchOptionalIntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.resize_",
    "summary": "Generated op for `aten::resize_ : (Tensor, int[], int?) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "size", "type": "AnyTorchListOfTorchIntType" },
      { "name": "memory_format", "type": "AnyTorchOptionalIntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.rms_norm",
    "summary": "Generated op for `aten::rms_norm : (Tensor, int[], Tensor?, float?) -> (Tensor)`",
    "inputs": [
      { "name": "input", "type": "AnyTorchTensorType" },
      { "name": "normalized_shape", "type": "AnyTorchListOfTorchIntType" },
      { "name": "weight", "type": "AnyTorchOptionalTensorType" },
      { "name": "eps", "type": "AnyTorchOptionalFloatType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.roll",
    "summary": "Generated op for `aten::roll : (Tensor, int[], int[]) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "shifts", "type": "AnyTorchListOfTorchIntType" },
      { "name": "dims", "type": "AnyTorchListOfTorchIntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.rot90",
    "summary": "Generated op for `aten::rot90 : (Tensor, int, int[]) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "k", "type": "Torch_IntType" },
      { "name": "dims", "type": "AnyTorchListOfTorchIntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.round",
    "summary": "Generated op for `aten::round : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.round_",
    "summary": "Generated op for `aten::round_ : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.round_.decimals",
    "summary": "Generated op for `aten::round_.decimals : (Tensor, int) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" },
      { "name": "decimals", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.round.decimals",
    "summary": "Generated op for `aten::round.decimals : (Tensor, int) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "decimals", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.rrelu",
    "summary": "Generated op for `aten::rrelu : (Tensor, Scalar, Scalar, bool, Generator?) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "lower", "type": "AnyTorchScalarType" },
      { "name": "upper", "type": "AnyTorchScalarType" },
      { "name": "training", "type": "Torch_BoolType" },
      { "name": "generator", "type": "AnyTorchOptionalGeneratorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.rrelu_",
    "summary": "Generated op for `aten::rrelu_ : (Tensor, Scalar, Scalar, bool, Generator?) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" },
      { "name": "lower", "type": "AnyTorchScalarType" },
      { "name": "upper", "type": "AnyTorchScalarType" },
      { "name": "training", "type": "Torch_BoolType" },
      { "name": "generator", "type": "AnyTorchOptionalGeneratorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.rrelu_with_noise",
    "summary": "Generated op for `aten::rrelu_with_noise : (Tensor, Tensor, Scalar, Scalar, bool, Generator?) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "noise", "type": "AnyTorchTensorType" },
      { "name": "lower", "type": "AnyTorchScalarType" },
      { "name": "upper", "type": "AnyTorchScalarType" },
      { "name": "training", "type": "Torch_BoolType" },
      { "name": "generator", "type": "AnyTorchOptionalGeneratorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.rrelu_with_noise_",
    "summary": "Generated op for `aten::rrelu_with_noise_ : (Tensor, Tensor, Scalar, Scalar, bool, Generator?) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" },
      { "name": "noise", "type": "Torch_NonValueTensorType" },
      { "name": "lower", "type": "AnyTorchScalarType" },
      { "name": "upper", "type": "AnyTorchScalarType" },
      { "name": "training", "type": "Torch_BoolType" },
      { "name": "generator", "type": "AnyTorchOptionalGeneratorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.rrelu_with_noise_backward",
    "summary": "Generated op for `aten::rrelu_with_noise_backward : (Tensor, Tensor, Tensor, Scalar, Scalar, bool, bool) -> (Tensor)`",
    "inputs": [
      { "name": "grad_output", "type": "AnyTorchTensorType" },
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "noise", "type": "AnyTorchTensorType" },
      { "name": "lower", "type": "AnyTorchScalarType" },
      { "name": "upper", "type": "AnyTorchScalarType" },
      { "name": "training", "type": "Torch_BoolType" },
      { "name": "self_is_result", "type": "Torch_BoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.rrelu_with_noise_functional",
    "summary": "Generated op for `aten::rrelu_with_noise_functional : (Tensor, Tensor, Scalar, Scalar, bool, Generator?) -> (Tensor, Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "noise", "type": "AnyTorchTensorType" },
      { "name": "lower", "type": "AnyTorchScalarType" },
      { "name": "upper", "type": "AnyTorchScalarType" },
      { "name": "training", "type": "Torch_BoolType" },
      { "name": "generator", "type": "AnyTorchOptionalGeneratorType" }
    ],
    "outputs": [
      { "name": "result0", "type": "AnyTorchOptionalTensorType" },
      { "name": "noise_out", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.rsqrt",
    "summary": "Generated op for `aten::rsqrt : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.rsqrt_",
    "summary": "Generated op for `aten::rsqrt_ : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.rsub.Scalar",
    "summary": "Generated op for `aten::rsub.Scalar : (Tensor, Scalar, Scalar) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "other", "type": "AnyTorchScalarType" },
      { "name": "alpha", "type": "AnyTorchScalarType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.scalar_tensor",
    "summary": "Generated op for `aten::scalar_tensor : (Scalar, int?, int?, Device?, bool?) -> (Tensor)`",
    "inputs": [
      { "name": "s", "type": "AnyTorchScalarType" },
      { "name": "dtype", "type": "AnyTorchOptionalIntType" },
      { "name": "layout", "type": "AnyTorchOptionalIntType" },
      { "name": "device", "type": "AnyTorchOptionalDeviceType" },
      { "name": "pin_memory", "type": "AnyTorchOptionalBoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.ScalarImplicit",
    "summary": "Generated op for `aten::ScalarImplicit : (Tensor) -> (Scalar)`",
    "inputs": [
      { "name": "a", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchScalarType" }
    ]
  },
  {
    "name": "torch.aten.scaled_dot_product_attention",
    "summary": "Generated op for `aten::scaled_dot_product_attention : (Tensor, Tensor, Tensor, Tensor?, float, bool, float?, bool) -> (Tensor)`",
    "inputs": [
      { "name": "query", "type": "AnyTorchTensorType" },
      { "name": "key", "type": "AnyTorchTensorType" },
      { "name": "value", "type": "AnyTorchTensorType" },
      { "name": "attn_mask", "type": "AnyTorchOptionalTensorType" },
      { "name": "dropout_p", "type": "Torch_FloatType" },
      { "name": "is_causal", "type": "Torch_BoolType" },
      { "name": "scale", "type": "AnyTorchOptionalFloatType" },
      { "name": "enable_gqa", "type": "Torch_BoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.scatter_.src",
    "summary": "Generated op for `aten::scatter_.src : (Tensor, int, Tensor, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" },
      { "name": "dim", "type": "Torch_IntType" },
      { "name": "index", "type": "Torch_NonValueTensorType" },
      { "name": "src", "type": "Torch_NonValueTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.scatter_.value",
    "summary": "Generated op for `aten::scatter_.value : (Tensor, int, Tensor, Scalar) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" },
      { "name": "dim", "type": "Torch_IntType" },
      { "name": "index", "type": "Torch_NonValueTensorType" },
      { "name": "value", "type": "AnyTorchScalarType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.scatter_add",
    "summary": "Generated op for `aten::scatter_add : (Tensor, int, Tensor, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "dim", "type": "Torch_IntType" },
      { "name": "index", "type": "AnyTorchTensorType" },
      { "name": "src", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.scatter_add_",
    "summary": "Generated op for `aten::scatter_add_ : (Tensor, int, Tensor, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" },
      { "name": "dim", "type": "Torch_IntType" },
      { "name": "index", "type": "Torch_NonValueTensorType" },
      { "name": "src", "type": "Torch_NonValueTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.scatter_reduce_.two",
    "summary": "Generated op for `aten::scatter_reduce_.two : (Tensor, int, Tensor, Tensor, str, bool) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" },
      { "name": "dim", "type": "Torch_IntType" },
      { "name": "index", "type": "Torch_NonValueTensorType" },
      { "name": "src", "type": "Torch_NonValueTensorType" },
      { "name": "reduce", "type": "Torch_StringType" },
      { "name": "include_self", "type": "Torch_BoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.scatter_reduce.two",
    "summary": "Generated op for `aten::scatter_reduce.two : (Tensor, int, Tensor, Tensor, str, bool) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "dim", "type": "Torch_IntType" },
      { "name": "index", "type": "AnyTorchTensorType" },
      { "name": "src", "type": "AnyTorchTensorType" },
      { "name": "reduce", "type": "Torch_StringType" },
      { "name": "include_self", "type": "Torch_BoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.scatter.reduce",
    "summary": "Generated op for `aten::scatter.reduce : (Tensor, int, Tensor, Tensor, str) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "dim", "type": "Torch_IntType" },
      { "name": "index", "type": "AnyTorchTensorType" },
      { "name": "src", "type": "AnyTorchTensorType" },
      { "name": "reduce", "type": "Torch_StringType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.scatter.src",
    "summary": "Generated op for `aten::scatter.src : (Tensor, int, Tensor, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "dim", "type": "Torch_IntType" },
      { "name": "index", "type": "AnyTorchTensorType" },
      { "name": "src", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.scatter.value",
    "summary": "Generated op for `aten::scatter.value : (Tensor, int, Tensor, Scalar) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "dim", "type": "Torch_IntType" },
      { "name": "index", "type": "AnyTorchTensorType" },
      { "name": "value", "type": "AnyTorchScalarType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.select_copy.int",
    "summary": "Generated op for `aten::select_copy.int : (Tensor, int, int) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "dim", "type": "Torch_IntType" },
      { "name": "index", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.select_scatter",
    "summary": "Generated op for `aten::select_scatter : (Tensor, Tensor, int, int) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "src", "type": "AnyTorchTensorType" },
      { "name": "dim", "type": "Torch_IntType" },
      { "name": "index", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.select.int",
    "summary": "Generated op for `aten::select.int : (Tensor, int, int) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "dim", "type": "Torch_IntType" },
      { "name": "index", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.selu",
    "summary": "Generated op for `aten::selu : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.selu_",
    "summary": "Generated op for `aten::selu_ : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.sgn",
    "summary": "Generated op for `aten::sgn : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.sgn_",
    "summary": "Generated op for `aten::sgn_ : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.sigmoid",
    "summary": "Generated op for `aten::sigmoid : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.sigmoid_",
    "summary": "Generated op for `aten::sigmoid_ : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.sigmoid_backward",
    "summary": "Generated op for `aten::sigmoid_backward : (Tensor, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "grad_output", "type": "AnyTorchTensorType" },
      { "name": "output", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.sign",
    "summary": "Generated op for `aten::sign : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.sign_",
    "summary": "Generated op for `aten::sign_ : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.signbit",
    "summary": "Generated op for `aten::signbit : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.silu",
    "summary": "Generated op for `aten::silu : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.silu_",
    "summary": "Generated op for `aten::silu_ : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.sin",
    "summary": "Generated op for `aten::sin : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.sin_",
    "summary": "Generated op for `aten::sin_ : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.sinh",
    "summary": "Generated op for `aten::sinh : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.sinh_",
    "summary": "Generated op for `aten::sinh_ : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.size",
    "summary": "Generated op for `aten::size : (Tensor) -> (int[])`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchListOfTorchIntType" }
    ]
  },
  {
    "name": "torch.aten.size.int",
    "summary": "Generated op for `aten::size.int : (Tensor, int) -> (int)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "dim", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "Torch_IntType" }
    ]
  },
  {
    "name": "torch.aten.slice_copy.Tensor",
    "summary": "Generated op for `aten::slice_copy.Tensor : (Tensor, int, int?, int?, int) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "dim", "type": "Torch_IntType" },
      { "name": "start", "type": "AnyTorchOptionalIntType" },
      { "name": "end", "type": "AnyTorchOptionalIntType" },
      { "name": "step", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.slice_scatter",
    "summary": "Generated op for `aten::slice_scatter : (Tensor, Tensor, int, int?, int?, int) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "src", "type": "AnyTorchTensorType" },
      { "name": "dim", "type": "Torch_IntType" },
      { "name": "start", "type": "AnyTorchOptionalIntType" },
      { "name": "end", "type": "AnyTorchOptionalIntType" },
      { "name": "step", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.slice.t",
    "summary": "Generated op for `aten::slice.t : (t[], int?, int?, int) -> (t[])`",
    "inputs": [
      { "name": "l", "type": "AnyTorchListType" },
      { "name": "start", "type": "AnyTorchOptionalIntType" },
      { "name": "end", "type": "AnyTorchOptionalIntType" },
      { "name": "step", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchListType" }
    ]
  },
  {
    "name": "torch.aten.slice.Tensor",
    "summary": "Generated op for `aten::slice.Tensor : (Tensor, int, int?, int?, int) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "dim", "type": "Torch_IntType" },
      { "name": "start", "type": "AnyTorchOptionalIntType" },
      { "name": "end", "type": "AnyTorchOptionalIntType" },
      { "name": "step", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.softmax.int",
    "summary": "Generated op for `aten::softmax.int : (Tensor, int, int?) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "dim", "type": "Torch_IntType" },
      { "name": "dtype", "type": "AnyTorchOptionalIntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.softplus",
    "summary": "Generated op for `aten::softplus : (Tensor, Scalar, Scalar) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "beta", "type": "AnyTorchScalarType" },
      { "name": "threshold", "type": "AnyTorchScalarType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.softshrink",
    "summary": "Generated op for `aten::softshrink : (Tensor, Scalar) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "lambd", "type": "AnyTorchScalarType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.sort",
    "summary": "Generated op for `aten::sort : (Tensor, int, bool) -> (Tensor, Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "dim", "type": "Torch_IntType" },
      { "name": "descending", "type": "Torch_BoolType" }
    ],
    "outputs": [
      { "name": "values", "type": "AnyTorchOptionalTensorType" },
      { "name": "indices", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.sort.int",
    "summary": "Generated op for `aten::sort.int : (int[], bool) -> ()`",
    "inputs": [
      { "name": "self", "type": "AnyTorchListOfTorchIntType" },
      { "name": "reverse", "type": "Torch_BoolType" }
    ]
  },
  {
    "name": "torch.aten.special_expm1",
    "summary": "Generated op for `aten::special_expm1 : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.split_copy.Tensor",
    "summary": "Generated op for `aten::split_copy.Tensor : (Tensor, int, int) -> (Tensor[])`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "split_size", "type": "Torch_IntType" },
      { "name": "dim", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchListOfTensorType" }
    ]
  },
  {
    "name": "torch.aten.split_with_sizes",
    "summary": "Generated op for `aten::split_with_sizes : (Tensor, int[], int) -> (Tensor[])`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "split_sizes", "type": "AnyTorchListOfTorchIntType" },
      { "name": "dim", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchListOfTensorType" }
    ]
  },
  {
    "name": "torch.aten.split_with_sizes_copy",
    "summary": "Generated op for `aten::split_with_sizes_copy : (Tensor, int[], int) -> (Tensor[])`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "split_sizes", "type": "AnyTorchListOfTorchIntType" },
      { "name": "dim", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchListOfTensorType" }
    ]
  },
  {
    "name": "torch.aten.split.sizes",
    "summary": "Generated op for `aten::split.sizes : (Tensor, int[], int) -> (Tensor[])`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "split_size", "type": "AnyTorchListOfTorchIntType" },
      { "name": "dim", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchListOfTensorType" }
    ]
  },
  {
    "name": "torch.aten.split.Tensor",
    "summary": "Generated op for `aten::split.Tensor : (Tensor, int, int) -> (Tensor[])`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "split_size", "type": "Torch_IntType" },
      { "name": "dim", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchListOfTensorType" }
    ]
  },
  {
    "name": "torch.aten.sqrt",
    "summary": "Generated op for `aten::sqrt : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.sqrt_",
    "summary": "Generated op for `aten::sqrt_ : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.sqrt.int",
    "summary": "Generated op for `aten::sqrt.int : (int) -> (float)`",
    "inputs": [
      { "name": "a", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "Torch_FloatType" }
    ]
  },
  {
    "name": "torch.aten.square",
    "summary": "Generated op for `aten::square : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.square_",
    "summary": "Generated op for `aten::square_ : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.squeeze",
    "summary": "Generated op for `aten::squeeze : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.squeeze_copy",
    "summary": "Generated op for `aten::squeeze_copy : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.squeeze_copy.dim",
    "summary": "Generated op for `aten::squeeze_copy.dim : (Tensor, int) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "dim", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.squeeze.dim",
    "summary": "Generated op for `aten::squeeze.dim : (Tensor, int) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "dim", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.stack",
    "summary": "Generated op for `aten::stack : (Tensor[], int) -> (Tensor)`",
    "inputs": [
      { "name": "tensors", "type": "AnyTorchListOfTensorType" },
      { "name": "dim", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.std",
    "summary": "Generated op for `aten::std : (Tensor, bool) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "unbiased", "type": "Torch_BoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.std.correction",
    "summary": "Generated op for `aten::std.correction : (Tensor, int[]?, Scalar?, bool) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "dim", "type": "AnyTorchOptionalListOfTorchIntType" },
      { "name": "correction", "type": "AnyTorchOptionalScalarType" },
      { "name": "keepdim", "type": "Torch_BoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.std.dim",
    "summary": "Generated op for `aten::std.dim : (Tensor, int[]?, bool, bool) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "dim", "type": "AnyTorchOptionalListOfTorchIntType" },
      { "name": "unbiased", "type": "Torch_BoolType" },
      { "name": "keepdim", "type": "Torch_BoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.stft",
    "summary": "Generated op for `aten::stft : (Tensor, int, int?, int?, Tensor?, bool, bool?, bool?, bool?) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "n_fft", "type": "Torch_IntType" },
      { "name": "hop_length", "type": "AnyTorchOptionalIntType" },
      { "name": "win_length", "type": "AnyTorchOptionalIntType" },
      { "name": "window", "type": "AnyTorchOptionalTensorType" },
      { "name": "normalized", "type": "Torch_BoolType" },
      { "name": "onesided", "type": "AnyTorchOptionalBoolType" },
      { "name": "return_complex", "type": "AnyTorchOptionalBoolType" },
      { "name": "align_to_window", "type": "AnyTorchOptionalBoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.stft.center",
    "summary": "Generated op for `aten::stft.center : (Tensor, int, int?, int?, Tensor?, bool, str, bool, bool?, bool?, bool?) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "n_fft", "type": "Torch_IntType" },
      { "name": "hop_length", "type": "AnyTorchOptionalIntType" },
      { "name": "win_length", "type": "AnyTorchOptionalIntType" },
      { "name": "window", "type": "AnyTorchOptionalTensorType" },
      { "name": "center", "type": "Torch_BoolType" },
      { "name": "pad_mode", "type": "Torch_StringType" },
      { "name": "normalized", "type": "Torch_BoolType" },
      { "name": "onesided", "type": "AnyTorchOptionalBoolType" },
      { "name": "return_complex", "type": "AnyTorchOptionalBoolType" },
      { "name": "align_to_window", "type": "AnyTorchOptionalBoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.str",
    "summary": "Generated op for `aten::str : (t) -> (str)`",
    "inputs": [
      { "name": "elem", "type": "AnyTorchType" }
    ],
    "outputs": [
      { "name": "result", "type": "Torch_StringType" }
    ]
  },
  {
    "name": "torch.aten.sub",
    "summary": "Generated op for `aten::sub : (Scalar, Scalar) -> (Scalar)`",
    "inputs": [
      { "name": "a", "type": "AnyTorchScalarType" },
      { "name": "b", "type": "AnyTorchScalarType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchScalarType" }
    ]
  },
  {
    "name": "torch.aten.sub_.Scalar",
    "summary": "Generated op for `aten::sub_.Scalar : (Tensor, Scalar, Scalar) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" },
      { "name": "other", "type": "AnyTorchScalarType" },
      { "name": "alpha", "type": "AnyTorchScalarType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.sub_.Tensor",
    "summary": "Generated op for `aten::sub_.Tensor : (Tensor, Tensor, Scalar) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" },
      { "name": "other", "type": "Torch_NonValueTensorType" },
      { "name": "alpha", "type": "AnyTorchScalarType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.sub.float",
    "summary": "Generated op for `aten::sub.float : (float, float) -> (float)`",
    "inputs": [
      { "name": "a", "type": "Torch_FloatType" },
      { "name": "b", "type": "Torch_FloatType" }
    ],
    "outputs": [
      { "name": "result", "type": "Torch_FloatType" }
    ]
  },
  {
    "name": "torch.aten.sub.int",
    "summary": "Generated op for `aten::sub.int : (int, int) -> (int)`",
    "inputs": [
      { "name": "a", "type": "Torch_IntType" },
      { "name": "b", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "Torch_IntType" }
    ]
  },
  {
    "name": "torch.aten.sub.Scalar",
    "summary": "Generated op for `aten::sub.Scalar : (Tensor, Scalar, Scalar) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "other", "type": "AnyTorchScalarType" },
      { "name": "alpha", "type": "AnyTorchScalarType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.sub.Tensor",
    "summary": "Generated op for `aten::sub.Tensor : (Tensor, Tensor, Scalar) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "other", "type": "AnyTorchTensorType" },
      { "name": "alpha", "type": "AnyTorchScalarType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.sum",
    "summary": "Generated op for `aten::sum : (Tensor, int?) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "dtype", "type": "AnyTorchOptionalIntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.sum.dim_IntList",
    "summary": "Generated op for `aten::sum.dim_IntList : (Tensor, int[]?, bool, int?) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "dim", "type": "AnyTorchOptionalListOfTorchIntType" },
      { "name": "keepdim", "type": "Torch_BoolType" },
      { "name": "dtype", "type": "AnyTorchOptionalIntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.sym_constrain_range",
    "summary": "Generated op for `aten::sym_constrain_range : (Scalar, int?, int?) -> ()`",
    "inputs": [
      { "name": "size", "type": "AnyTorchScalarType" },
      { "name": "min", "type": "AnyTorchOptionalIntType" },
      { "name": "max", "type": "AnyTorchOptionalIntType" }
    ]
  },
  {
    "name": "torch.aten.sym_constrain_range_for_size",
    "summary": "Generated op for `aten::sym_constrain_range_for_size : (Scalar, int?, int?) -> ()`",
    "inputs": [
      { "name": "size", "type": "AnyTorchScalarType" },
      { "name": "min", "type": "AnyTorchOptionalIntType" },
      { "name": "max", "type": "AnyTorchOptionalIntType" }
    ]
  },
  {
    "name": "torch.aten.t",
    "summary": "Generated op for `aten::t : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.t_copy",
    "summary": "Generated op for `aten::t_copy : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.tan",
    "summary": "Generated op for `aten::tan : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.tan_",
    "summary": "Generated op for `aten::tan_ : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.tanh",
    "summary": "Generated op for `aten::tanh : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.tanh_",
    "summary": "Generated op for `aten::tanh_ : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.tanh_backward",
    "summary": "Generated op for `aten::tanh_backward : (Tensor, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "grad_output", "type": "AnyTorchTensorType" },
      { "name": "output", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.tensor",
    "summary": "Generated op for `aten::tensor : (t[], int?, Device?, bool) -> (Tensor)`",
    "inputs": [
      { "name": "data", "type": "AnyTorchListType" },
      { "name": "dtype", "type": "AnyTorchOptionalIntType" },
      { "name": "device", "type": "AnyTorchOptionalDeviceType" },
      { "name": "requires_grad", "type": "Torch_BoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.tensor_split.sections",
    "summary": "Generated op for `aten::tensor_split.sections : (Tensor, int, int) -> (Tensor[])`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "sections", "type": "Torch_IntType" },
      { "name": "dim", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchListOfTensorType" }
    ]
  },
  {
    "name": "torch.aten.tensor.bool",
    "summary": "Generated op for `aten::tensor.bool : (bool, int?, Device?, bool) -> (Tensor)`",
    "inputs": [
      { "name": "t", "type": "Torch_BoolType" },
      { "name": "dtype", "type": "AnyTorchOptionalIntType" },
      { "name": "device", "type": "AnyTorchOptionalDeviceType" },
      { "name": "requires_grad", "type": "Torch_BoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.tensor.float",
    "summary": "Generated op for `aten::tensor.float : (float, int?, Device?, bool) -> (Tensor)`",
    "inputs": [
      { "name": "t", "type": "Torch_FloatType" },
      { "name": "dtype", "type": "AnyTorchOptionalIntType" },
      { "name": "device", "type": "AnyTorchOptionalDeviceType" },
      { "name": "requires_grad", "type": "Torch_BoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.tensor.int",
    "summary": "Generated op for `aten::tensor.int : (int, int?, Device?, bool) -> (Tensor)`",
    "inputs": [
      { "name": "t", "type": "Torch_IntType" },
      { "name": "dtype", "type": "AnyTorchOptionalIntType" },
      { "name": "device", "type": "AnyTorchOptionalDeviceType" },
      { "name": "requires_grad", "type": "Torch_BoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.threshold",
    "summary": "Generated op for `aten::threshold : (Tensor, Scalar, Scalar) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "threshold", "type": "AnyTorchScalarType" },
      { "name": "value", "type": "AnyTorchScalarType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.threshold_",
    "summary": "Generated op for `aten::threshold_ : (Tensor, Scalar, Scalar) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" },
      { "name": "threshold", "type": "AnyTorchScalarType" },
      { "name": "value", "type": "AnyTorchScalarType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.threshold_backward",
    "summary": "Generated op for `aten::threshold_backward : (Tensor, Tensor, Scalar) -> (Tensor)`",
    "inputs": [
      { "name": "grad_output", "type": "AnyTorchTensorType" },
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "threshold", "type": "AnyTorchScalarType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.tile",
    "summary": "Generated op for `aten::tile : (Tensor, int[]) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "dims", "type": "AnyTorchListOfTorchIntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.to.device",
    "summary": "Generated op for `aten::to.device : (Tensor, Device, int, bool, bool, int?) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "device", "type": "Torch_DeviceType" },
      { "name": "dtype", "type": "Torch_IntType" },
      { "name": "non_blocking", "type": "Torch_BoolType" },
      { "name": "copy", "type": "Torch_BoolType" },
      { "name": "memory_format", "type": "AnyTorchOptionalIntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.to.dtype",
    "summary": "Generated op for `aten::to.dtype : (Tensor, int, bool, bool, int?) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "dtype", "type": "Torch_IntType" },
      { "name": "non_blocking", "type": "Torch_BoolType" },
      { "name": "copy", "type": "Torch_BoolType" },
      { "name": "memory_format", "type": "AnyTorchOptionalIntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.to.dtype_layout",
    "summary": "Generated op for `aten::to.dtype_layout : (Tensor, int?, int?, Device?, bool?, bool, bool, int?) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "dtype", "type": "AnyTorchOptionalIntType" },
      { "name": "layout", "type": "AnyTorchOptionalIntType" },
      { "name": "device", "type": "AnyTorchOptionalDeviceType" },
      { "name": "pin_memory", "type": "AnyTorchOptionalBoolType" },
      { "name": "non_blocking", "type": "Torch_BoolType" },
      { "name": "copy", "type": "Torch_BoolType" },
      { "name": "memory_format", "type": "AnyTorchOptionalIntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.to.other",
    "summary": "Generated op for `aten::to.other : (Tensor, Tensor, bool, bool, int?) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "other", "type": "AnyTorchTensorType" },
      { "name": "non_blocking", "type": "Torch_BoolType" },
      { "name": "copy", "type": "Torch_BoolType" },
      { "name": "memory_format", "type": "AnyTorchOptionalIntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.to.prim_Device",
    "summary": "Generated op for `aten::to.prim_Device : (Tensor, Device?, int?, bool, bool) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "device", "type": "AnyTorchOptionalDeviceType" },
      { "name": "dtype", "type": "AnyTorchOptionalIntType" },
      { "name": "non_blocking", "type": "Torch_BoolType" },
      { "name": "copy", "type": "Torch_BoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.topk",
    "summary": "Generated op for `aten::topk : (Tensor, int, int, bool, bool) -> (Tensor, Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "k", "type": "Torch_IntType" },
      { "name": "dim", "type": "Torch_IntType" },
      { "name": "largest", "type": "Torch_BoolType" },
      { "name": "sorted", "type": "Torch_BoolType" }
    ],
    "outputs": [
      { "name": "values", "type": "AnyTorchOptionalTensorType" },
      { "name": "indices", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.trace",
    "summary": "Generated op for `aten::trace : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.transpose_copy.int",
    "summary": "Generated op for `aten::transpose_copy.int : (Tensor, int, int) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "dim0", "type": "Torch_IntType" },
      { "name": "dim1", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.transpose.int",
    "summary": "Generated op for `aten::transpose.int : (Tensor, int, int) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "dim0", "type": "Torch_IntType" },
      { "name": "dim1", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.tril",
    "summary": "Generated op for `aten::tril : (Tensor, int) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "diagonal", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.tril_",
    "summary": "Generated op for `aten::tril_ : (Tensor, int) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" },
      { "name": "diagonal", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.tril_indices",
    "summary": "Generated op for `aten::tril_indices : (int, int, int, int?, int?, Device?, bool?) -> (Tensor)`",
    "inputs": [
      { "name": "row", "type": "Torch_IntType" },
      { "name": "col", "type": "Torch_IntType" },
      { "name": "offset", "type": "Torch_IntType" },
      { "name": "dtype", "type": "AnyTorchOptionalIntType" },
      { "name": "layout", "type": "AnyTorchOptionalIntType" },
      { "name": "device", "type": "AnyTorchOptionalDeviceType" },
      { "name": "pin_memory", "type": "AnyTorchOptionalBoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.triu",
    "summary": "Generated op for `aten::triu : (Tensor, int) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "diagonal", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.triu_",
    "summary": "Generated op for `aten::triu_ : (Tensor, int) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" },
      { "name": "diagonal", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.triu_indices",
    "summary": "Generated op for `aten::triu_indices : (int, int, int, int?, int?, Device?, bool?) -> (Tensor)`",
    "inputs": [
      { "name": "row", "type": "Torch_IntType" },
      { "name": "col", "type": "Torch_IntType" },
      { "name": "offset", "type": "Torch_IntType" },
      { "name": "dtype", "type": "AnyTorchOptionalIntType" },
      { "name": "layout", "type": "AnyTorchOptionalIntType" },
      { "name": "device", "type": "AnyTorchOptionalDeviceType" },
      { "name": "pin_memory", "type": "AnyTorchOptionalBoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.trunc",
    "summary": "Generated op for `aten::trunc : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.trunc_",
    "summary": "Generated op for `aten::trunc_ : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.type_as",
    "summary": "Generated op for `aten::type_as : (Tensor, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "other", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.unbind_copy.int",
    "summary": "Generated op for `aten::unbind_copy.int : (Tensor, int) -> (Tensor[])`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "dim", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchListOfTensorType" }
    ]
  },
  {
    "name": "torch.aten.unbind.int",
    "summary": "Generated op for `aten::unbind.int : (Tensor, int) -> (Tensor[])`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "dim", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchListOfTensorType" }
    ]
  },
  {
    "name": "torch.aten.unflatten.int",
    "summary": "Generated op for `aten::unflatten.int : (Tensor, int, int[]) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "dim", "type": "Torch_IntType" },
      { "name": "sizes", "type": "AnyTorchListOfTorchIntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.unfold",
    "summary": "Generated op for `aten::unfold : (Tensor, int, int, int) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "dimension", "type": "Torch_IntType" },
      { "name": "size", "type": "Torch_IntType" },
      { "name": "step", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.unfold_copy",
    "summary": "Generated op for `aten::unfold_copy : (Tensor, int, int, int) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "dimension", "type": "Torch_IntType" },
      { "name": "size", "type": "Torch_IntType" },
      { "name": "step", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.uniform",
    "summary": "Generated op for `aten::uniform : (Tensor, float, float, Generator?) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "from", "type": "Torch_FloatType" },
      { "name": "to", "type": "Torch_FloatType" },
      { "name": "generator", "type": "AnyTorchOptionalGeneratorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.uniform_",
    "summary": "Generated op for `aten::uniform_ : (Tensor, float, float, Generator?) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" },
      { "name": "from", "type": "Torch_FloatType" },
      { "name": "to", "type": "Torch_FloatType" },
      { "name": "generator", "type": "AnyTorchOptionalGeneratorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.unique_consecutive",
    "summary": "Generated op for `aten::unique_consecutive : (Tensor, bool, bool, int?) -> (Tensor, Tensor, Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "return_inverse", "type": "Torch_BoolType" },
      { "name": "return_counts", "type": "Torch_BoolType" },
      { "name": "dim", "type": "AnyTorchOptionalIntType" }
    ],
    "outputs": [
      { "name": "result0", "type": "AnyTorchOptionalTensorType" },
      { "name": "result1", "type": "AnyTorchOptionalTensorType" },
      { "name": "result2", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.unique_dim",
    "summary": "Generated op for `aten::unique_dim : (Tensor, int, bool, bool, bool) -> (Tensor, Tensor, Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "dim", "type": "Torch_IntType" },
      { "name": "sorted", "type": "Torch_BoolType" },
      { "name": "return_inverse", "type": "Torch_BoolType" },
      { "name": "return_counts", "type": "Torch_BoolType" }
    ],
    "outputs": [
      { "name": "result0", "type": "AnyTorchOptionalTensorType" },
      { "name": "result1", "type": "AnyTorchOptionalTensorType" },
      { "name": "result2", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.unsqueeze",
    "summary": "Generated op for `aten::unsqueeze : (Tensor, int) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "dim", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.unsqueeze_",
    "summary": "Generated op for `aten::unsqueeze_ : (Tensor, int) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" },
      { "name": "dim", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.unsqueeze_copy",
    "summary": "Generated op for `aten::unsqueeze_copy : (Tensor, int) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "dim", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.upsample_bilinear2d",
    "summary": "Generated op for `aten::upsample_bilinear2d : (Tensor, int[], bool, float?, float?) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "output_size", "type": "AnyTorchListOfTorchIntType" },
      { "name": "align_corners", "type": "Torch_BoolType" },
      { "name": "scales_h", "type": "AnyTorchOptionalFloatType" },
      { "name": "scales_w", "type": "AnyTorchOptionalFloatType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.upsample_bilinear2d.vec",
    "summary": "Generated op for `aten::upsample_bilinear2d.vec : (Tensor, int[]?, bool, float[]?) -> (Tensor)`",
    "inputs": [
      { "name": "input", "type": "AnyTorchTensorType" },
      { "name": "output_size", "type": "AnyTorchOptionalListOfTorchIntType" },
      { "name": "align_corners", "type": "Torch_BoolType" },
      { "name": "scale_factors", "type": "AnyTorchOptionalListOfTorchFloatType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.upsample_nearest1d",
    "summary": "Generated op for `aten::upsample_nearest1d : (Tensor, int[], float?) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "output_size", "type": "AnyTorchListOfTorchIntType" },
      { "name": "scales", "type": "AnyTorchOptionalFloatType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.upsample_nearest1d.vec",
    "summary": "Generated op for `aten::upsample_nearest1d.vec : (Tensor, int[]?, float[]?) -> (Tensor)`",
    "inputs": [
      { "name": "input", "type": "AnyTorchTensorType" },
      { "name": "output_size", "type": "AnyTorchOptionalListOfTorchIntType" },
      { "name": "scale_factors", "type": "AnyTorchOptionalListOfTorchFloatType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.upsample_nearest2d",
    "summary": "Generated op for `aten::upsample_nearest2d : (Tensor, int[], float?, float?) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "output_size", "type": "AnyTorchListOfTorchIntType" },
      { "name": "scales_h", "type": "AnyTorchOptionalFloatType" },
      { "name": "scales_w", "type": "AnyTorchOptionalFloatType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.upsample_nearest2d_backward",
    "summary": "Generated op for `aten::upsample_nearest2d_backward : (Tensor, int[], int[], float?, float?) -> (Tensor)`",
    "inputs": [
      { "name": "grad_output", "type": "AnyTorchTensorType" },
      { "name": "output_size", "type": "AnyTorchListOfTorchIntType" },
      { "name": "input_size", "type": "AnyTorchListOfTorchIntType" },
      { "name": "scales_h", "type": "AnyTorchOptionalFloatType" },
      { "name": "scales_w", "type": "AnyTorchOptionalFloatType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.upsample_nearest2d.vec",
    "summary": "Generated op for `aten::upsample_nearest2d.vec : (Tensor, int[]?, float[]?) -> (Tensor)`",
    "inputs": [
      { "name": "input", "type": "AnyTorchTensorType" },
      { "name": "output_size", "type": "AnyTorchOptionalListOfTorchIntType" },
      { "name": "scale_factors", "type": "AnyTorchOptionalListOfTorchFloatType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.var",
    "summary": "Generated op for `aten::var : (Tensor, bool) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "unbiased", "type": "Torch_BoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.var_mean",
    "summary": "Generated op for `aten::var_mean : (Tensor, bool) -> (Tensor, Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "unbiased", "type": "Torch_BoolType" }
    ],
    "outputs": [
      { "name": "result0", "type": "AnyTorchOptionalTensorType" },
      { "name": "result1", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.var_mean.correction",
    "summary": "Generated op for `aten::var_mean.correction : (Tensor, int[]?, Scalar?, bool) -> (Tensor, Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "dim", "type": "AnyTorchOptionalListOfTorchIntType" },
      { "name": "correction", "type": "AnyTorchOptionalScalarType" },
      { "name": "keepdim", "type": "Torch_BoolType" }
    ],
    "outputs": [
      { "name": "result0", "type": "AnyTorchOptionalTensorType" },
      { "name": "result1", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.var_mean.dim",
    "summary": "Generated op for `aten::var_mean.dim : (Tensor, int[]?, bool, bool) -> (Tensor, Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "dim", "type": "AnyTorchOptionalListOfTorchIntType" },
      { "name": "unbiased", "type": "Torch_BoolType" },
      { "name": "keepdim", "type": "Torch_BoolType" }
    ],
    "outputs": [
      { "name": "result0", "type": "AnyTorchOptionalTensorType" },
      { "name": "result1", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.var.correction",
    "summary": "Generated op for `aten::var.correction : (Tensor, int[]?, Scalar?, bool) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "dim", "type": "AnyTorchOptionalListOfTorchIntType" },
      { "name": "correction", "type": "AnyTorchOptionalScalarType" },
      { "name": "keepdim", "type": "Torch_BoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.var.dim",
    "summary": "Generated op for `aten::var.dim : (Tensor, int[]?, bool, bool) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "dim", "type": "AnyTorchOptionalListOfTorchIntType" },
      { "name": "unbiased", "type": "Torch_BoolType" },
      { "name": "keepdim", "type": "Torch_BoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.view",
    "summary": "Generated op for `aten::view : (Tensor, int[]) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "size", "type": "AnyTorchListOfTorchIntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.view_as_complex",
    "summary": "Generated op for `aten::view_as_complex : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.view_as_real",
    "summary": "Generated op for `aten::view_as_real : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.view_copy",
    "summary": "Generated op for `aten::view_copy : (Tensor, int[]) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "size", "type": "AnyTorchListOfTorchIntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.view_copy.dtype",
    "summary": "Generated op for `aten::view_copy.dtype : (Tensor, int) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "dtype", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.view.dtype",
    "summary": "Generated op for `aten::view.dtype : (Tensor, int) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "dtype", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.warn",
    "summary": "Generated op for `aten::warn : (str, int) -> ()`",
    "inputs": [
      { "name": "message", "type": "Torch_StringType" },
      { "name": "stacklevel", "type": "Torch_IntType" }
    ]
  },
  {
    "name": "torch.aten.where.Scalar",
    "summary": "Generated op for `aten::where.Scalar : (Tensor, Scalar, Scalar) -> (Tensor)`",
    "inputs": [
      { "name": "condition", "type": "AnyTorchTensorType" },
      { "name": "self", "type": "AnyTorchScalarType" },
      { "name": "other", "type": "AnyTorchScalarType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.where.ScalarOther",
    "summary": "Generated op for `aten::where.ScalarOther : (Tensor, Tensor, Scalar) -> (Tensor)`",
    "inputs": [
      { "name": "condition", "type": "AnyTorchTensorType" },
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "other", "type": "AnyTorchScalarType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.where.ScalarSelf",
    "summary": "Generated op for `aten::where.ScalarSelf : (Tensor, Scalar, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "condition", "type": "AnyTorchTensorType" },
      { "name": "self", "type": "AnyTorchScalarType" },
      { "name": "other", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.where.self",
    "summary": "Generated op for `aten::where.self : (Tensor, Tensor, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "condition", "type": "AnyTorchTensorType" },
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "other", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.xlogy.Tensor",
    "summary": "Generated op for `aten::xlogy.Tensor : (Tensor, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "other", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.zero",
    "summary": "Generated op for `aten::zero : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.zero_",
    "summary": "Generated op for `aten::zero_ : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.zeros",
    "summary": "Generated op for `aten::zeros : (int[], int?, int?, Device?, bool?) -> (Tensor)`",
    "inputs": [
      { "name": "size", "type": "AnyTorchListOfTorchIntType" },
      { "name": "dtype", "type": "AnyTorchOptionalIntType" },
      { "name": "layout", "type": "AnyTorchOptionalIntType" },
      { "name": "device", "type": "AnyTorchOptionalDeviceType" },
      { "name": "pin_memory", "type": "AnyTorchOptionalBoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.zeros_like",
    "summary": "Generated op for `aten::zeros_like : (Tensor, int?, int?, Device?, bool?, int?) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "dtype", "type": "AnyTorchOptionalIntType" },
      { "name": "layout", "type": "AnyTorchOptionalIntType" },
      { "name": "device", "type": "AnyTorchOptionalDeviceType" },
      { "name": "pin_memory", "type": "AnyTorchOptionalBoolType" },
      { "name": "memory_format", "type": "AnyTorchOptionalIntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.attr",
    "summary": "Declare an attribute of a torch.class_type",
    "description": "This op declaratively specifies that torch.nn.Module's of the parent\n    torch.class_type must have an attribute `name` of type `type`.\n\n    If `private` is present, it indicates that the value of this attribute\n    cannot be accessed externally.",
    "attributes": [
      { "name": "name", "type": "StrAttr" },
      { "name": "type", "type": "TypeAttr" },
      { "name": "isPrivate", "type": "UnitAttr" }
    ],
    "assemblyFormat": "(`private` $isPrivate^)? $name `:` $type attr-dict"
  },
  {
    "name": "torch.bind_symbolic_shape",
    "summary": "Binds shape expressions to tensors using an affine map indexed by shape symbols",
    "description": "The `torch.bind_symbolic_shape` operation binds shape expressions\n    useful to compute the dynamic dimensions of a tensor. It takes a\n    variadic of SSA symbols that map 1:1 to the local symbols declared\n    in the affine map. The affine map contains a list of affine shape\n    expressions for each dim where the terminals are from the declared\n    symbols.\n\n    Example:\n    ```\n    torch.bind_symbolic_shape %arg0, [%0, %1], affine_map<()[s0, s1] -> (s0, s1, 3)> : !torch.vtensor<[?,?,3],f32>\n    torch.bind_symbolic_shape %out0, [%0, %1, %2], affine_map<()[s0, s1, s2] -> (s0, s1 * 2 + s2, 3)> : !torch.vtensor<[?,?,3],f32>\n    ```",
    "inputs": [
      { "name": "operand", "type": "Torch_ValueTensorType" },
      { "name": "shape_symbols", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "shape_expressions", "type": "Builtin_AffineMapAttr" }
    ]
  },
  {
    "name": "torch.class_type",
    "summary": "Constructs a torch.ClassType",
    "description": "Declares a class type. Class types are the types used to describe\n    TorchScript `torch.nn.Module`'s. The terminology \"class type\" is for\n    consistency with TorchScript (a better name in our context might be\n    \"nn module subtype\"). The `syn_name` of this op is the same string\n    as in the `!torch.nn.Module<\"...\">` type.\n\n    Example:\n\n    ```mlir\n    // A simple empty torch.class_type, with corresponding torch.nn_module.\n    torch.class_type @empty {}\n    %submodule = torch.nn_module {} : !torch.nn.Module<\"empty\">\n\n    // A class type with many members.\n    torch.class_type @test {\n      torch.attr \"b\" : !torch.bool\n      torch.attr \"i\" : !torch.int\n      torch.attr \"f\" : !torch.float\n      torch.attr \"t\" : !torch.tensor\n      torch.attr \"submodule\" : !torch.nn.Module<\"empty\">\n      torch.method \"method\", @f\n    }\n    torch.nn_module {\n      // These must match the order and names in the `torch.class_type`.\n      torch.slot \"b\", %bool_true : !torch.bool\n      torch.slot \"i\", %int3 : !torch.int\n      torch.slot \"f\", %float : !torch.float\n      torch.slot \"t\", %t : !torch.tensor\n      torch.slot \"submodule\", %submodule : !torch.nn.Module<\"empty\">\n    } : !torch.nn.Module<\"test\">\n    ```",
    "attributes": [
      { "name": "sym_name", "type": "SymbolNameAttr" }
    ],
    "assemblyFormat": "$sym_name $region attr-dict"
  },
  {
    "name": "torch.class_type_terminator",
    "summary": "Implicit terminator for torch.class_type",
    "assemblyFormat": "attr-dict"
  },
  {
    "name": "torch.constant.bool",
    "summary": "Materialize a constant `bool` value.",
    "outputs": [
      { "name": "result", "type": "Torch_BoolType" }
    ],
    "attributes": [
      { "name": "value", "type": "I1Attr" }
    ],
    "assemblyFormat": "$value attr-dict"
  },
  {
    "name": "torch.constant.device",
    "summary": "Materialize a constant Device value.",
    "outputs": [
      { "name": "result", "type": "Torch_DeviceType" }
    ],
    "attributes": [
      { "name": "value", "type": "StrAttr" }
    ],
    "assemblyFormat": "$value attr-dict"
  },
  {
    "name": "torch.constant.float",
    "summary": "Materialize a constant `float` value.",
    "description": "Note: TorchScript represents `float` as 64-bit floating point values.\n\n    TODO: Add a `!torch.float` type.",
    "outputs": [
      { "name": "result", "type": "Torch_FloatType" }
    ],
    "attributes": [
      { "name": "value", "type": "F64Attr" }
    ],
    "assemblyFormat": "$value attr-dict"
  },
  {
    "name": "torch.constant.int",
    "summary": "Materialize a constant `int` value.",
    "description": "Note: TorchScript represents integers as 64-bit signed values, unlike\n    Python where they are arbitrary precision.",
    "outputs": [
      { "name": "result", "type": "Torch_IntType" }
    ],
    "attributes": [
      { "name": "value", "type": "I64Attr" }
    ]
  },
  {
    "name": "torch.constant.none",
    "summary": "Get the singleton None value.",
    "description": "Not to be confused with the `mlir::NoneType`. Be careful to use\n    `Torch::NoneType` to avoid namespace ambiguity.",
    "outputs": [
      { "name": "result", "type": "Torch_NoneType" }
    ],
    "assemblyFormat": "attr-dict"
  },
  {
    "name": "torch.constant.number",
    "summary": "Materialize a constant `number` value.",
    "description": "This op is used as a workaround to the fact that the constant\n    materialization in MLIR must materialize a constant with a single op.\n    To materialize ops with a static `!torch.number` type, we must use this op,\n    even though we statically know if it is an integer or a float.\n\n    Note: This op unconditionally canonicalizes to\n    `torch.constant.{float,int}` + `torch.derefine`",
    "outputs": [
      { "name": "result", "type": "Torch_NumberType" }
    ],
    "attributes": [
      { "name": "value", "type": "AnyAttrOf" }
    ]
  },
  {
    "name": "torch.constant.str",
    "summary": "Materialize a constant str value.",
    "description": "Note: Strings in Python (and TorchScript) are immutable.",
    "outputs": [
      { "name": "result", "type": "Torch_StringType" }
    ],
    "attributes": [
      { "name": "value", "type": "StrAttr" }
    ],
    "assemblyFormat": "$value attr-dict"
  },
  {
    "name": "torch.copy.to_tensor",
    "summary": "Create a !torch.tensor with the same contents as the operand",
    "description": "This op is used to convert from !torch.vtensor to !torch.tensor.\n    It does so by allocating a new !torch.tensor and filling it with\n    the contents of the operand.\n\n    However, this op *does not* allow adding/removing static information about\n    sizes/dtype. For that, use `torch.tensor_static_info_cast`.\n\n    This op does not have the AllowsTypeRefinement trait because the operand\n    and result types are coupled. Only places that know how to simultaneously\n    update both types should be changing the type of this op.",
    "inputs": [
      { "name": "operand", "type": "Torch_ValueTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "Torch_NonValueTensorType" }
    ],
    "assemblyFormat": "$operand attr-dict `:` qualified(type($result))"
  },
  {
    "name": "torch.copy.to_vtensor",
    "summary": "Create a !torch.vtensor with the same contents as the operand",
    "description": "This op is used to convert from !torch.tensor to !torch.vtensor.\n\n    However, this op *does not* allow adding/removing static information about\n    sizes/dtype. For that, use `torch.tensor_static_info_cast`.\n\n    This op does not have the AllowsTypeRefinement trait because the operand\n    and result types are coupled. Only places that know how to simultaneously\n    update both types should be changing the type of this op.",
    "inputs": [
      { "name": "operand", "type": "Torch_NonValueTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "Torch_ValueTensorType" }
    ],
    "assemblyFormat": "$operand attr-dict `:` qualified(type($result))"
  },
  {
    "name": "torch.derefine",
    "summary": "De-refine a type",
    "description": "In terms of IR structure, TorchScript allows types to vary in many\n    circumstances where MLIR requires pointer-identical types. In particular,\n    it is valid to pass any subtype in place of a type. For example, if an\n    `Optional[int]` is required somewhere in the IR, it is legal to pass a\n    value of just `int` (but not the other way around; see\n    `torch.prim.unchecked_cast`). In effect, every *use* can have a different\n    type.\n\n    This op bridges that impedance mismatch. This op allows casting a value\n    from one type to a type that it is a subtype of to model this behavior.\n    This op uses the TorchScript notion of subtype, which matches the\n    Python notion of subtype presented in PEP 483.",
    "inputs": [
      { "name": "operand", "type": "AnyTorchType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchType" }
    ],
    "assemblyFormat": "$operand attr-dict `:` qualified(type($operand)) `to` qualified(type($result))"
  },
  {
    "name": "torch.dtype.calculate",
    "summary": "Dtype calculation encapsulation op",
    "description": "The `torch.dtype.calculate` op captures a dtype calculation\n    (in the region `calculation`) which calculates the dtypes for\n    the set of values yielded by the `body` region.\n\n    The `calculation` region yields a `!torch.int` for each\n    value yielded by the `body` region.\n\n    Conceptually, the `calculation` region executes first, then `body`\n    region. So the `calculation` region can also contain arbitrary\n    assertions or side-effecting code which guard the validity of the execution\n    of the body (typically by terminating the program with a\n    torch.prim.RaiseException op).\n\n    The program has undefined behavior if the values yielded by the `body`\n    region do not have the dtypes yielded by the `calculation` region.",
    "outputs": [
      { "name": "results", "type": "Variadic" }
    ],
    "assemblyFormat": "$body `dtypes` $calculation attr-dict `:` type($results)"
  },
  {
    "name": "torch.dtype.calculate.yield",
    "summary": "yield-like terminator for torch.dtype.calculate",
    "description": "This op terminates the `body` region of a `torch.dtype.calculate` op.",
    "inputs": [
      { "name": "results", "type": "Variadic" }
    ],
    "assemblyFormat": "attr-dict ($results^ `:` type($results))?"
  },
  {
    "name": "torch.dtype.calculate.yield.dtypes",
    "summary": "yield-like terminator for torch.dtype.calculate shape region",
    "description": "This op terminates the `dtypeCalculation` region of a\n    `torch.dtype.calculate` op.",
    "inputs": [
      { "name": "results", "type": "Variadic" }
    ],
    "assemblyFormat": "attr-dict ($results^ `:` type($results))?"
  },
  {
    "name": "torch.global_slot",
    "summary": "A slot with global storage",
    "description": "Represents a slot with global storage. The slot semantics are the same\n    as Python's: getting or setting a slot is done by object identity.\n\n    The `typeBound` is a type that the contained type is a subtype of.",
    "attributes": [
      { "name": "sym_name", "type": "SymbolNameAttr" },
      { "name": "sym_visibility", "type": "OptionalAttr" },
      { "name": "typeBound", "type": "TypeAttr" }
    ],
    "assemblyFormat": "($sym_visibility^)? $sym_name attr-dict `:` $typeBound"
  },
  {
    "name": "torch.global_slot.get",
    "summary": "Get the value stored in a torch.global_slot",
    "outputs": [
      { "name": "result", "type": "AnyTorchType" }
    ],
    "attributes": [
      { "name": "slot", "type": "FlatSymbolRefAttr" }
    ],
    "assemblyFormat": "$slot attr-dict `:` qualified(type($result))"
  },
  {
    "name": "torch.global_slot.init",
    "summary": "yield-like terminator for torch.initialize.global_slotsr region",
    "description": "The operand to this op becomes the initial value of the parent\n    torch.global_slot.",
    "inputs": [
      { "name": "initialValue", "type": "AnyTorchType" }
    ],
    "assemblyFormat": "$initialValue attr-dict `:` qualified(type($initialValue))"
  },
  {
    "name": "torch.global_slot.module_initializer",
    "summary": "Module initializer for all `torch.global_slot` ops",
    "description": "Initializer function that runs once at program startup to initialize\n    all `torch.global_slot` ops in the module.\n\n    The only ops that should be in the module initializer should be ops\n    generated by the IValue importer. This set avoids the need to define\n    the behavior in case of certain kinds of side effects in the initializer\n    (except for the side effect of updating the torch.global_slot ops with the\n    `torch.initialize.global_slots` op).",
    "assemblyFormat": "$initializer attr-dict"
  },
  {
    "name": "torch.global_slot.set",
    "summary": "Set the value stored in a torch.global_slot",
    "inputs": [
      { "name": "value", "type": "AnyTorchType" }
    ],
    "attributes": [
      { "name": "slot", "type": "FlatSymbolRefAttr" }
    ],
    "assemblyFormat": "$slot `=` $value attr-dict `:` qualified(type($value))"
  },
  {
    "name": "torch.initialize.global_slots",
    "summary": "Terminator for torch.global_slot.module_initializer region",
    "description": "Atomically updates the value of all the global slots named in `slotSymNames`\n    with the corresponding values provided in `initialValues`.",
    "inputs": [
      { "name": "initialValues", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "slotSymNames", "type": "SymbolRefArrayAttr" }
    ]
  },
  {
    "name": "torch.linear_params.create",
    "summary": "Create a `!torch.LinearParams`",
    "inputs": [
      { "name": "weight", "type": "AnyTorchTensorType" },
      { "name": "bias", "type": "Optional" }
    ],
    "outputs": [
      { "name": "result", "type": "Torch_LinearParamsType" }
    ],
    "assemblyFormat": "$weight (`,` $bias^)? attr-dict `:` qualified(type($weight)) (`,` qualified(type($bias))^)?"
  },
  {
    "name": "torch.method",
    "summary": "Declare a method of a torch.class_type",
    "description": "This op declaratively specifies that the parent torch.class_type has a\n    method `name` which calls `function`. `function` is an unbound function.\n    That is, it explicitly takes the torch.nn.Module as a parameter (no implicit\n    \"self\" object).\n\n    If `private` is present, it indicates that external calls cannot be made\n    to this method.",
    "attributes": [
      { "name": "name", "type": "StrAttr" },
      { "name": "function", "type": "FlatSymbolRefAttr" },
      { "name": "isPrivate", "type": "UnitAttr" }
    ],
    "assemblyFormat": "(`private` $isPrivate^)? $name `,` $function attr-dict"
  },
  {
    "name": "torch.nn_module",
    "summary": "Constructs a torch.nn.Module",
    "description": "This op is used to represent a torch.nn.Module when importing a\n    graph of Python objects.\n\n    This op returns a new torch.nn.Module as an SSA value, with a set of\n    declaratively specified properties.\n\n    Example:\n\n    ```mlir\n    %2 = torch.nn_module {\n      torch.slot \"b\", %bool_true : !torch.bool\n      torch.slot \"i\", %int3 : !torch.int\n      torch.slot \"f\", %float : !torch.float\n      torch.slot \"t\", %t : !torch.tensor\n      torch.slot \"submodule\", %1 : !torch.nn.Module\n    } : !torch.nn.Module<\"my_class_name\">\n    ```\n\n    This op is tightly coupled to the `torch.class_type` op named in the\n    `!torch.nn.Module<\"my_class_name\">` type. Each slot must match precisely\n    with the corresponding `torch.attr` in the `torch.class_type`.\n    See the documentation for `torch.class_type` for information.",
    "outputs": [
      { "name": "result", "type": "Torch_NnModuleType" }
    ],
    "assemblyFormat": "$region attr-dict `:` qualified(type($result))"
  },
  {
    "name": "torch.nn_module_terminator",
    "summary": "Implicit terminator for torch.nn_module",
    "assemblyFormat": "attr-dict"
  },
  {
    "name": "torch.onnx.rotary_embedding",
    "summary": "`rotary_embedding op : (Tensor, Tensor, Tensor, Tensor, int, int, int, int, float) -> (Tensor)`",
    "description": "The `torch.onnx.rotary_embedding` operation is an op which is used\n    specifically for supporting the Onnx's Rotary Embedding op. The\n    reason for this is that the Onnx ops can't be directly lowered to\n    Linalg and we have to map them to a legal Torch Dialect op, hence\n    this op is used for that purpose.",
    "inputs": [
      { "name": "input", "type": "AnyTorchTensorType" },
      { "name": "position_ids", "type": "AnyTorchTensorType" },
      { "name": "cos_cache", "type": "AnyTorchTensorType" },
      { "name": "sin_cache", "type": "AnyTorchTensorType" },
      { "name": "interleaved", "type": "Torch_IntType" },
      { "name": "is_packed_batching", "type": "Torch_IntType" },
      { "name": "num_heads", "type": "Torch_IntType" },
      { "name": "rotary_embedding_dim", "type": "Torch_IntType" },
      { "name": "scale", "type": "Torch_FloatType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchTensorType" }
    ]
  },
  {
    "name": "torch.operator",
    "summary": "Opaque torch operator",
    "description": "Represents an invocation of a `torch::jit::Operator` for which we don't\n    have a registered MLIR operation.\n\n    The `name` attribute contains the name that the MLIR op would have\n    (excluding `torch.`) if we did have it registered, which allows easy\n    cross referencing with `JITOperatorRegistryDump.txt`.",
    "inputs": [
      { "name": "operands", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "results", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "name", "type": "StrAttr" }
    ],
    "assemblyFormat": "$name `(` $operands `)` attr-dict `:` functional-type($operands, $results) $regions"
  },
  {
    "name": "torch.operator_terminator",
    "summary": "Implicit terminator for torch.operator",
    "inputs": [
      { "name": "operands", "type": "Variadic" }
    ],
    "assemblyFormat": "$operands attr-dict `:` type($operands)"
  },
  {
    "name": "torch.overwrite.tensor.contents",
    "summary": "Ovewrite the contents of tensor with values from another.",
    "description": "Replaces the contents of `overwritten` with corresponding values from\n    `value`.\n\n    Immediately after this op has completed, indexing `overwritten` will result\n    in identical values as indexing into `value`. Of course, later ops\n    might mutate `overwritten`, so this relationship need not hold for the\n    entire program. This op only updates the tensor data (not metadata).\n    In other words, it cannot change the (dynamic) shape of the overwritten tensor.\n\n    This op does not have the AllowsTypeRefinement trait because the types of the\n    two operands are coupled. Only places that know how to simultaneously update\n    both types should be changing the type of this op.",
    "inputs": [
      { "name": "value", "type": "Torch_ValueTensorType" },
      { "name": "overwritten", "type": "Torch_NonValueTensorType" }
    ],
    "assemblyFormat": "$value `overwrites` $overwritten attr-dict\n      `:` qualified(type($value)) `,` qualified(type($overwritten))"
  },
  {
    "name": "torch.per_tensor_affine.create",
    "summary": "Create a per-tensor-affine quantized tensor",
    "description": "Create a quantized tensor.\n\n    Quantization formula is:\n    ```\n    Q(x, scale, zero_point) = round(x/scale + zero_point)\n    ```\n\n    See:\n    https://pytorch.org/docs/stable/quantization.html#quantized-tensors",
    "inputs": [
      { "name": "int_repr", "type": "AnyTorchTensorType" },
      { "name": "scale", "type": "Torch_FloatType" },
      { "name": "offset", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchTensorType" }
    ],
    "assemblyFormat": "$int_repr `,` $scale `,` $offset attr-dict\n    `:` qualified(type($int_repr)) `,` qualified(type($scale)) `,` qualified(type($offset)) `->` qualified(type($result))"
  },
  {
    "name": "torch.prim.abs.Scalar",
    "summary": "Generated op for `prim::abs.Scalar : (Scalar) -> (Scalar)`",
    "inputs": [
      { "name": "a", "type": "AnyTorchScalarType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchScalarType" }
    ]
  },
  {
    "name": "torch.prim.CallMethod",
    "summary": "TorchScript prim::CallMethod op",
    "inputs": [
      { "name": "receiver", "type": "Torch_NnModuleType" },
      { "name": "methodOperands", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchType" }
    ],
    "attributes": [
      { "name": "name", "type": "StrAttr" }
    ],
    "assemblyFormat": "$receiver `[` $name `]` `(` $methodOperands `)` attr-dict `:` qualified(type($receiver)) `,` functional-type($methodOperands, $result)"
  },
  {
    "name": "torch.prim.CreateObject",
    "summary": "TorchScript prim::CreateObject op",
    "outputs": [
      { "name": "result", "type": "Torch_NnModuleType" }
    ],
    "assemblyFormat": "attr-dict qualified(type($result))"
  },
  {
    "name": "torch.prim.device",
    "summary": "Generated op for `prim::device : (Tensor) -> (Device)`",
    "inputs": [
      { "name": "a", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "Torch_DeviceType" }
    ]
  },
  {
    "name": "torch.prim.DictConstruct",
    "summary": "TorchScript prim::DictConstruct op",
    "inputs": [
      { "name": "keys", "type": "Variadic" },
      { "name": "values", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "result", "type": "Torch_DictType" }
    ],
    "assemblyFormat": "`keys` `(` ($keys^ `:` qualified(type($keys)))? `)` `values` `(` ($values^ `:` qualified(type($values)))? `)` attr-dict `->` qualified(type($result))"
  },
  {
    "name": "torch.prim.dtype",
    "summary": "Generated op for `prim::dtype : (Tensor) -> (int)`",
    "inputs": [
      { "name": "a", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "Torch_IntType" }
    ]
  },
  {
    "name": "torch.prim.Enter",
    "summary": "enter operation",
    "description": "This op represents a prim::Enter node in the Python object graph.",
    "inputs": [
      { "name": "inp", "type": "AnyTorchType" }
    ],
    "outputs": [
      { "name": "result", "type": "Torch_NoneType" }
    ],
    "assemblyFormat": "$inp attr-dict `:` qualified(type($inp))"
  },
  {
    "name": "torch.prim.Exit",
    "summary": "exit operation",
    "description": "This op represents a prim::Exit node in the Python object graph.",
    "inputs": [
      { "name": "inp", "type": "AnyTorchType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchTensorType" }
    ],
    "assemblyFormat": "$inp attr-dict `:` qualified(type($inp)) `->` qualified(type($result))"
  },
  {
    "name": "torch.prim.GetAttr",
    "summary": "TorchScript prim::GetAttr op",
    "inputs": [
      { "name": "receiver", "type": "Torch_NnModuleType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchType" }
    ],
    "attributes": [
      { "name": "name", "type": "StrAttr" }
    ],
    "assemblyFormat": "$receiver `[` $name `]` attr-dict `:` qualified(type($receiver)) `->` qualified(type($result))"
  },
  {
    "name": "torch.prim.If",
    "summary": "TorchScript prim::If op",
    "description": "This op (together with prim.If.yield) define a conditional control flow\n    construct. It is analogous to `scf.if` for MLIR folks that are familiar\n    with that. The main differences from that op are:\n\n    - `!torch.bool` condition value.\n    - The \"else\" region is always present. This is reflective of invariants of\n      the TorchScript IR.\n    - No special prettiness for the \"no yielded values\" case. These are\n      interesting for modeling mostly-non-SSA programs, but TorchScript IR\n      is already in SSA form.\n\n    See: https://github.com/pytorch/pytorch/blob/master/torch/csrc/jit/OVERVIEW.md#if",
    "inputs": [
      { "name": "condition", "type": "Torch_BoolType" }
    ],
    "outputs": [
      { "name": "results", "type": "Variadic" }
    ]
  },
  {
    "name": "torch.prim.If.yield",
    "summary": "yield-like terminator for torch.prim.If",
    "description": "Does not correspond to any torch prim op directly (the way that they model\n    blocks has a built-in notion of yield-like terminator).",
    "inputs": [
      { "name": "results", "type": "Variadic" }
    ],
    "assemblyFormat": "attr-dict ($results^ `:` qualified(type($results)))?"
  },
  {
    "name": "torch.prim.layout",
    "summary": "Generated op for `prim::layout : (Tensor) -> (int)`",
    "inputs": [
      { "name": "a", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "Torch_IntType" }
    ]
  },
  {
    "name": "torch.prim.ListConstruct",
    "summary": "TorchScript prim::ListConstruct op",
    "inputs": [
      { "name": "elements", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchListType" }
    ],
    "assemblyFormat": "$elements attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "torch.prim.ListUnpack",
    "summary": "TorchScript prim::ListUnpack op",
    "inputs": [
      { "name": "operand", "type": "AnyTorchType" }
    ],
    "outputs": [
      { "name": "results", "type": "Variadic" }
    ],
    "assemblyFormat": "$operand attr-dict `:` qualified(type($operand)) `->` qualified(type($results))"
  },
  {
    "name": "torch.prim.Load",
    "summary": "load operation",
    "description": "This op represents a prim::Load node in the Python object graph.",
    "outputs": [
      { "name": "result", "type": "AnyTorchType" }
    ],
    "attributes": [
      { "name": "name", "type": "StrAttr" }
    ],
    "assemblyFormat": "$name attr-dict `:` qualified(type($result))"
  },
  {
    "name": "torch.prim.Loop",
    "summary": "TorchScript prim::Loop op",
    "description": "This op (together with prim.Loop.condition) define a looping construct\n    that combines `for` and `while` behavior.\n\n    See: https://github.com/pytorch/pytorch/blob/master/torch/csrc/jit/OVERVIEW.md#loops",
    "inputs": [
      { "name": "maxTripCount", "type": "Torch_IntType" },
      { "name": "initialCondition", "type": "Torch_BoolType" },
      { "name": "iterArgsInit", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "results", "type": "Variadic" }
    ],
    "assemblyFormat": "$maxTripCount `,` $initialCondition `,` `init` `(` $iterArgsInit `)` $region\n    attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "torch.prim.Loop.condition",
    "summary": "yield-like terminator for torch.prim.Loop",
    "description": "Does not correspond to any torch prim op directly (the way that they model\n    blocks has a built-in notion of yield-like terminator).",
    "inputs": [
      { "name": "shouldContinue", "type": "Torch_BoolType" },
      { "name": "iterArgs", "type": "Variadic" }
    ],
    "assemblyFormat": "$shouldContinue `,`\n    `iter` `(` ($iterArgs^ `:` qualified(type($iterArgs)))? `)` attr-dict"
  },
  {
    "name": "torch.prim.max.int",
    "summary": "Generated op for `prim::max.int : (int, int) -> (int)`",
    "inputs": [
      { "name": "a", "type": "Torch_IntType" },
      { "name": "b", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "Torch_IntType" }
    ]
  },
  {
    "name": "torch.prim.max.self_int",
    "summary": "Generated op for `prim::max.self_int : (int[]) -> (int)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchListOfTorchIntType" }
    ],
    "outputs": [
      { "name": "result", "type": "Torch_IntType" }
    ]
  },
  {
    "name": "torch.prim.min.int",
    "summary": "Generated op for `prim::min.int : (int, int) -> (int)`",
    "inputs": [
      { "name": "a", "type": "Torch_IntType" },
      { "name": "b", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "Torch_IntType" }
    ]
  },
  {
    "name": "torch.prim.min.self_int",
    "summary": "Generated op for `prim::min.self_int : (int[]) -> (int)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchListOfTorchIntType" }
    ],
    "outputs": [
      { "name": "result", "type": "Torch_IntType" }
    ]
  },
  {
    "name": "torch.prim.NumToTensor.Scalar",
    "summary": "Generated op for `prim::NumToTensor.Scalar : (Scalar) -> (Tensor)`",
    "inputs": [
      { "name": "a", "type": "AnyTorchScalarType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.prim.Print",
    "summary": "Generated op for `prim::Print : (...) -> ()`",
    "inputs": [
      { "name": "operands", "type": "Variadic" }
    ],
    "assemblyFormat": "`(` $operands `)` attr-dict `:` qualified(type($operands))"
  },
  {
    "name": "torch.prim.RaiseException",
    "summary": "Generated op for `prim::RaiseException : (str, str?) -> ()`",
    "inputs": [
      { "name": "msg", "type": "Torch_StringType" },
      { "name": "cls", "type": "AnyTorchOptionalStringType" }
    ]
  },
  {
    "name": "torch.prim.SetAttr",
    "summary": "TorchScript prim::SetAttr op",
    "inputs": [
      { "name": "receiver", "type": "Torch_NnModuleType" },
      { "name": "value", "type": "AnyTorchType" }
    ],
    "attributes": [
      { "name": "name", "type": "StrAttr" }
    ],
    "assemblyFormat": "$receiver `[` $name `]` `=` $value attr-dict `:` qualified(type($receiver)) `,` qualified(type($value))"
  },
  {
    "name": "torch.prim.Store",
    "summary": "store operation",
    "description": "This op represents a prim::Store node in the Python object graph.",
    "inputs": [
      { "name": "value", "type": "AnyTorchType" }
    ],
    "attributes": [
      { "name": "name", "type": "StrAttr" }
    ],
    "assemblyFormat": "$name `,` $value attr-dict `:` qualified(type($value))"
  },
  {
    "name": "torch.prim.tolist",
    "summary": "Generated op for `prim::tolist : (...) -> (...)`",
    "inputs": [
      { "name": "operands", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "results", "type": "Variadic" }
    ],
    "assemblyFormat": "`(` $operands `)` attr-dict `:` qualified(type($operands)) `->` qualified(type($results))"
  },
  {
    "name": "torch.prim.TupleConstruct",
    "summary": "TorchScript prim::TupleConstruct op",
    "description": "Note: This op does not allow trivial type refinement, because the\n    operand types and the result types must be in correspondence.",
    "inputs": [
      { "name": "elements", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "result", "type": "Torch_TupleType" }
    ],
    "assemblyFormat": "$elements attr-dict `:` qualified(type($elements)) `->` qualified(type($result))"
  },
  {
    "name": "torch.prim.TupleIndex",
    "summary": "Generated op for `prim::TupleIndex : (Any, int) -> (Any)`",
    "inputs": [
      { "name": "tup", "type": "AnyTorchType" },
      { "name": "i", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchType" }
    ]
  },
  {
    "name": "torch.prim.TupleUnpack",
    "summary": "Generated op for `prim::TupleUnpack : (Any) -> (...)`",
    "inputs": [
      { "name": "tup", "type": "AnyTorchType" }
    ],
    "outputs": [
      { "name": "results", "type": "Variadic" }
    ],
    "assemblyFormat": "$tup attr-dict `:` qualified(type($tup)) `->` qualified(type($results))"
  },
  {
    "name": "torch.prim.unchecked_cast",
    "summary": "Generated op for `prim::unchecked_cast : (t) -> (t)`",
    "inputs": [
      { "name": "x", "type": "AnyTorchType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchType" }
    ]
  },
  {
    "name": "torch.prim.Uninitialized",
    "summary": "Generated op for `prim::Uninitialized : () -> (Any)`",
    "outputs": [
      { "name": "result", "type": "AnyTorchType" }
    ]
  },
  {
    "name": "torch.prims.collapse",
    "summary": "Generated op for `prims::collapse : (Tensor, int, int) -> (Tensor)`",
    "inputs": [
      { "name": "a", "type": "AnyTorchTensorType" },
      { "name": "start", "type": "Torch_IntType" },
      { "name": "end", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.prims.convert_element_type",
    "summary": "Generated op for `prims::convert_element_type : (Tensor, int) -> (Tensor)`",
    "inputs": [
      { "name": "a", "type": "AnyTorchTensorType" },
      { "name": "dtype", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.prims.iota",
    "summary": "Generated op for `prims::iota : (int, int, int, int, Device, bool) -> (Tensor)`",
    "inputs": [
      { "name": "length", "type": "Torch_IntType" },
      { "name": "start", "type": "Torch_IntType" },
      { "name": "step", "type": "Torch_IntType" },
      { "name": "dtype", "type": "Torch_IntType" },
      { "name": "device", "type": "Torch_DeviceType" },
      { "name": "requires_grad", "type": "Torch_BoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.prims.split_dim",
    "summary": "Generated op for `prims::split_dim : (Tensor, int, int) -> (Tensor)`",
    "inputs": [
      { "name": "a", "type": "AnyTorchTensorType" },
      { "name": "dim", "type": "Torch_IntType" },
      { "name": "outer_length", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.prims.sqrt",
    "summary": "Generated op for `prims::sqrt : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.prims.squeeze",
    "summary": "Generated op for `prims::squeeze : (Tensor, int[]) -> (Tensor)`",
    "inputs": [
      { "name": "a", "type": "AnyTorchTensorType" },
      { "name": "dimensions", "type": "AnyTorchListOfTorchIntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.prims.sum",
    "summary": "Generated op for `prims::sum : (Tensor, int[]?, int?) -> (Tensor)`",
    "inputs": [
      { "name": "inp", "type": "AnyTorchTensorType" },
      { "name": "dims", "type": "AnyTorchOptionalListOfTorchIntType" },
      { "name": "output_dtype", "type": "AnyTorchOptionalIntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.prims.var",
    "summary": "Generated op for `prims::var : (Tensor, int[]?, float?, int?) -> (Tensor)`",
    "inputs": [
      { "name": "inp", "type": "AnyTorchTensorType" },
      { "name": "dims", "type": "AnyTorchOptionalListOfTorchIntType" },
      { "name": "correction", "type": "AnyTorchOptionalFloatType" },
      { "name": "output_dtype", "type": "AnyTorchOptionalIntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.prims.view_of",
    "summary": "Generated op for `prims::view_of : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "a", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.promote_dtypes",
    "summary": "`promote_dtypes op : (int?[], int[]) -> (int)`",
    "description": "This op is generated when the python function\n    `__torch_mlir_internal_promote_dtypes` is used in a dtype refinement\n    function. It represents the type promotion logic used by PyTorch to\n    determine result types.\n\n    The first argument is a list of optional ranks for each of the inputs\n    being used for promotion. The ranks are optional to allow representing\n    `Scalar` inputs, which follow their own set of promotion rules.\n\n    The second argument is a list of dtypes for each of the inputs being used\n    for promotion.\n\n    The order of the values in each list must be the same. In other words,\n    the ith rank and the ith dtype must be from the same Scalar/Tensor.\n\n    It is an error to call this op with empty lists or lists of different size.",
    "inputs": [
      { "name": "ranks", "type": "AnyTorchListOfOptionalIntType" },
      { "name": "dtypes", "type": "AnyTorchListOfTorchIntType" }
    ],
    "outputs": [
      { "name": "result", "type": "Torch_IntType" }
    ],
    "assemblyFormat": "$ranks `,` $dtypes attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "torch.quantized.linear",
    "summary": "Generated op for `quantized::linear : (Tensor, __torch__.torch.classes.quantized.LinearPackedParamsBase, float, int) -> (Tensor)`",
    "inputs": [
      { "name": "X", "type": "AnyTorchTensorType" },
      { "name": "W_prepack", "type": "Torch_LinearParamsType" },
      { "name": "Y_scale_i", "type": "Torch_FloatType" },
      { "name": "Y_zero_point_i", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "Y", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.runtime.assert",
    "summary": "Runtime Assertion",
    "inputs": [
      { "name": "condition", "type": "Torch_BoolType" }
    ],
    "attributes": [
      { "name": "message", "type": "StrAttr" }
    ],
    "assemblyFormat": "$condition `,` $message attr-dict"
  },
  {
    "name": "torch.shape.calculate",
    "summary": "Shape calculation encapsulation op",
    "description": "The `torch.shape.calculate` op captures a shape calculation\n    (in the region `calculation`) which calculates the shapes for\n    the set of values yielded by the `body` region.\n\n    The `calculation` region yields a `!torch.list<int>` for each\n    value yielded by the `body` region.\n\n    Conceptually, the `calculation` region executes first, then `body`\n    region. So the `calculation` region can also contain arbitrary\n    assertions or side-effecting code which guard the validity of the execution\n    of the body (typically by terminating the program with a\n    torch.prim.RaiseException op).\n\n    The program has undefined behavior if the values yielded by the `body`\n    region do not have the shapes yielded by the `calculation` region.",
    "outputs": [
      { "name": "results", "type": "Variadic" }
    ],
    "assemblyFormat": "$body `shapes` $calculation attr-dict `:` type($results)"
  },
  {
    "name": "torch.shape.calculate.yield",
    "summary": "yield-like terminator for torch.shape.calculate",
    "description": "This op terminates the `body` region of a `torch.shape.calculate` op.",
    "inputs": [
      { "name": "results", "type": "Variadic" }
    ],
    "assemblyFormat": "attr-dict ($results^ `:` type($results))?"
  },
  {
    "name": "torch.shape.calculate.yield.shapes",
    "summary": "yield-like terminator for torch.shape.calculate shape region",
    "description": "This op terminates the `shapeCalculation` region of a\n    `torch.shape.calculate` op.",
    "inputs": [
      { "name": "results", "type": "Variadic" }
    ],
    "assemblyFormat": "attr-dict ($results^ `:` type($results))?"
  },
  {
    "name": "torch.slot",
    "summary": "Define the value of a slot of a torch.nn.Module",
    "description": "This op specifies that the initial value of the slot `name` of the\n    parent torch.nn_module should be `value`, which is allowed to be an\n    arbitrary Torch-compatible SSA value, including other !torch.nn.Module's.",
    "inputs": [
      { "name": "value", "type": "AnyTorchType" }
    ],
    "attributes": [
      { "name": "name", "type": "StrAttr" }
    ],
    "assemblyFormat": "$name `,` $value attr-dict `:` qualified(type($value))"
  },
  {
    "name": "torch.symbolic_int",
    "summary": "Symbolic int representing a dynamic dimension",
    "description": "The `torch.symbolic_int` operation captures a dynamic dimension on the\n    global function arguments as exported by TorchDynamo (torch.export).\n    It associates the shape symbols (i.e. \"s0\", \"s1\") with the\n    global SSA values (i.e. `%0`, `%1`) that is then referenced\n    to bind shapes on op results.\n\n    Additionally, the operation annotates `min_val` and `max_val` attributes\n    denoting the range constraints for the dynamic dimension. This may be\n    useful for modeling runtime shape guards, or compile-time optimizations\n    based on the shape bounds (min, opt, max) on results of ops / regions.\n\n    Example:\n    ```\n    %0 = torch.symbolic_int \"s0\" {min_val = 5, max_val = 10} : !torch.int\n    %1 = torch.symbolic_int \"s1\" {min_val = 2, max_val = 20} : !torch.int\n    ```\n\n    In this case, we see that `s0` has the range [5, 10] and `s1` has the\n    range [2, 20]. When unspecified, the range constraints feeding in from\n    TorchDynamo default to [0, INT_MAX] (or [2, INT_MAX] in older PyTorch\n    releases). In either case, the interpretation (as specified by TorchDynamo)\n    is that the dynamic dimension is assumed to be not 0 or 1. This is not a\n    bug, and does not necessarily mean that the exported program will not work\n    for dimensions 0 or 1. For an in-depth discussion of this topic, see\n    [The 0/1 Specialization Problem](https://docs.google.com/document/d/16VPOa3d-Liikf48teAOmxLc92rgvJdfosIy-yoT38Io/edit?fbclid=IwAR3HNwmmexcitV0pbZm_x1a4ykdXZ9th_eJWK-3hBtVgKnrkmemz6Pm5jRQ#heading=h.ez923tomjvyk).",
    "outputs": [
      { "name": "result", "type": "Torch_IntType" }
    ],
    "attributes": [
      { "name": "symbol_name", "type": "StrAttr" },
      { "name": "min_val", "type": "I64Attr" },
      { "name": "max_val", "type": "I64Attr" }
    ],
    "assemblyFormat": "$symbol_name ` ` `{` `min_val` `=` $min_val `,` `max_val` `=` $max_val `}` attr-dict `:` type($result)"
  },
  {
    "name": "torch.tensor_static_info_cast",
    "summary": "Adds/removes static information from a tensor type.",
    "description": "This op does not imply any runtime code. Semantically it is an identity\n    function. However, it statically annotates (or erases) shape and dtype\n    information from a tensor type.\n\n    This op *cannot* be used to add/remove value semantics from a tensor.\n    For converting between the value-semantic and non-value-semantic domains,\n    use `torch.copy.to_tensor` and `torch.copy.from_tensor`. This op is kept\n    separate to prevent canonicalizations from accidentally dropping static\n    information. In most cases, after running the `torch-refine-types` pass,\n    this op becomes a no-op (the pass will incorporate the static information\n    into other ops that allow type refinement).",
    "inputs": [
      { "name": "operand", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchTensorType" }
    ],
    "assemblyFormat": "$operand attr-dict `:` qualified(type($operand)) `to` qualified(type($result))"
  },
  {
    "name": "torch.tensor.literal",
    "summary": "Create a value of !torch.tensor type from a literal",
    "description": "Example:\n    ```\n    %0 = torch.tensor.literal(dense<0.0> : tensor<3x5xf32>) : !torch.tensor\n    %1 = torch.tensor.literal(dense<0.0> : tensor<3xf32>) : !torch.tensor<[3],f32>\n    ```\n\n    This op covers a typical frontend use case of creating a type-erased\n    `!torch.tensor`. Inside the compiler, we decompose it into\n    `torch.vtensor.literal` which is easier to analyze and transform.\n\n    Note: This op is not called \"constant\" because the created tensor is not\n    \"constant\" in any meaning of that word.",
    "outputs": [
      { "name": "result", "type": "Torch_NonValueTensorType" }
    ],
    "attributes": [
      { "name": "value", "type": "ElementsAttr" }
    ],
    "assemblyFormat": "`(` $value `)` attr-dict `:` qualified(type($result))"
  },
  {
    "name": "torch.torchvision.deform_conv2d",
    "summary": "Generated op for `torchvision::deform_conv2d : (Tensor, Tensor, Tensor, Tensor, Tensor, int, int, int, int, int, int, int, int, bool) -> (Tensor)`",
    "inputs": [
      { "name": "input", "type": "AnyTorchTensorType" },
      { "name": "weight", "type": "AnyTorchTensorType" },
      { "name": "offset", "type": "AnyTorchTensorType" },
      { "name": "mask", "type": "AnyTorchTensorType" },
      { "name": "bias", "type": "AnyTorchTensorType" },
      { "name": "stride_h", "type": "Torch_IntType" },
      { "name": "stride_w", "type": "Torch_IntType" },
      { "name": "pad_h", "type": "Torch_IntType" },
      { "name": "pad_w", "type": "Torch_IntType" },
      { "name": "dilation_h", "type": "Torch_IntType" },
      { "name": "dilation_w", "type": "Torch_IntType" },
      { "name": "groups", "type": "Torch_IntType" },
      { "name": "offset_groups", "type": "Torch_IntType" },
      { "name": "use_mask", "type": "Torch_BoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.torchvision.nms",
    "summary": "Generated op for `torchvision::nms : (Tensor, Tensor, float) -> (Tensor)`",
    "inputs": [
      { "name": "dets", "type": "AnyTorchTensorType" },
      { "name": "scores", "type": "AnyTorchTensorType" },
      { "name": "iou_threshold", "type": "Torch_FloatType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.torchvision.roi_align",
    "summary": "Generated op for `torchvision::roi_align : (Tensor, Tensor, float, int, int, int, bool) -> (Tensor)`",
    "inputs": [
      { "name": "input", "type": "AnyTorchTensorType" },
      { "name": "rois", "type": "AnyTorchTensorType" },
      { "name": "spatial_scale", "type": "Torch_FloatType" },
      { "name": "pooled_height", "type": "Torch_IntType" },
      { "name": "pooled_width", "type": "Torch_IntType" },
      { "name": "sampling_ratio", "type": "Torch_IntType" },
      { "name": "aligned", "type": "Torch_BoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.torchvision.roi_pool",
    "summary": "Generated op for `torchvision::roi_pool : (Tensor, Tensor, float, int, int) -> (Tensor, Tensor)`",
    "inputs": [
      { "name": "input", "type": "AnyTorchTensorType" },
      { "name": "rois", "type": "AnyTorchTensorType" },
      { "name": "spatial_scale", "type": "Torch_FloatType" },
      { "name": "pooled_height", "type": "Torch_IntType" },
      { "name": "pooled_width", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result0", "type": "AnyTorchOptionalTensorType" },
      { "name": "result1", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.valsem.aten.bernoulli.float",
    "summary": "`bernoulli.float op : (Tensor, float, Generator?) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "p", "type": "Torch_FloatType" },
      { "name": "generator", "type": "AnyTorchOptionalGeneratorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchTensorType" }
    ],
    "assemblyFormat": "$self `,` $p `,` $generator attr-dict `:` type($self) `,` type($p) `,` type($generator) `->` type($result)"
  },
  {
    "name": "torch.vtensor.literal",
    "summary": "Create a value of !torch.vtensor type from a literal",
    "description": "Example:\n    ```\n    %0 = torch.vtensor.literal(dense<0.0> : tensor<3x5xf32>) : !torch.vtensor<[3,5],f32>\n    %1 = torch.vtensor.literal(dense<0.0> : tensor<3xf32>) : !torch.vtensor<[3],f32>\n    ```\n\n    Unlike `torch.tensor.literal`, which covers a typical frontend use case\n    and allows type refinement, this op always has a maximally resolved type\n    (which is always possible, because it is created from a literal). This\n    has a stronger set of invariants that better fit the needs of the\n    compiler internals.",
    "outputs": [
      { "name": "result", "type": "Torch_ValueTensorType" }
    ],
    "attributes": [
      { "name": "value", "type": "ElementsAttr" }
    ],
    "assemblyFormat": "`(` $value `)` attr-dict `:` qualified(type($result))"
  },
  {
    "name": "tosa.abs",
    "summary": "Elementwise abs operator.",
    "description": "Elementwise absolute value operation.\n\n    Example:\n\n    ```mlir\n    %output = tosa.abs(%input1) : (tensor<21x3xf32>) -> tensor<21x3xf32>\n    ```",
    "inputs": [
      { "name": "input1", "type": "Tosa_Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "Tosa_Tensor" }
    ],
    "assemblyFormat": "operands attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "tosa.add",
    "summary": "Elementwise addition operator.",
    "description": "Elementwise addition of input1 and input2. Axis of size 1 will be broadcast,\n    as necessary. Rank of input tensors must match.\n\n    Example:\n\n    ```mlir\n    // Elementwise addition.\n    %out = tosa.add %input1, %input2 : tensor<12x6xf32>, tensor<12x6xf32> -> tensor<12x6xf32>\n\n    // Elementwise addition with broadcasting.\n    %out = tosa.add %input1, %input2 : tensor<12x6xsi32>, tensor<1x1xsi32> -> tensor<12x6xsi32>\n    ```",
    "inputs": [
      { "name": "input1", "type": "Tosa_Tensor" },
      { "name": "input2", "type": "Tosa_Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "Tosa_Tensor" }
    ],
    "assemblyFormat": "operands attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "tosa.apply_scale",
    "summary": "Rescale scalar operator for Tosa tensor operators",
    "description": "Applies rescaling for fixed point values. This behavior is replicated in\n    multiple quantized operations (mul, convolution, rescale, matmul, pooling).\n\n    The commonplace implementation is to use i64 operations to avoid integer\n    overflow with target specific implementations can use native operations to\n    avoid wider than necessary types.",
    "inputs": [
      { "name": "value", "type": "Tosa_IntLike" },
      { "name": "multiplier", "type": "Tosa_IntLike" },
      { "name": "shift", "type": "Tosa_Int8Like" }
    ],
    "outputs": [
      { "name": "output", "type": "Tosa_IntLike" }
    ],
    "attributes": [
      { "name": "rounding_mode", "type": "Tosa_RoundingModeAttr" }
    ]
  },
  {
    "name": "tosa.argmax",
    "summary": "Perform argmax on the input.",
    "description": "This returns the index with the largest value across the given axis of the\n    input tensor. If multiple locations have equal values, returns the first\n    match along the search axis.",
    "inputs": [
      { "name": "input", "type": "Tosa_TensorAtLeast1D" }
    ],
    "outputs": [
      { "name": "output", "type": "Tosa_Tensor" }
    ],
    "attributes": [
      { "name": "axis", "type": "I32Attr" },
      { "name": "nan_mode", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "tosa.arithmetic_right_shift",
    "summary": "Elementwise Arithmetic Right Shift.",
    "description": "Elementwise arithmetic right shift of input1 by the amount specified in\n    input2. Axis of size 1 will be broadcast, as necessary. Rank of input tensors\n    must match.",
    "inputs": [
      { "name": "input1", "type": "Tosa_Tensor" },
      { "name": "input2", "type": "Tosa_Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "Tosa_Tensor" }
    ],
    "attributes": [
      { "name": "round", "type": "BoolAttr" }
    ],
    "assemblyFormat": "operands attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "tosa.avg_pool2d",
    "summary": "Performs average pooling on the input.",
    "description": "This performs an average pooling over the given input tensor. A sliding\n    window of size given by <kernel size> is passed over the input tensor, with\n    the mean value being placed in the output tensor. When calculating the\n    average, only the number of valid input tensor values, but not padding, are\n    used to calculate the divisor.",
    "inputs": [
      { "name": "input", "type": "Tosa_Tensor4D" },
      { "name": "input_zp", "type": "Tosa_ScalarIntOrFloatTensor" },
      { "name": "output_zp", "type": "Tosa_ScalarIntOrFloatTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "Tosa_Tensor4D" }
    ],
    "attributes": [
      { "name": "kernel", "type": "Tosa_IntArrayAttr2" },
      { "name": "stride", "type": "Tosa_IntArrayAttr2" },
      { "name": "pad", "type": "Tosa_IntArrayAttr4" },
      { "name": "acc_type", "type": "TypeAttrOf" }
    ],
    "assemblyFormat": "operands attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "tosa.bitwise_and",
    "summary": "Bitwise AND operator.",
    "description": "Elementwise bitwise AND of input1 and input2. Axis of size 1\n    will be broadcast as necessary. Rank of input tensors must match.",
    "inputs": [
      { "name": "input1", "type": "Tosa_Tensor" },
      { "name": "input2", "type": "Tosa_Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "Tosa_Tensor" }
    ],
    "assemblyFormat": "operands attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "tosa.bitwise_not",
    "summary": "Bitwise NOT operator.",
    "description": "Elementwise bitwise NOT of input tensor.",
    "inputs": [
      { "name": "input1", "type": "Tosa_Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "Tosa_Tensor" }
    ],
    "assemblyFormat": "operands attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "tosa.bitwise_or",
    "summary": "Bitwise OR operator.",
    "description": "Elementwise bitwise OR of input1 and input2. Axis of size 1 will be\n    broadcast as necessary. Rank of input tensors must match.",
    "inputs": [
      { "name": "input1", "type": "Tosa_Tensor" },
      { "name": "input2", "type": "Tosa_Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "Tosa_Tensor" }
    ],
    "assemblyFormat": "operands attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "tosa.bitwise_xor",
    "summary": "Bitwise XOR operator.",
    "description": "Elementwise bitwise XOR of input1 and input2. Axis of size 1 will be\n    broadcast as necessary. Rank of input tensors must match.",
    "inputs": [
      { "name": "input1", "type": "Tosa_Tensor" },
      { "name": "input2", "type": "Tosa_Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "Tosa_Tensor" }
    ],
    "assemblyFormat": "operands attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "tosa.cast",
    "summary": "Cast operation.",
    "description": "Casts a tensor from one data type to another.\n    * This table is showing the supported conversions from the TOSA Specification.\n    * The MLIR dialect here can be used to represent other conversions.\n\n    | Mode                     | Input   | Output  |\n    |--------------------------|---------|---------|\n    | fp16 to fp32             | float16 | float32 |\n    | fp16 to int 16           | float16 | int16   |\n    | fp16 to int 32           | float16 | int32   |\n    | fp16 to int 8            | float16 | int8    |\n    | fp32 to fp16             | float32 | float16 |\n    | fp32 to int 16           | float32 | int16   |\n    | fp32 to int 32           | float32 | int32   |\n    | fp32 to int 8            | float32 | int8    |\n    | int 16 to fp16           | int16   | float16 |\n    | int 16 to fp32           | int16   | float32 |\n    | int 32 to fp16           | int32   | float16 |\n    | int 32 to fp32           | int32   | float32 |\n    | int 8 to fp16            | int8    | float16 |\n    | int 8 to fp32            | int8    | float32 |\n    | bool to int 16           | Boolean | int16   |\n    | bool to int 32           | Boolean | int32   |\n    | bool to int 8            | Boolean | int8    |\n    | int 16 to bool           | int16   | Boolean |\n    | int 16 to int 32         | int16   | int32   |\n    | int 16 to int 8          | int16   | int8    |\n    | int 32 to bool           | int32   | Boolean |\n    | int 32 to int 16         | int32   | int16   |\n    | int 32 to int 8          | int32   | int8    |\n    | int 8 to bool            | int8    | Boolean |\n    | int 8 to int 16          | int8    | int16   |\n    | int 8 to int 32          | int8    | int32   |\n    | bf16 to fp32             | bf16    | float32 |\n    | bf16 to int 16           | bf16    | int16   |\n    | bf16 to int 32           | bf16    | int32   |\n    | bf16 to int 8            | bf16    | int8    |\n    | fp32 to bf16             | float32 | bf16    |\n    | int 16 to bf16           | int16   | bf16    |\n    | int 32 to bf16           | int32   | bf16    |\n    | int 8 to bf16            | int8    | bf16    |\n    | bf16 to fp8e4m3          | bf16    | fp8e4m3 |\n    | fp8e4m3 to bf16          | fp8e4m3 | bf16    |\n    | bf16 to fp8e5m2          | bf16    | fp8e5m2 |\n    | fp8e5m2 to bf16          | fp8e5m2 | bf16    |\n    | fp16 to fp8e4m3          | float16 | fp8e4m3 |\n    | fp32 to fp8e4m3          | float32 | fp8e4m3 |\n    | fp8e4m3 to fp16          | fp8e4m3 | float16 |\n    | fp8e4m3 to fp32          | fp8e4m3 | float32 |\n    | fp16 to fp8e5m2          | float16 | fp8e5m2 |\n    | fp32 to fp8e5m2          | float32 | fp8e5m2 |\n    | fp8e5m2 to fp16          | fp8e5m2 | float16 |\n    | fp8e5m2 to fp32          | fp8e5m2 | float32 |",
    "inputs": [
      { "name": "input", "type": "Tosa_Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "Tosa_Tensor" }
    ],
    "assemblyFormat": "operands attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "tosa.ceil",
    "summary": "Elementwise ceil operator.",
    "description": "Elementwise ceiling operation.",
    "inputs": [
      { "name": "input1", "type": "Tosa_Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "Tosa_Tensor" }
    ],
    "assemblyFormat": "operands attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "tosa.clamp",
    "summary": "Computes clamp(features, min, max).",
    "description": "Clamp to an arbitrary minimum and maximum value.\n    Maximum and minimum values are specified as values in the range of the\n    input type.\n    No zero point subtraction is done to the values, thus to clamp to the zero\n    point value, the zero point itself should be supplied as the minimum value.",
    "inputs": [
      { "name": "input", "type": "Tosa_Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "Tosa_Tensor" }
    ],
    "attributes": [
      { "name": "min_val", "type": "Tosa_IntOrFloatAttr" },
      { "name": "max_val", "type": "Tosa_IntOrFloatAttr" },
      { "name": "nan_mode", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "tosa.clz",
    "summary": "Elementwise count leading zero operator.",
    "description": "Elementwise count leading zeros operation.",
    "inputs": [
      { "name": "input1", "type": "Tosa_Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "Tosa_Tensor" }
    ],
    "assemblyFormat": "operands attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "tosa.concat",
    "summary": "Concatenates tensors along one dimension.",
    "description": "Concatenate a list of tensors along a given axis.\n    No data conversion happens during a concat operation.",
    "inputs": [
      { "name": "input1", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "output", "type": "Tosa_TensorAtLeast1D" }
    ],
    "attributes": [
      { "name": "axis", "type": "I32Attr" }
    ],
    "assemblyFormat": "operands attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "tosa.cond_if",
    "summary": "Conditional if operator.",
    "description": "Evaluates a Boolean condition and then takes one of two distinct execution\n    paths. This implements the semantic If-then-else structure.",
    "inputs": [
      { "name": "condition", "type": "Tosa_I1Tensor" },
      { "name": "input_list", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "output_list", "type": "Variadic" }
    ]
  },
  {
    "name": "tosa.const",
    "summary": "Constant operator.",
    "description": "A node containing constant data for use as the input to an operation. May\n    hold data in any of the supported data formats.\n\n    Example:\n\n    ```mlir\n    // Generic form\n    %out = \"tosa.const\"() {values = dense<0> : tensor<2x3xi32>} : () -> tensor<2x3xi32>\n    ```",
    "outputs": [
      { "name": "output", "type": "Tosa_Tensor" }
    ],
    "attributes": [
      { "name": "values", "type": "ElementsAttr" }
    ]
  },
  {
    "name": "tosa.const_shape",
    "summary": "Constant Shape operator.",
    "description": "A node containing a constant shape.\n\n    Example:\n\n    ```mlir\n    // Generic form\n    %out = \"tosa.const_shape\"() {values = dense<0> : tensor<4xindex>} : () -> !tosa.shape<4>\n    ```",
    "outputs": [
      { "name": "output", "type": "Tosa_Shape" }
    ],
    "attributes": [
      { "name": "values", "type": "IndexElementsAttr" }
    ],
    "assemblyFormat": "operands attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "tosa.conv2d",
    "summary": "2D Convolution operator.",
    "description": "Performs a 2D convolution over the given tensor input, using the weight\n    tensor. Implementations may choose to skip calculation of multiplies in\n    the padding area.",
    "inputs": [
      { "name": "input", "type": "Tosa_Tensor4D" },
      { "name": "weight", "type": "Tosa_Tensor4D" },
      { "name": "bias", "type": "Tosa_Tensor1D" },
      { "name": "input_zp", "type": "Tosa_ScalarIntOrFloatTensor" },
      { "name": "weight_zp", "type": "Tosa_ScalarIntOrFloatTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "Tosa_Tensor4D" }
    ],
    "attributes": [
      { "name": "pad", "type": "Tosa_IntArrayAttr4" },
      { "name": "stride", "type": "Tosa_IntArrayAttr2" },
      { "name": "dilation", "type": "Tosa_IntArrayAttr2" },
      { "name": "acc_type", "type": "TypeAttrOf" },
      { "name": "local_bound", "type": "DefaultValuedOptionalAttr" }
    ],
    "assemblyFormat": "operands attr-dict `:` functional-type(operands, results)",
    "category": "Layer"
  },
  {
    "name": "tosa.conv3d",
    "summary": "3D Convolution operator.",
    "description": "Performs a 3D convolution over the given input tensor. Implementations\n    may choose to skip calculation of multiplies in the padding area.",
    "inputs": [
      { "name": "input", "type": "Tosa_Tensor5D" },
      { "name": "weight", "type": "Tosa_Tensor5D" },
      { "name": "bias", "type": "Tosa_Tensor1D" },
      { "name": "input_zp", "type": "Tosa_ScalarIntOrFloatTensor" },
      { "name": "weight_zp", "type": "Tosa_ScalarIntOrFloatTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "Tosa_Tensor5D" }
    ],
    "attributes": [
      { "name": "pad", "type": "Tosa_IntArrayAttr6" },
      { "name": "stride", "type": "Tosa_IntArrayAttr3" },
      { "name": "dilation", "type": "Tosa_IntArrayAttr3" },
      { "name": "acc_type", "type": "TypeAttrOf" },
      { "name": "local_bound", "type": "DefaultValuedOptionalAttr" }
    ],
    "assemblyFormat": "operands attr-dict `:` functional-type(operands, results)",
    "category": "Layer"
  },
  {
    "name": "tosa.cos",
    "summary": "Elementwise cos operator.",
    "description": "Elementwise cosine operation for values given in radians.",
    "inputs": [
      { "name": "input1", "type": "Tosa_FloatTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "Tosa_FloatTensor" }
    ],
    "assemblyFormat": "operands attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "tosa.custom",
    "summary": "Custom operator wrapper for Tosa",
    "description": "Hardware implementing TOSA may choose to add additional custom operators\n    that are not expressed in the existing TOSA operations. These operators are\n    not expected to be portable across TOSA implementations. The input and\n    output signatures must be expressed in the corresponding TOSA node.\n\n    `operator_name` is a string that tells the backend which custom operator is\n    being called.\n\n    `domain_name` is a string identifier which can help avoid name collisions on\n    the identifier field.\n\n    `implementation_attrs` is a string which is a backend and identifier specific\n    set of attributes to the custom operator.\n\n    `input_list` is the set of tensor inputs to the custom operator.\n\n    `output_list` is the list of tensors returned by the operator. The number of operators\n    is backend specific.\n\n    Example:\n\n    ```mlir\n    %out = tosa.custom %in {domain_name = \"tosa_mlir_test\", operator_name =\n           \"custom_test\", implementation_attrs = \"\"}: (tensor<10xi32>) ->\n           (tensor<10xi32>)\n    ```",
    "inputs": [
      { "name": "input_list", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "output_list", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "operator_name", "type": "StrAttr" },
      { "name": "domain_name", "type": "StrAttr" },
      { "name": "implementation_attrs", "type": "StrAttr" }
    ],
    "assemblyFormat": "operands attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "tosa.depthwise_conv2d",
    "summary": "Depthwise 2D Convolution operator.",
    "description": "Performs 2D convolutions separately over each channel of the given tensor\n    input, using the weight tensor. Implementations may choose to skip\n    calculation of multiplies in the padding area.",
    "inputs": [
      { "name": "input", "type": "Tosa_Tensor4D" },
      { "name": "weight", "type": "Tosa_Tensor4D" },
      { "name": "bias", "type": "Tosa_Tensor1D" },
      { "name": "input_zp", "type": "Tosa_ScalarIntOrFloatTensor" },
      { "name": "weight_zp", "type": "Tosa_ScalarIntOrFloatTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "Tosa_Tensor4D" }
    ],
    "attributes": [
      { "name": "pad", "type": "Tosa_IntArrayAttr4" },
      { "name": "stride", "type": "Tosa_IntArrayAttr2" },
      { "name": "dilation", "type": "Tosa_IntArrayAttr2" },
      { "name": "acc_type", "type": "TypeAttrOf" },
      { "name": "local_bound", "type": "DefaultValuedOptionalAttr" }
    ],
    "assemblyFormat": "operands attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "tosa.equal",
    "summary": "Returns the truth value of (input1 == input2) element-wise.",
    "description": "Elementwise comparison operation.",
    "inputs": [
      { "name": "input1", "type": "Tosa_Tensor" },
      { "name": "input2", "type": "Tosa_Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "Tosa_I1Tensor" }
    ],
    "assemblyFormat": "operands attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "tosa.erf",
    "summary": "Computes gauss error function of input.",
    "description": "Gauss error function: $ erf(x) = \\frac{2}{\\sqrt{\\pi}} \\int_{0}^{x} e^{-t^2} dt $\n    For quantized integer data types, the TABLE operator should be used instead\n    with the following definition. The ERF table has 513 entries each of\n    16-bit precision and covering the input range -4.0 to +4.0 in steps of 1/64.",
    "inputs": [
      { "name": "input", "type": "Tosa_Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "Tosa_Tensor" }
    ],
    "assemblyFormat": "operands attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "tosa.exp",
    "summary": "Elementwise exp operator.",
    "description": "Elementwise e to the x operation",
    "inputs": [
      { "name": "input1", "type": "Tosa_Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "Tosa_Tensor" }
    ],
    "assemblyFormat": "operands attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "tosa.fft2d",
    "summary": "Performs FFT2D operation on the input.",
    "description": "Performs a batched complex 2D Fast Fourier Transform over the input. The\n    complex input values are constructed from the corresponding values in the\n    input_real and input_imag tensors. The resulting values in the output are\n    split into the output_real and output_imag tensors. No normalization is\n    applied on either the forward or inverse versions of the operation.\n\n    Example:\n\n    ```mlir\n     %output_real, %output_imag = tosa.fft2d %input_real, %input_imag : (tensor<8x9xf32>, tensor<8x9xf32>) -> (tensor<8x9xf32>, tensor<8x9xf32>)\n    ```",
    "inputs": [
      { "name": "input_real", "type": "Tosa_Tensor3D" },
      { "name": "input_imag", "type": "Tosa_Tensor3D" }
    ],
    "outputs": [
      { "name": "output_real", "type": "Tosa_Tensor3D" },
      { "name": "output_imag", "type": "Tosa_Tensor3D" }
    ],
    "attributes": [
      { "name": "inverse", "type": "BoolAttr" },
      { "name": "local_bound", "type": "DefaultValuedOptionalAttr" }
    ],
    "assemblyFormat": "operands attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "tosa.floor",
    "summary": "Elementwise floor operator.",
    "description": "Elementwise floor operation.",
    "inputs": [
      { "name": "input1", "type": "Tosa_Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "Tosa_Tensor" }
    ],
    "assemblyFormat": "operands attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "tosa.gather",
    "summary": "Gather operation.",
    "description": "Generate a tensor for which each element in the output is a subtensor of the\n    values tensor based on the indices. N is the number of batches, W the number\n    of indices in each batch, K the range of each index and C the number data\n    channels for each index.",
    "inputs": [
      { "name": "values", "type": "Tosa_Tensor3D" },
      { "name": "indices", "type": "Tosa_Int32Tensor2D" }
    ],
    "outputs": [
      { "name": "output", "type": "Tosa_Tensor3D" }
    ],
    "assemblyFormat": "operands attr-dict `:` functional-type(operands, results)",
    "category": "Tensor"
  },
  {
    "name": "tosa.greater",
    "summary": "Returns the truth value of (input1 > input2) element-wise.",
    "description": "Elementwise greater than comparison operation.",
    "inputs": [
      { "name": "input1", "type": "Tosa_Tensor" },
      { "name": "input2", "type": "Tosa_Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "Tosa_I1Tensor" }
    ],
    "assemblyFormat": "operands attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "tosa.greater_equal",
    "summary": "Returns the truth value of (input1 >= input2) element-wise.",
    "description": "Elementwise comparison operation.",
    "inputs": [
      { "name": "input1", "type": "Tosa_Tensor" },
      { "name": "input2", "type": "Tosa_Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "Tosa_I1Tensor" }
    ],
    "assemblyFormat": "operands attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "tosa.identity",
    "summary": "Identity operator.",
    "description": "Returns a tensor with the same shape, type, and contents as the input.",
    "inputs": [
      { "name": "input1", "type": "Tosa_Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "Tosa_Tensor" }
    ],
    "assemblyFormat": "operands attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "tosa.intdiv",
    "summary": "Integer divide operator.",
    "description": "Elementwise integer divide of input1 by input2. Axis of size 1 will be\n    broadcast as necessary. Rank of input tensors must match. The result of the\n    divide is truncated towards zero. Expected use is for operations on\n    non-scaled integers. Floating point divide should use RECIPROCAL and MUL.\n    Quantized integer divide should use TABLE (for 1/x) and MUL.",
    "inputs": [
      { "name": "input1", "type": "Tosa_Int32Tensor" },
      { "name": "input2", "type": "Tosa_Int32Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "Tosa_Int32Tensor" }
    ],
    "assemblyFormat": "operands attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "tosa.log",
    "summary": "Elementwise log operator.",
    "description": "Elementwise natural logarithm operation",
    "inputs": [
      { "name": "input1", "type": "Tosa_Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "Tosa_Tensor" }
    ],
    "assemblyFormat": "operands attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "tosa.logical_and",
    "summary": "Returns the truth value of input1 AND input2 element-wise.",
    "description": "Elementwise logical AND of input1 and input2. Axis of size 1 will be\n    broadcast, as necessary. Rank of input tensors must match.",
    "inputs": [
      { "name": "input1", "type": "Tosa_I1Tensor" },
      { "name": "input2", "type": "Tosa_I1Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "Tosa_I1Tensor" }
    ],
    "assemblyFormat": "operands attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "tosa.logical_left_shift",
    "summary": "Elementwise Logical Left Shift.",
    "description": "Elementwise logical left-shift of input1 by the amount specified in input2.\n    Axis of size 1 will be broadcast, as necessary.\n    Rank of input tensors must match.",
    "inputs": [
      { "name": "input1", "type": "Tosa_Tensor" },
      { "name": "input2", "type": "Tosa_Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "Tosa_Tensor" }
    ],
    "assemblyFormat": "operands attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "tosa.logical_not",
    "summary": "Returns the truth value of NOT input1 element-wise.",
    "description": "Elementwise logical NOT of input.",
    "inputs": [
      { "name": "input1", "type": "Tosa_I1Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "Tosa_I1Tensor" }
    ],
    "assemblyFormat": "operands attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "tosa.logical_or",
    "summary": "Returns the truth value of x OR y element-wise.",
    "description": "Elementwise logical OR of input1 and input2. Axis of size 1 will be\n    broadcast as necessary. Rank of input tensors must match.",
    "inputs": [
      { "name": "input1", "type": "Tosa_I1Tensor" },
      { "name": "input2", "type": "Tosa_I1Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "Tosa_I1Tensor" }
    ],
    "assemblyFormat": "operands attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "tosa.logical_right_shift",
    "summary": "Elementwise Logical Right Shift.",
    "description": "Elementwise logical right shift of input1 by the amount specified in input2.\n    Axis of size 1 will be broadcast, as necessary. Rank of input tensors must\n    match.",
    "inputs": [
      { "name": "input1", "type": "Tosa_Tensor" },
      { "name": "input2", "type": "Tosa_Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "Tosa_Tensor" }
    ],
    "assemblyFormat": "operands attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "tosa.logical_xor",
    "summary": "Returns the truth value of input1 XOR input2 element-wise.",
    "description": "Elementwise logical XOR of input1 and input2. Axis of size 1 will be\n    broadcast as necessary. Rank of input tensors must match.",
    "inputs": [
      { "name": "input1", "type": "Tosa_I1Tensor" },
      { "name": "input2", "type": "Tosa_I1Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "Tosa_I1Tensor" }
    ],
    "assemblyFormat": "operands attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "tosa.matmul",
    "summary": "Matrix multiplication operator.",
    "description": "Performs two dimensional matrix multiplications.",
    "inputs": [
      { "name": "a", "type": "Tosa_Tensor3D" },
      { "name": "b", "type": "Tosa_Tensor3D" },
      { "name": "a_zp", "type": "Tosa_ScalarIntOrFloatTensor" },
      { "name": "b_zp", "type": "Tosa_ScalarIntOrFloatTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "Tosa_Tensor3D" }
    ],
    "assemblyFormat": "operands attr-dict `:` functional-type(operands, results)",
    "category": "Layer"
  },
  {
    "name": "tosa.matmul_t_block_scaled",
    "summary": "Performs two dimensional matrix multiplications using block scaled tensors.",
    "description": "Performs two dimensional matrix multiplications using block scaled tensors. The block\n    dimension is always the the last dimension of the tensor, so the result is effectively\n    a matrix multiply of A by the transposed B matrix. If the N dimension of input B is of\n    size 1, the B matrix will be broadcast.",
    "inputs": [
      { "name": "a_data", "type": "Tosa_MXFPDataTensor3D" },
      { "name": "a_scale", "type": "Tosa_MXFPScaleTensor3D" },
      { "name": "b_data", "type": "Tosa_MXFPDataTensor3D" },
      { "name": "b_scale", "type": "Tosa_MXFPScaleTensor3D" }
    ],
    "outputs": [
      { "name": "output_data", "type": "Tosa_Tensor3D" }
    ],
    "attributes": [
      { "name": "block_size", "type": "Tosa_BlockSizeAttr" }
    ]
  },
  {
    "name": "tosa.max_pool2d",
    "summary": "Performs max pooling on the input.",
    "description": "This performs a max pooling over the given input tensor. A sliding window of\n    size given by <kernel size> is passed over the input tensor, with the\n    maximum value being placed in the\n    output tensor.",
    "inputs": [
      { "name": "input", "type": "Tosa_Tensor4D" }
    ],
    "outputs": [
      { "name": "output", "type": "Tosa_Tensor4D" }
    ],
    "attributes": [
      { "name": "kernel", "type": "Tosa_IntArrayAttr2" },
      { "name": "stride", "type": "Tosa_IntArrayAttr2" },
      { "name": "pad", "type": "Tosa_IntArrayAttr4" },
      { "name": "nan_mode", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "tosa.maximum",
    "summary": "Elementwise Maximum.",
    "description": "Elementwise max of input1 and input2. Axis of size 1 will be broadcast, as\n    necessary. Rank of input tensors must match.",
    "inputs": [
      { "name": "input1", "type": "Tosa_Tensor" },
      { "name": "input2", "type": "Tosa_Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "Tosa_Tensor" }
    ],
    "attributes": [
      { "name": "nan_mode", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "tosa.minimum",
    "summary": "Elementwise Minimum.",
    "description": "Elementwise minimum of input1 and input2. Axis of size 1\n    will be broadcast, as necessary. Rank of input tensors must match.",
    "inputs": [
      { "name": "input1", "type": "Tosa_Tensor" },
      { "name": "input2", "type": "Tosa_Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "Tosa_Tensor" }
    ],
    "attributes": [
      { "name": "nan_mode", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "tosa.mul",
    "summary": "Multiplication operator.",
    "description": "Elementwise multiplication (Hadamard product) of input1 and input2.\n    Axis of size 1 will be broadcast, as necessary. Rank of input tensors must\n    match.",
    "inputs": [
      { "name": "input1", "type": "Tosa_Tensor" },
      { "name": "input2", "type": "Tosa_Tensor" },
      { "name": "shift", "type": "Tosa_ScalarInt8Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "Tosa_Tensor" }
    ],
    "assemblyFormat": "operands attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "tosa.negate",
    "summary": "Elementwise negate operator.",
    "description": "Elementwise negation operation.",
    "inputs": [
      { "name": "input1", "type": "Tosa_Tensor" },
      { "name": "input1_zp", "type": "Tosa_ScalarIntOrFloatTensor" },
      { "name": "output_zp", "type": "Tosa_ScalarIntOrFloatTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "Tosa_Tensor" }
    ],
    "assemblyFormat": "operands attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "tosa.pad",
    "summary": "Pads a tensor with value specified.",
    "description": "Pads a tensor along the borders of each dimension with a supplied value.\n    Returns a new tensor with the padding included. The pad_const value includes\n    the zero point if the tensor uses a zero point.\n\n    Example:\n\n    ```mlir\n    %pad_const = \"tosa.const\"() {values = dense<3.14> : tensor<1xf32>} : () -> tensor<1xf32>\n    %padding = tosa.const_shape {values = dense<[1, 2, 3, 4]> : tensor<4xindex>} : () -> !tosa.shape<4>\n    tosa.pad %arg0, %padding, %pad_const: (tensor<1x2xf32>, !tosa.shape<4>, tensor<1xf32>)  -> (tensor<4x9xf32>)\n    ```\n\n    Example 2:\n\n    ```mlir\n    %pad_const = \"tosa.const\"() {values = dense<3.14> : tensor<1xf32>} : () -> tensor<1xf32>\n    %padding = tosa.const_shape {values = dense<[-1, 2, 3, 4]> : tensor<4xindex>} : () -> !tosa.shape<4>\n    tosa.pad %arg0, %padding, %pad_const : (tensor<1x2xf32>, !tosa.shape<4>, tensor<1xf32>)  -> (tensor<?x9xf32>)\n    ```",
    "inputs": [
      { "name": "input1", "type": "Tosa_TensorAtLeast1D" },
      { "name": "padding", "type": "Tosa_Shape" },
      { "name": "pad_const", "type": "Tosa_ScalarTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "Tosa_TensorAtLeast1D" }
    ],
    "assemblyFormat": "operands attr-dict `:` functional-type(operands, results)",
    "category": "Transform"
  },
  {
    "name": "tosa.pow",
    "summary": "Computes the power of one value to another.",
    "description": "Elementwise input1 value raised to the power of input2.\n    Axis of size 1 will be broadcast, as necessary. Rank of input tensors must\n    match.",
    "inputs": [
      { "name": "input1", "type": "Tosa_Tensor" },
      { "name": "input2", "type": "Tosa_Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "Tosa_Tensor" }
    ],
    "assemblyFormat": "operands attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "tosa.reciprocal",
    "summary": "Elementwise reciprocal operator.",
    "description": "Elementwise reciprocal operation. For integer operation, a TABLE should be\n    used with the appropriate ranges.",
    "inputs": [
      { "name": "input1", "type": "Tosa_Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "Tosa_Tensor" }
    ],
    "assemblyFormat": "operands attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "tosa.reduce_all",
    "summary": "Reduce All operator.",
    "description": "Reduce a tensor along the given axis with a logical AND operation.",
    "inputs": [
      { "name": "input", "type": "Tosa_TensorAtLeast1D" }
    ],
    "outputs": [
      { "name": "output", "type": "Tosa_TensorAtLeast1D" }
    ],
    "attributes": [
      { "name": "axis", "type": "I32Attr" }
    ],
    "assemblyFormat": "operands attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "tosa.reduce_any",
    "summary": "Reduce Any operator.",
    "description": "Reduce a tensor along the given axis with a logical OR operation.",
    "inputs": [
      { "name": "input", "type": "Tosa_TensorAtLeast1D" }
    ],
    "outputs": [
      { "name": "output", "type": "Tosa_TensorAtLeast1D" }
    ],
    "attributes": [
      { "name": "axis", "type": "I32Attr" }
    ],
    "assemblyFormat": "operands attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "tosa.reduce_max",
    "summary": "Reduce Max operator.",
    "description": "Reduce a tensor along the given axis with a maximum operation.",
    "inputs": [
      { "name": "input", "type": "Tosa_TensorAtLeast1D" }
    ],
    "outputs": [
      { "name": "output", "type": "Tosa_TensorAtLeast1D" }
    ],
    "attributes": [
      { "name": "axis", "type": "I32Attr" },
      { "name": "nan_mode", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "tosa.reduce_min",
    "summary": "Reduce Min operator.",
    "description": "Reduce a tensor along the given axis with a minimum operation.",
    "inputs": [
      { "name": "input", "type": "Tosa_TensorAtLeast1D" }
    ],
    "outputs": [
      { "name": "output", "type": "Tosa_TensorAtLeast1D" }
    ],
    "attributes": [
      { "name": "axis", "type": "I32Attr" },
      { "name": "nan_mode", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "tosa.reduce_product",
    "summary": "Reduce Product operator.",
    "description": "Reduce a tensor along the given axis by computing the product of the axis.",
    "inputs": [
      { "name": "input", "type": "Tosa_TensorAtLeast1D" }
    ],
    "outputs": [
      { "name": "output", "type": "Tosa_TensorAtLeast1D" }
    ],
    "attributes": [
      { "name": "axis", "type": "I32Attr" }
    ],
    "assemblyFormat": "operands attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "tosa.reduce_sum",
    "summary": "Reduce Sum operator.",
    "description": "Reduce a tensor along the given axis by computing the sum of the axis.",
    "inputs": [
      { "name": "input", "type": "Tosa_TensorAtLeast1D" }
    ],
    "outputs": [
      { "name": "output", "type": "Tosa_TensorAtLeast1D" }
    ],
    "attributes": [
      { "name": "axis", "type": "I32Attr" }
    ],
    "assemblyFormat": "operands attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "tosa.rescale",
    "summary": "Tosa rescale operator.",
    "description": "RESCALE is defined using an integer multiply, add, and shift.\n\n    Rescale supports two precisions of multiplier: 16-bit and 32-bit. The 32-bit multiplier\n    version supports two rounding modes to enable simpler lowering of existing frameworks\n    that use two stage rounding. All arithmetic is designed so that it does not overflow a\n    64-bit accumulator and that the result fits in 32 bits. In particular, a 48-bit value\n    cannot be scaled with the 32-bit multiplier because the accumulator would need to have\n    80 bits.\n\n    The shift and value range are limited to allow a variety of implementations. The limit\n    of 62 on shift allows the shift to be decomposed as two right shifts of 31.\n\n    Supported rescalings:\n    * This table is showing the supported conversions from the TOSA Specification.\n    * The MLIR dialect here can be used to represent other conversions.\n\n    | Mode                   | Input | Output | Unsigned input | Unsigned output |\n    |------------------------|-------|--------|----------------|-----------------|\n    | signed 16 to 16        | int16 | int16  |  false         |  false          |\n    | signed 16 to 32        | int16 | int32  |  false         |  false          |\n    | signed 16 to 8         | int16 | int8   |  false         |  false          |\n    | signed 32 to 16        | int32 | int16  |  false         |  false          |\n    | signed 32 to 32        | int32 | int32  |  false         |  false          |\n    | signed 32 to 8         | int32 | int8   |  false         |  false          |\n    | signed 8 to 16         | int8  | int16  |  false         |  false          |\n    | signed 8 to 32         | int8  | int32  |  false         |  false          |\n    | signed 8 to 8          | int8  | int8   |  false         |  false          |\n    | signed 48 to 16        | int48 | int16  |  false         |  false          |\n    | signed 48 to 32        | int48 | int32  |  false         |  false          |\n    | signed 48 to 8         | int48 | int8   |  false         |  false          |\n    | unsigned 8 to signed 8 | uint8 | int8   |  true          |  false          |\n    | signed 8 to unsigned 8 | int8  | uint8  |  false         |  true           |",
    "inputs": [
      { "name": "input", "type": "Tosa_Tensor" },
      { "name": "multiplier", "type": "Tosa_1DInt16Or32Tensor" },
      { "name": "shift", "type": "Tosa_1DInt8Tensor" },
      { "name": "input_zp", "type": "Tosa_ScalarIntOrFloatTensor" },
      { "name": "output_zp", "type": "Tosa_ScalarIntOrFloatTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "Tosa_Tensor" }
    ],
    "attributes": [
      { "name": "scale32", "type": "BoolAttr" },
      { "name": "rounding_mode", "type": "Tosa_RoundingModeAttr" },
      { "name": "per_channel", "type": "BoolAttr" },
      { "name": "input_unsigned", "type": "BoolAttr" },
      { "name": "output_unsigned", "type": "BoolAttr" }
    ]
  },
  {
    "name": "tosa.reshape",
    "summary": "Reshape operator.",
    "description": "Returns a tensor with the same type/values as the input, with a new shape\n    specified by the shape argument. Reshape may operate on tensors of any rank.\n    No data conversion happens during a reshape operation.",
    "inputs": [
      { "name": "input1", "type": "Tosa_Tensor" },
      { "name": "shape", "type": "Tosa_Shape" }
    ],
    "outputs": [
      { "name": "output", "type": "Tosa_Tensor" }
    ],
    "assemblyFormat": "operands attr-dict `:` functional-type(operands, results)",
    "category": "Shape"
  },
  {
    "name": "tosa.resize",
    "summary": "Resize operation, supports various resize/upsample modes.",
    "description": "Resizes a tensor. Resize is only allowed in the H and W dimensions.\n\n    The height dimension is scaled by factor (scale_y_n/scale_y_d). The width\n    dimension is scaled by factor (scale_x_n/scale_x_d).\n\n    The NEAREST_NEIGHBOR mode returns the value of the input tensor closest to\n    the calculated sample position for both floating-point and integer data\n    formats.\n\n    Floating-point BILINEAR mode returns a bilinearly interpolated output value\n    based on the four closest input sample positions.\n\n    For integer BILINEAR interpolation mode, the output value must be scaled by\n    1/(scale_y_n * scale_x_n) in a following operation to complete the\n    interpolation (for example with a RESCALE operator).\n\n    The output dimensions can be derived from the input dimensions by inverting\n    the scale as described in the pseudocode. The [border_y, border_x] values\n    adjust the output size to allow fractional sampling beyond integer input\n    position (IH - 1,IW - 1).\n\n    The limit MAX_SCALE is applied to each scale ratio after reduction of the\n    ratio. Individual scale numerator and denominator values are allowed to be\n    larger than MAX_SCALE.",
    "inputs": [
      { "name": "input", "type": "Tosa_Tensor4D" },
      { "name": "scale", "type": "Rank4TosaShape" },
      { "name": "offset", "type": "Rank2TosaShape" },
      { "name": "border", "type": "Rank2TosaShape" }
    ],
    "outputs": [
      { "name": "output", "type": "Tosa_Tensor4D" }
    ],
    "attributes": [
      { "name": "mode", "type": "Tosa_ResizeModeAttr" }
    ]
  },
  {
    "name": "tosa.reverse",
    "summary": "Reverse operator.",
    "description": "Returns a tensor with the same type/values as the input, with the data\n    reversed along the given axis. No data conversion happens during a reverse\n    operation.",
    "inputs": [
      { "name": "input1", "type": "Tosa_TensorAtLeast1D" }
    ],
    "outputs": [
      { "name": "output", "type": "Tosa_TensorAtLeast1D" }
    ],
    "attributes": [
      { "name": "axis", "type": "I32Attr" }
    ],
    "assemblyFormat": "operands attr-dict `:` functional-type(operands, results)",
    "category": "Transform"
  },
  {
    "name": "tosa.rfft2d",
    "summary": "Performs RFFT2D operation on the input.",
    "description": "Performs a batched 2D real-valued Fast Fourier Transform over the input where\n    the input tensor consists of real values producing complex valued output. The\n    complex output values will be split into the output_real and output_imag\n    tensor arguments. RFFT2D takes advantage of Hermitian symmetry to only\n    calculate the first half of the final output axis. Implementations may choose\n    to skip calculation of the imaginary values at (0,0), (0,W/2), (H/2,0), and\n    (H/2, W/2). If the calculation is skipped, the result at that location must be\n    zero.\n\n    Example:\n\n    ```mlir\n     %ouput_real, %output_imag = tosa.rfft2d %input_real : (tensor<8x16xf32>) -> (tensor<8x9xf32>, tensor<8x9xf32>)\n    ```",
    "inputs": [
      { "name": "input_real", "type": "Tosa_Tensor3D" }
    ],
    "outputs": [
      { "name": "output_real", "type": "Tosa_Tensor3D" },
      { "name": "output_imag", "type": "Tosa_Tensor3D" }
    ],
    "attributes": [
      { "name": "local_bound", "type": "DefaultValuedOptionalAttr" }
    ],
    "assemblyFormat": "operands attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "tosa.rsqrt",
    "summary": "Elementwise 1/sqrt operator.",
    "description": "Elementwise reciprocal square root operation. For integer operation, a TABLE\n    should be used with the appropriate ranges.",
    "inputs": [
      { "name": "input1", "type": "Tosa_Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "Tosa_Tensor" }
    ],
    "assemblyFormat": "operands attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "tosa.scatter",
    "summary": "Scatter operation.",
    "description": "The values_out tensor is set to the values_in tensor with data modified as\n    follows: data from the input tensor is inserted at the positions specified\n    by the indices tensor. N is the number of batches, W the number of indices\n    in each batch, K the range of each index and C the number data channels for\n    each index. It is not permitted to repeat the same output index within a\n    single SCATTER operation and so each output index occurs at most once. It\n    follows that K >= W. In use cases that require multiple updates to the same\n    output position, these must be decomposed into multiple SCATTER operations.",
    "inputs": [
      { "name": "values_in", "type": "Tosa_Tensor3D" },
      { "name": "indices", "type": "Tosa_Int32Tensor2D" },
      { "name": "input", "type": "Tosa_Tensor3D" }
    ],
    "outputs": [
      { "name": "values_out", "type": "Tosa_Tensor3D" }
    ],
    "assemblyFormat": "operands attr-dict `:` functional-type(operands, results)",
    "category": "Tensor"
  },
  {
    "name": "tosa.select",
    "summary": "Elementwise select operator.",
    "description": "Elementwise select of the output based on a condition.",
    "inputs": [
      { "name": "input1", "type": "Tosa_I1Tensor" },
      { "name": "input2", "type": "Tosa_Tensor" },
      { "name": "input3", "type": "Tosa_Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "Tosa_Tensor" }
    ],
    "assemblyFormat": "operands attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "tosa.sigmoid",
    "summary": "Computes elementwise sigmoid of input.",
    "description": "Applies the sigmoid logistic function to each element of the input tensor:\n    $ sigmoid(x) = \\frac{1}{1 + e^{-x}} $.\n\n    For quantized integer data types, the TABLE operator should be used instead.\n    Each implementation may choose an appropriate TABLE given the scale and zero\n    point of the input data. Eight or sixteen bit precision tables may be used\n    based on the input tensor to the sigmoid function.",
    "inputs": [
      { "name": "input", "type": "Tosa_Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "Tosa_Tensor" }
    ],
    "assemblyFormat": "operands attr-dict `:` functional-type(operands, results)",
    "category": "Activation"
  },
  {
    "name": "tosa.sin",
    "summary": "Elementwise sin operator.",
    "description": "Elementwise sine operation for values given in radians.",
    "inputs": [
      { "name": "input1", "type": "Tosa_FloatTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "Tosa_FloatTensor" }
    ],
    "assemblyFormat": "operands attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "tosa.slice",
    "summary": "Slice operator.",
    "description": "Extracts a slice of input1, beginning at the start coordinates,\n    and extending for size elements in each direction.\n    No data conversion happens during a slice operation.",
    "inputs": [
      { "name": "input1", "type": "Tosa_TensorAtLeast1D" },
      { "name": "start", "type": "Tosa_Shape" },
      { "name": "size", "type": "Tosa_Shape" }
    ],
    "outputs": [
      { "name": "output", "type": "Tosa_TensorAtLeast1D" }
    ],
    "assemblyFormat": "operands attr-dict `:` functional-type(operands, results)",
    "category": "Tensor"
  },
  {
    "name": "tosa.sub",
    "summary": "Elementwise subtraction operator.",
    "description": "Elementwise subtraction of input1 and input2. Axis of size 1 will be\n    broadcast as necessary. Rank of input tensors must match.",
    "inputs": [
      { "name": "input1", "type": "Tosa_Tensor" },
      { "name": "input2", "type": "Tosa_Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "Tosa_Tensor" }
    ],
    "assemblyFormat": "operands attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "tosa.table",
    "summary": "Table lookup operator.",
    "description": "Table lookup operation. For int8_t TABLE operation, perform a 256 entry\n    table lookup returning an int8_t value. For int16_t tables, the int16_t\n    input is treated as a fixed-point 9.7 value. The most significant 9 bits\n    are used to index into the table. The fractional 7 bits are used to\n    interpolate based on table[index] and table[index+1]. For int16_t inputs,\n    the TABLE operator returns a 16.7 interpolated value in an int32_t. This\n    value can then be input to the RESCALE operator to scale to the required\n    output data type. Note that int16_t table has 513 values to handle\n    table[index+1] when index=511.\n\n    An int16_t to int16_t table lookup can be constructed in TOSA as follows:\n    * Use the TABLE operator to produce a fixed point 16.7 interpolated result\n    * Use RESCALE (in_t=int32_t, out_t=int16_t, scale=1<<14, shift=21) to\n      scale the output to int16_t range (or alternate scale as required)",
    "inputs": [
      { "name": "input1", "type": "Tosa_Tensor" },
      { "name": "table", "type": "Tosa_Tensor1D" }
    ],
    "outputs": [
      { "name": "output", "type": "Tosa_Tensor" }
    ],
    "assemblyFormat": "operands attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "tosa.tanh",
    "summary": "Computes elementwise hyperbolic tangent of input.",
    "description": "Parameterized hyperbolic tangent: $ tanh(x) = \\frac{1 - e^{-2x}}{1 + e^{-2x}} $.\n\n    For quantized integer data types, the TABLE operator should be used instead.\n    Each implementation may choose an appropriate TABLE given the scale and zero\n    point of the input data. Eight or sixteen bit precision tables may be used\n    based on the input tensor to the tanh function.",
    "inputs": [
      { "name": "input", "type": "Tosa_Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "Tosa_Tensor" }
    ],
    "assemblyFormat": "operands attr-dict `:` functional-type(operands, results)",
    "category": "Activation"
  },
  {
    "name": "tosa.tile",
    "summary": "Tile operator.",
    "description": "Replicates input1 multiples times along each dimension.",
    "inputs": [
      { "name": "input1", "type": "Tosa_TensorAtLeast1D" },
      { "name": "multiples", "type": "Tosa_Shape" }
    ],
    "outputs": [
      { "name": "output", "type": "Tosa_TensorAtLeast1D" }
    ],
    "assemblyFormat": "operands attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "tosa.transpose",
    "summary": "Transpose operator.",
    "description": "Permutes the dimensions of the input tensor input1 based on the perms\n    argument. Each value in the perms list must be a valid dimension of the\n    input tensor and may not be repeated.",
    "inputs": [
      { "name": "input1", "type": "Tosa_TensorAtLeast1D" }
    ],
    "outputs": [
      { "name": "output", "type": "Tosa_TensorAtLeast1D" }
    ],
    "attributes": [
      { "name": "perms", "type": "DenseI32ArrayAttr" }
    ],
    "assemblyFormat": "operands attr-dict `:` functional-type(operands, results)",
    "category": "Transform"
  },
  {
    "name": "tosa.transpose_conv2d",
    "summary": "Transpose 2D Convolution operator.",
    "description": "Performs a 2D transposed convolution over the given tensor input, using the\n    weights tensor. Implementations may choose to skip calculation of multiplies\n    by zero at fractional input positions.",
    "inputs": [
      { "name": "input", "type": "Tosa_Tensor4D" },
      { "name": "weight", "type": "Tosa_Tensor4D" },
      { "name": "bias", "type": "Tosa_Tensor1D" },
      { "name": "input_zp", "type": "Tosa_ScalarIntOrFloatTensor" },
      { "name": "weight_zp", "type": "Tosa_ScalarIntOrFloatTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "Tosa_Tensor4D" }
    ],
    "attributes": [
      { "name": "out_pad", "type": "Tosa_IntArrayAttr4" },
      { "name": "stride", "type": "Tosa_IntArrayAttr2" },
      { "name": "acc_type", "type": "TypeAttrOf" },
      { "name": "local_bound", "type": "DefaultValuedOptionalAttr" }
    ],
    "assemblyFormat": "operands attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "tosa.variable",
    "summary": "Defines a variable",
    "description": "Defines a new TOSA variable. This is a persistent mutable value across multiple\n    TOSA graph invocations. Modifications are expressed using read/write semantics.",
    "attributes": [
      { "name": "sym_name", "type": "SymbolNameAttr" },
      { "name": "var_shape", "type": "IndexElementsAttr" },
      { "name": "type", "type": "TypeAttr" },
      { "name": "initial_value", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "$sym_name\n    attr-dict\n    custom<VariableOpTypeOrInitialValue>($var_shape, $type, $initial_value)"
  },
  {
    "name": "tosa.variable_read",
    "summary": "read_buffer operator",
    "description": "Reads the value from a pseudo-buffer resource holding a persistent mutable tensor.",
    "outputs": [
      { "name": "output1", "type": "Tosa_Tensor" }
    ],
    "attributes": [
      { "name": "name", "type": "SymbolNameAttr" }
    ],
    "assemblyFormat": "$name attr-dict `:` type($output1)"
  },
  {
    "name": "tosa.variable_write",
    "summary": "write_buffer operator",
    "description": "Assigns a value to the pseudo-buffer resource holding a persistent mutable tensor.",
    "inputs": [
      { "name": "input1", "type": "Tosa_Tensor" }
    ],
    "attributes": [
      { "name": "name", "type": "SymbolNameAttr" }
    ],
    "assemblyFormat": "$name attr-dict `,` $input1 `:` type($input1)"
  },
  {
    "name": "tosa.while_loop",
    "summary": "output = input; While (Cond(output)) {output = Body(output)}",
    "description": "Generates and evaluates a Boolean condition and either executes a loop body\n    or exits the loop. This action is performed repeatedly after\n    updating and re-evaluating the Boolean condition every iteration. This\n    implements the semantic foreach or while iterative loop structure.",
    "inputs": [
      { "name": "input_list", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "output_list", "type": "Variadic" }
    ]
  },
  {
    "name": "tosa.yield",
    "summary": "yield operator",
    "description": "return operation within the conditional and body of\n    structured control flow. Operation takes variadic operands\n    but produces no results of its own.",
    "inputs": [
      { "name": "inputs", "type": "Variadic" }
    ],
    "assemblyFormat": "$inputs attr-dict `:` type($inputs)"
  },
  {
    "name": "vector.bitcast",
    "summary": "bitcast casts between vectors",
    "description": "The bitcast operation casts between vectors of the same rank, the minor 1-D\n    vector size is casted to a vector with a different element type but same\n    bitwidth. In case of 0-D vectors, the bitwidth of element types must be\n    equal.\n\n    Example:\n\n    ```mlir\n    // Example casting to a smaller element type.\n    %1 = vector.bitcast %0 : vector<5x1x4x3xf32> to vector<5x1x4x6xi16>\n\n    // Example casting to a bigger element type.\n    %3 = vector.bitcast %2 : vector<10x12x8xi8> to vector<10x12x2xi32>\n\n    // Example casting to an element type of the same size.\n    %5 = vector.bitcast %4 : vector<5x1x4x3xf32> to vector<5x1x4x3xi32>\n\n    // Example casting of 0-D vectors.\n    %7 = vector.bitcast %6 : vector<f32> to vector<i32>\n    ```",
    "assemblyFormat": "$source attr-dict `:` type($source) `to` type($result)"
  },
  {
    "name": "vector.broadcast",
    "summary": "broadcast operation",
    "description": "Broadcasts the scalar or k-D vector value in the source operand\n    to a n-D result vector such that the broadcast makes sense, i.e.,\n    the source operand is duplicated to match the given rank and sizes\n    in the result vector. The legality rules are:\n    * the source operand must have the same element type as the result type\n    * a k-D vector <s_1 x .. x s_k x type> can be broadcast to\n      a n-D vector <t_1 x .. x t_n x type> if\n       * k <= n, and\n       * the sizes in the trailing dimensions n-k < i <= n with j=i+k-n\n          match exactly as s_j = t_i or s_j = 1:\n       ```\n           t_1 x   ..  t_n-k x t_n-k+1 x .. x t_i x .. x t_n\n                               s_1     x .. x s_j x .. x s_k\n               <duplication>         <potential stretch>\n       ```\n       * in addition, any scalable unit dimension, `[1]`, must match exactly.\n\n    The source operand is duplicated over all the missing leading dimensions\n    and stretched over the trailing dimensions where the source has a non-equal\n    dimension of 1 (stretching a trailing dimension is also referred to as\n    \"dim-1\" broadcasting). These rules imply that any scalar broadcast (k=0) to\n    any shaped vector with the same element type is always legal.\n\n    Example:\n\n    ```mlir\n    %0 = arith.constant 0.0 : f32\n    %1 = vector.broadcast %0 : f32 to vector<16xf32>\n    %2 = vector.broadcast %1 : vector<16xf32> to vector<4x16xf32>\n    ```",
    "assemblyFormat": "$source attr-dict `:` type($source) `to` type($vector)"
  },
  {
    "name": "vector.compressstore",
    "summary": "writes elements selectively from a vector as defined by a mask",
    "description": "The compress store operation writes elements from a vector into memory as\n    defined by a base with indices and a mask vector. Compression only applies\n    to the innermost dimension. When the mask is set, the corresponding element\n    from the vector is written next to memory.  Otherwise, no action is taken\n    for the element. Informally the semantics are:\n\n    ```\n    index = i\n    if (mask[0]) base[index++] = value[0]\n    if (mask[1]) base[index++] = value[1]\n    etc.\n    ```\n\n    Note that the index increment is done conditionally.\n\n    If a mask bit is set and the corresponding index is out-of-bounds for the\n    given base, the behavior is undefined. If a mask bit is not set, no value\n    is stored regardless of the index, and the index is allowed to be\n    out-of-bounds.\n\n    The compress store can be used directly where applicable, or can be used\n    during progressively lowering to bring other memory operations closer to\n    hardware ISA support for a compress. The semantics of the operation closely\n    correspond to those of the `llvm.masked.compressstore`\n    [intrinsic](https://llvm.org/docs/LangRef.html#llvm-masked-compressstore-intrinsics).\n\n    An optional `alignment` attribute allows to specify the byte alignment of the\n    store operation. It must be a positive power of 2. The operation must access\n    memory at an address aligned to this boundary. Violating this requirement\n    triggers immediate undefined behavior.\n\n    Note, at the moment this Op is only available for fixed-width vectors.\n\n    Examples:\n\n    ```mlir\n    vector.compressstore %base[%i], %mask, %value\n      : memref<?xf32>, vector<8xi1>, vector<8xf32>\n\n    vector.compressstore %base[%i, %j], %mask, %value\n      : memref<?x?xf32>, vector<16xi1>, vector<16xf32>\n    ```",
    "assemblyFormat": "$base `[` $indices `]` `,` $mask `,` $valueToStore attr-dict `:` type($base) `,` type($mask) `,` type($valueToStore)"
  },
  {
    "name": "vector.constant_mask",
    "summary": "creates a constant vector mask",
    "description": "Creates and returns a vector mask where elements of the result vector\n    are set to '0' or '1', based on whether the element indices are contained\n    within a hyper-rectangular region specified by the 'mask_dim_sizes'\n    array attribute argument. Each element of the 'mask_dim_sizes' array,\n    specifies an exclusive upper bound [0, mask-dim-size-element-value)\n    for a unique dimension in the vector result. The conjunction of the ranges\n    define a hyper-rectangular region within which elements values are set to 1\n    (otherwise element values are set to 0). Each value of 'mask_dim_sizes' must\n    be non-negative and not greater than the size of the corresponding vector\n    dimension (as opposed to vector.create_mask which allows this). Sizes that\n    correspond to scalable dimensions are implicitly multiplied by vscale,\n    though currently only zero (none set) or the size of the dim/vscale\n    (all set) are supported.\n\n    Example:\n\n    ```mlir\n    // create a constant vector mask of size 4x3xi1 with elements in range\n    // 0 <= row <= 2 and 0 <= col <= 1 are set to 1 (others to 0).\n    %1 = vector.constant_mask [3, 2] : vector<4x3xi1>\n\n    print %1\n                  columns\n                0    1    2\n              |------------\n            0 | 1    1    0\n      rows  1 | 1    1    0\n            2 | 1    1    0\n            3 | 0    0    0\n    ```",
    "assemblyFormat": "$mask_dim_sizes attr-dict `:` type(results)"
  },
  {
    "name": "vector.contract",
    "summary": "vector contraction operation",
    "description": "Computes the sum of products of vector elements along contracting\n    dimension pairs from 2 vectors of rank M and N respectively, adds this\n    intermediate result to the accumulator argument of rank K, and returns a\n    vector result of rank K (where K = num_lhs_free_dims + num_rhs_free_dims +\n    num_batch_dims (see dimension type descriptions below)). For K = 0 (no\n    free or batch dimensions), the accumulator and output are a scalar.\n\n    If operands and the result have types of different bitwidths, operands are\n    promoted to have the same bitwidth as the result before performing the\n    contraction. For integer types, only signless integer types are supported,\n    and the promotion happens via sign extension.\n\n    An iterator type attribute list must be specified, where each element of\n    the list represents an iterator with one of the following types:\n\n    *   \"reduction\": reduction dimensions are present in the lhs and rhs\n        arguments but not in the output (and accumulator\n        argument). These are the dimensions along which the vector\n        contraction op computes the sum of products, and\n        contracting dimension pair dimension sizes must match\n        between lhs/rhs.\n\n    *   \"parallel\": Batch dimensions are iterator type \"parallel\", and\n        are non-contracting dimensions present in the lhs, rhs and\n        output. The lhs/rhs co-iterate along the batch dimensions,\n        which should be expressed in their indexing maps.\n\n        Free dimensions are iterator type \"parallel\", and are\n        non-contraction, non-batch dimensions accessed by either the\n        lhs or rhs (but not both). The lhs and rhs free dimensions\n        are unrelated to each other and do not co-iterate, which\n        should be expressed in their indexing maps.\n\n    An indexing map attribute list must be specified with an entry for lhs, rhs\n    and acc arguments. An indexing map attribute specifies a mapping from each\n    iterator in the iterator type list, to each dimension of an N-D vector.\n\n    An optional kind attribute may be used to specify the combining function\n    between the intermediate result and accumulator argument of rank K. This\n    attribute can take the values `add`/`mul`/`minsi`/`minui`/`maxsi`/`maxui`\n    /`and`/`or`/`xor` for integers, and `add`/`mul`/`minnumf`/`maxnumf`\n    /`minimumf`/`maximumf` for floats. The default is `add`.\n\n    Example:\n\n    ```mlir\n    // Simple DOT product (K = 0).\n    #contraction_accesses = [\n     affine_map<(i) -> (i)>,\n     affine_map<(i) -> (i)>,\n     affine_map<(i) -> ()>\n    ]\n    #contraction_trait = {\n      indexing_maps = #contraction_accesses,\n      iterator_types = [\"reduction\"]\n    }\n    %3 = vector.contract #contraction_trait %0, %1, %2\n      : vector<10xf32>, vector<10xf32> into f32\n\n    // 2D vector contraction with one contracting dimension (matmul, K = 2).\n    #contraction_accesses = [\n      affine_map<(i, j, k) -> (i, k)>,\n      affine_map<(i, j, k) -> (k, j)>,\n      affine_map<(i, j, k) -> (i, j)>\n    ]\n    #contraction_trait = {\n      indexing_maps = #contraction_accesses,\n      iterator_types = [\"parallel\", \"parallel\", \"reduction\"]\n    }\n\n    %3 = vector.contract #contraction_trait %0, %1, %2\n      : vector<4x3xf32>, vector<3x7xf32> into vector<4x7xf32>\n\n    // 4D to 3D vector contraction with two contracting dimensions and\n    // one batch dimension (K = 3).\n    #contraction_accesses = [\n      affine_map<(b0, f0, f1, c0, c1) -> (c0, b0, c1, f0)>,\n      affine_map<(b0, f0, f1, c0, c1) -> (b0, c1, c0, f1)>,\n      affine_map<(b0, f0, f1, c0, c1) -> (b0, f0, f1)>\n    ]\n    #contraction_trait = {\n      indexing_maps = #contraction_accesses,\n      iterator_types = [\"parallel\", \"parallel\", \"parallel\",\n                        \"reduction\", \"reduction\"]\n    }\n\n    %4 = vector.contract #contraction_trait %0, %1, %2\n        : vector<7x8x16x15xf32>, vector<8x16x7x5xf32> into vector<8x15x5xf32>\n\n    // Vector contraction with mixed typed. lhs/rhs have different element\n    // types than accumulator/result.\n    %5 = vector.contract #contraction_trait %0, %1, %2\n      : vector<10xf16>, vector<10xf16> into f32\n\n    // Contract with max (K = 0).\n    #contraction_accesses = [\n     affine_map<(i) -> (i)>,\n     affine_map<(i) -> (i)>,\n     affine_map<(i) -> ()>\n    ]\n    #contraction_trait = {\n      indexing_maps = #contraction_accesses,\n      iterator_types = [\"reduction\"],\n      kind = #vector.kind<maxnumf>\n    }\n    %6 = vector.contract #contraction_trait %0, %1, %2\n      : vector<10xf32>, vector<10xf32> into f32\n    ```"
  },
  {
    "name": "vector.create_mask",
    "summary": "creates a vector mask",
    "description": "Creates and returns a vector mask where elements of the result vector\n    are set to '0' or '1', based on whether the element indices are contained\n    within a hyper-rectangular region specified by the operands. Specifically,\n    each operand specifies a range [0, operand-value) for a unique dimension in\n    the vector result. The conjunction of the operand ranges define a\n    hyper-rectangular region within which elements values are set to 1\n    (otherwise element values are set to 0). If operand-value is negative, it is\n    treated as if it were zero, and if it is greater than the corresponding\n    dimension size, it is treated as if it were equal to the dimension size.\n\n    Example:\n\n    ```mlir\n    // create a vector mask of size 4x3xi1 where elements in range\n    // 0 <= row <= 2 and 0 <= col <= 1 are set to 1 (others to 0).\n    %1 = vector.create_mask %c3, %c2 : vector<4x3xi1>\n\n    print %1\n                  columns\n                0    1    2\n              |------------\n            0 | 1    1    0\n      rows  1 | 1    1    0\n            2 | 1    1    0\n            3 | 0    0    0\n    ```",
    "assemblyFormat": "$operands attr-dict `:` type(results)"
  },
  {
    "name": "vector.deinterleave",
    "summary": "constructs two vectors by deinterleaving an input vector",
    "description": "The deinterleave operation constructs two vectors from a single input\n        vector. The first result vector contains the elements from even indexes\n        of the input, and the second contains elements from odd indexes. This is\n        the inverse of a `vector.interleave` operation.\n\n        Each output's trailing dimension is half of the size of the input\n        vector's trailing dimension. This operation requires the input vector\n        to have a rank > 0 and an even number of elements in its trailing\n        dimension.\n\n        The operation supports scalable vectors.\n\n        Example:\n        ```mlir\n        %0, %1 = vector.deinterleave %a\n                   : vector<8xi8> -> vector<4xi8>\n        %2, %3 = vector.deinterleave %b\n                   : vector<2x8xi8> -> vector<2x4xi8>\n        %4, %5 = vector.deinterleave %c\n                   : vector<2x8x4xi8> -> vector<2x8x2xi8>\n        %6, %7 = vector.deinterleave %d\n                   : vector<[8]xf32> -> vector<[4]xf32>\n        %8, %9 = vector.deinterleave %e\n                   : vector<2x[6]xf64> -> vector<2x[3]xf64>\n        %10, %11 = vector.deinterleave %f\n                   : vector<2x4x[6]xf64> -> vector<2x4x[3]xf64>\n        ```",
    "inputs": [
      { "name": "source", "type": "AnyVectorOfNonZeroRank" }
    ],
    "outputs": [
      { "name": "res1", "type": "AnyVectorOfNonZeroRank" },
      { "name": "res2", "type": "AnyVectorOfNonZeroRank" }
    ],
    "assemblyFormat": "$source attr-dict `:` type($source) `->` type($res1)"
  },
  {
    "name": "vector.expandload",
    "summary": "reads elements from memory and spreads them into a vector as defined by a mask",
    "description": "The expand load reads elements from memory into a vector as defined by a\n    base with indices and a mask vector. Expansion only applies to the innermost\n    dimension. When the mask is set, the next element is read from memory.\n    Otherwise, the corresponding element is taken from a pass-through vector.\n    Informally the semantics are:\n\n    ```\n    index = i\n    result[0] := if mask[0] then base[index++] else pass_thru[0]\n    result[1] := if mask[1] then base[index++] else pass_thru[1]\n    etc.\n    ```\n\n    Note that the index increment is done conditionally.\n\n    If a mask bit is set and the corresponding index is out-of-bounds for the\n    given base, the behavior is undefined. If a mask bit is not set, the value\n    comes from the pass-through vector regardless of the index, and the index is\n    allowed to be out-of-bounds.\n\n    The expand load can be used directly where applicable, or can be used\n    during progressively lowering to bring other memory operations closer to\n    hardware ISA support for an expand. The semantics of the operation closely\n    correspond to those of the `llvm.masked.expandload`\n    [intrinsic](https://llvm.org/docs/LangRef.html#llvm-masked-expandload-intrinsics).\n\n    An optional `alignment` attribute allows to specify the byte alignment of the\n    load operation. It must be a positive power of 2. The operation must access\n    memory at an address aligned to this boundary. Violating this requirement\n    triggers immediate undefined behavior.\n\n    Note, at the moment this Op is only available for fixed-width vectors.\n\n    Examples:\n\n    ```mlir\n    %0 = vector.expandload %base[%i], %mask, %pass_thru\n       : memref<?xf32>, vector<8xi1>, vector<8xf32> into vector<8xf32>\n\n    %1 = vector.expandload %base[%i, %j], %mask, %pass_thru\n       : memref<?x?xf32>, vector<16xi1>, vector<16xf32> into vector<16xf32>\n    ```",
    "assemblyFormat": "$base `[` $indices `]` `,` $mask `,` $pass_thru attr-dict `:` type($base) `,` type($mask) `,` type($pass_thru) `into` type($result)"
  },
  {
    "name": "vector.extract",
    "summary": "extract operation",
    "description": "Extracts an (n − k)-D result sub-vector from an n-D source vector at a\n    specified k-D position. When n = k, the result degenerates to a scalar\n    element.\n\n    Static and dynamic indices must be greater or equal to zero and less than\n    the size of the corresponding dimension. The result is undefined if any\n    index is out-of-bounds. The value `-1` represents a poison index, which\n    specifies that the extracted element is poison.\n\n    Example:\n\n    ```mlir\n    %1 = vector.extract %0[3]: vector<8x16xf32> from vector<4x8x16xf32>\n    %2 = vector.extract %0[2, 1, 3]: f32 from vector<4x8x16xf32>\n    %4 = vector.extract %0[%a, %b, %c]: f32 from vector<4x8x16xf32>\n    %5 = vector.extract %0[2, %b]: vector<16xf32> from vector<4x8x16xf32>\n    %6 = vector.extract %10[-1, %c]: f32 from vector<4x16xf32>\n    ```",
    "inputs": [
      { "name": "source", "type": "AnyVectorOfAnyRank" },
      { "name": "dynamic_position", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyType" }
    ],
    "attributes": [
      { "name": "static_position", "type": "DenseI64ArrayAttr" }
    ],
    "assemblyFormat": "$source ``\n    custom<DynamicIndexList>($dynamic_position, $static_position)\n    attr-dict `:` type($result) `from` type($source)"
  },
  {
    "name": "vector.extract_strided_slice",
    "summary": "extract_strided_slice operation",
    "description": "Takes an n-D vector, k-D `offsets` integer array attribute, a k-sized\n    `sizes` integer array attribute, a k-sized `strides` integer array\n    attribute and extracts the n-D subvector at the proper offset.\n\n    At the moment strides must contain only 1s.\n\n    Returns an n-D vector where the first k-D dimensions match the `sizes`\n    attribute. The returned subvector contains the elements starting at offset\n    `offsets` and ending at `offsets + sizes`.\n\n    Example:\n\n    ```mlir\n    %1 = vector.extract_strided_slice %0\n        {offsets = [0, 2], sizes = [2, 4], strides = [1, 1]}:\n      vector<4x8x16xf32> to vector<2x4x16xf32>\n\n    // TODO: Evolve to a range form syntax similar to:\n    %1 = vector.extract_strided_slice %0[0:2:1][2:4:1]\n      vector<4x8x16xf32> to vector<2x4x16xf32>\n    ```\n\n    TODO: Implement support for poison indices.",
    "assemblyFormat": "$source attr-dict `:` type($source) `to` type(results)"
  },
  {
    "name": "vector.fma",
    "summary": "vector fused multiply-add",
    "description": "Multiply-add expressions operate on n-D vectors and compute a fused\n    pointwise multiply-and-accumulate: `$result = $lhs * $rhs + $acc`.\n    All operands and result have the same vector type. The semantics\n    of the operation correspond to those of the `llvm.fma`\n    [intrinsic](https://llvm.org/docs/LangRef.html#int-fma). In the\n    particular case of lowering to LLVM, this is guaranteed to lower\n    to the `llvm.fma.*` intrinsic.\n\n    Example:\n\n    ```mlir\n    %3 = vector.fma %0, %1, %2: vector<8x16xf32>\n    ```",
    "assemblyFormat": "$lhs `,` $rhs `,` $acc attr-dict `:` type($lhs)"
  },
  {
    "name": "vector.from_elements",
    "summary": "operation that defines a vector from scalar elements",
    "description": "This operation defines a vector from one or multiple scalar elements. The\n    scalar elements are arranged in row-major within the vector. The number of\n    elements must match the number of elements in the result type. All elements\n    must have the same type, which must match the element type of the result\n    vector type. Scalable vectors are not supported.\n\n    Examples:\n\n    ```mlir\n    // Define a 0-D vector.\n    %0 = vector.from_elements %f1 : vector<f32>\n    // [%f1]\n\n    // Define a 1-D vector.\n    %1 = vector.from_elements %f1, %f2 : vector<2xf32>\n    // [%f1, %f2]\n\n    // Define a 2-D vector.\n    %2 = vector.from_elements %f1, %f2, %f3, %f4, %f5, %f6 : vector<2x3xf32>\n    // [[%f1, %f2, %f3], [%f4, %f5, %f6]]\n\n    // Define a 3-D vector.\n    %3 = vector.from_elements %f1, %f2, %f3, %f4, %f5, %f6 : vector<3x1x2xf32>\n    // [[[%f1, %f2]], [[%f3, %f4]], [[%f5, %f6]]]\n    ```",
    "inputs": [
      { "name": "elements", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "dest", "type": "AnyFixedVectorOfAnyRank" }
    ],
    "assemblyFormat": "$elements attr-dict `:` type($dest)"
  },
  {
    "name": "vector.gather",
    "summary": "Gathers elements from memory or ranked tensor into a vector as defined by an\n    index vector and a mask vector.",
    "description": "The gather operation returns an n-D vector whose elements are either loaded\n    from a k-D memref or tensor, or taken from an n-D pass-through vector, depending\n    on the values of an n-D mask vector.\n\n    If a mask bit is set, the corresponding result element is taken from `base`\n    at an index defined by k indices and n-D `index_vec`. Otherwise, the element\n    is taken from the pass-through vector. As an example, suppose that `base` is\n    3-D and the result is 2-D:\n\n    ```mlir\n    func.func @gather_3D_to_2D(\n        %base: memref<?x10x?xf32>, %ofs_0: index, %ofs_1: index, %ofs_2: index,\n        %indices: vector<2x3xi32>, %mask: vector<2x3xi1>,\n        %fall_thru: vector<2x3xf32>) -> vector<2x3xf32> {\n            %result = vector.gather %base[%ofs_0, %ofs_1, %ofs_2]\n                                   [%indices], %mask, %fall_thru : [...]\n            return %result : vector<2x3xf32>\n    }\n    ```\n\n    The indexing semantics are then,\n\n    ```\n    result[i,j] := if mask[i,j] then base[i0, i1, i2 + indices[i,j]]\n                   else pass_thru[i,j]\n    ```\n    The index into `base` only varies in the innermost ((k-1)-th) dimension.\n\n    If a mask bit is set and the corresponding index is out-of-bounds for the\n    given base, the behavior is undefined. If a mask bit is not set, the value\n    comes from the pass-through vector regardless of the index, and the index is\n    allowed to be out-of-bounds.\n\n    The gather operation can be used directly where applicable, or can be used\n    during progressively lowering to bring other memory operations closer to\n    hardware ISA support for a gather.\n\n    An optional `alignment` attribute allows to specify the byte alignment of the\n    gather operation. It must be a positive power of 2. The operation must access\n    memory at an address aligned to this boundary. Violating this requirement\n    triggers immediate undefined behavior.\n\n    Examples:\n\n    ```mlir\n    // 1-D memref gathered to 2-D vector.\n    %0 = vector.gather %base[%c0][%v], %mask, %pass_thru\n       : memref<?xf32>, vector<2x16xi32>, vector<2x16xi1>, vector<2x16xf32> into vector<2x16xf32>\n\n    // 2-D memref gathered to 1-D vector.\n    %1 = vector.gather %base[%i, %j][%v], %mask, %pass_thru\n       : memref<16x16xf32>, vector<16xi32>, vector<16xi1>, vector<16xf32> into vector<16xf32>\n    ```",
    "assemblyFormat": "$base `[` $offsets `]` `[` $indices `]` `,` $mask `,` $pass_thru attr-dict `:` type($base) `,` type($indices)  `,` type($mask) `,` type($pass_thru) `into` type($result)",
    "category": "Tensor"
  },
  {
    "name": "vector.insert",
    "summary": "insert operation",
    "description": "Inserts an (n - k)-D sub-vector (value-to-store) into an n-D destination\n    vector at a specified k-D position. When n = 0, value-to-store degenerates\n    to a scalar element inserted into the n-D destination vector.\n\n    Static and dynamic indices must be greater or equal to zero and less than\n    the size of the corresponding dimension. The result is undefined if any\n    index is out-of-bounds. The value `-1` represents a poison index, which\n    specifies that the resulting vector is poison.\n\n    Example:\n\n    ```mlir\n    %2 = vector.insert %0, %1[3] : vector<8x16xf32> into vector<4x8x16xf32>\n    %5 = vector.insert %3, %4[2, 1, 3] : f32 into vector<4x8x16xf32>\n    %11 = vector.insert %9, %10[%a, %b, %c] : f32 into vector<4x8x16xf32>\n    %12 = vector.insert %4, %10[2, %b] : vector<16xf32> into vector<4x8x16xf32>\n    %13 = vector.insert %20, %1[-1, %c] : f32 into vector<4x16xf32>\n    ```",
    "inputs": [
      { "name": "valueToStore", "type": "AnyType" },
      { "name": "dest", "type": "AnyVectorOfAnyRank" },
      { "name": "dynamic_position", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyVectorOfAnyRank" }
    ],
    "attributes": [
      { "name": "static_position", "type": "DenseI64ArrayAttr" }
    ],
    "assemblyFormat": "$valueToStore `,` $dest custom<DynamicIndexList>($dynamic_position, $static_position)\n    attr-dict `:` type($valueToStore) `into` type($dest)"
  },
  {
    "name": "vector.insert_strided_slice",
    "summary": "strided_slice operation",
    "description": "Takes a k-D valueToStore vector, an n-D destination vector (n >= k), n-sized\n    `offsets` integer array attribute, a k-sized `strides` integer array attribute\n    and inserts the k-D valueToStore vector as a strided subvector at the proper offset\n    into the n-D destination vector.\n\n    At the moment strides must contain only 1s.\n\n    Returns an n-D vector that is a copy of the n-D destination vector in which\n    the last k-D dimensions contain the k-D valueToStore vector elements strided at\n    the proper location as specified by the offsets.\n\n    Example:\n\n    ```mlir\n    %2 = vector.insert_strided_slice %0, %1\n        {offsets = [0, 0, 2], strides = [1, 1]}:\n      vector<2x4xf32> into vector<16x4x8xf32>\n    ```",
    "assemblyFormat": "$valueToStore `,` $dest attr-dict `:` type($valueToStore) `into` type($dest)"
  },
  {
    "name": "vector.interleave",
    "summary": "constructs a vector by interleaving two input vectors",
    "description": "The interleave operation constructs a new vector by interleaving the\n    elements from the trailing (or final) dimension of two input vectors,\n    returning a new vector where the trailing dimension is twice the size.\n\n    Note that for the n-D case this differs from the interleaving possible with\n    `vector.shuffle`, which would only operate on the leading dimension.\n\n    Another key difference is this operation supports scalable vectors, though\n    currently a general LLVM lowering is limited to the case where only the\n    trailing dimension is scalable.\n\n    Example:\n    ```mlir\n    %a = arith.constant dense<[0, 1]> : vector<2xi32>\n    %b = arith.constant dense<[2, 3]> : vector<2xi32>\n    // The value of `%0` is `[0, 2, 1, 3]`.\n    %0 = vector.interleave %a, %b : vector<2xi32> -> vector<4xi32>\n\n    // Examples showing allowed input and result types.\n    %1 = vector.interleave %c, %d : vector<f16> -> vector<2xf16>\n    %2 = vector.interleave %e, %f : vector<6x3xf32> -> vector<6x6xf32>\n    %3 = vector.interleave %g, %h : vector<[4]xi32> -> vector<[8]xi32>\n    %4 = vector.interleave %i, %j : vector<2x4x[2]xf64> -> vector<2x4x[4]xf64>\n    ```",
    "inputs": [
      { "name": "lhs", "type": "AnyVectorOfAnyRank" },
      { "name": "rhs", "type": "AnyVectorOfAnyRank" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyVectorOfNonZeroRank" }
    ],
    "assemblyFormat": "$lhs `,` $rhs  attr-dict `:` type($lhs) `->` type($result)"
  },
  {
    "name": "vector.load",
    "summary": "reads an n-D slice of memory into an n-D vector",
    "description": "The 'vector.load' operation reads an n-D slice of memory into an n-D\n    vector. It takes a 'base' memref, an index for each memref dimension and a\n    result vector type as arguments. It returns a value of the result vector\n    type. The 'base' memref and indices determine the start memory address from\n    which to read. Each index provides an offset for each memref dimension\n    based on the element type of the memref. The shape of the result vector\n    type determines the shape of the slice read from the start memory address.\n    The elements along each dimension of the slice are strided by the memref\n    strides. When loading more than 1 element, only unit strides are allowed\n    along the most minor memref dimension. These constraints guarantee that\n    elements read along the first dimension of the slice are contiguous in\n    memory.\n\n    The memref element type can be a scalar or a vector type. If the memref\n    element type is a scalar, it should match the element type of the result\n    vector. If the memref element type is vector, it should match the result\n    vector type.\n\n    Example: 0-D vector load on a scalar memref.\n    ```mlir\n    %result = vector.load %base[%i, %j] : memref<100x100xf32>, vector<f32>\n    ```\n\n    Example: 1-D vector load on a scalar memref.\n    ```mlir\n    %result = vector.load %base[%i, %j] : memref<100x100xf32>, vector<8xf32>\n    ```\n\n    Example: 1-D vector load on a vector memref.\n    ```mlir\n    %result = vector.load %memref[%i, %j] : memref<200x100xvector<8xf32>>, vector<8xf32>\n    ```\n\n    Example:  2-D vector load on a scalar memref.\n    ```mlir\n    %result = vector.load %memref[%i, %j] : memref<200x100xf32>, vector<4x8xf32>\n    ```\n\n    Example:  2-D vector load on a vector memref.\n    ```mlir\n    %result = vector.load %memref[%i, %j] : memref<200x100xvector<4x8xf32>>, vector<4x8xf32>\n    ```\n\n    Representation-wise, the 'vector.load' operation permits out-of-bounds\n    reads. Support and implementation of out-of-bounds vector loads is\n    target-specific. No assumptions should be made on the value of elements\n    loaded out of bounds. Not all targets may support out-of-bounds vector\n    loads.\n\n    Example:  Potential out-of-bound vector load.\n    ```mlir\n    %result = vector.load %memref[%index] : memref<?xf32>, vector<8xf32>\n    ```\n\n    Example:  Explicit out-of-bound vector load.\n    ```mlir\n    %result = vector.load %memref[%c0] : memref<7xf32>, vector<8xf32>\n    ```\n\n    An optional `alignment` attribute allows to specify the byte alignment of the\n    load operation. It must be a positive power of 2. The operation must access\n    memory at an address aligned to this boundary. Violating this requirement\n    triggers immediate undefined behavior.",
    "inputs": [
      { "name": "base", "type": "Arg" },
      { "name": "indices", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyVectorOfAnyRank" }
    ],
    "attributes": [
      { "name": "nontemporal", "type": "DefaultValuedOptionalAttr" },
      { "name": "alignment", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "$base `[` $indices `]` attr-dict `:` type($base) `,` type($result)"
  },
  {
    "name": "vector.mask",
    "summary": "Predicates a maskable vector operation",
    "description": "The `vector.mask` is a `MaskingOpInterface` operation that predicates the\n    execution of another operation. It takes an `i1` vector mask and an\n    optional passthru vector as arguments.\n\n    A implicitly `vector.yield`-terminated region encloses the operation to be\n    masked. Values used within the region are captured from above. Only one\n    *maskable* operation can be masked with a `vector.mask` operation at a time.\n    An operation is *maskable* if it implements the `MaskableOpInterface`. The\n    terminator yields all results from the maskable operation to the result of\n    this operation. No other values are allowed to be yielded.\n\n    An empty `vector.mask` operation is currently legal to enable optimizations\n    across the `vector.mask` region. However, this might change in the future\n    once vector transformations gain better support for `vector.mask`.\n    TODO: Consider making empty `vector.mask` illegal.\n\n    The vector mask argument holds a bit for each vector lane and determines\n    which vector lanes should execute the maskable operation and which ones\n    should not. The `vector.mask` operation returns the value produced by the\n    masked execution of the nested operation, if any. The masked-off lanes in\n    the result vector are taken from the corresponding lanes of the pass-thru\n    argument, if provided, or left unmodified, otherwise. At this point, 0-D\n    vectors are not supported by `vector.mask`. They may be supported in the\n    future.\n\n    The `vector.mask` operation does not prescribe how a maskable operation\n    should be masked or how a masked operation should be lowered. Masking\n    constraints and some semantic details are provided by each maskable\n    operation through the `MaskableOpInterface`. Lowering of masked operations\n    is implementation defined. For instance, scalarizing the masked operation\n    or executing the operation for the masked-off lanes are valid lowerings as\n    long as the execution of masked-off lanes does not change the observable\n    behavior of the program.\n\n    Examples:\n\n    ```\n      %0 = vector.mask %mask { vector.reduction <add>, %a : vector<8xi32> into i32 } : vector<8xi1> -> i32\n    ```\n\n    ```\n      %0 = vector.mask %mask, %passthru { arith.divsi %a, %b : vector<8xi32> } : vector<8xi1> -> vector<8xi32>\n    ```\n\n    ```\n      vector.mask %mask { vector.transfer_write %val, %t0[%idx] : vector<16xf32>, memref<?xf32> } : vector<16xi1>\n    ```\n\n    ```\n      vector.mask %mask { vector.transfer_write %val, %t0[%idx] : vector<16xf32>, tensor<?xf32> } : vector<16xi1> -> tensor<?xf32>\n    ```",
    "inputs": [
      { "name": "mask", "type": "VectorOfNonZeroRankOf" },
      { "name": "passthru", "type": "Optional" }
    ],
    "outputs": [
      { "name": "results", "type": "Variadic" }
    ]
  },
  {
    "name": "vector.maskedload",
    "summary": "loads elements from memory into a vector as defined by a mask vector",
    "description": "The masked load reads elements from memory into a vector as defined\n    by a base with indices and a mask vector. When the mask is set, the\n    element is read from memory. Otherwise, the corresponding element is taken\n    from a pass-through vector. Informally the semantics are:\n    ```\n    result[0] := if mask[0] then base[i + 0] else pass_thru[0]\n    result[1] := if mask[1] then base[i + 1] else pass_thru[1]\n    etc.\n    ```\n\n    If a mask bit is set and the corresponding index is out-of-bounds for the\n    given base, the behavior is undefined. If a mask bit is not set, the value\n    comes from the pass-through vector regardless of the index, and the index is\n    allowed to be out-of-bounds.\n\n    The masked load can be used directly where applicable, or can be used\n    during progressively lowering to bring other memory operations closer to\n    hardware ISA support for a masked load. The semantics of the operation\n    closely correspond to those of the `llvm.masked.load`\n    [intrinsic](https://llvm.org/docs/LangRef.html#llvm-masked-load-intrinsics).\n\n    Examples:\n\n    ```mlir\n    %0 = vector.maskedload %base[%i], %mask, %pass_thru\n       : memref<?xf32>, vector<8xi1>, vector<8xf32> into vector<8xf32>\n\n    %1 = vector.maskedload %base[%i, %j], %mask, %pass_thru\n       : memref<?x?xf32>, vector<16xi1>, vector<16xf32> into vector<16xf32>\n    ```\n\n    An optional `alignment` attribute allows to specify the byte alignment of the\n    load operation. It must be a positive power of 2. The operation must access\n    memory at an address aligned to this boundary. Violating this requirement\n    triggers immediate undefined behavior.",
    "assemblyFormat": "$base `[` $indices `]` `,` $mask `,` $pass_thru attr-dict `:` type($base) `,` type($mask) `,` type($pass_thru) `into` type($result)"
  },
  {
    "name": "vector.maskedstore",
    "summary": "stores elements from a vector into memory as defined by a mask vector",
    "description": "The masked store operation writes elements from a vector into memory\n    as defined by a base with indices and a mask vector. When the mask is\n    set, the corresponding element from the vector is written to memory. Otherwise,\n    no action is taken for the element. Informally the semantics are:\n    ```\n    if (mask[0]) base[i+0] = value[0]\n    if (mask[1]) base[i+1] = value[1]\n    etc.\n    ```\n\n    If a mask bit is set and the corresponding index is out-of-bounds for the\n    given base, the behavior is undefined. If a mask bit is not set, no value\n    is stored regardless of the index, and the index is allowed to be\n    out-of-bounds.\n\n    The masked store can be used directly where applicable, or can be used\n    during progressively lowering to bring other memory operations closer to\n    hardware ISA support for a masked store. The semantics of the operation\n    closely correspond to those of the `llvm.masked.store`\n    [intrinsic](https://llvm.org/docs/LangRef.html#llvm-masked-store-intrinsics).\n\n    Examples:\n\n    ```mlir\n    vector.maskedstore %base[%i], %mask, %value\n      : memref<?xf32>, vector<8xi1>, vector<8xf32>\n\n    vector.maskedstore %base[%i, %j], %mask, %value\n      : memref<?x?xf32>, vector<16xi1>, vector<16xf32>\n    ```\n\n    An optional `alignment` attribute allows to specify the byte alignment of the\n    store operation. It must be a positive power of 2. The operation must access\n    memory at an address aligned to this boundary. Violating this requirement\n    triggers immediate undefined behavior.",
    "assemblyFormat": "$base `[` $indices `]` `,` $mask `,` $valueToStore attr-dict `:` type($base) `,` type($mask) `,` type($valueToStore)"
  },
  {
    "name": "vector.multi_reduction",
    "summary": "Multi-dimensional reduction operation",
    "description": "Reduces an n-D vector into an (n-k)-D vector (or a scalar when k == n)\n    using the given operation: `add`/`mul`/`minsi`/`minui`/`maxsi`/`maxui`\n    /`and`/`or`/`xor` for integers, and `add`/`mul`/`minnumf`/`maxnumf`/`minimumf`\n    /`maximumf` for floats.\n    Takes an initial accumulator operand.\n\n    Example:\n\n    ```mlir\n    %1 = vector.multi_reduction <add>, %0, %acc0 [1, 3] :\n      vector<4x8x16x32xf32> to vector<4x16xf32>\n    %2 = vector.multi_reduction <add>, %1, %acc1 [0, 1] :\n      vector<4x16xf32> to f32\n    ```",
    "assemblyFormat": "$kind `,` $source `,` $acc attr-dict $reduction_dims `:` type($source) `to` type($dest)"
  },
  {
    "name": "vector.outerproduct",
    "summary": "vector outerproduct with optional fused add",
    "description": "Takes 2 1-D vectors and returns the 2-D vector containing the outer-product,\n    as illustrated below:\n    ```\n     outer |   [c, d]\n     ------+------------\n       [a, | [ [a*c, a*d],\n        b] |   [b*c, b*d] ]\n    ```\n    This operation also accepts a 1-D vector lhs and a scalar rhs. In this\n    case a simple AXPY operation is performed, which returns a 1-D vector.\n    ```\n        [a, b] * c = [a*c, b*c]\n    ```\n\n    An optional extra vector argument with the same shape as the output\n    vector may be specified in which case the operation returns the sum of\n    the outer-product and the extra vector. In this multiply-accumulate\n    scenario for floating-point arguments, the rounding mode is enforced\n    by guaranteeing that a fused-multiply add operation is emitted. When\n    lowered to the LLVMIR dialect, this form emits `llvm.intr.fma`, which\n    is guaranteed to lower to actual `fma` instructions on x86.\n\n    An optional kind attribute may be specified to be: `add`/`mul`/`minsi`\n    /`minui`/`maxsi`/`maxui`/`and`/`or`/`xor` for integers, and `add`/`mul`\n    /`minnumf`/`maxnumf`/`minimumf`/`maximumf` for floats. The default is\n    `add`.\n\n    Example:\n\n    ```\n    %2 = vector.outerproduct %0, %1: vector<4xf32>, vector<8xf32>\n    return %2: vector<4x8xf32>\n\n    %3 = vector.outerproduct %0, %1, %2:\n      vector<4xf32>, vector<8xf32>, vector<4x8xf32>\n    return %3: vector<4x8xf32>\n\n    %4 = vector.outerproduct %0, %1, %2 {kind = #vector.kind<maxnumf>}:\n      vector<4xf32>, vector<8xf32>, vector<4x8xf32>\n    return %3: vector<4x8xf32>\n\n    %6 = vector.outerproduct %4, %5: vector<10xf32>, f32\n    return %6: vector<10xf32>\n\n    ```"
  },
  {
    "name": "vector.print",
    "summary": "print operation (for testing and debugging)",
    "description": "Prints the source vector (or scalar) to stdout in a human-readable format\n    (for testing and debugging). No return value.\n\n    Example:\n\n    ```mlir\n    %v = arith.constant dense<0.0> : vector<4xf32>\n    vector.print %v : vector<4xf32>\n    ```\n\n    When lowered to LLVM, the vector print is decomposed into elementary\n    printing method calls that at runtime will yield:\n\n    ```\n    ( 0.0, 0.0, 0.0, 0.0 )\n    ```\n\n    This is printed to stdout via a small runtime support library, which only\n    needs to provide a few printing methods (single value for all data\n    types, opening/closing bracket, comma, newline).\n\n    By default `vector.print` adds a newline after the vector, but this can be\n    controlled by the `punctuation` attribute. For example, to print a comma\n    after instead do:\n\n    ```mlir\n    vector.print %v : vector<4xf32> punctuation <comma>\n    ```\n\n    Note that it is possible to use the punctuation attribute alone. The\n    following will print a single newline:\n\n    ```mlir\n    vector.print punctuation <newline>\n    ```\n\n    Additionally, to aid with debugging and testing `vector.print` can also\n    print constant strings:\n\n    ```mlir\n    vector.print str \"Hello, World!\"\n    ```",
    "assemblyFormat": "($source^ `:` type($source))?\n        oilist(\n            `str` $stringLiteral\n          | `punctuation` $punctuation)\n        attr-dict"
  },
  {
    "name": "vector.reduction",
    "summary": "reduction operation",
    "description": "Reduces an 1-D vector \"horizontally\" into a scalar using the given\n    operation: `add`/`mul`/`minsi`/`minui`/`maxsi`/`maxui`/`and`/`or`/`xor` for\n    integers, and `add`/`mul`/`minnumf`/`maxnumf`/`minimumf`/`maximumf` for\n    floats. Reductions also allow an optional fused accumulator.\n\n    Note that these operations are restricted to 1-D vectors to remain\n    close to the corresponding LLVM intrinsics:\n\n    http://llvm.org/docs/LangRef.html#vector-reduction-intrinsics\n\n    Example:\n\n    ```mlir\n    %1 = vector.reduction <add>, %0 : vector<16xf32> into f32\n\n    %3 = vector.reduction <xor>, %2 : vector<4xi32> into i32\n\n    %4 = vector.reduction <mul>, %0, %1 : vector<16xf32> into f32\n    ```",
    "assemblyFormat": "$kind `,` $vector (`,` $acc^)? (`fastmath` `` $fastmath^)? attr-dict `:` type($vector) `into` type($dest)"
  },
  {
    "name": "vector.scalable.extract",
    "summary": "extract subvector from scalable vector operation",
    "description": "Takes rank-1 source vector and a position `pos` within the source\n    vector, and extracts a subvector starting from that position.\n\n    The extraction position must be a multiple of the minimum size of the result\n    vector. For the operation to be well defined, the destination vector must\n    fit within the source vector from the specified position. Since the source\n    vector is scalable and its runtime length is unknown, the validity of the\n    operation can't be verified nor guaranteed at compile time.\n\n    Example:\n\n    ```mlir\n    %1 = vector.scalable.extract %0[8] : vector<4xf32> from vector<[8]xf32>\n    %3 = vector.scalable.extract %2[0] : vector<[4]xf32> from vector<[8]xf32>\n    ```\n\n    Invalid example:\n    ```mlir\n    %1 = vector.scalable.extract %0[5] : vector<4xf32> from vector<[16]xf32>\n    ```",
    "assemblyFormat": "$source `[` $pos `]` attr-dict `:` type($result) `from` type($source)"
  },
  {
    "name": "vector.scalable.insert",
    "summary": "insert subvector into scalable vector operation",
    "description": "This operations takes a rank-1 fixed-length or scalable subvector and\n    inserts it within the destination scalable vector starting from the\n    position specificed by `pos`. If the source vector is scalable, the\n    insertion position will be scaled by the runtime scaling factor of the\n    source subvector.\n\n    The insertion position must be a multiple of the minimum size of the source\n    vector. For the operation to be well defined, the source vector must fit in\n    the destination vector from the specified position. Since the destination\n    vector is scalable and its runtime length is unknown, the validity of the\n    operation can't be verified nor guaranteed at compile time.\n\n    Example:\n\n    ```mlir\n    %2 = vector.scalable.insert %0, %1[8] : vector<4xf32> into vector<[16]xf32>\n    %5 = vector.scalable.insert %3, %4[0] : vector<8xf32> into vector<[4]xf32>\n    %8 = vector.scalable.insert %6, %7[0] : vector<[4]xf32> into vector<[8]xf32>\n    ```\n\n    Invalid example:\n    ```mlir\n    %2 = vector.scalable.insert %0, %1[5] : vector<4xf32> into vector<[16]xf32>\n    ```",
    "assemblyFormat": "$valueToStore `,` $dest `[` $pos `]` attr-dict `:` type($valueToStore) `into` type($dest)"
  },
  {
    "name": "vector.scan",
    "summary": "Scan operation",
    "description": "Performs an inclusive/exclusive scan on an n-D vector along a single\n    dimension returning an n-D result vector using the given\n    operation (`add`/`mul`/`minsi`/`minui`/`maxsi`/`maxui`/`and`/`or`/`xor` for\n    integers, and `add`/`mul`/`minnumf`/`maxnumf`/`minimumf`/`maximumf` for\n    floats), and a specified value for the initial value. The operator returns\n    the result of scan as well as the result of the last reduction in the scan.\n\n    Example:\n\n    ```mlir\n    %1:2 = vector.scan <add>, %0, %acc {inclusive = false, reduction_dim = 1 : i64} :\n      vector<4x8x16x32xf32>, vector<4x16x32xf32>\n    ```",
    "assemblyFormat": "$kind `,` $source `,` $initial_value attr-dict `:` type($source) `,` type($initial_value)"
  },
  {
    "name": "vector.scatter",
    "summary": "scatters elements from a vector into memory as defined by an index vector\n    and a mask vector",
    "description": "The scatter operation stores elements from a n-D vector into memory as\n    defined by a base with indices and an additional n-D index vector, but\n    only if the corresponding bit in a n-D mask vector is set. Otherwise, no\n    action is taken for that element. Informally the semantics are:\n    ```\n    if (mask[0]) base[index[0]] = value[0]\n    if (mask[1]) base[index[1]] = value[1]\n    etc.\n    ```\n\n    If a mask bit is set and the corresponding index is out-of-bounds for the\n    given base, the behavior is undefined. If a mask bit is not set, no value\n    is stored regardless of the index, and the index is allowed to be\n    out-of-bounds.\n\n    If the index vector contains two or more duplicate indices, the behavior is\n    undefined. Underlying implementation may enforce strict sequential\n    semantics.\n    TODO: always enforce strict sequential semantics?\n\n    The scatter operation can be used directly where applicable, or can be used\n    during progressively lowering to bring other memory operations closer to\n    hardware ISA support for a scatter. The semantics of the operation closely\n    correspond to those of the `llvm.masked.scatter`\n    [intrinsic](https://llvm.org/docs/LangRef.html#llvm-masked-scatter-intrinsics).\n\n    An optional `alignment` attribute allows to specify the byte alignment of the\n    scatter operation. It must be a positive power of 2. The operation must access\n    memory at an address aligned to this boundary. Violating this requirement\n    triggers immediate undefined behavior.\n\n    Examples:\n\n    ```mlir\n    vector.scatter %base[%c0][%v], %mask, %value\n        : memref<?xf32>, vector<16xi32>, vector<16xi1>, vector<16xf32>\n\n    vector.scatter %base[%i, %j][%v], %mask, %value\n        : memref<16x16xf32>, vector<16xi32>, vector<16xi1>, vector<16xf32>\n    ```",
    "assemblyFormat": "$base `[` $offsets `]` `[` $indices `]` `,` $mask `,` $valueToStore attr-dict `:` type($base) `,` type($indices)  `,` type($mask) `,` type($valueToStore)",
    "category": "Tensor"
  },
  {
    "name": "vector.shape_cast",
    "summary": "shape_cast casts between vector shapes",
    "description": "Casts to a vector with the same number of elements, element type, and\n    number of scalable dimensions.\n\n    It is currently assumed that this operation does not require moving data,\n    and that it will be folded away before lowering vector operations.\n\n    There is an exception to the folding expectation when targeting\n    llvm.intr.matrix operations. We need a type conversion back and forth from a\n    2-D MLIR vector to a 1-D flattened LLVM vector.shape_cast lowering to LLVM\n    is supported in that particular case, for now.\n\n    Examples:\n\n    ```mlir\n    %1 = vector.shape_cast %0 : vector<4x3xf32> to vector<3x2x2xf32>\n\n    // with 2 scalable dimensions (number of which must be preserved).\n    %3 = vector.shape_cast %2 : vector<[2]x3x[4]xi8> to vector<3x[1]x[8]xi8>\n    ```",
    "assemblyFormat": "$source attr-dict `:` type($source) `to` type($result)"
  },
  {
    "name": "vector.shuffle",
    "summary": "shuffle operation",
    "description": "The shuffle operation constructs a permutation (or duplication) of elements\n    from two input vectors, returning a vector with the same element type as\n    the input and a length that is the same as the shuffle mask. The two input\n    vectors must have the same element type, same rank, and trailing dimension\n    sizes and shuffles their values in the leading dimension (which may differ\n    in size) according to the given mask. The legality rules are:\n    * the two operands must have the same element type as the result\n      - Either, the two operands and the result must have the same\n        rank and trailing dimension sizes, viz. given two k-D operands\n                v1 : <s_1 x s_2 x .. x s_k x type> and\n                v2 : <t_1 x t_2 x .. x t_k x type>\n        we have s_i = t_i for all 1 < i <= k\n      - Or, the two operands must be 0-D vectors and the result is a 1-D vector.\n    * the mask length equals the leading dimension size of the result\n    * numbering the input vector indices left to right across the operands, all\n      mask values must be within range, viz. given two k-D operands v1 and v2\n      above, all mask values are in the range [0,s_1+t_1). The value `-1`\n      represents a poison mask value, which specifies that the selected element\n      is poison.\n\n    Note, scalable vectors are not supported.\n\n    Example:\n\n    ```mlir\n    %0 = vector.shuffle %a, %b[0, 3]\n               : vector<2xf32>, vector<2xf32>       ; yields vector<2xf32>\n    %1 = vector.shuffle %c, %b[0, 1, 2]\n               : vector<2x16xf32>, vector<1x16xf32> ; yields vector<3x16xf32>\n    %2 = vector.shuffle %a, %b[3, 2, 1, 0]\n               : vector<2xf32>, vector<2xf32>       ; yields vector<4xf32>\n    %3 = vector.shuffle %a, %b[0, 1]\n               : vector<f32>, vector<f32>           ; yields vector<2xf32>\n    %4 = vector.shuffle %a, %b[0, 4, -1, -1, -1, -1]\n               : vector<4xf32>, vector<4xf32>       ; yields vector<6xf32>\n    ```",
    "assemblyFormat": "operands $mask attr-dict `:` type(operands)"
  },
  {
    "name": "vector.step",
    "summary": "A linear sequence of values from 0 to N",
    "description": "A `step` operation produces an index vector, i.e. a 1-D vector of values of\n    index type that represents a linear sequence from 0 to N-1, where N is the\n    number of elements in the `result` vector.\n\n    Supports fixed-width and scalable vectors.\n\n    Examples:\n\n    ```mlir\n    %0 = vector.step : vector<4xindex> ; [0, 1, 2, 3]\n    %1 = vector.step : vector<[4]xindex> ; [0, 1, .., <vscale * 4 - 1>]\n    ```",
    "outputs": [
      { "name": "result", "type": "VectorOfRankAndType" }
    ],
    "assemblyFormat": "attr-dict `:` type($result)"
  },
  {
    "name": "vector.store",
    "summary": "writes an n-D vector to an n-D slice of memory",
    "description": "The 'vector.store' operation writes an n-D vector to an n-D slice of memory.\n    It takes the vector value to be stored, a 'base' memref and an index for\n    each memref dimension. The 'base' memref and indices determine the start\n    memory address from which to write. Each index provides an offset for each\n    memref dimension based on the element type of the memref. The shape of the\n    vector value to store determines the shape of the slice written from the\n    start memory address. The elements along each dimension of the slice are\n    strided by the memref strides. When storing more than 1 element, only unit\n    strides are allowed along the most minor memref dimension. These constraints\n    guarantee that elements written along the first dimension of the slice are\n    contiguous in memory.\n\n    The memref element type can be a scalar or a vector type. If the memref\n    element type is a scalar, it should match the element type of the value\n    to store. If the memref element type is vector, it should match the type\n    of the value to store.\n\n    Example: 0-D vector store on a scalar memref.\n    ```mlir\n    vector.store %valueToStore, %memref[%i, %j] : memref<200x100xf32>, vector<f32>\n    ```\n\n    Example: 1-D vector store on a scalar memref.\n    ```mlir\n    vector.store %valueToStore, %memref[%i, %j] : memref<200x100xf32>, vector<8xf32>\n    ```\n\n    Example: 1-D vector store on a vector memref.\n    ```mlir\n    vector.store %valueToStore, %memref[%i, %j] : memref<200x100xvector<8xf32>>, vector<8xf32>\n    ```\n\n    Example:  2-D vector store on a scalar memref.\n    ```mlir\n    vector.store %valueToStore, %memref[%i, %j] : memref<200x100xf32>, vector<4x8xf32>\n    ```\n\n    Example:  2-D vector store on a vector memref.\n    ```mlir\n    vector.store %valueToStore, %memref[%i, %j] : memref<200x100xvector<4x8xf32>>, vector<4x8xf32>\n    ```\n\n    Representation-wise, the 'vector.store' operation permits out-of-bounds\n    writes. Support and implementation of out-of-bounds vector stores are\n    target-specific. No assumptions should be made on the memory written out of\n    bounds. Not all targets may support out-of-bounds vector stores.\n\n    Example:  Potential out-of-bounds vector store.\n    ```mlir\n    vector.store %valueToStore, %memref[%index] : memref<?xf32>, vector<8xf32>\n    ```\n\n    Example:  Explicit out-of-bounds vector store.\n    ```mlir\n    vector.store %valueToStore, %memref[%c0] : memref<7xf32>, vector<8xf32>\n    ```\n\n    An optional `alignment` attribute allows to specify the byte alignment of the\n    store operation. It must be a positive power of 2. The operation must access\n    memory at an address aligned to this boundary. Violating this requirement\n    triggers immediate undefined behavior.",
    "inputs": [
      { "name": "valueToStore", "type": "AnyVectorOfAnyRank" },
      { "name": "base", "type": "Arg" },
      { "name": "indices", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "nontemporal", "type": "DefaultValuedOptionalAttr" },
      { "name": "alignment", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "$valueToStore `,` $base `[` $indices `]` attr-dict `:` type($base) `,` type($valueToStore)"
  },
  {
    "name": "vector.to_elements",
    "summary": "operation that decomposes a vector into all its scalar elements",
    "description": "This operation decomposes all the scalar elements from a vector. The\n    decomposed scalar elements are returned in row-major order. The number of\n    scalar results must match the number of elements in the input vector type.\n    All the result elements have the same result type, which must match the\n    element type of the input vector. Scalable vectors are not supported.\n\n    Examples:\n\n    ```mlir\n    // Decompose a 0-D vector.\n    %0 = vector.to_elements %v0 : vector<f32>\n    // %0 = %v0[0]\n\n    // Decompose a 1-D vector.\n    %0:2 = vector.to_elements %v1 : vector<2xf32>\n    // %0#0 = %v1[0]\n    // %0#1 = %v1[1]\n\n    // Decompose a 2-D.\n    %0:6 = vector.to_elements %v2 : vector<2x3xf32>\n    // %0#0 = %v2[0, 0]\n    // %0#1 = %v2[0, 1]\n    // %0#2 = %v2[0, 2]\n    // %0#3 = %v2[1, 0]\n    // %0#4 = %v2[1, 1]\n    // %0#5 = %v2[1, 2]\n\n    // Decompose a 3-D vector.\n    %0:6 = vector.to_elements %v3 : vector<3x1x2xf32>\n    // %0#0 = %v3[0, 0, 0]\n    // %0#1 = %v3[0, 0, 1]\n    // %0#2 = %v3[1, 0, 0]\n    // %0#3 = %v3[1, 0, 1]\n    // %0#4 = %v3[2, 0, 0]\n    // %0#5 = %v3[2, 0, 1]\n    ```",
    "inputs": [
      { "name": "source", "type": "AnyVectorOfAnyRank" }
    ],
    "outputs": [
      { "name": "elements", "type": "Variadic" }
    ],
    "assemblyFormat": "$source attr-dict `:` type($source)"
  },
  {
    "name": "vector.transfer_read",
    "summary": "Reads a supervector from memory into an SSA vector value.",
    "description": "The `vector.transfer_read` op performs a read from a slice within a\n    [MemRef](../LangRef.md#memref-type) or a Ranked\n    [Tensor](../LangRef.md#tensor-type) supplied as its first operand\n    into a [vector](../LangRef.md#vector-type) of the same base elemental type.\n\n    A memref/tensor operand with vector element type, must have its vector\n    element type match a suffix (shape and element type) of the vector (e.g.\n    memref<3x2x6x4x3xf32>, vector<1x1x4x3xf32>).\n\n    The slice is further defined by a full-rank index within the MemRef/Tensor,\n    supplied as the operands `[1 .. 1 + rank(memref/tensor))` that defines the\n    starting point of the transfer (e.g. `%A[%i0, %i1, %i2]`).\n\n    The permutation_map [attribute](../LangRef.md#attributes) is an\n    [affine-map](Affine.md#affine-maps) which specifies the transposition on the\n    slice to match the vector shape. The permutation map may be implicit and\n    omitted from parsing and printing if it is the canonical minor identity map\n    (i.e. if it does not permute or broadcast any dimension).\n\n    The size of the slice is specified by the size of the vector, given as the\n    return type.\n\n    An SSA value `padding` of the same elemental type as the MemRef/Tensor is\n    provided to specify a fallback value in the case of out-of-bounds accesses\n    and/or masking.\n\n    An optional SSA value `mask` may be specified to mask out elements read from\n    the MemRef/Tensor. The `mask` type is an `i1` vector with a shape that\n    matches how elements are read from the MemRef/Tensor, *before* any\n    permutation or broadcasting. Elements whose corresponding mask element is\n    `0` are masked out and replaced with `padding`.\n\n    For every vector dimension, the boolean array attribute `in_bounds`\n    specifies if the transfer is guaranteed to be within the source bounds. If\n    set to \"false\", accesses (including the starting point) may run\n    out-of-bounds along the respective vector dimension as the index increases.\n    Non-vector dimensions *must* always be in-bounds. The `in_bounds` array\n    length has to be equal to the vector rank. This attribute has a default\n    value: `false` (i.e. \"out-of-bounds\"). When skipped in the textual IR, the\n    default value is assumed. Similarly, the OP printer will omit this\n    attribute when all dimensions are out-of-bounds (i.e. the default value is\n    used).\n\n    A `vector.transfer_read` can be lowered to a simple load if all dimensions\n    are specified to be within bounds and no `mask` was specified.\n\n    This operation is called 'read' by opposition to 'load' because the\n    super-vector granularity is generally not representable with a single\n    hardware register. A `vector.transfer_read` is thus a mid-level abstraction\n    that supports super-vectorization with non-effecting padding for full-tile\n    only operations.\n\n    More precisely, let's dive deeper into the permutation_map for the following\n    MLIR:\n\n    ```mlir\n    vector.transfer_read %A[%expr1, %expr2, %expr3, %expr4]\n      { permutation_map : (d0,d1,d2,d3) -> (d2,0,d0) } :\n      memref<?x?x?x?xf32>, vector<3x4x5xf32>\n    ```\n\n    This operation always reads a slice starting at `%A[%expr1, %expr2, %expr3,\n    %expr4]`. The size of the slice can be inferred from the resulting vector\n    shape and walking back through the permutation map: 3 along d2 and 5 along\n    d0, so the slice is: `%A[%expr1 : %expr1 + 5, %expr2, %expr3:%expr3 + 3, %expr4]`\n\n    That slice needs to be read into a `vector<3x4x5xf32>`. Since the\n    permutation map is not full rank, there must be a broadcast along vector\n    dimension `1`.\n\n    A notional lowering of vector.transfer_read could generate code resembling:\n\n    ```mlir\n    // %expr1, %expr2, %expr3, %expr4 defined before this point\n    // alloc a temporary buffer for performing the \"gather\" of the slice.\n    %tmp = memref.alloc() : memref<vector<3x4x5xf32>>\n    for %i = 0 to 3 {\n      affine.for %j = 0 to 4 {\n        affine.for %k = 0 to 5 {\n          // Note that this load does not involve %j.\n          %a = load %A[%expr1 + %k, %expr2, %expr3 + %i, %expr4] : memref<?x?x?x?xf32>\n          // Update the temporary gathered slice with the individual element\n          %slice = memref.load %tmp : memref<vector<3x4x5xf32>> -> vector<3x4x5xf32>\n          %updated = vector.insert %a, %slice[%i, %j, %k] : f32 into vector<3x4x5xf32>\n          memref.store %updated, %tmp : memref<vector<3x4x5xf32>>\n    }}}\n    // At this point we gathered the elements from the original\n    // memref into the desired vector layout, stored in the `%tmp` allocation.\n    %vec = memref.load %tmp : memref<vector<3x4x5xf32>> -> vector<3x4x5xf32>\n    ```\n\n    On a GPU one could then map `i`, `j`, `k` to blocks and threads. Notice that\n    the temporary storage footprint could conceptually be only `3 * 5` values but\n    `3 * 4 * 5` values are actually transferred between `%A` and `%tmp`.\n\n    Alternatively, if a notional vector broadcast operation were available, we\n    could avoid the loop on `%j` and the lowered code would resemble:\n\n    ```mlir\n    // %expr1, %expr2, %expr3, %expr4 defined before this point\n    %tmp = memref.alloc() : memref<vector<3x4x5xf32>>\n    for %i = 0 to 3 {\n      affine.for %k = 0 to 5 {\n        %a = load %A[%expr1 + %k, %expr2, %expr3 + %i, %expr4] : memref<?x?x?x?xf32>\n        %slice = memref.load %tmp : memref<vector<3x4x5xf32>> -> vector<3x4x5xf32>\n        // Here we only store to the first element in dimension one\n        %updated = vector.insert %a, %slice[%i, 0, %k] : f32 into vector<3x4x5xf32>\n        memref.store %updated, %tmp : memref<vector<3x4x5xf32>>\n    }}\n    // At this point we gathered the elements from the original\n    // memref into the desired vector layout, stored in the `%tmp` allocation.\n    // However we haven't replicated them alongside the first dimension, we need\n    // to broadcast now.\n    %partialVec = load %tmp : memref<vector<3x4x5xf32>> -> vector<3x4x5xf32>\n    %vec = broadcast %tmpvec, 1 : vector<3x4x5xf32>\n    ```\n\n    where `broadcast` broadcasts from element 0 to all others along the\n    specified dimension. This time, the number of loaded element is `3 * 5`\n    values.\n    An additional `1` broadcast is required. On a GPU this broadcast could be\n    implemented using a warp-shuffle if loop `j` were mapped to `threadIdx.x`.\n\n    Syntax\n    ```\n    operation ::= ssa-id `=` `vector.transfer_read` ssa-use-list\n      `{` attribute-entry `} :` memref-type `,` vector-type\n    ```\n\n    Example:\n\n    ```mlir\n    // Read the slice `%A[%i0, %i1:%i1+256, %i2:%i2+32]` into vector<32x256xf32>\n    // and pad with %f0 to handle the boundary case:\n    %f0 = arith.constant 0.0f : f32\n    affine.for %i0 = 0 to %0 {\n      affine.for %i1 = 0 to %1 step 256 {\n        affine.for %i2 = 0 to %2 step 32 {\n          %v = vector.transfer_read %A[%i0, %i1, %i2], (%f0)\n               {permutation_map: (d0, d1, d2) -> (d2, d1)} :\n               memref<?x?x?xf32>, vector<32x256xf32>\n    }}}\n\n    // or equivalently (rewrite with vector.transpose)\n    %f0 = arith.constant 0.0f : f32\n    affine.for %i0 = 0 to %0 {\n      affine.for %i1 = 0 to %1 step 256 {\n        affine.for %i2 = 0 to %2 step 32 {\n          %v0 = vector.transfer_read %A[%i0, %i1, %i2], (%f0)\n               {permutation_map: (d0, d1, d2) -> (d1, d2)} :\n               memref<?x?x?xf32>, vector<256x32xf32>\n          %v = vector.transpose %v0, [1, 0] :\n              vector<256x32xf32> to vector<32x256f32>\n    }}}\n\n    // Read the slice `%A[%i0, %i1]` (i.e. the element `%A[%i0, %i1]`) into\n    // vector<128xf32>. The underlying implementation will require a 1-D vector\n    // broadcast:\n    affine.for %i0 = 0 to %0 {\n      affine.for %i1 = 0 to %1 {\n        %3 = vector.transfer_read %A[%i0, %i1]\n             {permutation_map: (d0, d1) -> (0)} :\n             memref<?x?xf32>, vector<128xf32>\n      }\n    }\n\n    // Read from a memref with vector element type.\n    %4 = vector.transfer_read %arg1[%c3, %c3], %vf0\n      {permutation_map = (d0, d1)->(d0, d1)}\n        : memref<?x?xvector<4x3xf32>>, vector<1x1x4x3xf32>\n\n    // Read from a tensor with vector element type.\n    %4 = vector.transfer_read %arg1[%c3, %c3], %vf0\n      {permutation_map = (d0, d1)->(d0, d1)}\n        : tensor<?x?xvector<4x3xf32>>, vector<1x1x4x3xf32>\n\n    // Special encoding for 0-d transfer with 0-d tensor/memref, vector shape\n    // {1} and permutation_map () -> (0).\n    %0 = vector.transfer_read %arg0[], %f0 {permutation_map = affine_map<()->(0)>} :\n      tensor<f32>, vector<1xf32>\n    ```"
  },
  {
    "name": "vector.transfer_write",
    "summary": "The vector.transfer_write op writes a supervector to memory.",
    "description": "The `vector.transfer_write` op performs a write from a\n    [vector](../LangRef.md#vector-type), supplied as its first operand, into a\n    slice within a [MemRef](../LangRef.md#memref-type) or a Ranked\n    [Tensor](../LangRef.md#tensor-type) of the same base elemental type,\n    supplied as its second operand.\n\n    A vector memref/tensor operand must have its vector element type match a\n    suffix (shape and element type) of the vector (e.g. memref<3x2x6x4x3xf32>,\n    vector<1x1x4x3xf32>). If the operand is a tensor, the operation returns a\n    new tensor of the same type.\n\n    The slice is further defined by a full-rank index within the MemRef/Tensor,\n    supplied as the operands `[2 .. 2 + rank(memref/tensor))` that defines the\n    starting point of the transfer (e.g. `%A[%i0, %i1, %i2, %i3]`).\n\n    The permutation_map [attribute](../LangRef.md#attributes) is an\n    [affine-map](Affine.md#affine-maps) which specifies the transposition on the\n    slice to match the vector shape. The permutation map may be implicit and\n    omitted from parsing and printing if it is the canonical minor identity map\n    (i.e. if it does not permute any dimension). In contrast to `transfer_read`,\n    write ops cannot have broadcast dimensions.\n\n    The size of the slice is specified by the size of the vector.\n\n    An optional SSA value `mask` may be specified to mask out elements written\n    to the MemRef/Tensor. The `mask` type is an `i1` vector with a shape that\n    matches how elements are written into the MemRef/Tensor, *after* applying\n    any permutation. Elements whose corresponding mask element is `0` are\n    masked out.\n\n    For every vector dimension, the boolean array attribute `in_bounds`\n    specifies if the transfer is guaranteed to be within the source bounds. If\n    set to \"false\", accesses (including the starting point) may run\n    out-of-bounds along the respective vector dimension as the index increases.\n    Non-vector dimensions *must* always be in-bounds. The `in_bounds` array\n    length has to be equal to the vector rank. This attribute has a default\n    value: `false` (i.e. \"out-of-bounds\"). When skipped in the textual IR, the\n    default value is assumed. Similarly, the OP printer will omit this\n    attribute when all dimensions are out-of-bounds (i.e. the default value is\n    used).\n\n     A `vector.transfer_write` can be lowered to a simple store if all\n     dimensions are specified to be within bounds and no `mask` was specified.\n\n    This operation is called 'write' by opposition to 'store' because the\n    super-vector granularity is generally not representable with a single\n    hardware register. A `vector.transfer_write` is thus a\n    mid-level abstraction that supports super-vectorization with non-effecting\n    padding for full-tile-only code. It is the responsibility of\n    `vector.transfer_write`'s implementation to ensure the memory writes are\n    valid. Different lowerings may be pertinent depending on the hardware\n    support.\n\n    Example:\n\n    ```mlir\n    // write vector<16x32x64xf32> into the slice\n    //   `%A[%i0, %i1:%i1+32, %i2:%i2+64, %i3:%i3+16]`:\n    for %i0 = 0 to %0 {\n      affine.for %i1 = 0 to %1 step 32 {\n        affine.for %i2 = 0 to %2 step 64 {\n          affine.for %i3 = 0 to %3 step 16 {\n            %val = `ssa-value` : vector<16x32x64xf32>\n            vector.transfer_write %val, %A[%i0, %i1, %i2, %i3]\n              {permutation_map: (d0, d1, d2, d3) -> (d3, d1, d2)} :\n              vector<16x32x64xf32>, memref<?x?x?x?xf32>\n    }}}}\n\n    // or equivalently (rewrite with vector.transpose)\n    for %i0 = 0 to %0 {\n      affine.for %i1 = 0 to %1 step 32 {\n        affine.for %i2 = 0 to %2 step 64 {\n          affine.for %i3 = 0 to %3 step 16 {\n            %val = `ssa-value` : vector<16x32x64xf32>\n            %valt = vector.transpose %val, [1, 2, 0] :\n                  vector<16x32x64xf32> -> vector<32x64x16xf32>\n            vector.transfer_write %valt, %A[%i0, %i1, %i2, %i3]\n              {permutation_map: (d0, d1, d2, d3) -> (d1, d2, d3)} :\n              vector<32x64x16xf32>, memref<?x?x?x?xf32>\n    }}}}\n\n    // write to a memref with vector element type.\n    vector.transfer_write %4, %arg1[%c3, %c3]\n      {permutation_map = (d0, d1)->(d0, d1)}\n        : vector<1x1x4x3xf32>, memref<?x?xvector<4x3xf32>>\n\n    // return a tensor where the vector is inserted into the source tensor.\n    %5 = vector.transfer_write %4, %arg1[%c3, %c3]\n      {permutation_map = (d0, d1)->(d0, d1)}\n        : vector<1x1x4x3xf32>, tensor<?x?xvector<4x3xf32>>\n\n    // Special encoding for 0-d transfer with 0-d tensor/memref, vector shape\n    // {1} and permutation_map () -> (0).\n    %1 = vector.transfer_write %0, %arg0[] {permutation_map = affine_map<()->(0)>} :\n      vector<1xf32>, tensor<f32>\n    ```"
  },
  {
    "name": "vector.transpose",
    "summary": "vector transpose operation",
    "description": "Takes a n-D vector and returns the transposed n-D vector defined by\n    the permutation of ranks in the n-sized integer array attribute (in case\n    of 0-D vectors the array attribute must be empty).\n\n    In the operation\n\n    ```mlir\n    %1 = vector.transpose %0, [i_1, .., i_n]\n      : vector<d_1 x .. x d_n x f32>\n      to vector<d_trans[0] x .. x d_trans[n-1] x f32>\n    ```\n\n    the `permutation` array [i_1, .., i_n] must be a permutation of [0, .., n-1].\n\n    Example:\n\n    ```mlir\n    %1 = vector.transpose %0, [1, 0] : vector<2x3xf32> to vector<3x2xf32>\n\n     [ [a, b, c],       [ [a, d],\n       [d, e, f] ]  ->    [b, e],\n                          [c, f] ]\n    ```",
    "inputs": [
      { "name": "vector", "type": "AnyVectorOfAnyRank" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyVectorOfAnyRank" }
    ],
    "attributes": [
      { "name": "permutation", "type": "DenseI64ArrayAttr" }
    ],
    "assemblyFormat": "$vector `,` $permutation attr-dict `:` type($vector) `to` type($result)",
    "category": "Transform"
  },
  {
    "name": "vector.type_cast",
    "summary": "type_cast op converts a scalar memref to a vector memref",
    "description": "Performs a conversion from a memref with scalar element to a memref with a\n    *single* vector element, copying the shape of the memref to the vector. This\n    is the minimal viable operation that is required to makeke\n    super-vectorization operational. It can be seen as a special case of the\n    `view` operation but scoped in the super-vectorization context.\n\n    Example:\n\n    ```mlir\n    %A  = memref.alloc() : memref<5x4x3xf32>\n    %VA = vector.type_cast %A : memref<5x4x3xf32> to memref<vector<5x4x3xf32>>\n    ```",
    "assemblyFormat": "$memref attr-dict `:` type($memref) `to` type($result)"
  },
  {
    "name": "vector.vscale",
    "summary": "Load vector scale size",
    "description": "The `vscale` op returns the scale of the scalable vectors, a positive\n    integer value that is constant at runtime but unknown at compile-time.\n    The scale of the vector indicates the multiplicity of the vectors and\n    vector operations. For example, a `vector<[4]xi32>` is equivalent to\n    `vscale` consecutive `vector<4xi32>`; and an operation on a\n    `vector<[4]xi32>` is equivalent to performing that operation `vscale`\n    times, once on each `<4xi32>` segment of the scalable vector. The `vscale`\n    op can be used to calculate the step in vector-length agnostic (VLA) loops.\n    Right now we only support one contiguous set of scalable dimensions, all of\n    them grouped and scaled with the value returned by 'vscale'.",
    "outputs": [
      { "name": "res", "type": "Index" }
    ],
    "assemblyFormat": "attr-dict"
  },
  {
    "name": "vector.yield",
    "summary": "Terminates and yields values from vector regions.",
    "description": "\"vector.yield\" yields an SSA value from the Vector dialect op region and\n    terminates the regions. The semantics of how the values are yielded is\n    defined by the parent operation.\n    If \"vector.yield\" has any operands, the operands must correspond to the\n    parent operation's results.\n    If the parent operation defines no value the vector.yield may be omitted\n    when printing the region.",
    "inputs": [
      { "name": "operands", "type": "Variadic" }
    ],
    "assemblyFormat": "attr-dict ($operands^ `:` type($operands))?"
  }
]