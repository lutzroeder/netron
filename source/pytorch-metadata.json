[
  {
    "name": "__torch__.torch.classes.rnn.CellParamsBase",
    "inputs": [
      { "name": "type", "type": "string" },
      { "name": "tensors", "type": "Tensor[]" },
      { "name": "doubles", "type": "float64[]" },
      { "name": "longs", "type": "int64[]" },
      { "name": "packed_params", "type": "__torch__.torch.classes.quantized.LinearPackedParamsBase[]" }
    ]
  },
  {
    "name": "__torch__.torch.classes.xnnpack.Conv2dOpContext",
    "inputs": [
      { "name": "weight", "type": "Tensor" },
      { "name": "bias", "type": "Tensor", "optional": true },
      { "name": "stride", "type": "int64[]" },
      { "name": "padding", "type": "int64[]" },
      { "name": "dilation", "type": "int64[]" },
      { "name": "groups", "type": "int64" },
      { "name": "output_min", "type": "int64[]", "optional": true },
      { "name": "output_max", "type": "int64[]", "optional": true }
    ]
  },
  {
    "name": "__torch__.torch.classes.xnnpack.LinearOpContext",
    "inputs": [
      { "name": "weight", "type": "Tensor" },
      { "name": "bias", "type": "Tensor", "optional": true },
      { "name": "output_min", "type": "int64[]", "optional": true },
      { "name": "output_max", "type": "int64[]", "optional": true }
    ]
  },
  {
    "name": "_caffe2::BBoxTransform(Tensor rois, Tensor deltas, Tensor im_info, float[] weights, bool apply_scale, bool rotated, bool angle_bound_on, int angle_bound_lo, int angle_bound_hi, float clip_angle_thresh, bool legacy_plus_one, Tensor[]? _caffe2_preallocated_outputs=None) -> (Tensor output_0, Tensor output_1)"
  },
  {
    "name": "_caffe2::BatchPermutation(Tensor X, Tensor indices, Tensor[]? _caffe2_preallocated_outputs=None) -> Tensor"
  },
  {
    "name": "_caffe2::BoxWithNMSLimit(Tensor scores, Tensor boxes, Tensor batch_splits, float score_thresh, float nms, int detections_per_im, bool soft_nms_enabled, str soft_nms_method, float soft_nms_sigma, float soft_nms_min_score_thres, bool rotated, bool cls_agnostic_bbox_reg, bool input_boxes_include_bg_cls, bool output_classes_include_bg_cls, bool legacy_plus_one, Tensor[]? _caffe2_preallocated_outputs=None) -> (Tensor scores, Tensor boxes, Tensor classes, Tensor batch_splits, Tensor keeps, Tensor keeps_size)"
  },
  {
    "name": "_caffe2::CollectAndDistributeFpnRpnProposals(Tensor[] input_list, int roi_canonical_scale, int roi_canonical_level, int roi_max_level, int roi_min_level, int rpn_max_level, int rpn_min_level, int rpn_post_nms_topN, bool legacy_plus_one, Tensor[]? _caffe2_preallocated_outputs=None) -> (Tensor rois, Tensor rois_fpn2, Tensor rois_fpn3, Tensor rois_fpn4, Tensor rois_fpn5, Tensor rois_idx_restore_int32)"
  },
  {
    "name": "_caffe2::CollectRpnProposals(Tensor[] input_list, int rpn_max_level, int rpn_min_level, int rpn_post_nms_topN, Tensor[]? _caffe2_preallocated_outputs=None) -> (Tensor rois)"
  },
  {
    "name": "_caffe2::CopyCPUToGPU(Tensor input, Tensor[]? _caffe2_preallocated_outputs=None) -> Tensor"
  },
  {
    "name": "_caffe2::CopyGPUToCPU(Tensor input, Tensor[]? _caffe2_preallocated_outputs=None) -> Tensor"
  },
  {
    "name": "_caffe2::DistributeFpnProposals(Tensor rois, int roi_canonical_scale, int roi_canonical_level, int roi_max_level, int roi_min_level, bool legacy_plus_one, Tensor[]? _caffe2_preallocated_outputs=None) -> (Tensor rois_fpn2, Tensor rois_fpn3, Tensor rois_fpn4, Tensor rois_fpn5, Tensor rois_idx_restore_int32)"
  },
  {
    "name": "_caffe2::GenerateProposals(Tensor scores, Tensor bbox_deltas, Tensor im_info, Tensor anchors, float spatial_scale, int pre_nms_topN, int post_nms_topN, float nms_thresh, float min_size, bool angle_bound_on, int angle_bound_lo, int angle_bound_hi, float clip_angle_thresh, bool legacy_plus_one, Tensor[]? _caffe2_preallocated_outputs=None) -> (Tensor output_0, Tensor output_1)"
  },
  {
    "name": "_caffe2::RoIAlign(Tensor features, Tensor rois, str order, float spatial_scale, int pooled_h, int pooled_w, int sampling_ratio, bool aligned, Tensor[]? _caffe2_preallocated_outputs=None) -> Tensor"
  },
  {
    "name": "aten::Bool.Tensor(Tensor a) -> bool"
  },
  {
    "name": "aten::Bool.float(float a) -> bool"
  },
  {
    "name": "aten::Bool.int(int a) -> bool"
  },
  {
    "name": "aten::Complex.Scalar(Scalar a) -> complex"
  },
  {
    "name": "aten::Complex.Tensor_Tensor(Tensor a, Tensor b) -> complex"
  },
  {
    "name": "aten::Complex.Tensor_bool(Tensor x, bool y) -> complex"
  },
  {
    "name": "aten::Complex.Tensor_float(Tensor x, float y) -> complex"
  },
  {
    "name": "aten::Complex.Tensor_int(Tensor x, int y) -> complex"
  },
  {
    "name": "aten::Complex.bool_Tensor(bool x, Tensor y) -> complex"
  },
  {
    "name": "aten::Complex.bool_bool(bool x, bool y) -> complex"
  },
  {
    "name": "aten::Complex.bool_float(bool x, float y) -> complex"
  },
  {
    "name": "aten::Complex.bool_int(bool x, int y) -> complex"
  },
  {
    "name": "aten::Complex.float_Tensor(float x, Tensor y) -> complex"
  },
  {
    "name": "aten::Complex.float_bool(float x, bool y) -> complex"
  },
  {
    "name": "aten::Complex.float_float(float x, float y) -> complex"
  },
  {
    "name": "aten::Complex.float_int(float x, int y) -> complex"
  },
  {
    "name": "aten::Complex.int_Tensor(int x, Tensor y) -> complex"
  },
  {
    "name": "aten::Complex.int_bool(int x, bool y) -> complex"
  },
  {
    "name": "aten::Complex.int_float(int x, float y) -> complex"
  },
  {
    "name": "aten::Complex.int_int(int x, int y) -> complex"
  },
  {
    "name": "aten::ComplexImplicit(Tensor a) -> complex"
  },
  {
    "name": "aten::Float.Scalar(Scalar a) -> float"
  },
  {
    "name": "aten::Float.Tensor(Tensor a) -> float"
  },
  {
    "name": "aten::Float.bool(bool a) -> float"
  },
  {
    "name": "aten::Float.int(int a) -> float"
  },
  {
    "name": "aten::Float.str(str a) -> float"
  },
  {
    "name": "aten::FloatImplicit(Tensor a) -> float"
  },
  {
    "name": "aten::Int.Scalar(Scalar a) -> int"
  },
  {
    "name": "aten::Int.Tensor(Tensor a) -> int"
  },
  {
    "name": "aten::Int.bool(bool a) -> int"
  },
  {
    "name": "aten::Int.float(float a) -> int"
  },
  {
    "name": "aten::Int.str(str a) -> int"
  },
  {
    "name": "aten::IntImplicit(Tensor a) -> int"
  },
  {
    "name": "aten::ScalarImplicit(Tensor a) -> Scalar"
  },
  {
    "name": "aten::__and__.Scalar(Tensor self, Scalar other) -> Tensor"
  },
  {
    "name": "aten::__and__.Tensor(Tensor self, Tensor other) -> Tensor"
  },
  {
    "name": "aten::__and__.bool(bool a, bool b) -> bool"
  },
  {
    "name": "aten::__and__.int(int a, int b) -> int"
  },
  {
    "name": "aten::__contains__.Tensor(Dict(Tensor, t) dict, Tensor key) -> bool"
  },
  {
    "name": "aten::__contains__.bool(Dict(bool, t) dict, bool key) -> bool"
  },
  {
    "name": "aten::__contains__.complex(Dict(complex, t) dict, complex key) -> bool"
  },
  {
    "name": "aten::__contains__.float(Dict(float, t) dict, float key) -> bool"
  },
  {
    "name": "aten::__contains__.float_list(float[] l, float item) -> bool"
  },
  {
    "name": "aten::__contains__.int(Dict(int, t) dict, int key) -> bool"
  },
  {
    "name": "aten::__contains__.int_list(int[] l, int item) -> bool"
  },
  {
    "name": "aten::__contains__.str(Dict(str, t) dict, str key) -> bool"
  },
  {
    "name": "aten::__contains__.str_list(str[] l, str item) -> bool"
  },
  {
    "name": "aten::__getitem__.Dict_Tensor(Dict(Tensor, t) self, Tensor key) -> t(*)"
  },
  {
    "name": "aten::__getitem__.Dict_bool(Dict(bool, t) self, bool key) -> t(*)"
  },
  {
    "name": "aten::__getitem__.Dict_complex(Dict(complex, t) self, complex key) -> t(*)"
  },
  {
    "name": "aten::__getitem__.Dict_float(Dict(float, t) self, float key) -> t(*)"
  },
  {
    "name": "aten::__getitem__.Dict_int(Dict(int, t) self, int key) -> t(*)"
  },
  {
    "name": "aten::__getitem__.Dict_str(Dict(str, t) self, str key) -> t(*)"
  },
  {
    "name": "aten::__getitem__.str(str s, int index) -> str"
  },
  {
    "name": "aten::__getitem__.t(t[](a) list, int idx) -> t(*)"
  },
  {
    "name": "aten::__iand__.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)"
  },
  {
    "name": "aten::__iand__.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)"
  },
  {
    "name": "aten::__ilshift__.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)"
  },
  {
    "name": "aten::__ilshift__.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)"
  },
  {
    "name": "aten::__interpolate(Tensor input, int? size=None, float? scale_factor=None, str mode=\"nearest\", bool? align_corners=None, bool? recompute_scale_factor=None, bool antialias=False) -> Tensor"
  },
  {
    "name": "aten::__interpolate.scale_list(Tensor input, int? size=None, float[]? scale_factor=None, str mode=\"nearest\", bool? align_corners=None, bool? recompute_scale_factor=None, bool antialias=False) -> Tensor"
  },
  {
    "name": "aten::__interpolate.size_list(Tensor input, int[]? size=None, float? scale_factor=None, str mode=\"nearest\", bool? align_corners=None, bool? recompute_scale_factor=None, bool antialias=False) -> Tensor"
  },
  {
    "name": "aten::__interpolate.size_list_scale_list(Tensor input, int[]? size=None, float[]? scale_factor=None, str mode=\"nearest\", bool? align_corners=None, bool? recompute_scale_factor=None, bool antialias=False) -> Tensor"
  },
  {
    "name": "aten::__ior__.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)"
  },
  {
    "name": "aten::__ior__.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)"
  },
  {
    "name": "aten::__irshift__.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)"
  },
  {
    "name": "aten::__irshift__.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)"
  },
  {
    "name": "aten::__is__(t1 self, t2 obj) -> bool"
  },
  {
    "name": "aten::__isnot__(t1 self, t2 obj) -> bool"
  },
  {
    "name": "aten::__ixor__.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)"
  },
  {
    "name": "aten::__ixor__.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)"
  },
  {
    "name": "aten::__lshift__.Scalar(Tensor self, Scalar other) -> Tensor"
  },
  {
    "name": "aten::__lshift__.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::__lshift__.Tensor(Tensor self, Tensor other) -> Tensor"
  },
  {
    "name": "aten::__lshift__.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::__lshift__.int(int a, int b) -> int"
  },
  {
    "name": "aten::__not__(bool self) -> bool"
  },
  {
    "name": "aten::__or__.Scalar(Tensor self, Scalar other) -> Tensor"
  },
  {
    "name": "aten::__or__.Tensor(Tensor self, Tensor other) -> Tensor"
  },
  {
    "name": "aten::__or__.bool(bool a, bool b) -> bool"
  },
  {
    "name": "aten::__or__.int(int a, int b) -> int"
  },
  {
    "name": "aten::__rshift__.Scalar(Tensor self, Scalar other) -> Tensor"
  },
  {
    "name": "aten::__rshift__.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::__rshift__.Tensor(Tensor self, Tensor other) -> Tensor"
  },
  {
    "name": "aten::__rshift__.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::__rshift__.int(int a, int b) -> int"
  },
  {
    "name": "aten::__upsample(Tensor input, int? size=None, int? scale_factor=None, str mode=\"nearest\", bool? align_corners=None) -> Tensor",
    "category": "Layer"
  },
  {
    "name": "aten::__upsample.size_list(Tensor input, int[]? size=None, int? scale_factor=None, str mode=\"nearest\", bool? align_corners=None) -> Tensor",
    "category": "Layer"
  },
  {
    "name": "aten::__upsample_bilinear(Tensor input, int? size=None, int? scale_factor=None) -> Tensor"
  },
  {
    "name": "aten::__upsample_bilinear.scale_list(Tensor input, int? size=None, int[]? scale_factor=None) -> Tensor"
  },
  {
    "name": "aten::__upsample_bilinear.size_list(Tensor input, int[]? size=None, int? scale_factor=None) -> Tensor"
  },
  {
    "name": "aten::__upsample_bilinear.size_list_scale_list(Tensor input, int[]? size=None, int[]? scale_factor=None) -> Tensor"
  },
  {
    "name": "aten::__xor__.Scalar(Tensor self, Scalar other) -> Tensor"
  },
  {
    "name": "aten::__xor__.Tensor(Tensor self, Tensor other) -> Tensor"
  },
  {
    "name": "aten::__xor__.bool(bool a, bool b) -> bool"
  },
  {
    "name": "aten::__xor__.int(int a, int b) -> int"
  },
  {
    "name": "aten::_adaptive_avg_pool2d(Tensor self, SymInt[2] output_size) -> Tensor"
  },
  {
    "name": "aten::_adaptive_avg_pool2d.out(Tensor self, SymInt[2] output_size, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::_add_relu.Scalar(Tensor self, Scalar other, Scalar alpha=1) -> Tensor"
  },
  {
    "name": "aten::_add_relu.Scalar_out(Tensor self, Scalar other, Scalar alpha=1, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::_add_relu.Tensor(Tensor self, Tensor other, *, Scalar alpha=1) -> Tensor"
  },
  {
    "name": "aten::_add_relu.out(Tensor self, Tensor other, *, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::_add_relu_.Scalar(Tensor(a!) self, Scalar other, Scalar alpha=1) -> Tensor(a!)"
  },
  {
    "name": "aten::_add_relu_.Tensor(Tensor(a!) self, Tensor other, *, Scalar alpha=1) -> Tensor(a!)"
  },
  {
    "name": "aten::_aminmax(Tensor self) -> (Tensor, Tensor)"
  },
  {
    "name": "aten::_aminmax.dim(Tensor self, int dim, bool keepdim=False) -> (Tensor, Tensor)"
  },
  {
    "name": "aten::_aminmax.dim_out(Tensor self, int dim, bool keepdim=False, *, Tensor(a!) out0, Tensor(b!) out1) -> (Tensor(a!), Tensor(b!))"
  },
  {
    "name": "aten::_aminmax.out(Tensor self, *, Tensor(a!) out0, Tensor(b!) out1) -> (Tensor(a!), Tensor(b!))"
  },
  {
    "name": "aten::_autocast_to_reduced_precision(Tensor(a) self, bool cuda_enabled, bool cpu_enabled, ScalarType cuda_dtype, ScalarType cpu_dtype) -> Tensor(a)"
  },
  {
    "name": "aten::_cast_Byte(Tensor self, bool non_blocking=False) -> Tensor"
  },
  {
    "name": "aten::_cast_Char(Tensor self, bool non_blocking=False) -> Tensor"
  },
  {
    "name": "aten::_cast_Double(Tensor self, bool non_blocking=False) -> Tensor"
  },
  {
    "name": "aten::_cast_Float(Tensor self, bool non_blocking=False) -> Tensor"
  },
  {
    "name": "aten::_cast_Half(Tensor self, bool non_blocking=False) -> Tensor"
  },
  {
    "name": "aten::_cast_Int(Tensor self, bool non_blocking=False) -> Tensor"
  },
  {
    "name": "aten::_cast_Long(Tensor self, bool non_blocking=False) -> Tensor"
  },
  {
    "name": "aten::_cast_Short(Tensor self, bool non_blocking=False) -> Tensor"
  },
  {
    "name": "aten::_cat(Tensor[] tensors, int dim=0) -> Tensor"
  },
  {
    "name": "aten::_cat.out(Tensor[] tensors, int dim=0, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::_cdist_forward(Tensor x1, Tensor x2, float p, int? compute_mode) -> Tensor"
  },
  {
    "name": "aten::_cdist_forward.out(Tensor x1, Tensor x2, float p, int? compute_mode, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::_coalesce(Tensor self) -> Tensor"
  },
  {
    "name": "aten::_coalesce.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::_conj(Tensor(a) self) -> Tensor(a)"
  },
  {
    "name": "aten::_convolution(Tensor input, Tensor weight, Tensor? bias, SymInt[] stride, SymInt[] padding, SymInt[] dilation, bool transposed, SymInt[] output_padding, SymInt groups, bool benchmark, bool deterministic, bool cudnn_enabled, bool allow_tf32) -> Tensor",
    "category": "Layer"
  },
  {
    "name": "aten::_convolution.deprecated(Tensor input, Tensor weight, Tensor? bias, SymInt[] stride, SymInt[] padding, SymInt[] dilation, bool transposed, int[] output_padding, SymInt groups, bool benchmark, bool deterministic, bool cudnn_enabled) -> Tensor",
    "category": "Layer"
  },
  {
    "name": "aten::_convolution.out(Tensor input, Tensor weight, Tensor? bias, SymInt[] stride, SymInt[] padding, SymInt[] dilation, bool transposed, SymInt[] output_padding, SymInt groups, bool benchmark, bool deterministic, bool cudnn_enabled, bool allow_tf32, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::_convolution_mode(Tensor input, Tensor weight, Tensor? bias, SymInt[] stride, str padding, SymInt[] dilation, SymInt groups) -> Tensor"
  },
  {
    "name": "aten::_ctc_loss(Tensor log_probs, Tensor targets, int[] input_lengths, int[] target_lengths, int blank=0, bool zero_infinity=False) -> (Tensor, Tensor)"
  },
  {
    "name": "aten::_ctc_loss.Tensor(Tensor log_probs, Tensor targets, Tensor input_lengths, Tensor target_lengths, int blank=0, bool zero_infinity=False) -> (Tensor, Tensor)"
  },
  {
    "name": "aten::_ctc_loss.Tensor_out(Tensor log_probs, Tensor targets, Tensor input_lengths, Tensor target_lengths, int blank=0, bool zero_infinity=False, *, Tensor(a!) out0, Tensor(b!) out1) -> (Tensor(a!), Tensor(b!))"
  },
  {
    "name": "aten::_ctc_loss.out(Tensor log_probs, Tensor targets, int[] input_lengths, int[] target_lengths, int blank=0, bool zero_infinity=False, *, Tensor(a!) out0, Tensor(b!) out1) -> (Tensor(a!), Tensor(b!))"
  },
  {
    "name": "aten::_dim_arange(Tensor like, int dim) -> Tensor"
  },
  {
    "name": "aten::_fake_quantize_learnable_per_tensor_affine(Tensor self, Tensor scale, Tensor zero_point, int quant_min, int quant_max, float grad_factor=1.) -> Tensor",
    "category": "Quantization"
  },
  {
    "name": "aten::_fake_quantize_learnable_per_tensor_affine.out(Tensor self, Tensor scale, Tensor zero_point, int quant_min, int quant_max, float grad_factor=1., *, Tensor(a!) out) -> Tensor(a!)",
    "category": "Quantization"
  },
  {
    "name": "aten::_fake_quantize_learnable_per_tensor_affine_backward(Tensor grad, Tensor self, Tensor scale, Tensor zero_point, int quant_min, int quant_max, float grad_factor=1.) -> (Tensor, Tensor, Tensor)",
    "category": "Quantization"
  },
  {
    "name": "aten::_make_per_tensor_quantized_tensor(Tensor self, float scale, int zero_point) -> Tensor"
  },
  {
    "name": "aten::_make_per_tensor_quantized_tensor.out(Tensor self, float scale, int zero_point, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::_native_batch_norm_legit_functional(Tensor input, Tensor? weight, Tensor? bias, Tensor running_mean, Tensor running_var, bool training, float momentum, float eps) -> (Tensor, Tensor, Tensor, Tensor running_mean_out, Tensor running_var_out)",
    "category": "Normalization"
  },
  {
    "name": "aten::_native_batch_norm_legit_no_training(Tensor input, Tensor? weight, Tensor? bias, Tensor running_mean, Tensor running_var, float momentum, float eps) -> (Tensor, Tensor, Tensor)",
    "category": "Normalization"
  },
  {
    "name": "aten::_native_batch_norm_legit_no_training.out(Tensor input, Tensor? weight, Tensor? bias, Tensor running_mean, Tensor running_var, float momentum, float eps, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2) -> (Tensor(a!), Tensor(b!), Tensor(c!))"
  },
  {
    "name": "aten::_native_multi_head_attention(Tensor query, Tensor key, Tensor value, int embed_dim, int num_head, Tensor qkv_weight, Tensor qkv_bias, Tensor proj_weight, Tensor proj_bias, Tensor? mask=None, bool need_weights=True, bool average_attn_weights=True, int? mask_type=None) -> (Tensor, Tensor)",
    "category": "Attention"
  },
  {
    "name": "aten::_native_multi_head_attention.out(Tensor query, Tensor key, Tensor value, int embed_dim, int num_head, Tensor qkv_weight, Tensor qkv_bias, Tensor proj_weight, Tensor proj_bias, Tensor? mask=None, bool need_weights=True, bool average_attn_weights=True, int? mask_type=None, *, Tensor(a!) out0, Tensor(b!) out1) -> (Tensor(a!), Tensor(b!))"
  },
  {
    "name": "aten::_nested_tensor_from_mask(Tensor t, Tensor mask, bool mask_check=True) -> Tensor"
  },
  {
    "name": "aten::_nested_tensor_from_mask.out(Tensor t, Tensor mask, bool mask_check=True, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::_pack_padded_sequence(Tensor input, Tensor lengths, bool batch_first) -> (Tensor, Tensor)"
  },
  {
    "name": "aten::_pack_padded_sequence.out(Tensor input, Tensor lengths, bool batch_first, *, Tensor(a!) out0, Tensor(b!) out1) -> (Tensor(a!), Tensor(b!))"
  },
  {
    "name": "aten::_pad_packed_sequence(Tensor data, Tensor batch_sizes, bool batch_first, Scalar padding_value, int total_length) -> (Tensor, Tensor)"
  },
  {
    "name": "aten::_scaled_dot_product_efficient_attention(Tensor query, Tensor key, Tensor value, Tensor? attn_bias, bool compute_log_sumexp, float dropout_p=0., bool is_causal=False, *, float? scale=None) -> (Tensor output, Tensor log_sumexp, Tensor philox_seed, Tensor philox_offset)"
  },
  {
    "name": "aten::_scaled_dot_product_flash_attention_for_cpu(Tensor query, Tensor key, Tensor value, float dropout_p=0., bool is_causal=False, *, Tensor? attn_mask=None, float? scale=None) -> (Tensor output, Tensor logsumexp)"
  },
  {
    "name": "aten::_shape_as_tensor(Tensor self) -> Tensor"
  },
  {
    "name": "aten::_softmax(Tensor self, int dim, bool half_to_float) -> Tensor",
    "category": "Activation"
  },
  {
    "name": "aten::_softmax.out(Tensor self, int dim, bool half_to_float, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::_sparse_coo_tensor_unsafe(Tensor indices, Tensor values, SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, bool? is_coalesced=None) -> Tensor",
    "category": "Tensor"
  },
  {
    "name": "aten::_test_serialization_subcmul(Tensor self, Tensor other, Scalar alpha=1) -> Tensor"
  },
  {
    "name": "aten::_thnn_fused_gru_cell(Tensor input_gates, Tensor hidden_gates, Tensor hx, Tensor? input_bias=None, Tensor? hidden_bias=None) -> (Tensor, Tensor)"
  },
  {
    "name": "aten::_thnn_fused_gru_cell.out(Tensor input_gates, Tensor hidden_gates, Tensor hx, Tensor? input_bias=None, Tensor? hidden_bias=None, *, Tensor(a!) out0, Tensor(b!) out1) -> (Tensor(a!), Tensor(b!))"
  },
  {
    "name": "aten::_thnn_fused_lstm_cell(Tensor input_gates, Tensor hidden_gates, Tensor cx, Tensor? input_bias=None, Tensor? hidden_bias=None) -> (Tensor, Tensor, Tensor)"
  },
  {
    "name": "aten::_thnn_fused_lstm_cell.out(Tensor input_gates, Tensor hidden_gates, Tensor cx, Tensor? input_bias=None, Tensor? hidden_bias=None, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2) -> (Tensor(a!), Tensor(b!), Tensor(c!))"
  },
  {
    "name": "aten::_to_copy(Tensor self, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, bool non_blocking=False, MemoryFormat? memory_format=None) -> Tensor"
  },
  {
    "name": "aten::_to_copy.out(Tensor self, *, bool non_blocking=False, MemoryFormat? memory_format=None, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::_transformer_encoder_layer_fwd(Tensor src, int embed_dim, int num_heads, Tensor qkv_weight, Tensor qkv_bias, Tensor proj_weight, Tensor proj_bias, bool use_gelu, bool norm_first, float eps, Tensor norm_weight_1, Tensor norm_bias_1, Tensor norm_weight_2, Tensor norm_bias_2, Tensor ffn_weight_1, Tensor ffn_bias_1, Tensor ffn_weight_2, Tensor ffn_bias_2, Tensor? mask=None, int? mask_type=None) -> Tensor"
  },
  {
    "name": "aten::_transformer_encoder_layer_fwd.out(Tensor src, int embed_dim, int num_heads, Tensor qkv_weight, Tensor qkv_bias, Tensor proj_weight, Tensor proj_bias, bool use_gelu, bool norm_first, float eps, Tensor norm_weight_1, Tensor norm_bias_1, Tensor norm_weight_2, Tensor norm_bias_2, Tensor ffn_weight_1, Tensor ffn_bias_1, Tensor ffn_weight_2, Tensor ffn_bias_2, Tensor? mask=None, int? mask_type=None, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::_unique2(Tensor self, bool sorted=True, bool return_inverse=False, bool return_counts=False) -> (Tensor, Tensor, Tensor)"
  },
  {
    "name": "aten::_unique2.out(Tensor self, bool sorted=True, bool return_inverse=False, bool return_counts=False, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2) -> (Tensor(a!), Tensor(b!), Tensor(c!))"
  },
  {
    "name": "aten::_unsafe_view(Tensor self, SymInt[] size) -> Tensor"
  },
  {
    "name": "aten::_unsafe_view.out(Tensor self, SymInt[] size, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::_unwrap_optional(t(a)? optional) -> t(a)"
  },
  {
    "name": "aten::_upsample_bicubic2d_aa(Tensor self, SymInt[2] output_size, bool align_corners, float? scales_h=None, float? scales_w=None) -> Tensor"
  },
  {
    "name": "aten::_upsample_bicubic2d_aa.out(Tensor self, SymInt[2] output_size, bool align_corners, float? scales_h=None, float? scales_w=None, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::_upsample_bicubic2d_aa.vec(Tensor input, SymInt[]? output_size, bool align_corners, float[]? scale_factors) -> Tensor"
  },
  {
    "name": "aten::_upsample_bilinear2d_aa(Tensor self, SymInt[2] output_size, bool align_corners, float? scales_h=None, float? scales_w=None) -> Tensor"
  },
  {
    "name": "aten::_upsample_bilinear2d_aa.out(Tensor self, SymInt[2] output_size, bool align_corners, float? scales_h=None, float? scales_w=None, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::_upsample_bilinear2d_aa.vec(Tensor input, SymInt[]? output_size, bool align_corners, float[]? scale_factors) -> Tensor"
  },
  {
    "name": "aten::_upsample_nearest_exact1d(Tensor self, SymInt[1] output_size, float? scales=None) -> Tensor"
  },
  {
    "name": "aten::_upsample_nearest_exact1d.out(Tensor self, SymInt[1] output_size, float? scales=None, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::_upsample_nearest_exact1d.vec(Tensor input, SymInt[]? output_size, float[]? scale_factors) -> Tensor"
  },
  {
    "name": "aten::_upsample_nearest_exact2d(Tensor self, SymInt[2] output_size, float? scales_h=None, float? scales_w=None) -> Tensor"
  },
  {
    "name": "aten::_upsample_nearest_exact2d.out(Tensor self, SymInt[2] output_size, float? scales_h=None, float? scales_w=None, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::_upsample_nearest_exact2d.vec(Tensor input, SymInt[]? output_size, float[]? scale_factors) -> Tensor"
  },
  {
    "name": "aten::_upsample_nearest_exact3d(Tensor self, SymInt[3] output_size, float? scales_d=None, float? scales_h=None, float? scales_w=None) -> Tensor"
  },
  {
    "name": "aten::_upsample_nearest_exact3d.out(Tensor self, SymInt[3] output_size, float? scales_d=None, float? scales_h=None, float? scales_w=None, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::_upsample_nearest_exact3d.vec(Tensor input, SymInt[]? output_size, float[]? scale_factors) -> Tensor"
  },
  {
    "name": "aten::_weight_norm(Tensor v, Tensor g, int dim=0) -> Tensor"
  },
  {
    "name": "aten::_weight_norm_differentiable_backward(Tensor grad_w, Tensor saved_v, Tensor saved_g, Tensor saved_norms, int dim) -> (Tensor, Tensor)"
  },
  {
    "name": "aten::_weight_norm_interface(Tensor v, Tensor g, int dim=0) -> (Tensor, Tensor)"
  },
  {
    "name": "aten::_weight_norm_interface.out(Tensor v, Tensor g, int dim=0, *, Tensor(a!) out0, Tensor(b!) out1) -> (Tensor(a!), Tensor(b!))"
  },
  {
    "name": "aten::_weight_norm_interface_backward(Tensor grad_w, Tensor saved_v, Tensor saved_g, Tensor saved_norms, int dim) -> (Tensor, Tensor)"
  },
  {
    "name": "aten::_weight_norm_interface_backward.out(Tensor grad_w, Tensor saved_v, Tensor saved_g, Tensor saved_norms, int dim, *, Tensor(a!) out0, Tensor(b!) out1) -> (Tensor(a!), Tensor(b!))"
  },
  {
    "name": "aten::abs(Tensor self) -> Tensor"
  },
  {
    "name": "aten::abs.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::abs_(Tensor(a!) self) -> Tensor(a!)"
  },
  {
    "name": "aten::acos(Tensor self) -> Tensor"
  },
  {
    "name": "aten::acos.Scalar(Scalar a) -> Scalar"
  },
  {
    "name": "aten::acos.complex(complex a) -> complex"
  },
  {
    "name": "aten::acos.float(float a) -> float"
  },
  {
    "name": "aten::acos.int(int a) -> float"
  },
  {
    "name": "aten::acos.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::acos_(Tensor(a!) self) -> Tensor(a!)"
  },
  {
    "name": "aten::acosh(Tensor self) -> Tensor"
  },
  {
    "name": "aten::acosh.Scalar(Scalar a) -> Scalar"
  },
  {
    "name": "aten::acosh.complex(complex a) -> complex"
  },
  {
    "name": "aten::acosh.float(float a) -> float"
  },
  {
    "name": "aten::acosh.int(int a) -> float"
  },
  {
    "name": "aten::acosh.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::acosh_(Tensor(a!) self) -> Tensor(a!)"
  },
  {
    "name": "aten::adaptive_avg_pool1d(Tensor self, int[1] output_size) -> Tensor",
    "category": "Pool"
  },
  {
    "name": "aten::adaptive_avg_pool1d.out(Tensor self, int[1] output_size, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::adaptive_avg_pool2d(Tensor self, SymInt[2] output_size) -> Tensor",
    "category": "Pool"
  },
  {
    "name": "aten::adaptive_avg_pool2d.out(Tensor self, SymInt[2] output_size, *, Tensor(a!) out) -> Tensor(a!)",
    "category": "Pool"
  },
  {
    "name": "aten::adaptive_avg_pool3d(Tensor self, SymInt[3] output_size) -> Tensor",
    "category": "Pool"
  },
  {
    "name": "aten::adaptive_avg_pool3d.out(Tensor self, SymInt[3] output_size, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::adaptive_max_pool1d(Tensor self, int[1] output_size) -> (Tensor, Tensor)",
    "category": "Pool"
  },
  {
    "name": "aten::adaptive_max_pool2d(Tensor self, int[2] output_size) -> (Tensor, Tensor)",
    "category": "Pool"
  },
  {
    "name": "aten::adaptive_max_pool2d.out(Tensor self, int[2] output_size, *, Tensor(a!) out, Tensor(b!) indices) -> (Tensor(a!), Tensor(b!))"
  },
  {
    "name": "aten::adaptive_max_pool3d(Tensor self, int[3] output_size) -> (Tensor, Tensor)",
    "category": "Pool"
  },
  {
    "name": "aten::adaptive_max_pool3d.out(Tensor self, int[3] output_size, *, Tensor(a!) out, Tensor(b!) indices) -> (Tensor(a!), Tensor(b!))"
  },
  {
    "name": "aten::add(Scalar a, Scalar b) -> Scalar"
  },
  {
    "name": "aten::add.Scalar(Tensor self, Scalar other, Scalar alpha=1) -> Tensor"
  },
  {
    "name": "aten::add.Scalar_out(Tensor self, Scalar other, Scalar alpha=1, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::add.Tensor(Tensor self, Tensor other, *, Scalar alpha=1) -> Tensor"
  },
  {
    "name": "aten::add.complex(complex a, complex b) -> complex"
  },
  {
    "name": "aten::add.complex_float(complex a, float b) -> complex"
  },
  {
    "name": "aten::add.complex_int(complex a, int b) -> complex"
  },
  {
    "name": "aten::add.float(float a, float b) -> float"
  },
  {
    "name": "aten::add.float_complex(float a, complex b) -> complex"
  },
  {
    "name": "aten::add.float_int(float a, int b) -> float"
  },
  {
    "name": "aten::add.int(int a, int b) -> int"
  },
  {
    "name": "aten::add.int_complex(int a, complex b) -> complex"
  },
  {
    "name": "aten::add.int_float(int a, float b) -> float"
  },
  {
    "name": "aten::add.out(Tensor self, Tensor other, *, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::add.str(str a, str b) -> str"
  },
  {
    "name": "aten::add.t(t[] a, t[] b) -> t[]"
  },
  {
    "name": "aten::add_.Scalar(Tensor(a!) self, Scalar other, Scalar alpha=1) -> Tensor(a!)"
  },
  {
    "name": "aten::add_.Tensor(Tensor(a!) self, Tensor other, *, Scalar alpha=1) -> Tensor(a!)"
  },
  {
    "name": "aten::add_.t(t[](a!) self, t[] b) -> t[]"
  },
  {
    "name": "aten::addcdiv(Tensor self, Tensor tensor1, Tensor tensor2, *, Scalar value=1) -> Tensor"
  },
  {
    "name": "aten::addcdiv.out(Tensor self, Tensor tensor1, Tensor tensor2, *, Scalar value=1, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::addcmul(Tensor self, Tensor tensor1, Tensor tensor2, *, Scalar value=1) -> Tensor"
  },
  {
    "name": "aten::addcmul.out(Tensor self, Tensor tensor1, Tensor tensor2, *, Scalar value=1, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::addcmul_(Tensor(a!) self, Tensor tensor1, Tensor tensor2, *, Scalar value=1) -> Tensor(a!)"
  },
  {
    "name": "aten::addmm(Tensor self, Tensor mat1, Tensor mat2, *, Scalar beta=1, Scalar alpha=1) -> Tensor",
    "category": "Layer"
  },
  {
    "name": "aten::addmm.out(Tensor self, Tensor mat1, Tensor mat2, *, Scalar beta=1, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::addmm_(Tensor(a!) self, Tensor mat1, Tensor mat2, *, Scalar beta=1, Scalar alpha=1) -> Tensor(a!)"
  },
  {
    "name": "aten::addmv(Tensor self, Tensor mat, Tensor vec, *, Scalar beta=1, Scalar alpha=1) -> Tensor"
  },
  {
    "name": "aten::addmv.out(Tensor self, Tensor mat, Tensor vec, *, Scalar beta=1, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::addmv_(Tensor(a!) self, Tensor mat, Tensor vec, *, Scalar beta=1, Scalar alpha=1) -> Tensor(a!)"
  },
  {
    "name": "aten::affine_grid_generator(Tensor theta, SymInt[] size, bool align_corners) -> Tensor"
  },
  {
    "name": "aten::affine_grid_generator.out(Tensor theta, SymInt[] size, bool align_corners, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::alias(Tensor(a) self) -> Tensor(a)"
  },
  {
    "name": "aten::alias_copy(Tensor self) -> Tensor"
  },
  {
    "name": "aten::alias_copy.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::all(Tensor self) -> Tensor"
  },
  {
    "name": "aten::all.all_out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::all.bool(bool[] self) -> bool"
  },
  {
    "name": "aten::all.dim(Tensor self, int dim, bool keepdim=False) -> Tensor"
  },
  {
    "name": "aten::all.dimname(Tensor self, str dim, bool keepdim=False) -> Tensor"
  },
  {
    "name": "aten::all.dimname_out(Tensor self, str dim, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::all.dims(Tensor self, int[]? dim=None, bool keepdim=False) -> Tensor"
  },
  {
    "name": "aten::all.dims_out(Tensor self, int[]? dim=None, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::all.float(float[] self) -> bool"
  },
  {
    "name": "aten::all.int(int[] self) -> bool"
  },
  {
    "name": "aten::all.out(Tensor self, int dim, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::alpha_dropout(Tensor input, float p, bool train) -> Tensor",
    "category": "Dropout"
  },
  {
    "name": "aten::alpha_dropout_(Tensor(a!) self, float p, bool train) -> Tensor(a!)",
    "category": "Dropout"
  },
  {
    "name": "aten::amax(Tensor self, int[1] dim=[], bool keepdim=False) -> Tensor"
  },
  {
    "name": "aten::amax.out(Tensor self, int[1] dim=[], bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::amin(Tensor self, int[1] dim=[], bool keepdim=False) -> Tensor"
  },
  {
    "name": "aten::amin.out(Tensor self, int[1] dim=[], bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::aminmax(Tensor self, *, int? dim=None, bool keepdim=False) -> (Tensor min, Tensor max)"
  },
  {
    "name": "aten::aminmax.out(Tensor self, *, int? dim=None, bool keepdim=False, Tensor(a!) min, Tensor(b!) max) -> (Tensor(a!) min, Tensor(b!) max)"
  },
  {
    "name": "aten::angle(Tensor self) -> Tensor"
  },
  {
    "name": "aten::angle.Scalar(Scalar a) -> Scalar"
  },
  {
    "name": "aten::angle.complex(complex a) -> float"
  },
  {
    "name": "aten::angle.float(float a) -> float"
  },
  {
    "name": "aten::angle.int(int a) -> float"
  },
  {
    "name": "aten::angle.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::any(Tensor self) -> Tensor"
  },
  {
    "name": "aten::any.all_out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::any.bool(bool[] self) -> bool"
  },
  {
    "name": "aten::any.dim(Tensor self, int dim, bool keepdim=False) -> Tensor"
  },
  {
    "name": "aten::any.dimname(Tensor self, str dim, bool keepdim=False) -> Tensor"
  },
  {
    "name": "aten::any.dimname_out(Tensor self, str dim, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::any.dims(Tensor self, int[]? dim=None, bool keepdim=False) -> Tensor"
  },
  {
    "name": "aten::any.dims_out(Tensor self, int[]? dim=None, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::any.float(float[] self) -> bool"
  },
  {
    "name": "aten::any.int(int[] self) -> bool"
  },
  {
    "name": "aten::any.out(Tensor self, int dim, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::any.str(str[] self) -> bool"
  },
  {
    "name": "aten::append.t(t[](a!) self, t(c -> *) el) -> t[](a!)"
  },
  {
    "name": "aten::arange(Scalar end, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor"
  },
  {
    "name": "aten::arange.out(Scalar end, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::arange.start(Scalar start, Scalar end, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor"
  },
  {
    "name": "aten::arange.start_out(Scalar start, Scalar end, Scalar step=1, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::arange.start_out_(Scalar start, Scalar end) -> Tensor"
  },
  {
    "name": "aten::arange.start_step(Scalar start, Scalar end, Scalar step=1, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor"
  },
  {
    "name": "aten::arctan(Tensor self) -> Tensor"
  },
  {
    "name": "aten::arctan.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::arctan_(Tensor(a!) self) -> Tensor(a!)"
  },
  {
    "name": "aten::argmax(Tensor self, int? dim=None, bool keepdim=False) -> Tensor"
  },
  {
    "name": "aten::argmax.out(Tensor self, int? dim=None, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::argmin(Tensor self, int? dim=None, bool keepdim=False) -> Tensor"
  },
  {
    "name": "aten::argmin.out(Tensor self, int? dim=None, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::argsort(Tensor self, int dim=-1, bool descending=False) -> Tensor"
  },
  {
    "name": "aten::argsort.dimname(Tensor self, str dim, bool descending=False) -> Tensor"
  },
  {
    "name": "aten::argsort.stable(Tensor self, *, bool stable, int dim=-1, bool descending=False) -> Tensor"
  },
  {
    "name": "aten::argsort.stable_out(Tensor self, *, bool stable, int dim=-1, bool descending=False, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::argwhere(Tensor self) -> Tensor"
  },
  {
    "name": "aten::as_strided(Tensor(a) self, SymInt[] size, SymInt[] stride, SymInt? storage_offset=None) -> Tensor(a)"
  },
  {
    "name": "aten::as_strided_(Tensor(a!) self, SymInt[] size, SymInt[] stride, SymInt? storage_offset=None) -> Tensor(a!)"
  },
  {
    "name": "aten::as_strided_copy(Tensor self, SymInt[] size, SymInt[] stride, SymInt? storage_offset=None) -> Tensor"
  },
  {
    "name": "aten::as_strided_copy.out(Tensor self, SymInt[] size, SymInt[] stride, SymInt? storage_offset=None, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::as_strided_scatter(Tensor self, Tensor src, SymInt[] size, SymInt[] stride, SymInt? storage_offset=None) -> Tensor"
  },
  {
    "name": "aten::as_strided_scatter.out(Tensor self, Tensor src, SymInt[] size, SymInt[] stride, SymInt? storage_offset=None, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::as_tensor(Tensor(a) data, *, ScalarType? dtype=None, Device? device=None) -> Tensor(a|b)"
  },
  {
    "name": "aten::as_tensor.bool(bool t, *, ScalarType? dtype=None, Device? device=None) -> Tensor"
  },
  {
    "name": "aten::as_tensor.complex(complex t, *, ScalarType? dtype=None, Device? device=None) -> Tensor"
  },
  {
    "name": "aten::as_tensor.float(float t, *, ScalarType? dtype=None, Device? device=None) -> Tensor"
  },
  {
    "name": "aten::as_tensor.int(int t, *, ScalarType? dtype=None, Device? device=None) -> Tensor"
  },
  {
    "name": "aten::as_tensor.list(t[] data, *, ScalarType? dtype=None, Device? device=None) -> Tensor"
  },
  {
    "name": "aten::asin(Tensor self) -> Tensor"
  },
  {
    "name": "aten::asin.Scalar(Scalar a) -> Scalar"
  },
  {
    "name": "aten::asin.complex(complex a) -> complex"
  },
  {
    "name": "aten::asin.float(float a) -> float"
  },
  {
    "name": "aten::asin.int(int a) -> float"
  },
  {
    "name": "aten::asin.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::asinh(Tensor self) -> Tensor"
  },
  {
    "name": "aten::asinh.Scalar(Scalar a) -> Scalar"
  },
  {
    "name": "aten::asinh.complex(complex a) -> complex"
  },
  {
    "name": "aten::asinh.float(float a) -> float"
  },
  {
    "name": "aten::asinh.int(int a) -> float"
  },
  {
    "name": "aten::asinh.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::atan(Tensor self) -> Tensor"
  },
  {
    "name": "aten::atan.Scalar(Scalar a) -> Scalar"
  },
  {
    "name": "aten::atan.complex(complex a) -> complex"
  },
  {
    "name": "aten::atan.float(float a) -> float"
  },
  {
    "name": "aten::atan.int(int a) -> float"
  },
  {
    "name": "aten::atan.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::atan2(Tensor self, Tensor other) -> Tensor"
  },
  {
    "name": "aten::atan2.Scalar_Scalar(Scalar a, Scalar b) -> float"
  },
  {
    "name": "aten::atan2.float(float a, float b) -> float"
  },
  {
    "name": "aten::atan2.float_int(float a, int b) -> float"
  },
  {
    "name": "aten::atan2.int(int a, int b) -> float"
  },
  {
    "name": "aten::atan2.int_float(int a, float b) -> float"
  },
  {
    "name": "aten::atan2.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::atan2_(Tensor(a!) self, Tensor other) -> Tensor(a!)"
  },
  {
    "name": "aten::atan_(Tensor(a!) self) -> Tensor(a!)"
  },
  {
    "name": "aten::atanh(Tensor self) -> Tensor"
  },
  {
    "name": "aten::atanh.Scalar(Scalar a) -> Scalar"
  },
  {
    "name": "aten::atanh.complex(complex a) -> complex"
  },
  {
    "name": "aten::atanh.float(float a) -> float"
  },
  {
    "name": "aten::atanh.int(int a) -> float"
  },
  {
    "name": "aten::atanh.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::atanh_(Tensor(a!) self) -> Tensor(a!)"
  },
  {
    "name": "aten::avg_pool1d(Tensor self, int[1] kernel_size, int[1] stride=[], int[1] padding=[0], bool ceil_mode=False, bool count_include_pad=True) -> Tensor",
    "category": "Pool"
  },
  {
    "name": "aten::avg_pool1d.out(Tensor self, int[1] kernel_size, int[1] stride=[], int[1] padding=[0], bool ceil_mode=False, bool count_include_pad=True, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::avg_pool2d(Tensor self, int[2] kernel_size, int[2] stride=[], int[2] padding=0, bool ceil_mode=False, bool count_include_pad=True, int? divisor_override=None) -> Tensor",
    "category": "Pool"
  },
  {
    "name": "aten::avg_pool2d.out(Tensor self, int[2] kernel_size, int[2] stride=[], int[2] padding=0, bool ceil_mode=False, bool count_include_pad=True, int? divisor_override=None, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::avg_pool3d(Tensor self, int[3] kernel_size, int[3] stride=[], int[3] padding=0, bool ceil_mode=False, bool count_include_pad=True, int? divisor_override=None) -> Tensor",
    "category": "Pool"
  },
  {
    "name": "aten::avg_pool3d.out(Tensor self, int[3] kernel_size, int[3] stride=[], int[3] padding=0, bool ceil_mode=False, bool count_include_pad=True, int? divisor_override=None, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::baddbmm(Tensor self, Tensor batch1, Tensor batch2, *, Scalar beta=1, Scalar alpha=1) -> Tensor"
  },
  {
    "name": "aten::baddbmm.out(Tensor self, Tensor batch1, Tensor batch2, *, Scalar beta=1, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::baddbmm_(Tensor(a!) self, Tensor batch1, Tensor batch2, *, Scalar beta=1, Scalar alpha=1) -> Tensor(a!)"
  },
  {
    "name": "aten::batch_norm(Tensor input, Tensor? weight, Tensor? bias, Tensor? running_mean, Tensor? running_var, bool training, float momentum, float eps, bool cudnn_enabled) -> Tensor",
    "category": "Normalization"
  },
  {
    "name": "aten::bernoulli(Tensor self, *, Generator? generator=None) -> Tensor"
  },
  {
    "name": "aten::bernoulli.Tensor(Tensor self, Tensor p, *, Generator? generator=None) -> Tensor"
  },
  {
    "name": "aten::bernoulli.Tensor_out(Tensor self, Tensor p, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::bernoulli.float_out(Tensor self, float p=0.5, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::bernoulli.out(Tensor self, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::bernoulli.p(Tensor self, float p, *, Generator? generator=None) -> Tensor"
  },
  {
    "name": "aten::bernoulli_.Tensor(Tensor(a!) self, Tensor p, *, Generator? generator=None) -> Tensor(a!)"
  },
  {
    "name": "aten::bernoulli_.float(Tensor(a!) self, float p=0.5, *, Generator? generator=None) -> Tensor(a!)"
  },
  {
    "name": "aten::bilinear(Tensor input1, Tensor input2, Tensor weight, Tensor? bias=None) -> Tensor"
  },
  {
    "name": "aten::bin(int i) -> str"
  },
  {
    "name": "aten::binary_cross_entropy(Tensor self, Tensor target, Tensor? weight=None, int reduction=1) -> Tensor"
  },
  {
    "name": "aten::binary_cross_entropy.out(Tensor self, Tensor target, Tensor? weight=None, int reduction=1, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::binary_cross_entropy_backward(Tensor grad_output, Tensor self, Tensor target, Tensor? weight=None, int reduction=1) -> Tensor"
  },
  {
    "name": "aten::binary_cross_entropy_backward.grad_input(Tensor grad_output, Tensor self, Tensor target, Tensor? weight=None, int reduction=1, *, Tensor(a!) grad_input) -> Tensor(a!)"
  },
  {
    "name": "aten::binary_cross_entropy_with_logits(Tensor self, Tensor target, Tensor? weight=None, Tensor? pos_weight=None, int reduction=1) -> Tensor"
  },
  {
    "name": "aten::binary_cross_entropy_with_logits.out(Tensor self, Tensor target, Tensor? weight=None, Tensor? pos_weight=None, int reduction=1, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::bincount(Tensor self, Tensor? weights=None, int minlength=0) -> Tensor"
  },
  {
    "name": "aten::bincount.out(Tensor self, Tensor? weights=None, int minlength=0, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::binomial(Tensor count, Tensor prob, Generator? generator=None) -> Tensor"
  },
  {
    "name": "aten::binomial.out(Tensor count, Tensor prob, Generator? generator=None, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::bitwise_and.Scalar(Tensor self, Scalar other) -> Tensor"
  },
  {
    "name": "aten::bitwise_and.Scalar_Tensor(Scalar self, Tensor other) -> Tensor"
  },
  {
    "name": "aten::bitwise_and.Scalar_Tensor_out(Scalar self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::bitwise_and.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::bitwise_and.Tensor(Tensor self, Tensor other) -> Tensor"
  },
  {
    "name": "aten::bitwise_and.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::bitwise_and_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)"
  },
  {
    "name": "aten::bitwise_and_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)"
  },
  {
    "name": "aten::bitwise_left_shift.Scalar_Tensor(Scalar self, Tensor other) -> Tensor"
  },
  {
    "name": "aten::bitwise_left_shift.Scalar_Tensor_out(Scalar self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::bitwise_left_shift.Tensor(Tensor self, Tensor other) -> Tensor"
  },
  {
    "name": "aten::bitwise_left_shift.Tensor_Scalar(Tensor self, Scalar other) -> Tensor"
  },
  {
    "name": "aten::bitwise_left_shift.Tensor_Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::bitwise_left_shift.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::bitwise_left_shift_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)"
  },
  {
    "name": "aten::bitwise_left_shift_.Tensor_Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)"
  },
  {
    "name": "aten::bitwise_not(Tensor self) -> Tensor"
  },
  {
    "name": "aten::bitwise_not.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::bitwise_not_(Tensor(a!) self) -> Tensor(a!)"
  },
  {
    "name": "aten::bitwise_or.Scalar(Tensor self, Scalar other) -> Tensor"
  },
  {
    "name": "aten::bitwise_or.Scalar_Tensor(Scalar self, Tensor other) -> Tensor"
  },
  {
    "name": "aten::bitwise_or.Scalar_Tensor_out(Scalar self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::bitwise_or.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::bitwise_or.Tensor(Tensor self, Tensor other) -> Tensor"
  },
  {
    "name": "aten::bitwise_or.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::bitwise_or_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)"
  },
  {
    "name": "aten::bitwise_or_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)"
  },
  {
    "name": "aten::bitwise_right_shift.Scalar_Tensor(Scalar self, Tensor other) -> Tensor"
  },
  {
    "name": "aten::bitwise_right_shift.Scalar_Tensor_out(Scalar self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::bitwise_right_shift.Tensor(Tensor self, Tensor other) -> Tensor"
  },
  {
    "name": "aten::bitwise_right_shift.Tensor_Scalar(Tensor self, Scalar other) -> Tensor"
  },
  {
    "name": "aten::bitwise_right_shift.Tensor_Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::bitwise_right_shift.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::bitwise_right_shift_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)"
  },
  {
    "name": "aten::bitwise_right_shift_.Tensor_Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)"
  },
  {
    "name": "aten::bitwise_xor.Scalar(Tensor self, Scalar other) -> Tensor"
  },
  {
    "name": "aten::bitwise_xor.Scalar_Tensor(Scalar self, Tensor other) -> Tensor"
  },
  {
    "name": "aten::bitwise_xor.Scalar_Tensor_out(Scalar self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::bitwise_xor.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::bitwise_xor.Tensor(Tensor self, Tensor other) -> Tensor"
  },
  {
    "name": "aten::bitwise_xor.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::bitwise_xor_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)"
  },
  {
    "name": "aten::bitwise_xor_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)"
  },
  {
    "name": "aten::block_diag(Tensor[] tensors) -> Tensor"
  },
  {
    "name": "aten::block_diag.out(Tensor[] tensors, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::bmm(Tensor self, Tensor mat2) -> Tensor"
  },
  {
    "name": "aten::bmm.out(Tensor self, Tensor mat2, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::broadcast_tensors(Tensor[] tensors) -> Tensor[]"
  },
  {
    "name": "aten::broadcast_to(Tensor(a) self, SymInt[] size) -> Tensor(a)"
  },
  {
    "name": "aten::bucketize.Scalar(Scalar self, Tensor boundaries, *, bool out_int32=False, bool right=False) -> Tensor"
  },
  {
    "name": "aten::bucketize.Scalar_out(Scalar self, Tensor boundaries, *, bool out_int32=False, bool right=False, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::bucketize.Tensor(Tensor self, Tensor boundaries, *, bool out_int32=False, bool right=False) -> Tensor"
  },
  {
    "name": "aten::bucketize.Tensor_out(Tensor self, Tensor boundaries, *, bool out_int32=False, bool right=False, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::cartesian_prod(Tensor[] tensors) -> Tensor"
  },
  {
    "name": "aten::cat(Tensor[] tensors, int dim=0) -> Tensor",
    "category": "Tensor"
  },
  {
    "name": "aten::cat.names(Tensor[] tensors, str dim) -> Tensor",
    "category": "Tensor"
  },
  {
    "name": "aten::cat.names_out(Tensor[] tensors, str dim, *, Tensor(a!) out) -> Tensor(a!)",
    "category": "Tensor"
  },
  {
    "name": "aten::cat.out(Tensor[] tensors, int dim=0, *, Tensor(a!) out) -> Tensor(a!)",
    "category": "Tensor"
  },
  {
    "name": "aten::cauchy_(Tensor(a!) self, float median=0., float sigma=1., *, Generator? generator=None) -> Tensor(a!)"
  },
  {
    "name": "aten::cdist(Tensor x1, Tensor x2, float p=2., int? compute_mode=None) -> Tensor"
  },
  {
    "name": "aten::ceil(Tensor self) -> Tensor"
  },
  {
    "name": "aten::ceil.Scalar(Scalar a) -> Scalar"
  },
  {
    "name": "aten::ceil.float(float a) -> int"
  },
  {
    "name": "aten::ceil.int(int a) -> int"
  },
  {
    "name": "aten::ceil.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::ceil_(Tensor(a!) self) -> Tensor(a!)"
  },
  {
    "name": "aten::celu(Tensor self, Scalar alpha=1.) -> Tensor",
    "category": "Activation"
  },
  {
    "name": "aten::celu.out(Tensor self, Scalar alpha=1., *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::celu_(Tensor(a!) self, Scalar alpha=1.) -> Tensor(a!)"
  },
  {
    "name": "aten::channel_shuffle(Tensor self, SymInt groups) -> Tensor"
  },
  {
    "name": "aten::channel_shuffle.out(Tensor self, SymInt groups, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::chunk(Tensor(a -> *) self, int chunks, int dim=0) -> Tensor(a)[]"
  },
  {
    "name": "aten::clamp(Tensor self, Scalar? min=None, Scalar? max=None) -> Tensor"
  },
  {
    "name": "aten::clamp.Tensor(Tensor self, Tensor? min=None, Tensor? max=None) -> Tensor"
  },
  {
    "name": "aten::clamp.Tensor_out(Tensor self, Tensor? min=None, Tensor? max=None, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::clamp.out(Tensor self, Scalar? min=None, Scalar? max=None, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::clamp_(Tensor(a!) self, Scalar? min=None, Scalar? max=None) -> Tensor(a!)"
  },
  {
    "name": "aten::clamp_.Tensor(Tensor(a!) self, Tensor? min=None, Tensor? max=None) -> Tensor(a!)"
  },
  {
    "name": "aten::clamp_max(Tensor self, Scalar max) -> Tensor"
  },
  {
    "name": "aten::clamp_max.Tensor(Tensor self, Tensor max) -> Tensor"
  },
  {
    "name": "aten::clamp_max.Tensor_out(Tensor self, Tensor max, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::clamp_max.out(Tensor self, Scalar max, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::clamp_max_(Tensor(a!) self, Scalar max) -> Tensor(a!)"
  },
  {
    "name": "aten::clamp_max_.Tensor(Tensor(a!) self, Tensor max) -> Tensor(a!)"
  },
  {
    "name": "aten::clamp_min(Tensor self, Scalar min) -> Tensor"
  },
  {
    "name": "aten::clamp_min.Tensor(Tensor self, Tensor min) -> Tensor"
  },
  {
    "name": "aten::clamp_min.Tensor_out(Tensor self, Tensor min, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::clamp_min.out(Tensor self, Scalar min, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::clamp_min_(Tensor(a!) self, Scalar min) -> Tensor(a!)"
  },
  {
    "name": "aten::clamp_min_.Tensor(Tensor(a!) self, Tensor min) -> Tensor(a!)"
  },
  {
    "name": "aten::clear.Tensor(Dict(Tensor, t)(a!) self) -> ()"
  },
  {
    "name": "aten::clear.bool(Dict(bool, t)(a!) self) -> ()"
  },
  {
    "name": "aten::clear.complex(Dict(complex, t)(a!) self) -> ()"
  },
  {
    "name": "aten::clear.float(Dict(float, t)(a!) self) -> ()"
  },
  {
    "name": "aten::clear.int(Dict(int, t)(a!) self) -> ()"
  },
  {
    "name": "aten::clear.str(Dict(str, t)(a!) self) -> ()"
  },
  {
    "name": "aten::clear.t(t[](a!) self) -> ()"
  },
  {
    "name": "aten::clip(Tensor self, Scalar? min=None, Scalar? max=None) -> Tensor"
  },
  {
    "name": "aten::clip.Tensor(Tensor self, Tensor? min=None, Tensor? max=None) -> Tensor"
  },
  {
    "name": "aten::clip.Tensor_out(Tensor self, Tensor? min=None, Tensor? max=None, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::clip.out(Tensor self, Scalar? min=None, Scalar? max=None, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::clip_(Tensor(a!) self, Scalar? min=None, Scalar? max=None) -> Tensor(a!)"
  },
  {
    "name": "aten::clip_.Tensor(Tensor(a!) self, Tensor? min=None, Tensor? max=None) -> Tensor(a!)"
  },
  {
    "name": "aten::clone(Tensor self, *, MemoryFormat? memory_format=None) -> Tensor"
  },
  {
    "name": "aten::clone.out(Tensor self, *, MemoryFormat? memory_format=None, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::coalesce(Tensor(a) self) -> Tensor(a)"
  },
  {
    "name": "aten::col2im(Tensor self, SymInt[2] output_size, int[2] kernel_size, int[2] dilation, int[2] padding, int[2] stride) -> Tensor"
  },
  {
    "name": "aten::col2im.out(Tensor self, SymInt[2] output_size, int[2] kernel_size, int[2] dilation, int[2] padding, int[2] stride, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::column_stack(Tensor[] tensors) -> Tensor"
  },
  {
    "name": "aten::column_stack.out(Tensor[] tensors, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::complex(Tensor real, Tensor imag) -> Tensor"
  },
  {
    "name": "aten::complex.out(Tensor real, Tensor imag, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::concat(Tensor[] tensors, int dim=0) -> Tensor",
    "category": "Tensor"
  },
  {
    "name": "aten::concat.names(Tensor[] tensors, str dim) -> Tensor",
    "category": "Tensor"
  },
  {
    "name": "aten::concat.names_out(Tensor[] tensors, str dim, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::concat.out(Tensor[] tensors, int dim=0, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::concatenate(Tensor[] tensors, int dim=0) -> Tensor"
  },
  {
    "name": "aten::concatenate.names(Tensor[] tensors, str dim) -> Tensor"
  },
  {
    "name": "aten::concatenate.names_out(Tensor[] tensors, str dim, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::concatenate.out(Tensor[] tensors, int dim=0, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::conj(Tensor(a) self) -> Tensor(a)"
  },
  {
    "name": "aten::constant_pad_nd(Tensor self, SymInt[] pad, Scalar value=0) -> Tensor",
    "category": "Tensor"
  },
  {
    "name": "aten::constant_pad_nd.out(Tensor self, SymInt[] pad, Scalar value=0, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::contiguous(Tensor(a) self, *, MemoryFormat memory_format=0) -> Tensor(a)"
  },
  {
    "name": "aten::conv1d(Tensor input, Tensor weight, Tensor? bias=None, SymInt[1] stride=[1], SymInt[1] padding=[0], SymInt[1] dilation=[1], SymInt groups=1) -> Tensor",
    "category": "Layer"
  },
  {
    "name": "aten::conv1d.padding(Tensor input, Tensor weight, Tensor? bias=None, SymInt[1] stride=[1], str padding=\"valid\", SymInt[1] dilation=[1], SymInt groups=1) -> Tensor",
    "category": "Layer"
  },
  {
    "name": "aten::conv2d(Tensor input, Tensor weight, Tensor? bias=None, SymInt[2] stride=[1, 1], SymInt[2] padding=[0, 0], SymInt[2] dilation=[1, 1], SymInt groups=1) -> Tensor",
    "category": "Layer"
  },
  {
    "name": "aten::conv2d.padding(Tensor input, Tensor weight, Tensor? bias=None, SymInt[2] stride=[1, 1], str padding=\"valid\", SymInt[2] dilation=[1, 1], SymInt groups=1) -> Tensor",
    "category": "Layer"
  },
  {
    "name": "aten::conv3d(Tensor input, Tensor weight, Tensor? bias=None, SymInt[3] stride=[1, 1, 1], SymInt[3] padding=[0, 0, 0], SymInt[3] dilation=[1, 1, 1], SymInt groups=1) -> Tensor",
    "category": "Layer"
  },
  {
    "name": "aten::conv3d.padding(Tensor input, Tensor weight, Tensor? bias=None, SymInt[3] stride=[1, 1, 1], str padding=\"valid\", SymInt[3] dilation=[1, 1, 1], SymInt groups=1) -> Tensor",
    "category": "Layer"
  },
  {
    "name": "aten::conv_transpose1d(Tensor input, Tensor weight, Tensor? bias=None, SymInt[1] stride=[1], SymInt[1] padding=[0], SymInt[1] output_padding=[0], SymInt groups=1, SymInt[1] dilation=[1]) -> Tensor",
    "category": "Layer"
  },
  {
    "name": "aten::conv_transpose2d.input(Tensor input, Tensor weight, Tensor? bias=None, SymInt[2] stride=[1, 1], SymInt[2] padding=[0, 0], SymInt[2] output_padding=[0, 0], SymInt groups=1, SymInt[2] dilation=[1, 1]) -> Tensor",
    "category": "Layer"
  },
  {
    "name": "aten::conv_transpose3d.input(Tensor input, Tensor weight, Tensor? bias=None, SymInt[3] stride=[1, 1, 1], SymInt[3] padding=[0, 0, 0], SymInt[3] output_padding=[0, 0, 0], SymInt groups=1, SymInt[3] dilation=[1, 1, 1]) -> Tensor",
    "category": "Layer"
  },
  {
    "name": "aten::convolution(Tensor input, Tensor weight, Tensor? bias, SymInt[] stride, SymInt[] padding, SymInt[] dilation, bool transposed, SymInt[] output_padding, SymInt groups) -> Tensor",
    "category": "Layer"
  },
  {
    "name": "aten::convolution.out(Tensor input, Tensor weight, Tensor? bias, SymInt[] stride, SymInt[] padding, SymInt[] dilation, bool transposed, SymInt[] output_padding, SymInt groups, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::convolution_backward(Tensor grad_output, Tensor input, Tensor weight, SymInt[]? bias_sizes, SymInt[] stride, SymInt[] padding, SymInt[] dilation, bool transposed, SymInt[] output_padding, SymInt groups, bool[3] output_mask) -> (Tensor, Tensor, Tensor)"
  },
  {
    "name": "aten::convolution_backward.out(Tensor grad_output, Tensor input, Tensor weight, SymInt[]? bias_sizes, SymInt[] stride, SymInt[] padding, SymInt[] dilation, bool transposed, SymInt[] output_padding, SymInt groups, bool[3] output_mask, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2) -> (Tensor(a!), Tensor(b!), Tensor(c!))"
  },
  {
    "name": "aten::convolution_backward_overrideable(Tensor grad_output, Tensor input, Tensor weight, SymInt[] stride, SymInt[] padding, SymInt[] dilation, bool transposed, SymInt[] output_padding, SymInt groups, bool[3] output_mask) -> (Tensor grad_input, Tensor grad_weight, Tensor grad_bias)"
  },
  {
    "name": "aten::convolution_backward_overrideable.out(Tensor grad_output, Tensor input, Tensor weight, SymInt[] stride, SymInt[] padding, SymInt[] dilation, bool transposed, SymInt[] output_padding, SymInt groups, bool[3] output_mask, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2) -> (Tensor(a!), Tensor(b!), Tensor(c!))"
  },
  {
    "name": "aten::convolution_overrideable(Tensor input, Tensor weight, Tensor? bias, SymInt[] stride, SymInt[] padding, SymInt[] dilation, bool transposed, SymInt[] output_padding, SymInt groups) -> Tensor"
  },
  {
    "name": "aten::convolution_overrideable.out(Tensor input, Tensor weight, Tensor? bias, SymInt[] stride, SymInt[] padding, SymInt[] dilation, bool transposed, SymInt[] output_padding, SymInt groups, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::copy_(Tensor(a!) self, Tensor src, bool non_blocking=False) -> Tensor(a!)"
  },
  {
    "name": "aten::copy_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)"
  },
  {
    "name": "aten::copy_.float(Tensor(a!) self, float other) -> Tensor(a!)"
  },
  {
    "name": "aten::copy_.int(Tensor(a!) self, int other) -> Tensor(a!)"
  },
  {
    "name": "aten::cos(Tensor self) -> Tensor"
  },
  {
    "name": "aten::cos.Scalar(Scalar a) -> Scalar"
  },
  {
    "name": "aten::cos.complex(complex a) -> complex"
  },
  {
    "name": "aten::cos.float(float a) -> float"
  },
  {
    "name": "aten::cos.int(int a) -> float"
  },
  {
    "name": "aten::cos.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::cosh(Tensor self) -> Tensor"
  },
  {
    "name": "aten::cosh.Scalar(Scalar a) -> Scalar"
  },
  {
    "name": "aten::cosh.complex(complex a) -> complex"
  },
  {
    "name": "aten::cosh.float(float a) -> float"
  },
  {
    "name": "aten::cosh.int(int a) -> float"
  },
  {
    "name": "aten::cosh.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::cosine_similarity(Tensor x1, Tensor x2, int dim=1, float eps=1e-08) -> Tensor"
  },
  {
    "name": "aten::count_nonzero(Tensor self, int? dim=None) -> Tensor"
  },
  {
    "name": "aten::count_nonzero.dim_IntList(Tensor self, int[] dim) -> Tensor"
  },
  {
    "name": "aten::count_nonzero.dim_IntList_out(Tensor self, int[] dim, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::count_nonzero.out(Tensor self, int? dim=None, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::cpu(Tensor(a) self) -> Tensor(a|b)"
  },
  {
    "name": "aten::cross(Tensor self, Tensor other, int? dim=None) -> Tensor"
  },
  {
    "name": "aten::cross.out(Tensor self, Tensor other, int? dim=None, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::cross_entropy_loss(Tensor self, Tensor target, Tensor? weight=None, int reduction=1, SymInt ignore_index=-100, float label_smoothing=0.) -> Tensor"
  },
  {
    "name": "aten::ctc_loss.IntList(Tensor log_probs, Tensor targets, int[] input_lengths, int[] target_lengths, int blank=0, int reduction=1, bool zero_infinity=False) -> Tensor"
  },
  {
    "name": "aten::ctc_loss.Tensor(Tensor log_probs, Tensor targets, Tensor input_lengths, Tensor target_lengths, int blank=0, int reduction=1, bool zero_infinity=False) -> Tensor"
  },
  {
    "name": "aten::cudnn_convolution_add_relu(Tensor self, Tensor weight, Tensor z, Scalar? alpha, Tensor? bias, SymInt[] stride, SymInt[] padding, SymInt[] dilation, SymInt groups) -> Tensor"
  },
  {
    "name": "aten::cudnn_convolution_add_relu.out(Tensor self, Tensor weight, Tensor z, Scalar? alpha, Tensor? bias, SymInt[] stride, SymInt[] padding, SymInt[] dilation, SymInt groups, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::cudnn_convolution_relu(Tensor self, Tensor weight, Tensor? bias, SymInt[] stride, SymInt[] padding, SymInt[] dilation, SymInt groups) -> Tensor"
  },
  {
    "name": "aten::cudnn_convolution_relu.out(Tensor self, Tensor weight, Tensor? bias, SymInt[] stride, SymInt[] padding, SymInt[] dilation, SymInt groups, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::cummax(Tensor self, int dim) -> (Tensor values, Tensor indices)"
  },
  {
    "name": "aten::cummax.dimname(Tensor self, str dim) -> (Tensor values, Tensor indices)"
  },
  {
    "name": "aten::cummax.dimname_out(Tensor self, str dim, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)"
  },
  {
    "name": "aten::cummax.out(Tensor self, int dim, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)"
  },
  {
    "name": "aten::cummaxmin_backward(Tensor grad, Tensor input, Tensor indices, int dim) -> Tensor"
  },
  {
    "name": "aten::cumsum(Tensor self, int dim, *, ScalarType? dtype=None) -> Tensor"
  },
  {
    "name": "aten::cumsum.dimname(Tensor self, str dim, *, ScalarType? dtype=None) -> Tensor"
  },
  {
    "name": "aten::cumsum.dimname_out(Tensor self, str dim, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::cumsum.out(Tensor self, int dim, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::cumsum_(Tensor(a!) self, int dim, *, ScalarType? dtype=None) -> Tensor(a!)"
  },
  {
    "name": "aten::cumsum_.dimname(Tensor(a!) self, str dim, *, ScalarType? dtype=None) -> Tensor(a!)"
  },
  {
    "name": "aten::dequantize.any(Any tensors) -> Any",
    "category": "Quantization"
  },
  {
    "name": "aten::dequantize.list(Tensor[] qtensors) -> Tensor[]",
    "category": "Quantization"
  },
  {
    "name": "aten::dequantize.self(Tensor self) -> Tensor",
    "category": "Quantization"
  },
  {
    "name": "aten::dequantize.self_out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::dequantize.tensor(Tensor qtensor) -> Tensor",
    "category": "Quantization"
  },
  {
    "name": "aten::dequantize.tensors(Tensor[] tensors) -> Tensor[]",
    "category": "Quantization"
  },
  {
    "name": "aten::dequantize.tensors_out(Tensor[] tensors, *, Tensor(a!)[] out) -> ()"
  },
  {
    "name": "aten::detach(Tensor(a) self) -> Tensor(a)"
  },
  {
    "name": "aten::detach_(Tensor(a!) self) -> Tensor(a!)"
  },
  {
    "name": "aten::detach_copy(Tensor self) -> Tensor"
  },
  {
    "name": "aten::detach_copy.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::device(str a) -> Device"
  },
  {
    "name": "aten::device.with_index(str type, int index) -> Device"
  },
  {
    "name": "aten::diag(Tensor self, int diagonal=0) -> Tensor"
  },
  {
    "name": "aten::diag.out(Tensor self, int diagonal=0, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::diag_embed(Tensor self, int offset=0, int dim1=-2, int dim2=-1) -> Tensor"
  },
  {
    "name": "aten::diag_embed.out(Tensor self, int offset=0, int dim1=-2, int dim2=-1, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::diagflat(Tensor self, int offset=0) -> Tensor"
  },
  {
    "name": "aten::diagonal(Tensor(a) self, int offset=0, int dim1=0, int dim2=1) -> Tensor(a)"
  },
  {
    "name": "aten::diagonal.Dimname(Tensor(a) self, *, str outdim, str dim1, str dim2, int offset=0) -> Tensor(a)"
  },
  {
    "name": "aten::diagonal_backward(Tensor grad_output, SymInt[] input_sizes, int offset, int dim1, int dim2) -> Tensor"
  },
  {
    "name": "aten::diagonal_backward.out(Tensor grad_output, SymInt[] input_sizes, int offset, int dim1, int dim2, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::diagonal_copy(Tensor self, int offset=0, int dim1=0, int dim2=1) -> Tensor"
  },
  {
    "name": "aten::diagonal_copy.out(Tensor self, int offset=0, int dim1=0, int dim2=1, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::diagonal_scatter(Tensor self, Tensor src, int offset=0, int dim1=0, int dim2=1) -> Tensor"
  },
  {
    "name": "aten::diagonal_scatter.out(Tensor self, Tensor src, int offset=0, int dim1=0, int dim2=1, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::dict() -> Dict(str, Tensor)"
  },
  {
    "name": "aten::dict.Dict_Tensor(Dict(Tensor, t)(a) self) -> Dict(Tensor, t)"
  },
  {
    "name": "aten::dict.Dict_bool(Dict(bool, t)(a) self) -> Dict(bool, t)"
  },
  {
    "name": "aten::dict.Dict_complex(Dict(complex, t)(a) self) -> Dict(complex, t)"
  },
  {
    "name": "aten::dict.Dict_float(Dict(float, t)(a) self) -> Dict(float, t)"
  },
  {
    "name": "aten::dict.Dict_int(Dict(int, t)(a) self) -> Dict(int, t)"
  },
  {
    "name": "aten::dict.Dict_str(Dict(str, t)(a) self) -> Dict(str, t)"
  },
  {
    "name": "aten::dict.Tensor((Tensor, tVal)[] inputs) -> Dict(Tensor, tVal)"
  },
  {
    "name": "aten::dict.bool((bool, tVal)[] inputs) -> Dict(bool, tVal)"
  },
  {
    "name": "aten::dict.complex((complex, tVal)[] inputs) -> Dict(complex, tVal)"
  },
  {
    "name": "aten::dict.float((float, tVal)[] inputs) -> Dict(float, tVal)"
  },
  {
    "name": "aten::dict.int((int, tVal)[] inputs) -> Dict(int, tVal)"
  },
  {
    "name": "aten::dict.str((str, tVal)[] inputs) -> Dict(str, tVal)"
  },
  {
    "name": "aten::diff(Tensor self, int n=1, int dim=-1, Tensor? prepend=None, Tensor? append=None) -> Tensor"
  },
  {
    "name": "aten::diff.out(Tensor self, int n=1, int dim=-1, Tensor? prepend=None, Tensor? append=None, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::dim(Tensor self) -> int"
  },
  {
    "name": "aten::dist(Tensor self, Tensor other, Scalar p=2) -> Tensor"
  },
  {
    "name": "aten::dist.out(Tensor self, Tensor other, Scalar p=2, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::div(Scalar a, Scalar b) -> float"
  },
  {
    "name": "aten::div.Scalar(Tensor self, Scalar other) -> Tensor"
  },
  {
    "name": "aten::div.Scalar_mode(Tensor self, Scalar other, *, str? rounding_mode) -> Tensor"
  },
  {
    "name": "aten::div.Scalar_mode_out(Tensor self, Scalar other, *, str? rounding_mode, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::div.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::div.Tensor(Tensor self, Tensor other) -> Tensor"
  },
  {
    "name": "aten::div.Tensor_mode(Tensor self, Tensor other, *, str? rounding_mode) -> Tensor"
  },
  {
    "name": "aten::div.complex(complex a, complex b) -> complex"
  },
  {
    "name": "aten::div.float(float a, float b) -> float"
  },
  {
    "name": "aten::div.int(int a, int b) -> float"
  },
  {
    "name": "aten::div.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::div.out_mode(Tensor self, Tensor other, *, str? rounding_mode, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::div_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)"
  },
  {
    "name": "aten::div_.Scalar_mode(Tensor(a!) self, Scalar other, *, str? rounding_mode) -> Tensor(a!)"
  },
  {
    "name": "aten::div_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)"
  },
  {
    "name": "aten::div_.Tensor_mode(Tensor(a!) self, Tensor other, *, str? rounding_mode) -> Tensor(a!)"
  },
  {
    "name": "aten::divide.Scalar(Tensor self, Scalar other) -> Tensor"
  },
  {
    "name": "aten::divide.Scalar_mode(Tensor self, Scalar other, *, str? rounding_mode) -> Tensor"
  },
  {
    "name": "aten::divide.Tensor(Tensor self, Tensor other) -> Tensor"
  },
  {
    "name": "aten::divide.Tensor_mode(Tensor self, Tensor other, *, str? rounding_mode) -> Tensor"
  },
  {
    "name": "aten::divide.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::divide.out_mode(Tensor self, Tensor other, *, str? rounding_mode, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::divide_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)"
  },
  {
    "name": "aten::divide_.Scalar_mode(Tensor(a!) self, Scalar other, *, str? rounding_mode) -> Tensor(a!)"
  },
  {
    "name": "aten::divide_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)"
  },
  {
    "name": "aten::divide_.Tensor_mode(Tensor(a!) self, Tensor other, *, str? rounding_mode) -> Tensor(a!)"
  },
  {
    "name": "aten::divmod.float(float x, float y) -> (float, float)"
  },
  {
    "name": "aten::divmod.float_int(float x, int y) -> (float, float)"
  },
  {
    "name": "aten::divmod.int(int x, int y) -> (int, int)"
  },
  {
    "name": "aten::divmod.int_float(int x, float y) -> (float, float)"
  },
  {
    "name": "aten::dot(Tensor self, Tensor tensor) -> Tensor"
  },
  {
    "name": "aten::dot.out(Tensor self, Tensor tensor, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::dropout(Tensor input, float p, bool train) -> Tensor",
    "category": "Dropout"
  },
  {
    "name": "aten::dropout_(Tensor(a!) self, float p, bool train) -> Tensor(a!)",
    "category": "Dropout"
  },
  {
    "name": "aten::einsum(str equation, Tensor[] tensors, *, int[]? path=None) -> Tensor"
  },
  {
    "name": "aten::einsum.sublist(Tensor a, ...) -> Tensor"
  },
  {
    "name": "aten::elu(Tensor self, Scalar alpha=1, Scalar scale=1, Scalar input_scale=1) -> Tensor",
    "category": "Activation"
  },
  {
    "name": "aten::elu.out(Tensor self, Scalar alpha=1, Scalar scale=1, Scalar input_scale=1, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::elu_(Tensor(a!) self, Scalar alpha=1, Scalar scale=1, Scalar input_scale=1) -> Tensor(a!)",
    "category": "Activation"
  },
  {
    "name": "aten::embedding(Tensor weight, Tensor indices, SymInt padding_idx=-1, bool scale_grad_by_freq=False, bool sparse=False) -> Tensor",
    "category": "Transform"
  },
  {
    "name": "aten::embedding.out(Tensor weight, Tensor indices, SymInt padding_idx=-1, bool scale_grad_by_freq=False, bool sparse=False, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::embedding_bag(Tensor weight, Tensor indices, Tensor offsets, bool scale_grad_by_freq=False, int mode=0, bool sparse=False, Tensor? per_sample_weights=None, bool include_last_offset=False) -> (Tensor, Tensor, Tensor, Tensor)",
    "category": "Transform"
  },
  {
    "name": "aten::embedding_bag.padding_idx(Tensor weight, Tensor indices, Tensor offsets, bool scale_grad_by_freq, int mode, bool sparse, Tensor? per_sample_weights, bool include_last_offset, int? padding_idx) -> (Tensor, Tensor, Tensor, Tensor)"
  },
  {
    "name": "aten::embedding_renorm_(Tensor(a!) self, Tensor indices, float max_norm, float norm_type) -> Tensor(a!)"
  },
  {
    "name": "aten::empty.memory_format(SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor"
  },
  {
    "name": "aten::empty.names(int[] size, *, str[]? names, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor"
  },
  {
    "name": "aten::empty.names_out(int[] size, *, str[]? names, MemoryFormat? memory_format=None, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::empty.out(SymInt[] size, *, MemoryFormat? memory_format=None, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::empty_like(Tensor self, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor"
  },
  {
    "name": "aten::empty_like.out(Tensor self, *, MemoryFormat? memory_format=None, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::eq(Scalar a, Scalar b) -> bool"
  },
  {
    "name": "aten::eq.Scalar(Tensor self, Scalar other) -> Tensor"
  },
  {
    "name": "aten::eq.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::eq.Tensor(Tensor self, Tensor other) -> Tensor"
  },
  {
    "name": "aten::eq.Tensor_list(Tensor[] a, Tensor[] b) -> bool"
  },
  {
    "name": "aten::eq.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::eq.bool(bool a, bool b) -> bool"
  },
  {
    "name": "aten::eq.bool_list(bool[] a, bool[] b) -> bool"
  },
  {
    "name": "aten::eq.complex(complex a, complex b) -> bool"
  },
  {
    "name": "aten::eq.complex_float(complex a, float b) -> bool"
  },
  {
    "name": "aten::eq.device(Device a, Device b) -> bool"
  },
  {
    "name": "aten::eq.enum(AnyEnumType a, AnyEnumType b) -> bool"
  },
  {
    "name": "aten::eq.float(float a, float b) -> bool"
  },
  {
    "name": "aten::eq.float_complex(float a, complex b) -> bool"
  },
  {
    "name": "aten::eq.float_int(float a, int b) -> bool"
  },
  {
    "name": "aten::eq.float_list(float[] a, float[] b) -> bool"
  },
  {
    "name": "aten::eq.int(int a, int b) -> bool"
  },
  {
    "name": "aten::eq.int_float(int a, float b) -> bool"
  },
  {
    "name": "aten::eq.int_list(int[] a, int[] b) -> bool"
  },
  {
    "name": "aten::eq.str(str a, str b) -> bool"
  },
  {
    "name": "aten::eq.str_list(str[] a, str[] b) -> bool"
  },
  {
    "name": "aten::equal(Tensor self, Tensor other) -> bool"
  },
  {
    "name": "aten::erf(Tensor self) -> Tensor"
  },
  {
    "name": "aten::erf.Scalar(Scalar a) -> Scalar"
  },
  {
    "name": "aten::erf.float(float a) -> float"
  },
  {
    "name": "aten::erf.int(int a) -> float"
  },
  {
    "name": "aten::erf.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::erfc(Tensor self) -> Tensor"
  },
  {
    "name": "aten::erfc.Scalar(Scalar a) -> Scalar"
  },
  {
    "name": "aten::erfc.float(float a) -> float"
  },
  {
    "name": "aten::erfc.int(int a) -> float"
  },
  {
    "name": "aten::erfc.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::exp(Tensor self) -> Tensor"
  },
  {
    "name": "aten::exp.Scalar(Scalar a) -> Scalar"
  },
  {
    "name": "aten::exp.complex(complex a) -> complex"
  },
  {
    "name": "aten::exp.float(float a) -> float"
  },
  {
    "name": "aten::exp.int(int a) -> float"
  },
  {
    "name": "aten::exp.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::exp_(Tensor(a!) self) -> Tensor(a!)"
  },
  {
    "name": "aten::expand(Tensor(a) self, SymInt[] size, *, bool implicit=False) -> Tensor(a)"
  },
  {
    "name": "aten::expand_as(Tensor(a) self, Tensor other) -> Tensor(a)"
  },
  {
    "name": "aten::expm1(Tensor self) -> Tensor"
  },
  {
    "name": "aten::expm1.Scalar(Scalar a) -> Scalar"
  },
  {
    "name": "aten::expm1.float(float a) -> float"
  },
  {
    "name": "aten::expm1.int(int a) -> float"
  },
  {
    "name": "aten::expm1.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::expm1_(Tensor(a!) self) -> Tensor(a!)"
  },
  {
    "name": "aten::exponential_(Tensor(a!) self, float lambd=1., *, Generator? generator=None) -> Tensor(a!)"
  },
  {
    "name": "aten::extend.t(t[](a!) self, t[] other) -> ()"
  },
  {
    "name": "aten::eye(SymInt n, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor"
  },
  {
    "name": "aten::eye.m(SymInt n, SymInt m, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor"
  },
  {
    "name": "aten::eye.m_out(SymInt n, SymInt m, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::eye.out(SymInt n, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::fake_quantize_per_channel_affine(Tensor self, Tensor scale, Tensor zero_point, int axis, int quant_min, int quant_max) -> Tensor",
    "category": "Quantization"
  },
  {
    "name": "aten::fake_quantize_per_tensor_affine(Tensor self, float scale, int zero_point, int quant_min, int quant_max) -> Tensor",
    "category": "Quantization"
  },
  {
    "name": "aten::fake_quantize_per_tensor_affine.tensor_qparams(Tensor self, Tensor scale, Tensor zero_point, int quant_min, int quant_max) -> Tensor",
    "category": "Quantization"
  },
  {
    "name": "aten::fake_quantize_per_tensor_affine_cachemask(Tensor self, float scale, int zero_point, int quant_min, int quant_max) -> (Tensor output, Tensor mask)",
    "category": "Quantization"
  },
  {
    "name": "aten::fake_quantize_per_tensor_affine_cachemask.out(Tensor self, float scale, int zero_point, int quant_min, int quant_max, *, Tensor(a!) out0, Tensor(b!) out1) -> (Tensor(a!), Tensor(b!))",
    "category": "Quantization"
  },
  {
    "name": "aten::fake_quantize_per_tensor_affine_cachemask_backward(Tensor grad, Tensor mask) -> Tensor",
    "category": "Quantization"
  },
  {
    "name": "aten::feature_alpha_dropout(Tensor input, float p, bool train) -> Tensor",
    "category": "Dropout"
  },
  {
    "name": "aten::feature_alpha_dropout_(Tensor(a!) self, float p, bool train) -> Tensor(a!)",
    "category": "Dropout"
  },
  {
    "name": "aten::feature_dropout(Tensor input, float p, bool train) -> Tensor",
    "category": "Dropout"
  },
  {
    "name": "aten::feature_dropout_(Tensor(a!) self, float p, bool train) -> Tensor(a!)",
    "category": "Dropout"
  },
  {
    "name": "aten::fft(Tensor self, int signal_ndim, bool normalized=False) -> Tensor"
  },
  {
    "name": "aten::fft_fft(Tensor self, SymInt? n=None, int dim=-1, str? norm=None) -> Tensor"
  },
  {
    "name": "aten::fft_fft.out(Tensor self, SymInt? n=None, int dim=-1, str? norm=None, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::fft_fft2(Tensor self, SymInt[1]? s=None, int[1] dim=[-2, -1], str? norm=None) -> Tensor"
  },
  {
    "name": "aten::fft_fft2.out(Tensor self, SymInt[1]? s=None, int[1] dim=[-2, -1], str? norm=None, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::fft_fftn(Tensor self, SymInt[1]? s=None, int[1]? dim=None, str? norm=None) -> Tensor"
  },
  {
    "name": "aten::fft_fftn.out(Tensor self, SymInt[1]? s=None, int[1]? dim=None, str? norm=None, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::fft_fftshift(Tensor self, int[1]? dim=None) -> Tensor"
  },
  {
    "name": "aten::fft_hfft2(Tensor self, SymInt[1]? s=None, int[1] dim=[-2, -1], str? norm=None) -> Tensor"
  },
  {
    "name": "aten::fft_hfft2.out(Tensor self, SymInt[1]? s=None, int[1] dim=[-2, -1], str? norm=None, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::fft_hfftn(Tensor self, SymInt[1]? s=None, int[1]? dim=None, str? norm=None) -> Tensor"
  },
  {
    "name": "aten::fft_hfftn.out(Tensor self, SymInt[1]? s=None, int[1]? dim=None, str? norm=None, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::fft_ifft(Tensor self, SymInt? n=None, int dim=-1, str? norm=None) -> Tensor"
  },
  {
    "name": "aten::fft_ifft.out(Tensor self, SymInt? n=None, int dim=-1, str? norm=None, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::fft_ifft2(Tensor self, SymInt[1]? s=None, int[1] dim=[-2, -1], str? norm=None) -> Tensor"
  },
  {
    "name": "aten::fft_ifft2.out(Tensor self, SymInt[1]? s=None, int[1] dim=[-2, -1], str? norm=None, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::fft_ifftn(Tensor self, SymInt[1]? s=None, int[1]? dim=None, str? norm=None) -> Tensor"
  },
  {
    "name": "aten::fft_ifftn.out(Tensor self, SymInt[1]? s=None, int[1]? dim=None, str? norm=None, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::fft_ifftshift(Tensor self, int[1]? dim=None) -> Tensor"
  },
  {
    "name": "aten::fft_ihfft2(Tensor self, SymInt[1]? s=None, int[1] dim=[-2, -1], str? norm=None) -> Tensor"
  },
  {
    "name": "aten::fft_ihfft2.out(Tensor self, SymInt[1]? s=None, int[1] dim=[-2, -1], str? norm=None, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::fft_ihfftn(Tensor self, SymInt[1]? s=None, int[1]? dim=None, str? norm=None) -> Tensor"
  },
  {
    "name": "aten::fft_ihfftn.out(Tensor self, SymInt[1]? s=None, int[1]? dim=None, str? norm=None, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::fft_irfft(Tensor self, SymInt? n=None, int dim=-1, str? norm=None) -> Tensor"
  },
  {
    "name": "aten::fft_irfft.out(Tensor self, SymInt? n=None, int dim=-1, str? norm=None, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::fft_irfft2(Tensor self, SymInt[1]? s=None, int[1] dim=[-2, -1], str? norm=None) -> Tensor"
  },
  {
    "name": "aten::fft_irfft2.out(Tensor self, SymInt[1]? s=None, int[1] dim=[-2, -1], str? norm=None, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::fft_irfftn(Tensor self, SymInt[1]? s=None, int[1]? dim=None, str? norm=None) -> Tensor"
  },
  {
    "name": "aten::fft_irfftn.out(Tensor self, SymInt[1]? s=None, int[1]? dim=None, str? norm=None, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::fft_rfft(Tensor self, SymInt? n=None, int dim=-1, str? norm=None) -> Tensor"
  },
  {
    "name": "aten::fft_rfft.out(Tensor self, SymInt? n=None, int dim=-1, str? norm=None, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::fft_rfft2(Tensor self, SymInt[1]? s=None, int[1] dim=[-2, -1], str? norm=None) -> Tensor"
  },
  {
    "name": "aten::fft_rfft2.out(Tensor self, SymInt[1]? s=None, int[1] dim=[-2, -1], str? norm=None, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::fft_rfftn(Tensor self, SymInt[1]? s=None, int[1]? dim=None, str? norm=None) -> Tensor"
  },
  {
    "name": "aten::fft_rfftn.out(Tensor self, SymInt[1]? s=None, int[1]? dim=None, str? norm=None, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::fill_.Scalar(Tensor(a!) self, Scalar value) -> Tensor(a!)"
  },
  {
    "name": "aten::fill_.Tensor(Tensor(a!) self, Tensor value) -> Tensor(a!)"
  },
  {
    "name": "aten::find(str self, str substr, int start=0, int end=-1) -> int"
  },
  {
    "name": "aten::flatten.DimnameList(Tensor(a) self, str[] dims, str out_dim) -> Tensor(a)",
    "category": "Shape"
  },
  {
    "name": "aten::flatten.named_out_dim(Tensor(a) self, int start_dim, int end_dim, str out_dim) -> Tensor(a)",
    "category": "Shape"
  },
  {
    "name": "aten::flatten.using_ints(Tensor(a) self, int start_dim=0, int end_dim=-1) -> Tensor(a)",
    "category": "Shape"
  },
  {
    "name": "aten::flatten.using_names(Tensor(a) self, str start_dim, str end_dim, str out_dim) -> Tensor(a)",
    "category": "Shape"
  },
  {
    "name": "aten::flip(Tensor self, int[] dims) -> Tensor"
  },
  {
    "name": "aten::flip.out(Tensor self, int[] dims, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::floor(Tensor self) -> Tensor"
  },
  {
    "name": "aten::floor.Scalar(Scalar a) -> Scalar"
  },
  {
    "name": "aten::floor.float(float a) -> int"
  },
  {
    "name": "aten::floor.int(int a) -> int"
  },
  {
    "name": "aten::floor.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::floor_(Tensor(a!) self) -> Tensor(a!)"
  },
  {
    "name": "aten::floor_divide(Tensor self, Tensor other) -> Tensor"
  },
  {
    "name": "aten::floor_divide.Scalar(Tensor self, Scalar other) -> Tensor"
  },
  {
    "name": "aten::floor_divide.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::floor_divide.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::floor_divide_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)"
  },
  {
    "name": "aten::floor_divide_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)"
  },
  {
    "name": "aten::floordiv(Scalar a, Scalar b) -> Scalar"
  },
  {
    "name": "aten::floordiv.float(float a, float b) -> float"
  },
  {
    "name": "aten::floordiv.float_int(float a, int b) -> float"
  },
  {
    "name": "aten::floordiv.int(int a, int b) -> int"
  },
  {
    "name": "aten::floordiv.int_float(int a, float b) -> float"
  },
  {
    "name": "aten::fmod(Scalar a, Scalar b) -> float"
  },
  {
    "name": "aten::fmod.Scalar(Tensor self, Scalar other) -> Tensor"
  },
  {
    "name": "aten::fmod.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::fmod.Tensor(Tensor self, Tensor other) -> Tensor"
  },
  {
    "name": "aten::fmod.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::fmod.float(float a, float b) -> float"
  },
  {
    "name": "aten::fmod.float_int(float a, int b) -> float"
  },
  {
    "name": "aten::fmod.int(int a, int b) -> float"
  },
  {
    "name": "aten::fmod.int_float(int a, float b) -> float"
  },
  {
    "name": "aten::format(str self, ...) -> str",
    "is_vararg": true
  },
  {
    "name": "aten::frobenius_norm.dim(Tensor self, int[1] dim, bool keepdim=False) -> Tensor",
    "category": "Normalization"
  },
  {
    "name": "aten::frobenius_norm.out(Tensor self, int[1] dim, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::full(SymInt[] size, Scalar fill_value, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor"
  },
  {
    "name": "aten::full.names(int[] size, Scalar fill_value, *, str[]? names, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor"
  },
  {
    "name": "aten::full.names_out(int[] size, Scalar fill_value, *, str[]? names, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::full.out(SymInt[] size, Scalar fill_value, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::full_like(Tensor self, Scalar fill_value, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor"
  },
  {
    "name": "aten::full_like.out(Tensor self, Scalar fill_value, *, MemoryFormat? memory_format=None, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::fused_moving_avg_obs_fake_quant(Tensor self, Tensor observer_on, Tensor fake_quant_on, Tensor(a!) running_min, Tensor(b!) running_max, Tensor(c!) scale, Tensor(d!) zero_point, float averaging_const, int quant_min, int quant_max, int ch_axis, bool per_row_fake_quant=False, bool symmetric_quant=False) -> Tensor"
  },
  {
    "name": "aten::gather(Tensor self, int dim, Tensor index, *, bool sparse_grad=False) -> Tensor",
    "category": "Transform"
  },
  {
    "name": "aten::gather.dimname(Tensor self, str dim, Tensor index, *, bool sparse_grad=False) -> Tensor",
    "category": "Transform"
  },
  {
    "name": "aten::gather.dimname_out(Tensor self, str dim, Tensor index, *, bool sparse_grad=False, Tensor(a!) out) -> Tensor(a!)",
    "category": "Transform"
  },
  {
    "name": "aten::gather.out(Tensor self, int dim, Tensor index, *, bool sparse_grad=False, Tensor(a!) out) -> Tensor(a!)",
    "category": "Transform"
  },
  {
    "name": "aten::gather_backward(Tensor grad, Tensor self, int dim, Tensor index, bool sparse_grad) -> Tensor"
  },
  {
    "name": "aten::gcd(Tensor self, Tensor other) -> Tensor"
  },
  {
    "name": "aten::gcd.int(int a, int b) -> int"
  },
  {
    "name": "aten::gcd.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::gcd_(Tensor(a!) self, Tensor other) -> Tensor(a!)"
  },
  {
    "name": "aten::ge(Scalar a, Scalar b) -> bool"
  },
  {
    "name": "aten::ge.Scalar(Tensor self, Scalar other) -> Tensor"
  },
  {
    "name": "aten::ge.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::ge.Tensor(Tensor self, Tensor other) -> Tensor"
  },
  {
    "name": "aten::ge.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::ge.float(float a, float b) -> bool"
  },
  {
    "name": "aten::ge.float_int(float a, int b) -> bool"
  },
  {
    "name": "aten::ge.int(int a, int b) -> bool"
  },
  {
    "name": "aten::ge.int_float(int a, float b) -> bool"
  },
  {
    "name": "aten::ge.str(str a, str b) -> bool"
  },
  {
    "name": "aten::ge_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)"
  },
  {
    "name": "aten::ge_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)"
  },
  {
    "name": "aten::gelu(Tensor self, *, str approximate=\"none\") -> Tensor",
    "category": "Activation"
  },
  {
    "name": "aten::gelu.out(Tensor self, *, str approximate=\"none\", Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::gelu_(Tensor(a!) self, *, str approximate=\"none\") -> Tensor(a!)",
    "category": "Activation"
  },
  {
    "name": "aten::gelu_backward(Tensor grad_output, Tensor self, *, str approximate=\"none\") -> Tensor"
  },
  {
    "name": "aten::gelu_backward.grad_input(Tensor grad_output, Tensor self, *, str approximate=\"none\", Tensor(a!) grad_input) -> Tensor(a!)"
  },
  {
    "name": "aten::geometric_(Tensor(a!) self, float p, *, Generator? generator=None) -> Tensor(a!)"
  },
  {
    "name": "aten::geqrf(Tensor self) -> (Tensor a, Tensor tau)"
  },
  {
    "name": "aten::geqrf.a(Tensor self, *, Tensor(a!) a, Tensor(b!) tau) -> (Tensor(a!) a, Tensor(b!) tau)"
  },
  {
    "name": "aten::ger(Tensor self, Tensor vec2) -> Tensor"
  },
  {
    "name": "aten::ger.out(Tensor self, Tensor vec2, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::get.Tensor(Dict(Tensor, t) self, Tensor key) -> t(*)?"
  },
  {
    "name": "aten::get.bool(Dict(bool, t) self, bool key) -> t(*)?"
  },
  {
    "name": "aten::get.complex(Dict(complex, t) self, complex key) -> t(*)?"
  },
  {
    "name": "aten::get.default_Tensor(Dict(Tensor, t) self, Tensor key, t default_value) -> t(*)"
  },
  {
    "name": "aten::get.default_bool(Dict(bool, t) self, bool key, t default_value) -> t(*)"
  },
  {
    "name": "aten::get.default_complex(Dict(complex, t) self, complex key, t default_value) -> t(*)"
  },
  {
    "name": "aten::get.default_float(Dict(float, t) self, float key, t default_value) -> t(*)"
  },
  {
    "name": "aten::get.default_int(Dict(int, t) self, int key, t default_value) -> t(*)"
  },
  {
    "name": "aten::get.default_str(Dict(str, t) self, str key, t default_value) -> t(*)"
  },
  {
    "name": "aten::get.float(Dict(float, t) self, float key) -> t(*)?"
  },
  {
    "name": "aten::get.int(Dict(int, t) self, int key) -> t(*)?"
  },
  {
    "name": "aten::get.str(Dict(str, t) self, str key) -> t(*)?"
  },
  {
    "name": "aten::get_device(Tensor self) -> int"
  },
  {
    "name": "aten::glu(Tensor self, int dim=-1) -> Tensor",
    "category": "Activation"
  },
  {
    "name": "aten::glu.out(Tensor self, int dim=-1, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::greater_equal.Scalar(Tensor self, Scalar other) -> Tensor"
  },
  {
    "name": "aten::greater_equal.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::greater_equal.Tensor(Tensor self, Tensor other) -> Tensor"
  },
  {
    "name": "aten::greater_equal.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::greater_equal_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)"
  },
  {
    "name": "aten::greater_equal_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)"
  },
  {
    "name": "aten::grid_sampler(Tensor input, Tensor grid, int interpolation_mode, int padding_mode, bool align_corners) -> Tensor"
  },
  {
    "name": "aten::grid_sampler.legacy(Tensor input, Tensor grid, int interpolation_mode, int padding_mode) -> Tensor"
  },
  {
    "name": "aten::group_norm(Tensor input, int num_groups, Tensor? weight=None, Tensor? bias=None, float eps=1.0000000000000001e-05, bool cudnn_enabled=True) -> Tensor",
    "category": "Normalization"
  },
  {
    "name": "aten::gru.data(Tensor data, Tensor batch_sizes, Tensor hx, Tensor[] params, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional) -> (Tensor, Tensor)",
    "category": "Layer"
  },
  {
    "name": "aten::gru.input(Tensor input, Tensor hx, Tensor[] params, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional, bool batch_first) -> (Tensor, Tensor)",
    "category": "Layer"
  },
  {
    "name": "aten::gt(Scalar a, Scalar b) -> bool"
  },
  {
    "name": "aten::gt.Scalar(Tensor self, Scalar other) -> Tensor"
  },
  {
    "name": "aten::gt.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::gt.Tensor(Tensor self, Tensor other) -> Tensor"
  },
  {
    "name": "aten::gt.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::gt.float(float a, float b) -> bool"
  },
  {
    "name": "aten::gt.float_int(float a, int b) -> bool"
  },
  {
    "name": "aten::gt.int(int a, int b) -> bool"
  },
  {
    "name": "aten::gt.int_float(int a, float b) -> bool"
  },
  {
    "name": "aten::gt.str(str a, str b) -> bool"
  },
  {
    "name": "aten::hamming_window(int window_length, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor"
  },
  {
    "name": "aten::hamming_window.out(int window_length, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::hamming_window.periodic(int window_length, bool periodic, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor"
  },
  {
    "name": "aten::hamming_window.periodic_alpha(int window_length, bool periodic, float alpha, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor"
  },
  {
    "name": "aten::hamming_window.periodic_alpha_beta(int window_length, bool periodic, float alpha, float beta, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor"
  },
  {
    "name": "aten::hamming_window.periodic_alpha_beta_out(int window_length, bool periodic, float alpha, float beta, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::hamming_window.periodic_alpha_out(int window_length, bool periodic, float alpha, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::hamming_window.periodic_out(int window_length, bool periodic, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::hann_window(int window_length, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor"
  },
  {
    "name": "aten::hann_window.out(int window_length, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::hann_window.periodic(int window_length, bool periodic, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor"
  },
  {
    "name": "aten::hann_window.periodic_out(int window_length, bool periodic, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::hardshrink(Tensor self, Scalar lambd=0.5) -> Tensor"
  },
  {
    "name": "aten::hardshrink.out(Tensor self, Scalar lambd=0.5, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::hardsigmoid(Tensor self) -> Tensor",
    "category": "Activation"
  },
  {
    "name": "aten::hardsigmoid.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::hardsigmoid_(Tensor(a!) self) -> Tensor(a!)",
    "category": "Activation"
  },
  {
    "name": "aten::hardswish(Tensor self) -> Tensor",
    "category": "Activation"
  },
  {
    "name": "aten::hardswish.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::hardswish_(Tensor(a!) self) -> Tensor(a!)",
    "category": "Activation"
  },
  {
    "name": "aten::hardswish_backward(Tensor grad_output, Tensor self) -> Tensor"
  },
  {
    "name": "aten::hardswish_backward.out(Tensor grad_output, Tensor self, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::hardtanh(Tensor self, Scalar min_val=-1, Scalar max_val=1) -> Tensor",
    "category": "Activation"
  },
  {
    "name": "aten::hardtanh.out(Tensor self, Scalar min_val=-1, Scalar max_val=1, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::hardtanh_(Tensor(a!) self, Scalar min_val=-1, Scalar max_val=1) -> Tensor(a!)",
    "category": "Activation"
  },
  {
    "name": "aten::hardtanh_backward(Tensor grad_output, Tensor self, Scalar min_val, Scalar max_val) -> Tensor"
  },
  {
    "name": "aten::hardtanh_backward.grad_input(Tensor grad_output, Tensor self, Scalar min_val, Scalar max_val, *, Tensor(a!) grad_input) -> Tensor(a!)"
  },
  {
    "name": "aten::histc(Tensor self, int bins=100, Scalar min=0, Scalar max=0) -> Tensor"
  },
  {
    "name": "aten::histc.out(Tensor self, int bins=100, Scalar min=0, Scalar max=0, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::hstack(Tensor[] tensors) -> Tensor"
  },
  {
    "name": "aten::hstack.out(Tensor[] tensors, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::huber_loss(Tensor self, Tensor target, int reduction=1, float delta=1.) -> Tensor"
  },
  {
    "name": "aten::huber_loss.out(Tensor self, Tensor target, int reduction=1, float delta=1., *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::huber_loss_backward(Tensor grad_output, Tensor self, Tensor target, int reduction, float delta) -> Tensor"
  },
  {
    "name": "aten::huber_loss_backward.out(Tensor grad_output, Tensor self, Tensor target, int reduction, float delta, *, Tensor(a!) grad_input) -> Tensor(a!)"
  },
  {
    "name": "aten::im2col(Tensor self, int[2] kernel_size, int[2] dilation, int[2] padding, int[2] stride) -> Tensor"
  },
  {
    "name": "aten::im2col.out(Tensor self, int[2] kernel_size, int[2] dilation, int[2] padding, int[2] stride, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::imag(Tensor(a) self) -> Tensor(a)"
  },
  {
    "name": "aten::index.Tensor(Tensor self, Tensor?[] indices) -> Tensor"
  },
  {
    "name": "aten::index.Tensor_hacked_twin(Tensor self, Tensor[] indices) -> Tensor"
  },
  {
    "name": "aten::index.Tensor_out(Tensor self, Tensor?[] indices, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::index.list_Tensor(Tensor[] self, Tensor el) -> int"
  },
  {
    "name": "aten::index.list_bool(bool[] self, bool el) -> int"
  },
  {
    "name": "aten::index.list_float(float[] self, float el) -> int"
  },
  {
    "name": "aten::index.list_int(int[] self, int el) -> int"
  },
  {
    "name": "aten::index.list_str(str[] self, str el) -> int"
  },
  {
    "name": "aten::index.str(str self, str substr, int start=0, int end=-1) -> int"
  },
  {
    "name": "aten::index_add(Tensor self, int dim, Tensor index, Tensor source, *, Scalar alpha=1) -> Tensor"
  },
  {
    "name": "aten::index_add.dimname(Tensor self, str dim, Tensor index, Tensor source, *, Scalar alpha=1) -> Tensor"
  },
  {
    "name": "aten::index_add.out(Tensor self, int dim, Tensor index, Tensor source, *, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::index_add_(Tensor(a!) self, int dim, Tensor index, Tensor source, *, Scalar alpha=1) -> Tensor(a!)"
  },
  {
    "name": "aten::index_copy(Tensor self, int dim, Tensor index, Tensor source) -> Tensor"
  },
  {
    "name": "aten::index_copy.dimname(Tensor self, str dim, Tensor index, Tensor source) -> Tensor"
  },
  {
    "name": "aten::index_copy.out(Tensor self, int dim, Tensor index, Tensor source, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::index_copy_(Tensor(a!) self, int dim, Tensor index, Tensor source) -> Tensor(a!)"
  },
  {
    "name": "aten::index_copy_.dimname(Tensor(a!) self, str dim, Tensor index, Tensor source) -> Tensor(a!)"
  },
  {
    "name": "aten::index_fill.Dimname_Scalar(Tensor self, str dim, Tensor index, Scalar value) -> Tensor"
  },
  {
    "name": "aten::index_fill.Dimname_Tensor(Tensor self, str dim, Tensor index, Tensor value) -> Tensor"
  },
  {
    "name": "aten::index_fill.int_Scalar(Tensor self, int dim, Tensor index, Scalar value) -> Tensor"
  },
  {
    "name": "aten::index_fill.int_Scalar_out(Tensor self, int dim, Tensor index, Scalar value, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::index_fill.int_Tensor(Tensor self, int dim, Tensor index, Tensor value) -> Tensor"
  },
  {
    "name": "aten::index_fill.int_Tensor_out(Tensor self, int dim, Tensor index, Tensor value, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::index_fill_.Dimname_Scalar(Tensor(a!) self, str dim, Tensor index, Scalar value) -> Tensor(a!)"
  },
  {
    "name": "aten::index_fill_.Dimname_Tensor(Tensor(a!) self, str dim, Tensor index, Tensor value) -> Tensor(a!)"
  },
  {
    "name": "aten::index_fill_.int_Scalar(Tensor(a!) self, int dim, Tensor index, Scalar value) -> Tensor(a!)"
  },
  {
    "name": "aten::index_fill_.int_Tensor(Tensor(a!) self, int dim, Tensor index, Tensor value) -> Tensor(a!)"
  },
  {
    "name": "aten::index_put(Tensor self, Tensor?[] indices, Tensor values, bool accumulate=False) -> Tensor"
  },
  {
    "name": "aten::index_put.hacked_twin(Tensor self, Tensor[] indices, Tensor values, bool accumulate=False) -> Tensor"
  },
  {
    "name": "aten::index_put.out(Tensor self, Tensor?[] indices, Tensor values, bool accumulate=False, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::index_put_(Tensor(a!) self, Tensor?[] indices, Tensor values, bool accumulate=False) -> Tensor(a!)"
  },
  {
    "name": "aten::index_put_.hacked_twin(Tensor(a!) self, Tensor[] indices, Tensor values, bool accumulate=False) -> Tensor(a!)"
  },
  {
    "name": "aten::index_reduce(Tensor self, int dim, Tensor index, Tensor source, str reduce, *, bool include_self=True) -> Tensor"
  },
  {
    "name": "aten::index_reduce.out(Tensor self, int dim, Tensor index, Tensor source, str reduce, *, bool include_self=True, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::index_reduce_(Tensor(a!) self, int dim, Tensor index, Tensor source, str reduce, *, bool include_self=True) -> Tensor(a!)"
  },
  {
    "name": "aten::index_select(Tensor self, int dim, Tensor index) -> Tensor"
  },
  {
    "name": "aten::index_select.dimname(Tensor self, str dim, Tensor index) -> Tensor"
  },
  {
    "name": "aten::index_select.dimname_out(Tensor self, str dim, Tensor index, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::index_select.out(Tensor self, int dim, Tensor index, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::index_select_backward(Tensor grad, SymInt[] self_sizes, int dim, Tensor index) -> Tensor"
  },
  {
    "name": "aten::instance_norm(Tensor input, Tensor? weight, Tensor? bias, Tensor? running_mean, Tensor? running_var, bool use_input_stats, float momentum, float eps, bool cudnn_enabled) -> Tensor",
    "category": "Normalization"
  },
  {
    "name": "aten::int_repr(Tensor self) -> Tensor"
  },
  {
    "name": "aten::int_repr.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::inverse(Tensor self) -> Tensor"
  },
  {
    "name": "aten::inverse.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::is_contiguous(Tensor self) -> bool"
  },
  {
    "name": "aten::is_contiguous.memory_format(Tensor self, MemoryFormat memory_format) -> bool"
  },
  {
    "name": "aten::is_floating_point(Tensor self) -> bool"
  },
  {
    "name": "aten::isfinite(Tensor self) -> Tensor"
  },
  {
    "name": "aten::isfinite.complex(complex a) -> bool"
  },
  {
    "name": "aten::isfinite.float(float a) -> bool"
  },
  {
    "name": "aten::isinf(Tensor self) -> Tensor"
  },
  {
    "name": "aten::isinf.complex(complex a) -> bool"
  },
  {
    "name": "aten::isinf.float(float a) -> bool"
  },
  {
    "name": "aten::isinf.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::isnan(Tensor self) -> Tensor"
  },
  {
    "name": "aten::isnan.complex(complex a) -> bool"
  },
  {
    "name": "aten::isnan.float(float a) -> bool"
  },
  {
    "name": "aten::isnan.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::istft(Tensor self, int n_fft, int? hop_length=None, int? win_length=None, Tensor? window=None, bool center=True, bool normalized=False, bool? onesided=None, int? length=None, bool return_complex=False) -> Tensor"
  },
  {
    "name": "aten::item(Tensor self) -> Scalar"
  },
  {
    "name": "aten::items.Tensor(Dict(Tensor, t) self) -> ((Tensor, t)[])"
  },
  {
    "name": "aten::items.bool(Dict(bool, t) self) -> ((bool, t)[])"
  },
  {
    "name": "aten::items.complex(Dict(complex, t) self) -> ((complex, t)[])"
  },
  {
    "name": "aten::items.float(Dict(float, t) self) -> ((float, t)[])"
  },
  {
    "name": "aten::items.int(Dict(int, t) self) -> ((int, t)[])"
  },
  {
    "name": "aten::items.str(Dict(str, t) self) -> ((str, t)[])"
  },
  {
    "name": "aten::join(str self, str[] values) -> str"
  },
  {
    "name": "aten::keys.Tensor(Dict(Tensor, t) self) -> Tensor[](*)"
  },
  {
    "name": "aten::keys.bool(Dict(bool, t) self) -> bool[](*)"
  },
  {
    "name": "aten::keys.complex(Dict(complex, t) self) -> complex[](*)"
  },
  {
    "name": "aten::keys.float(Dict(float, t) self) -> float[](*)"
  },
  {
    "name": "aten::keys.int(Dict(int, t) self) -> int[](*)"
  },
  {
    "name": "aten::keys.str(Dict(str, t) self) -> str[](*)"
  },
  {
    "name": "aten::kl_div(Tensor self, Tensor target, int reduction=1, *, bool log_target=False) -> Tensor"
  },
  {
    "name": "aten::kthvalue(Tensor self, int k, int dim=-1, bool keepdim=False) -> (Tensor values, Tensor indices)"
  },
  {
    "name": "aten::kthvalue.dimname(Tensor self, int k, str dim, bool keepdim=False) -> (Tensor values, Tensor indices)"
  },
  {
    "name": "aten::kthvalue.dimname_out(Tensor self, int k, str dim, bool keepdim=False, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)"
  },
  {
    "name": "aten::kthvalue.values(Tensor self, int k, int dim=-1, bool keepdim=False, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)"
  },
  {
    "name": "aten::l1_loss(Tensor self, Tensor target, int reduction=1) -> Tensor"
  },
  {
    "name": "aten::layer_norm(Tensor input, SymInt[] normalized_shape, Tensor? weight=None, Tensor? bias=None, float eps=1.0000000000000001e-05, bool cudnn_enable=True) -> Tensor",
    "category": "Normalization"
  },
  {
    "name": "aten::le(Scalar a, Scalar b) -> bool"
  },
  {
    "name": "aten::le.Scalar(Tensor self, Scalar other) -> Tensor"
  },
  {
    "name": "aten::le.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::le.Tensor(Tensor self, Tensor other) -> Tensor"
  },
  {
    "name": "aten::le.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::le.float(float a, float b) -> bool"
  },
  {
    "name": "aten::le.float_int(float a, int b) -> bool"
  },
  {
    "name": "aten::le.int(int a, int b) -> bool"
  },
  {
    "name": "aten::le.int_float(int a, float b) -> bool"
  },
  {
    "name": "aten::le.str(str a, str b) -> bool"
  },
  {
    "name": "aten::leaky_relu(Tensor self, Scalar negative_slope=0.01) -> Tensor",
    "category": "Activation"
  },
  {
    "name": "aten::leaky_relu.out(Tensor self, Scalar negative_slope=0.01, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::leaky_relu_(Tensor(a!) self, Scalar negative_slope=0.01) -> Tensor(a!)",
    "category": "Activation"
  },
  {
    "name": "aten::len.Dict_Tensor(Dict(Tensor, t) self) -> int"
  },
  {
    "name": "aten::len.Dict_bool(Dict(bool, t) self) -> int"
  },
  {
    "name": "aten::len.Dict_complex(Dict(complex, t) self) -> int"
  },
  {
    "name": "aten::len.Dict_float(Dict(float, t) self) -> int"
  },
  {
    "name": "aten::len.Dict_int(Dict(int, t) self) -> int"
  },
  {
    "name": "aten::len.Dict_str(Dict(str, t) self) -> int"
  },
  {
    "name": "aten::len.Tensor(Tensor t) -> int"
  },
  {
    "name": "aten::len.any(Any[] a) -> int"
  },
  {
    "name": "aten::len.str(str s) -> int"
  },
  {
    "name": "aten::len.t(t[] a) -> int"
  },
  {
    "name": "aten::lerp.Scalar(Tensor self, Tensor end, Scalar weight) -> Tensor"
  },
  {
    "name": "aten::lerp.Scalar_out(Tensor self, Tensor end, Scalar weight, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::lerp.Tensor(Tensor self, Tensor end, Tensor weight) -> Tensor"
  },
  {
    "name": "aten::lerp.Tensor_out(Tensor self, Tensor end, Tensor weight, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::lift_fresh(Tensor(a) self) -> Tensor(a)"
  },
  {
    "name": "aten::lift_fresh_copy(Tensor self) -> Tensor"
  },
  {
    "name": "aten::lift_fresh_copy.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::linalg_cross(Tensor self, Tensor other, *, int dim=-1) -> Tensor"
  },
  {
    "name": "aten::linalg_cross.out(Tensor self, Tensor other, *, int dim=-1, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::linalg_inv(Tensor A) -> Tensor"
  },
  {
    "name": "aten::linalg_inv.out(Tensor A, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::linalg_inv_ex(Tensor A, *, bool check_errors=False) -> (Tensor inverse, Tensor info)"
  },
  {
    "name": "aten::linalg_inv_ex.inverse(Tensor A, *, bool check_errors=False, Tensor(a!) inverse, Tensor(b!) info) -> (Tensor(a!) inverse, Tensor(b!) info)"
  },
  {
    "name": "aten::linalg_norm(Tensor self, Scalar? ord=None, int[1]? dim=None, bool keepdim=False, *, ScalarType? dtype=None) -> Tensor"
  },
  {
    "name": "aten::linalg_norm.ord_str(Tensor self, str ord, int[1]? dim=None, bool keepdim=False, *, ScalarType? dtype=None) -> Tensor"
  },
  {
    "name": "aten::linalg_norm.ord_str_out(Tensor self, str ord, int[1]? dim=None, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::linalg_norm.out(Tensor self, Scalar? ord=None, int[1]? dim=None, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::linalg_qr(Tensor A, str mode=\"reduced\") -> (Tensor Q, Tensor R)"
  },
  {
    "name": "aten::linalg_qr.out(Tensor A, str mode=\"reduced\", *, Tensor(a!) Q, Tensor(b!) R) -> (Tensor(a!) Q, Tensor(b!) R)"
  },
  {
    "name": "aten::linalg_slogdet(Tensor A) -> (Tensor sign, Tensor logabsdet)"
  },
  {
    "name": "aten::linalg_slogdet.out(Tensor A, *, Tensor(a!) sign, Tensor(b!) logabsdet) -> (Tensor(a!) sign, Tensor(b!) logabsdet)"
  },
  {
    "name": "aten::linalg_solve(Tensor A, Tensor B, *, bool left=True) -> Tensor"
  },
  {
    "name": "aten::linalg_solve.out(Tensor A, Tensor B, *, bool left=True, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::linalg_solve_ex(Tensor A, Tensor B, *, bool left=True, bool check_errors=False) -> (Tensor result, Tensor info)"
  },
  {
    "name": "aten::linalg_solve_ex.out(Tensor A, Tensor B, *, bool left=True, bool check_errors=False, Tensor(a!) result, Tensor(b!) info) -> (Tensor(a!) result, Tensor(b!) info)"
  },
  {
    "name": "aten::linalg_solve_triangular(Tensor self, Tensor B, *, bool upper, bool left=True, bool unitriangular=False) -> Tensor"
  },
  {
    "name": "aten::linalg_solve_triangular.out(Tensor self, Tensor B, *, bool upper, bool left=True, bool unitriangular=False, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::linalg_svd(Tensor A, bool full_matrices=True, *, str? driver=None) -> (Tensor U, Tensor S, Tensor Vh)"
  },
  {
    "name": "aten::linalg_svd.U(Tensor A, bool full_matrices=True, *, str? driver=None, Tensor(a!) U, Tensor(b!) S, Tensor(c!) Vh) -> (Tensor(a!) U, Tensor(b!) S, Tensor(c!) Vh)"
  },
  {
    "name": "aten::linalg_tensorinv(Tensor self, int ind=2) -> Tensor"
  },
  {
    "name": "aten::linalg_tensorinv.out(Tensor self, int ind=2, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::linalg_tensorsolve(Tensor self, Tensor other, int[]? dims=None) -> Tensor"
  },
  {
    "name": "aten::linalg_tensorsolve.out(Tensor self, Tensor other, int[]? dims=None, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::linalg_vector_norm(Tensor self, Scalar ord=2, int[1]? dim=None, bool keepdim=False, *, ScalarType? dtype=None) -> Tensor"
  },
  {
    "name": "aten::linalg_vector_norm.out(Tensor self, Scalar ord=2, int[1]? dim=None, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::linear(Tensor input, Tensor weight, Tensor? bias=None) -> Tensor",
    "category": "Layer"
  },
  {
    "name": "aten::linear.out(Tensor input, Tensor weight, Tensor? bias=None, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::linear_backward(Tensor self, Tensor grad_output, Tensor weight, bool[3] output_mask) -> (Tensor, Tensor, Tensor)"
  },
  {
    "name": "aten::linear_backward.out(Tensor self, Tensor grad_output, Tensor weight, bool[3] output_mask, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2) -> (Tensor(a!), Tensor(b!), Tensor(c!))"
  },
  {
    "name": "aten::linspace(Scalar start, Scalar end, int steps, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor"
  },
  {
    "name": "aten::linspace.Scalar_Tensor(Scalar start, Tensor end, int steps, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor"
  },
  {
    "name": "aten::linspace.Scalar_Tensor_out(Scalar start, Tensor end, int steps, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::linspace.Tensor_Scalar(Tensor start, Scalar end, int steps, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor"
  },
  {
    "name": "aten::linspace.Tensor_Scalar_out(Tensor start, Scalar end, int steps, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::linspace.Tensor_Tensor(Tensor start, Tensor end, int steps, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor"
  },
  {
    "name": "aten::linspace.Tensor_Tensor_out(Tensor start, Tensor end, int steps, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::linspace.out(Scalar start, Scalar end, int steps, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::list(str t) -> str[]"
  },
  {
    "name": "aten::list.t(t[] l) -> t[]"
  },
  {
    "name": "aten::log(Tensor self) -> Tensor"
  },
  {
    "name": "aten::log.Scalar(Scalar a) -> Scalar"
  },
  {
    "name": "aten::log.Scalar_Scalar(Scalar a, Scalar b) -> float"
  },
  {
    "name": "aten::log.complex(complex a) -> complex"
  },
  {
    "name": "aten::log.complex_complex(complex a, complex b) -> complex"
  },
  {
    "name": "aten::log.complex_float(complex a, float b) -> complex"
  },
  {
    "name": "aten::log.complex_int(complex a, int b) -> complex"
  },
  {
    "name": "aten::log.float(float a) -> float"
  },
  {
    "name": "aten::log.float_complex(float a, complex b) -> complex"
  },
  {
    "name": "aten::log.float_float(float a, float b) -> float"
  },
  {
    "name": "aten::log.float_int(float a, int b) -> float"
  },
  {
    "name": "aten::log.int(int a) -> float"
  },
  {
    "name": "aten::log.int_complex(int a, complex b) -> complex"
  },
  {
    "name": "aten::log.int_float(int a, float b) -> float"
  },
  {
    "name": "aten::log.int_int(int a, int b) -> float"
  },
  {
    "name": "aten::log.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::log10(Tensor self) -> Tensor"
  },
  {
    "name": "aten::log10.Scalar(Scalar a) -> Scalar"
  },
  {
    "name": "aten::log10.complex(complex a) -> complex"
  },
  {
    "name": "aten::log10.float(float a) -> float"
  },
  {
    "name": "aten::log10.int(int a) -> float"
  },
  {
    "name": "aten::log10.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::log10_(Tensor(a!) self) -> Tensor(a!)"
  },
  {
    "name": "aten::log1p(Tensor self) -> Tensor"
  },
  {
    "name": "aten::log1p.Scalar(Scalar a) -> Scalar"
  },
  {
    "name": "aten::log1p.float(float a) -> float"
  },
  {
    "name": "aten::log1p.int(int a) -> float"
  },
  {
    "name": "aten::log1p.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::log1p_(Tensor(a!) self) -> Tensor(a!)"
  },
  {
    "name": "aten::log2(Tensor self) -> Tensor"
  },
  {
    "name": "aten::log2.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::log2_(Tensor(a!) self) -> Tensor(a!)"
  },
  {
    "name": "aten::log_(Tensor(a!) self) -> Tensor(a!)"
  },
  {
    "name": "aten::log_normal_(Tensor(a!) self, float mean=1., float std=2., *, Generator? generator=None) -> Tensor(a!)"
  },
  {
    "name": "aten::log_sigmoid(Tensor self) -> Tensor"
  },
  {
    "name": "aten::log_sigmoid.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::log_sigmoid_backward(Tensor grad_output, Tensor self, Tensor buffer) -> Tensor"
  },
  {
    "name": "aten::log_sigmoid_backward.grad_input(Tensor grad_output, Tensor self, Tensor buffer, *, Tensor(a!) grad_input) -> Tensor(a!)"
  },
  {
    "name": "aten::log_sigmoid_forward(Tensor self) -> (Tensor output, Tensor buffer)"
  },
  {
    "name": "aten::log_sigmoid_forward.output(Tensor self, *, Tensor(a!) output, Tensor(b!) buffer) -> (Tensor(a!), Tensor(b!))"
  },
  {
    "name": "aten::log_softmax.Dimname(Tensor self, str dim, *, ScalarType? dtype=None) -> Tensor",
    "category": "Activation"
  },
  {
    "name": "aten::log_softmax.int(Tensor self, int dim, ScalarType? dtype=None) -> Tensor",
    "category": "Activation"
  },
  {
    "name": "aten::log_softmax.int_out(Tensor self, int dim, ScalarType? dtype=None, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::logaddexp(Tensor self, Tensor other) -> Tensor"
  },
  {
    "name": "aten::logaddexp.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::logaddexp2(Tensor self, Tensor other) -> Tensor"
  },
  {
    "name": "aten::logaddexp2.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::logcumsumexp(Tensor self, int dim) -> Tensor"
  },
  {
    "name": "aten::logcumsumexp.dimname(Tensor self, str dim) -> Tensor"
  },
  {
    "name": "aten::logcumsumexp.dimname_out(Tensor self, str dim, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::logcumsumexp.out(Tensor self, int dim, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::logdet(Tensor self) -> Tensor"
  },
  {
    "name": "aten::logical_and(Tensor self, Tensor other) -> Tensor"
  },
  {
    "name": "aten::logical_and.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::logical_and_(Tensor(a!) self, Tensor other) -> Tensor(a!)"
  },
  {
    "name": "aten::logical_not(Tensor self) -> Tensor"
  },
  {
    "name": "aten::logical_not.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::logical_not_(Tensor(a!) self) -> Tensor(a!)"
  },
  {
    "name": "aten::logical_or(Tensor self, Tensor other) -> Tensor"
  },
  {
    "name": "aten::logical_or.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::logical_or_(Tensor(a!) self, Tensor other) -> Tensor(a!)"
  },
  {
    "name": "aten::logical_xor(Tensor self, Tensor other) -> Tensor"
  },
  {
    "name": "aten::logical_xor.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::logical_xor_(Tensor(a!) self, Tensor other) -> Tensor(a!)"
  },
  {
    "name": "aten::logit(Tensor self, float? eps=None) -> Tensor"
  },
  {
    "name": "aten::logit.out(Tensor self, float? eps=None, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::logit_(Tensor(a!) self, float? eps=None) -> Tensor(a!)"
  },
  {
    "name": "aten::logit_backward(Tensor grad_output, Tensor self, float? eps=None) -> Tensor"
  },
  {
    "name": "aten::logit_backward.grad_input(Tensor grad_output, Tensor self, float? eps=None, *, Tensor(a!) grad_input) -> Tensor(a!)"
  },
  {
    "name": "aten::logspace(Scalar start, Scalar end, int steps, float base=10., *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor"
  },
  {
    "name": "aten::logspace.Scalar_Tensor(Scalar start, Tensor end, int steps, float base=10., *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor"
  },
  {
    "name": "aten::logspace.Scalar_Tensor_out(Scalar start, Tensor end, int steps, float base=10., *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::logspace.Tensor_Scalar(Tensor start, Scalar end, int steps, float base=10., *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor"
  },
  {
    "name": "aten::logspace.Tensor_Scalar_out(Tensor start, Scalar end, int steps, float base=10., *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::logspace.Tensor_Tensor(Tensor start, Tensor end, int steps, float base=10., *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor"
  },
  {
    "name": "aten::logspace.Tensor_Tensor_out(Tensor start, Tensor end, int steps, float base=10., *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::logspace.out(Scalar start, Scalar end, int steps, float base=10., *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::logsumexp(Tensor self, int[1] dim, bool keepdim=False) -> Tensor"
  },
  {
    "name": "aten::logsumexp.names(Tensor self, str[1] dim, bool keepdim=False) -> Tensor"
  },
  {
    "name": "aten::logsumexp.names_out(Tensor self, str[1] dim, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::logsumexp.out(Tensor self, int[1] dim, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::lstm.data(Tensor data, Tensor batch_sizes, Tensor[] hx, Tensor[] params, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional) -> (Tensor, Tensor, Tensor)",
    "category": "Layer"
  },
  {
    "name": "aten::lstm.input(Tensor input, Tensor[] hx, Tensor[] params, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional, bool batch_first) -> (Tensor, Tensor, Tensor)",
    "category": "Layer"
  },
  {
    "name": "aten::lstm_cell(Tensor input, Tensor[] hx, Tensor w_ih, Tensor w_hh, Tensor? b_ih=None, Tensor? b_hh=None) -> (Tensor, Tensor)",
    "category": "Layer"
  },
  {
    "name": "aten::lstrip(str self, str chars=\" \\n\\t\\f\\v\") -> str"
  },
  {
    "name": "aten::lt(Scalar a, Scalar b) -> bool"
  },
  {
    "name": "aten::lt.Scalar(Tensor self, Scalar other) -> Tensor"
  },
  {
    "name": "aten::lt.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::lt.Tensor(Tensor self, Tensor other) -> Tensor"
  },
  {
    "name": "aten::lt.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::lt.float(float a, float b) -> bool"
  },
  {
    "name": "aten::lt.float_int(float a, int b) -> bool"
  },
  {
    "name": "aten::lt.int(int a, int b) -> bool"
  },
  {
    "name": "aten::lt.int_float(int a, float b) -> bool"
  },
  {
    "name": "aten::lt.str(str a, str b) -> bool"
  },
  {
    "name": "aten::manual_seed(int seed) -> ()"
  },
  {
    "name": "aten::manual_seed.generator(Generator(a!) self, int seed) -> Generator(a!)"
  },
  {
    "name": "aten::masked_fill.Scalar(Tensor self, Tensor mask, Scalar value) -> Tensor"
  },
  {
    "name": "aten::masked_fill.Scalar_out(Tensor self, Tensor mask, Scalar value, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::masked_fill.Tensor(Tensor self, Tensor mask, Tensor value) -> Tensor"
  },
  {
    "name": "aten::masked_fill.Tensor_out(Tensor self, Tensor mask, Tensor value, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::masked_fill_.Scalar(Tensor(a!) self, Tensor mask, Scalar value) -> Tensor(a!)"
  },
  {
    "name": "aten::masked_fill_.Tensor(Tensor(a!) self, Tensor mask, Tensor value) -> Tensor(a!)"
  },
  {
    "name": "aten::masked_scatter(Tensor self, Tensor mask, Tensor source) -> Tensor"
  },
  {
    "name": "aten::masked_scatter.out(Tensor self, Tensor mask, Tensor source, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::masked_scatter_(Tensor(a!) self, Tensor mask, Tensor source) -> Tensor(a!)"
  },
  {
    "name": "aten::masked_select(Tensor self, Tensor mask) -> Tensor"
  },
  {
    "name": "aten::masked_select.out(Tensor self, Tensor mask, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::matmul(Tensor self, Tensor other) -> Tensor"
  },
  {
    "name": "aten::matmul.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::max(Tensor self) -> Tensor"
  },
  {
    "name": "aten::max.dim(Tensor self, int dim, bool keepdim=False) -> (Tensor values, Tensor indices)"
  },
  {
    "name": "aten::max.dim_max(Tensor self, int dim, bool keepdim=False, *, Tensor(a!) max, Tensor(b!) max_values) -> (Tensor(a!) values, Tensor(b!) indices)"
  },
  {
    "name": "aten::max.names_dim(Tensor self, str dim, bool keepdim=False) -> (Tensor values, Tensor indices)"
  },
  {
    "name": "aten::max.names_dim_max(Tensor self, str dim, bool keepdim=False, *, Tensor(a!) max, Tensor(b!) max_values) -> (Tensor(a!) values, Tensor(b!) indices)"
  },
  {
    "name": "aten::max.other(Tensor self, Tensor other) -> Tensor"
  },
  {
    "name": "aten::max.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::max.unary_out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::max_pool1d(Tensor self, int[1] kernel_size, int[1] stride=[], int[1] padding=[0], int[1] dilation=[1], bool ceil_mode=False) -> Tensor",
    "category": "Pool"
  },
  {
    "name": "aten::max_pool1d_with_indices(Tensor self, int[1] kernel_size, int[1] stride=[], int[1] padding=[0], int[1] dilation=[1], bool ceil_mode=False) -> (Tensor, Tensor)",
    "category": "Pool"
  },
  {
    "name": "aten::max_pool2d(Tensor self, int[2] kernel_size, int[2] stride=[], int[2] padding=0, int[2] dilation=1, bool ceil_mode=False) -> Tensor",
    "category": "Pool"
  },
  {
    "name": "aten::max_pool2d_with_indices(Tensor self, int[2] kernel_size, int[2] stride=[], int[2] padding=0, int[2] dilation=1, bool ceil_mode=False) -> (Tensor, Tensor)",
    "category": "Pool"
  },
  {
    "name": "aten::max_pool2d_with_indices.out(Tensor self, int[2] kernel_size, int[2] stride=[], int[2] padding=0, int[2] dilation=1, bool ceil_mode=False, *, Tensor(a!) out, Tensor(b!) indices) -> (Tensor(a!), Tensor(b!))"
  },
  {
    "name": "aten::max_pool3d(Tensor self, int[3] kernel_size, int[3] stride=[], int[3] padding=0, int[3] dilation=1, bool ceil_mode=False) -> Tensor",
    "category": "Pool"
  },
  {
    "name": "aten::max_pool3d_with_indices(Tensor self, int[3] kernel_size, int[3] stride=[], int[3] padding=0, int[3] dilation=1, bool ceil_mode=False) -> (Tensor, Tensor)"
  },
  {
    "name": "aten::max_pool3d_with_indices.out(Tensor self, int[3] kernel_size, int[3] stride=[], int[3] padding=0, int[3] dilation=1, bool ceil_mode=False, *, Tensor(a!) out, Tensor(b!) indices) -> (Tensor(a!), Tensor(b!))"
  },
  {
    "name": "aten::max_pool3d_with_indices_backward(Tensor grad_output, Tensor self, int[3] kernel_size, int[3] stride, int[3] padding, int[3] dilation, bool ceil_mode, Tensor indices) -> Tensor"
  },
  {
    "name": "aten::max_pool3d_with_indices_backward.grad_input(Tensor grad_output, Tensor self, int[3] kernel_size, int[3] stride, int[3] padding, int[3] dilation, bool ceil_mode, Tensor indices, *, Tensor(a!) grad_input) -> Tensor(a!)"
  },
  {
    "name": "aten::max_unpool2d(Tensor self, Tensor indices, SymInt[2] output_size) -> Tensor",
    "category": "Pool"
  },
  {
    "name": "aten::max_unpool2d.out(Tensor self, Tensor indices, SymInt[2] output_size, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::max_unpool3d(Tensor self, Tensor indices, SymInt[3] output_size, int[3] stride, int[3] padding) -> Tensor",
    "category": "Pool"
  },
  {
    "name": "aten::max_unpool3d.out(Tensor self, Tensor indices, SymInt[3] output_size, int[3] stride, int[3] padding, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::maximum(Tensor self, Tensor other) -> Tensor"
  },
  {
    "name": "aten::maximum.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::mean(Tensor self, *, ScalarType? dtype=None) -> Tensor"
  },
  {
    "name": "aten::mean.dim(Tensor self, int[1]? dim, bool keepdim=False, *, ScalarType? dtype=None) -> Tensor"
  },
  {
    "name": "aten::mean.dtype_out(Tensor self, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::mean.names_dim(Tensor self, str[1] dim, bool keepdim=False, *, ScalarType? dtype=None) -> Tensor"
  },
  {
    "name": "aten::mean.names_out(Tensor self, str[1] dim, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::mean.out(Tensor self, int[1]? dim, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::median(Tensor self) -> Tensor"
  },
  {
    "name": "aten::median.dim(Tensor self, int dim, bool keepdim=False) -> (Tensor values, Tensor indices)"
  },
  {
    "name": "aten::median.dim_values(Tensor self, int dim, bool keepdim=False, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)"
  },
  {
    "name": "aten::median.names_dim(Tensor self, str dim, bool keepdim=False) -> (Tensor values, Tensor indices)"
  },
  {
    "name": "aten::median.names_dim_values(Tensor self, str dim, bool keepdim=False, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)"
  },
  {
    "name": "aten::median.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::meshgrid(Tensor[] tensors) -> Tensor[]",
    "category": "Tensor"
  },
  {
    "name": "aten::meshgrid.indexing(Tensor[] tensors, *, str indexing) -> Tensor[]",
    "category": "Tensor"
  },
  {
    "name": "aten::min(Tensor self) -> Tensor"
  },
  {
    "name": "aten::min.dim(Tensor self, int dim, bool keepdim=False) -> (Tensor values, Tensor indices)"
  },
  {
    "name": "aten::min.dim_min(Tensor self, int dim, bool keepdim=False, *, Tensor(a!) min, Tensor(b!) min_indices) -> (Tensor(a!) values, Tensor(b!) indices)"
  },
  {
    "name": "aten::min.names_dim(Tensor self, str dim, bool keepdim=False) -> (Tensor values, Tensor indices)"
  },
  {
    "name": "aten::min.names_dim_min(Tensor self, str dim, bool keepdim=False, *, Tensor(a!) min, Tensor(b!) min_indices) -> (Tensor(a!) values, Tensor(b!) indices)"
  },
  {
    "name": "aten::min.other(Tensor self, Tensor other) -> Tensor"
  },
  {
    "name": "aten::min.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::min.unary_out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::minimum(Tensor self, Tensor other) -> Tensor"
  },
  {
    "name": "aten::minimum.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::mish(Tensor self) -> Tensor",
    "category": "Activation"
  },
  {
    "name": "aten::mish.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::mish_(Tensor(a!) self) -> Tensor(a!)",
    "category": "Activation"
  },
  {
    "name": "aten::mkldnn_reorder_conv2d_weight(Tensor self, SymInt[2] padding=[0, 0], SymInt[2] stride=[1, 1], SymInt[2] dilation=[1, 1], SymInt groups=1, SymInt[]? input_size=None) -> Tensor"
  },
  {
    "name": "aten::mkldnn_reorder_conv2d_weight.out(Tensor self, SymInt[2] padding=[0, 0], SymInt[2] stride=[1, 1], SymInt[2] dilation=[1, 1], SymInt groups=1, SymInt[]? input_size=None, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::mm(Tensor self, Tensor mat2) -> Tensor"
  },
  {
    "name": "aten::mm.out(Tensor self, Tensor mat2, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::moveaxis.int(Tensor(a) self, int source, int destination) -> Tensor(a)"
  },
  {
    "name": "aten::moveaxis.intlist(Tensor(a) self, int[] source, int[] destination) -> Tensor(a)"
  },
  {
    "name": "aten::movedim.int(Tensor(a) self, int source, int destination) -> Tensor(a)"
  },
  {
    "name": "aten::movedim.intlist(Tensor(a) self, int[] source, int[] destination) -> Tensor(a)"
  },
  {
    "name": "aten::mse_loss(Tensor self, Tensor target, int reduction=1) -> Tensor"
  },
  {
    "name": "aten::mse_loss.out(Tensor self, Tensor target, int reduction=1, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::mse_loss_backward(Tensor grad_output, Tensor self, Tensor target, int reduction) -> Tensor"
  },
  {
    "name": "aten::mse_loss_backward.grad_input(Tensor grad_output, Tensor self, Tensor target, int reduction, *, Tensor(a!) grad_input) -> Tensor(a!)"
  },
  {
    "name": "aten::mul(Scalar a, Scalar b) -> Scalar"
  },
  {
    "name": "aten::mul.Scalar(Tensor self, Scalar other) -> Tensor"
  },
  {
    "name": "aten::mul.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::mul.Tensor(Tensor self, Tensor other) -> Tensor"
  },
  {
    "name": "aten::mul.complex(complex a, complex b) -> complex"
  },
  {
    "name": "aten::mul.complex_float(complex a, float b) -> complex"
  },
  {
    "name": "aten::mul.complex_int(complex a, int b) -> complex"
  },
  {
    "name": "aten::mul.float(float a, float b) -> float"
  },
  {
    "name": "aten::mul.float_complex(float a, complex b) -> complex"
  },
  {
    "name": "aten::mul.float_int(float a, int b) -> float"
  },
  {
    "name": "aten::mul.int(int a, int b) -> int"
  },
  {
    "name": "aten::mul.int_complex(int a, complex b) -> complex"
  },
  {
    "name": "aten::mul.int_float(int a, float b) -> float"
  },
  {
    "name": "aten::mul.left_t(t[] l, int n) -> t[]"
  },
  {
    "name": "aten::mul.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::mul.right_(int n, t[] l) -> t[]"
  },
  {
    "name": "aten::mul_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)"
  },
  {
    "name": "aten::mul_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)"
  },
  {
    "name": "aten::mul_.t(t[](a!) l, int n) -> t[](a!)"
  },
  {
    "name": "aten::multinomial(Tensor self, int num_samples, bool replacement=False, *, Generator? generator=None) -> Tensor"
  },
  {
    "name": "aten::multinomial.out(Tensor self, int num_samples, bool replacement=False, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::multiply.Scalar(Tensor self, Scalar other) -> Tensor"
  },
  {
    "name": "aten::multiply.Tensor(Tensor self, Tensor other) -> Tensor"
  },
  {
    "name": "aten::multiply.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::multiply_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)"
  },
  {
    "name": "aten::multiply_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)"
  },
  {
    "name": "aten::mv(Tensor self, Tensor vec) -> Tensor"
  },
  {
    "name": "aten::mv.out(Tensor self, Tensor vec, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::mvlgamma(Tensor self, int p) -> Tensor"
  },
  {
    "name": "aten::mvlgamma.out(Tensor self, int p, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::mvlgamma_(Tensor(a!) self, int p) -> Tensor(a!)"
  },
  {
    "name": "aten::nan_to_num(Tensor self, float? nan=None, float? posinf=None, float? neginf=None) -> Tensor"
  },
  {
    "name": "aten::nan_to_num.out(Tensor self, float? nan=None, float? posinf=None, float? neginf=None, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::nan_to_num_(Tensor(a!) self, float? nan=None, float? posinf=None, float? neginf=None) -> Tensor(a!)"
  },
  {
    "name": "aten::narrow(Tensor(a) self, int dim, SymInt start, SymInt length) -> Tensor(a)"
  },
  {
    "name": "aten::narrow.Tensor(Tensor(a) self, int dim, Tensor start, SymInt length) -> Tensor(a)"
  },
  {
    "name": "aten::narrow_copy(Tensor self, int dim, SymInt start, SymInt length) -> Tensor"
  },
  {
    "name": "aten::narrow_copy.out(Tensor self, int dim, SymInt start, SymInt length, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::native_layer_norm(Tensor input, SymInt[] normalized_shape, Tensor? weight, Tensor? bias, float eps) -> (Tensor, Tensor, Tensor)",
    "category": "Normalization"
  },
  {
    "name": "aten::native_layer_norm.out(Tensor input, SymInt[] normalized_shape, Tensor? weight, Tensor? bias, float eps, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2) -> (Tensor(a!), Tensor(b!), Tensor(c!))"
  },
  {
    "name": "aten::ne(Scalar a, Scalar b) -> bool"
  },
  {
    "name": "aten::ne.Scalar(Tensor self, Scalar other) -> Tensor"
  },
  {
    "name": "aten::ne.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::ne.Tensor(Tensor self, Tensor other) -> Tensor"
  },
  {
    "name": "aten::ne.Tensor_list(Tensor[] a, Tensor[] b) -> bool"
  },
  {
    "name": "aten::ne.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::ne.bool(bool a, bool b) -> bool"
  },
  {
    "name": "aten::ne.bool_list(bool[] a, bool[] b) -> bool"
  },
  {
    "name": "aten::ne.complex(complex a, complex b) -> bool"
  },
  {
    "name": "aten::ne.complex_float(complex a, float b) -> bool"
  },
  {
    "name": "aten::ne.device(Device a, Device b) -> bool"
  },
  {
    "name": "aten::ne.enum(AnyEnumType a, AnyEnumType b) -> bool"
  },
  {
    "name": "aten::ne.float(float a, float b) -> bool"
  },
  {
    "name": "aten::ne.float_complex(float a, complex b) -> bool"
  },
  {
    "name": "aten::ne.float_int(float a, int b) -> bool"
  },
  {
    "name": "aten::ne.float_list(float[] a, float[] b) -> bool"
  },
  {
    "name": "aten::ne.int(int a, int b) -> bool"
  },
  {
    "name": "aten::ne.int_float(int a, float b) -> bool"
  },
  {
    "name": "aten::ne.int_list(int[] a, int[] b) -> bool"
  },
  {
    "name": "aten::ne.str(str a, str b) -> bool"
  },
  {
    "name": "aten::ne.str_list(str[] a, str[] b) -> bool"
  },
  {
    "name": "aten::neg(Tensor self) -> Tensor"
  },
  {
    "name": "aten::neg.Scalar(Scalar a) -> Scalar"
  },
  {
    "name": "aten::neg.complex(complex a) -> complex"
  },
  {
    "name": "aten::neg.float(float a) -> float"
  },
  {
    "name": "aten::neg.int(int a) -> int"
  },
  {
    "name": "aten::neg.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::nested_to_padded_tensor(Tensor self, float padding, int[]? output_size=None) -> Tensor"
  },
  {
    "name": "aten::new_empty(Tensor self, SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor"
  },
  {
    "name": "aten::new_empty.out(Tensor self, SymInt[] size, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::new_empty_strided(Tensor self, SymInt[] size, SymInt[] stride, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor"
  },
  {
    "name": "aten::new_empty_strided.out(Tensor self, SymInt[] size, SymInt[] stride, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::new_full(Tensor self, SymInt[] size, Scalar fill_value, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor"
  },
  {
    "name": "aten::new_full.out(Tensor self, SymInt[] size, Scalar fill_value, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::new_ones(Tensor self, SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor"
  },
  {
    "name": "aten::new_ones.out(Tensor self, SymInt[] size, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::new_zeros(Tensor self, SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor"
  },
  {
    "name": "aten::new_zeros.out(Tensor self, SymInt[] size, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::nll_loss(Tensor self, Tensor target, Tensor? weight=None, int reduction=1, SymInt ignore_index=-100) -> Tensor"
  },
  {
    "name": "aten::nll_loss.out(Tensor self, Tensor target, Tensor? weight=None, int reduction=1, SymInt ignore_index=-100, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::nll_loss2d(Tensor self, Tensor target, Tensor? weight=None, int reduction=1, SymInt ignore_index=-100) -> Tensor"
  },
  {
    "name": "aten::nll_loss2d.out(Tensor self, Tensor target, Tensor? weight=None, int reduction=1, SymInt ignore_index=-100, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::nll_loss_nd(Tensor self, Tensor target, Tensor? weight=None, int reduction=1, SymInt ignore_index=-100) -> Tensor"
  },
  {
    "name": "aten::nonzero(Tensor self) -> Tensor"
  },
  {
    "name": "aten::nonzero.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::nonzero_numpy(Tensor self) -> Tensor[]"
  },
  {
    "name": "aten::norm.Scalar(Tensor self, Scalar p=2) -> Tensor"
  },
  {
    "name": "aten::norm.ScalarOpt_dim(Tensor self, Scalar? p, int[1] dim, bool keepdim=False) -> Tensor"
  },
  {
    "name": "aten::norm.ScalarOpt_dim_dtype(Tensor self, Scalar? p, int[1] dim, bool keepdim, *, ScalarType dtype) -> Tensor"
  },
  {
    "name": "aten::norm.ScalarOpt_dtype(Tensor self, Scalar? p, *, ScalarType dtype) -> Tensor"
  },
  {
    "name": "aten::norm.ScalarOpt_dtype_out(Tensor self, Scalar? p, *, ScalarType dtype, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::norm.Scalar_out(Tensor self, Scalar p=2, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::norm.dtype_out(Tensor self, Scalar? p, int[1] dim, bool keepdim, *, ScalarType dtype, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::norm.names_ScalarOpt_dim(Tensor self, Scalar? p, str[1] dim, bool keepdim=False) -> Tensor"
  },
  {
    "name": "aten::norm.names_ScalarOpt_dim_dtype(Tensor self, Scalar? p, str[1] dim, bool keepdim, *, ScalarType dtype) -> Tensor"
  },
  {
    "name": "aten::norm.names_dtype_out(Tensor self, Scalar? p, str[1] dim, bool keepdim, *, ScalarType dtype, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::norm.names_out(Tensor self, Scalar? p, str[1] dim, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::norm.out(Tensor self, Scalar? p, int[1] dim, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::normal.Tensor_Tensor(Tensor mean, Tensor std, *, Generator? generator=None) -> Tensor"
  },
  {
    "name": "aten::normal.Tensor_Tensor_out(Tensor mean, Tensor std, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::normal.Tensor_float(Tensor mean, float std=1., *, Generator? generator=None) -> Tensor"
  },
  {
    "name": "aten::normal.Tensor_float_out(Tensor mean, float std=1., *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::normal.float_Tensor(float mean, Tensor std, *, Generator? generator=None) -> Tensor"
  },
  {
    "name": "aten::normal.float_Tensor_out(float mean, Tensor std, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::normal.float_float(float mean, float std, SymInt[] size, *, Generator? generator=None, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor"
  },
  {
    "name": "aten::normal.float_float_out(float mean, float std, SymInt[] size, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::normal.out(Tensor self, float mean=0., float std=1., *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::normal_(Tensor(a!) self, float mean=0., float std=1., *, Generator? generator=None) -> Tensor(a!)"
  },
  {
    "name": "aten::numel(Tensor self) -> int"
  },
  {
    "name": "aten::numpy_T(Tensor(a) self) -> Tensor(a)"
  },
  {
    "name": "aten::numpy_T.a(Tensor(a) self) -> Tensor(a)"
  },
  {
    "name": "aten::one_hot(Tensor self, int num_classes=-1) -> Tensor"
  },
  {
    "name": "aten::ones(SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor"
  },
  {
    "name": "aten::ones.names(int[] size, *, str[]? names, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor"
  },
  {
    "name": "aten::ones.names_out(int[] size, *, str[]? names, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::ones.out(SymInt[] size, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::ones_like(Tensor self, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor"
  },
  {
    "name": "aten::ones_like.out(Tensor self, *, MemoryFormat? memory_format=None, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::outer(Tensor self, Tensor vec2) -> Tensor"
  },
  {
    "name": "aten::outer.out(Tensor self, Tensor vec2, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::pad(Tensor self, SymInt[] pad, str mode=\"constant\", float? value=None) -> Tensor",
    "category": "Tensor"
  },
  {
    "name": "aten::pad_sequence(Tensor[] sequences, bool batch_first=False, float padding_value=0., str padding_side=\"right\") -> Tensor"
  },
  {
    "name": "aten::pairwise_distance(Tensor x1, Tensor x2, float p=2., float eps=9.9999999999999995e-07, bool keepdim=False) -> Tensor"
  },
  {
    "name": "aten::pdist(Tensor self, float p=2.) -> Tensor"
  },
  {
    "name": "aten::permute(Tensor(a) self, int[] dims) -> Tensor(a)",
    "category": "Shape"
  },
  {
    "name": "aten::pin_memory(Tensor(a) self, Device? device=None) -> Tensor(a)"
  },
  {
    "name": "aten::pinverse(Tensor self, float rcond=1.0000000000000001e-15) -> Tensor"
  },
  {
    "name": "aten::pixel_shuffle(Tensor self, int upscale_factor) -> Tensor"
  },
  {
    "name": "aten::pixel_shuffle.out(Tensor self, int upscale_factor, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::pixel_unshuffle(Tensor self, int downscale_factor) -> Tensor"
  },
  {
    "name": "aten::pixel_unshuffle.out(Tensor self, int downscale_factor, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::polar(Tensor abs, Tensor angle) -> Tensor"
  },
  {
    "name": "aten::polar.Scalar_Scalar(Scalar a, Scalar b) -> Scalar"
  },
  {
    "name": "aten::polar.float(float a, float b) -> complex"
  },
  {
    "name": "aten::polar.float_int(float a, int b) -> complex"
  },
  {
    "name": "aten::polar.int(int a, int b) -> complex"
  },
  {
    "name": "aten::polar.int_float(int a, float b) -> complex"
  },
  {
    "name": "aten::polar.out(Tensor abs, Tensor angle, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::pop.Dict_Tensor(Dict(Tensor, t)(a!) self, Tensor key) -> t(*)"
  },
  {
    "name": "aten::pop.Dict_bool(Dict(bool, t)(a!) self, bool key) -> t(*)"
  },
  {
    "name": "aten::pop.Dict_complex(Dict(complex, t)(a!) self, complex key) -> t(*)"
  },
  {
    "name": "aten::pop.Dict_default_Tensor(Dict(Tensor, t)(a!) self, Tensor key, t default_value) -> t(*)"
  },
  {
    "name": "aten::pop.Dict_default_bool(Dict(bool, t)(a!) self, bool key, t default_value) -> t(*)"
  },
  {
    "name": "aten::pop.Dict_default_complex(Dict(complex, t)(a!) self, complex key, t default_value) -> t(*)"
  },
  {
    "name": "aten::pop.Dict_default_float(Dict(float, t)(a!) self, float key, t default_value) -> t(*)"
  },
  {
    "name": "aten::pop.Dict_default_int(Dict(int, t)(a!) self, int key, t default_value) -> t(*)"
  },
  {
    "name": "aten::pop.Dict_default_str(Dict(str, t)(a!) self, str key, t default_value) -> t(*)"
  },
  {
    "name": "aten::pop.Dict_float(Dict(float, t)(a!) self, float key) -> t(*)"
  },
  {
    "name": "aten::pop.Dict_int(Dict(int, t)(a!) self, int key) -> t(*)"
  },
  {
    "name": "aten::pop.Dict_str(Dict(str, t)(a!) self, str key) -> t(*)"
  },
  {
    "name": "aten::pop.t(t[](a!) self, int idx=-1) -> t(*)"
  },
  {
    "name": "aten::pow.Scalar(Scalar self, Tensor exponent) -> Tensor"
  },
  {
    "name": "aten::pow.Scalar_Scalar(Scalar a, Scalar b) -> float"
  },
  {
    "name": "aten::pow.Scalar_out(Scalar self, Tensor exponent, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::pow.Tensor_Scalar(Tensor self, Scalar exponent) -> Tensor"
  },
  {
    "name": "aten::pow.Tensor_Scalar_out(Tensor self, Scalar exponent, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::pow.Tensor_Tensor(Tensor self, Tensor exponent) -> Tensor"
  },
  {
    "name": "aten::pow.Tensor_Tensor_out(Tensor self, Tensor exponent, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::pow.complex(complex a, complex b) -> complex"
  },
  {
    "name": "aten::pow.complex_float(complex a, float b) -> complex"
  },
  {
    "name": "aten::pow.float(float a, float b) -> float"
  },
  {
    "name": "aten::pow.float_complex(float a, complex b) -> complex"
  },
  {
    "name": "aten::pow.float_int(float a, int b) -> float"
  },
  {
    "name": "aten::pow.int(int a, int b) -> float"
  },
  {
    "name": "aten::pow.int_float(int a, float b) -> float"
  },
  {
    "name": "aten::pow.int_to_int(int a, int b) -> int"
  },
  {
    "name": "aten::pow_.Scalar(Tensor(a!) self, Scalar exponent) -> Tensor(a!)"
  },
  {
    "name": "aten::pow_.Tensor(Tensor(a!) self, Tensor exponent) -> Tensor(a!)"
  },
  {
    "name": "aten::prelu(Tensor self, Tensor weight) -> Tensor",
    "category": "Activation"
  },
  {
    "name": "aten::prod(Tensor self, *, ScalarType? dtype=None) -> Tensor"
  },
  {
    "name": "aten::prod.Dimname_out(Tensor self, str dim, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::prod.dim_Dimname(Tensor self, str dim, bool keepdim=False, *, ScalarType? dtype=None) -> Tensor"
  },
  {
    "name": "aten::prod.dim_int(Tensor self, int dim, bool keepdim=False, *, ScalarType? dtype=None) -> Tensor"
  },
  {
    "name": "aten::prod.int_out(Tensor self, int dim, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::prod.out(Tensor self, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::put_(Tensor(a!) self, Tensor index, Tensor source, bool accumulate=False) -> Tensor(a!)"
  },
  {
    "name": "aten::quantile(Tensor self, Tensor q, int? dim=None, bool keepdim=False, *, str interpolation=\"linear\") -> Tensor"
  },
  {
    "name": "aten::quantile.out(Tensor self, Tensor q, int? dim=None, bool keepdim=False, *, str interpolation=\"linear\", Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::quantile.scalar(Tensor self, float q, int? dim=None, bool keepdim=False, *, str interpolation=\"linear\") -> Tensor"
  },
  {
    "name": "aten::quantile.scalar_out(Tensor self, float q, int? dim=None, bool keepdim=False, *, str interpolation=\"linear\", Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::quantize_per_channel(Tensor self, Tensor scales, Tensor zero_points, int axis, ScalarType dtype) -> Tensor",
    "category": "Quantization"
  },
  {
    "name": "aten::quantize_per_channel.out(Tensor self, Tensor scales, Tensor zero_points, int axis, ScalarType dtype, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::quantize_per_tensor(Tensor self, float scale, int zero_point, ScalarType dtype) -> Tensor",
    "category": "Quantization"
  },
  {
    "name": "aten::quantize_per_tensor.out(Tensor self, float scale, int zero_point, ScalarType dtype, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::quantize_per_tensor.tensor_qparams(Tensor self, Tensor scale, Tensor zero_point, ScalarType dtype) -> Tensor",
    "category": "Quantization"
  },
  {
    "name": "aten::quantize_per_tensor.tensor_qparams_out(Tensor self, Tensor scale, Tensor zero_point, ScalarType dtype, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::quantize_per_tensor.tensors(Tensor[] tensors, Tensor scales, Tensor zero_points, ScalarType dtype) -> Tensor[]",
    "category": "Quantization"
  },
  {
    "name": "aten::quantize_per_tensor.tensors_out(Tensor[] tensors, Tensor scales, Tensor zero_points, ScalarType dtype, *, Tensor(a!)[] out) -> ()"
  },
  {
    "name": "aten::quantize_per_tensor_dynamic(Tensor self, ScalarType dtype, bool reduce_range) -> Tensor",
    "category": "Quantization"
  },
  {
    "name": "aten::quantize_per_tensor_dynamic.out(Tensor self, ScalarType dtype, bool reduce_range, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::quantized_gru.data(Tensor data, Tensor batch_sizes, Tensor hx, __torch__.torch.classes.rnn.CellParamsBase[] params, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional) -> (Tensor, Tensor)",
    "category": "Layer"
  },
  {
    "name": "aten::quantized_gru.data_legacy(Tensor data, Tensor batch_sizes, Tensor hx, Tensor[] params, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional) -> (Tensor, Tensor)",
    "category": "Layer"
  },
  {
    "name": "aten::quantized_gru.input(Tensor input, Tensor hx, __torch__.torch.classes.rnn.CellParamsBase[] params, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional, bool batch_first) -> (Tensor, Tensor)",
    "category": "Layer"
  },
  {
    "name": "aten::quantized_gru.input_legacy(Tensor input, Tensor hx, Tensor[] params, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional, bool batch_first) -> (Tensor, Tensor)",
    "category": "Layer"
  },
  {
    "name": "aten::quantized_lstm.data(Tensor data, Tensor batch_sizes, Tensor[] hx, __torch__.torch.classes.rnn.CellParamsBase[] params, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional, *, ScalarType? dtype=None, bool use_dynamic=False) -> (Tensor, Tensor, Tensor)",
    "category": "Layer"
  },
  {
    "name": "aten::quantized_lstm.data_legacy(Tensor data, Tensor batch_sizes, Tensor[] hx, Tensor[] params, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional, *, ScalarType? dtype=None, bool use_dynamic=False) -> (Tensor, Tensor, Tensor)",
    "category": "Layer"
  },
  {
    "name": "aten::quantized_lstm.input(Tensor input, Tensor[] hx, __torch__.torch.classes.rnn.CellParamsBase[] params, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional, bool batch_first, *, ScalarType? dtype=None, bool use_dynamic=False) -> (Tensor, Tensor, Tensor)",
    "category": "Layer"
  },
  {
    "name": "aten::quantized_lstm.input_legacy(Tensor input, Tensor[] hx, Tensor[] params, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional, bool batch_first, *, ScalarType? dtype=None, bool use_dynamic=False) -> (Tensor, Tensor, Tensor)",
    "category": "Layer"
  },
  {
    "name": "aten::quantized_lstm_cell(Tensor input, Tensor[] hx, Tensor w_ih, Tensor w_hh, Tensor b_ih, Tensor b_hh, Tensor packed_ih, Tensor packed_hh, Tensor col_offsets_ih, Tensor col_offsets_hh, Scalar scale_ih, Scalar scale_hh, Scalar zero_point_ih, Scalar zero_point_hh) -> (Tensor, Tensor)"
  },
  {
    "name": "aten::rand(SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor"
  },
  {
    "name": "aten::rand.generator(SymInt[] size, *, Generator? generator, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor"
  },
  {
    "name": "aten::rand.generator_out(SymInt[] size, *, Generator? generator, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::rand.generator_with_names(SymInt[] size, *, Generator? generator, str[]? names, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor"
  },
  {
    "name": "aten::rand.generator_with_names_out(SymInt[] size, *, Generator? generator, str[]? names, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::rand.names(SymInt[] size, *, str[]? names, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor"
  },
  {
    "name": "aten::rand.names_out(SymInt[] size, *, str[]? names, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::rand.out(SymInt[] size, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::rand_like(Tensor self, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor"
  },
  {
    "name": "aten::rand_like.out(Tensor self, *, MemoryFormat? memory_format=None, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::randint(SymInt high, SymInt[] size, *, ScalarType? dtype=4, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor"
  },
  {
    "name": "aten::randint.generator(SymInt high, SymInt[] size, *, Generator? generator, ScalarType? dtype=4, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor"
  },
  {
    "name": "aten::randint.generator_out(SymInt high, SymInt[] size, *, Generator? generator, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::randint.low(SymInt low, SymInt high, SymInt[] size, *, ScalarType? dtype=4, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor"
  },
  {
    "name": "aten::randint.low_generator(SymInt low, SymInt high, SymInt[] size, *, Generator? generator, ScalarType? dtype=4, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor"
  },
  {
    "name": "aten::randint.low_generator_out(SymInt low, SymInt high, SymInt[] size, *, Generator? generator, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::randint.low_out(SymInt low, SymInt high, SymInt[] size, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::randint.out(SymInt high, SymInt[] size, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::randint_like(Tensor self, SymInt high, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor"
  },
  {
    "name": "aten::randint_like.low_dtype(Tensor self, SymInt low, SymInt high, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor"
  },
  {
    "name": "aten::randint_like.low_dtype_out(Tensor self, SymInt low, SymInt high, *, MemoryFormat? memory_format=None, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::randint_like.out(Tensor self, SymInt high, *, MemoryFormat? memory_format=None, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::randn(SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor"
  },
  {
    "name": "aten::randn.generator(SymInt[] size, *, Generator? generator, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor"
  },
  {
    "name": "aten::randn.generator_out(SymInt[] size, *, Generator? generator, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::randn.generator_with_names(SymInt[] size, *, Generator? generator, str[]? names, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor"
  },
  {
    "name": "aten::randn.generator_with_names_out(SymInt[] size, *, Generator? generator, str[]? names, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::randn.names(SymInt[] size, *, str[]? names, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor"
  },
  {
    "name": "aten::randn.names_out(SymInt[] size, *, str[]? names, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::randn.out(SymInt[] size, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::randn_like(Tensor self, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor"
  },
  {
    "name": "aten::randn_like.out(Tensor self, *, MemoryFormat? memory_format=None, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::random_(Tensor(a!) self, *, Generator? generator=None) -> Tensor(a!)"
  },
  {
    "name": "aten::random_.from(Tensor(a!) self, int from, int? to, *, Generator? generator=None) -> Tensor(a!)"
  },
  {
    "name": "aten::random_.to(Tensor(a!) self, int to, *, Generator? generator=None) -> Tensor(a!)"
  },
  {
    "name": "aten::randperm(SymInt n, *, ScalarType? dtype=4, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor"
  },
  {
    "name": "aten::randperm.generator(SymInt n, *, Generator? generator, ScalarType? dtype=4, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor"
  },
  {
    "name": "aten::randperm.generator_out(SymInt n, *, Generator? generator, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::randperm.out(SymInt n, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::real(Tensor(a) self) -> Tensor(a)"
  },
  {
    "name": "aten::reciprocal(Tensor self) -> Tensor"
  },
  {
    "name": "aten::reciprocal.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::reflection_pad1d(Tensor self, SymInt[2] padding) -> Tensor",
    "category": "Tensor"
  },
  {
    "name": "aten::reflection_pad1d.out(Tensor self, SymInt[2] padding, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::reflection_pad2d(Tensor self, SymInt[4] padding) -> Tensor",
    "category": "Tensor"
  },
  {
    "name": "aten::reflection_pad2d.out(Tensor self, SymInt[4] padding, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::reflection_pad3d(Tensor self, SymInt[6] padding) -> Tensor",
    "category": "Tensor"
  },
  {
    "name": "aten::reflection_pad3d.out(Tensor self, SymInt[6] padding, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::relu(Tensor self) -> Tensor",
    "category": "Activation"
  },
  {
    "name": "aten::relu.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::relu6(Tensor self) -> Tensor",
    "category": "Activation"
  },
  {
    "name": "aten::relu6_(Tensor(a!) self) -> Tensor(a!)",
    "category": "Activation"
  },
  {
    "name": "aten::relu_(Tensor(a!) self) -> Tensor(a!)",
    "category": "Activation"
  },
  {
    "name": "aten::remainder(Scalar a, Scalar b) -> Scalar"
  },
  {
    "name": "aten::remainder.Scalar(Tensor self, Scalar other) -> Tensor"
  },
  {
    "name": "aten::remainder.Scalar_Tensor(Scalar self, Tensor other) -> Tensor"
  },
  {
    "name": "aten::remainder.Scalar_Tensor_out(Scalar self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::remainder.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::remainder.Tensor(Tensor self, Tensor other) -> Tensor"
  },
  {
    "name": "aten::remainder.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::remainder.float(float a, float b) -> float"
  },
  {
    "name": "aten::remainder.float_int(float a, int b) -> float"
  },
  {
    "name": "aten::remainder.int(int a, int b) -> int"
  },
  {
    "name": "aten::remainder.int_float(int a, float b) -> float"
  },
  {
    "name": "aten::remainder_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)"
  },
  {
    "name": "aten::remainder_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)"
  },
  {
    "name": "aten::renorm(Tensor self, Scalar p, int dim, Scalar maxnorm) -> Tensor"
  },
  {
    "name": "aten::renorm.out(Tensor self, Scalar p, int dim, Scalar maxnorm, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::repeat(Tensor self, SymInt[] repeats) -> Tensor"
  },
  {
    "name": "aten::repeat.out(Tensor self, SymInt[] repeats, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::repeat_interleave.Tensor(Tensor repeats, *, SymInt? output_size=None) -> Tensor"
  },
  {
    "name": "aten::repeat_interleave.Tensor_out(Tensor repeats, *, SymInt? output_size=None, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::repeat_interleave.self_Tensor(Tensor self, Tensor repeats, int? dim=None, *, SymInt? output_size=None) -> Tensor"
  },
  {
    "name": "aten::repeat_interleave.self_int(Tensor self, SymInt repeats, int? dim=None, *, SymInt? output_size=None) -> Tensor"
  },
  {
    "name": "aten::replace(str self, str old, str new, int max=-1) -> str"
  },
  {
    "name": "aten::replication_pad1d(Tensor self, SymInt[2] padding) -> Tensor",
    "category": "Tensor"
  },
  {
    "name": "aten::replication_pad1d.out(Tensor self, SymInt[2] padding, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::replication_pad2d(Tensor self, SymInt[4] padding) -> Tensor",
    "category": "Tensor"
  },
  {
    "name": "aten::replication_pad2d.out(Tensor self, SymInt[4] padding, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::replication_pad3d(Tensor self, SymInt[6] padding) -> Tensor",
    "category": "Tensor"
  },
  {
    "name": "aten::replication_pad3d.out(Tensor self, SymInt[6] padding, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::requires_grad_(Tensor(a!) self, bool requires_grad=True) -> Tensor(a!)"
  },
  {
    "name": "aten::reshape(Tensor(a) self, SymInt[] shape) -> Tensor(a)",
    "category": "Shape"
  },
  {
    "name": "aten::reshape_as(Tensor(a) self, Tensor other) -> Tensor(a)",
    "category": "Shape"
  },
  {
    "name": "aten::resize_(Tensor(a!) self, SymInt[] size, *, MemoryFormat? memory_format=None) -> Tensor(a!)"
  },
  {
    "name": "aten::resolve_conj(Tensor(a) self) -> Tensor(a)"
  },
  {
    "name": "aten::resolve_neg(Tensor(a) self) -> Tensor(a)"
  },
  {
    "name": "aten::reverse.t(t[](a!) self) -> ()"
  },
  {
    "name": "aten::rnn_relu.data(Tensor data, Tensor batch_sizes, Tensor hx, Tensor[] params, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional) -> (Tensor, Tensor)"
  },
  {
    "name": "aten::rnn_relu.input(Tensor input, Tensor hx, Tensor[] params, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional, bool batch_first) -> (Tensor, Tensor)",
    "category": "Layer"
  },
  {
    "name": "aten::rnn_tanh.data(Tensor data, Tensor batch_sizes, Tensor hx, Tensor[] params, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional) -> (Tensor, Tensor)"
  },
  {
    "name": "aten::rnn_tanh.input(Tensor input, Tensor hx, Tensor[] params, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional, bool batch_first) -> (Tensor, Tensor)",
    "category": "Layer"
  },
  {
    "name": "aten::rnn_tanh_cell(Tensor input, Tensor hx, Tensor w_ih, Tensor w_hh, Tensor? b_ih=None, Tensor? b_hh=None) -> Tensor"
  },
  {
    "name": "aten::roll(Tensor self, SymInt[1] shifts, int[1] dims=[]) -> Tensor",
    "category": "Layer"
  },
  {
    "name": "aten::roll.out(Tensor self, SymInt[1] shifts, int[1] dims=[], *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::rot90(Tensor self, int k=1, int[] dims=[0, 1]) -> Tensor"
  },
  {
    "name": "aten::rot90.out(Tensor self, int k=1, int[] dims=[0, 1], *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::round(Tensor self) -> Tensor"
  },
  {
    "name": "aten::round.Scalar(Scalar a) -> Scalar"
  },
  {
    "name": "aten::round.decimals(Tensor self, *, int decimals) -> Tensor"
  },
  {
    "name": "aten::round.decimals_out(Tensor self, *, int decimals, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::round.float(float a) -> float"
  },
  {
    "name": "aten::round.int(int a) -> float"
  },
  {
    "name": "aten::round.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::round_(Tensor(a!) self) -> Tensor(a!)"
  },
  {
    "name": "aten::round_.decimals(Tensor(a!) self, *, int decimals) -> Tensor(a!)"
  },
  {
    "name": "aten::rrelu(Tensor self, Scalar lower=0.125, Scalar upper=0.33333333333333331, bool training=False, Generator? generator=None) -> Tensor"
  },
  {
    "name": "aten::rsqrt(Tensor self) -> Tensor"
  },
  {
    "name": "aten::rsqrt.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::rsqrt_(Tensor(a!) self) -> Tensor(a!)"
  },
  {
    "name": "aten::rstrip(str self, str chars=\" \\n\\t\\f\\v\") -> str"
  },
  {
    "name": "aten::rsub.Scalar(Tensor self, Scalar other, Scalar alpha=1) -> Tensor"
  },
  {
    "name": "aten::rsub.Scalar_out(Tensor self, Scalar other, Scalar alpha=1, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::rsub.Tensor(Tensor self, Tensor other, *, Scalar alpha=1) -> Tensor"
  },
  {
    "name": "aten::rsub.Tensor_out(Tensor self, Tensor other, *, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::scalar_tensor(Scalar s, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor"
  },
  {
    "name": "aten::scalar_tensor.out(Scalar s, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::scaled_dot_product_attention(Tensor query, Tensor key, Tensor value, Tensor? attn_mask=None, float dropout_p=0., bool is_causal=False, *, float? scale=None, bool enable_gqa=False) -> Tensor",
    "category": "Attention"
  },
  {
    "name": "aten::scatter.dimname_src(Tensor self, str dim, Tensor index, Tensor src) -> Tensor"
  },
  {
    "name": "aten::scatter.dimname_value(Tensor self, str dim, Tensor index, Scalar value) -> Tensor"
  },
  {
    "name": "aten::scatter.reduce(Tensor self, int dim, Tensor index, Tensor src, *, str reduce) -> Tensor"
  },
  {
    "name": "aten::scatter.reduce_out(Tensor self, int dim, Tensor index, Tensor src, *, str reduce, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::scatter.src(Tensor self, int dim, Tensor index, Tensor src) -> Tensor"
  },
  {
    "name": "aten::scatter.src_out(Tensor self, int dim, Tensor index, Tensor src, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::scatter.value(Tensor self, int dim, Tensor index, Scalar value) -> Tensor"
  },
  {
    "name": "aten::scatter.value_out(Tensor self, int dim, Tensor index, Scalar value, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::scatter.value_reduce(Tensor self, int dim, Tensor index, Scalar value, *, str reduce) -> Tensor"
  },
  {
    "name": "aten::scatter.value_reduce_out(Tensor self, int dim, Tensor index, Scalar value, *, str reduce, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::scatter_.reduce(Tensor(a!) self, int dim, Tensor index, Tensor src, *, str reduce) -> Tensor(a!)"
  },
  {
    "name": "aten::scatter_.src(Tensor(a!) self, int dim, Tensor index, Tensor src) -> Tensor(a!)"
  },
  {
    "name": "aten::scatter_.value(Tensor(a!) self, int dim, Tensor index, Scalar value) -> Tensor(a!)"
  },
  {
    "name": "aten::scatter_.value_reduce(Tensor(a!) self, int dim, Tensor index, Scalar value, *, str reduce) -> Tensor(a!)"
  },
  {
    "name": "aten::scatter_add(Tensor self, int dim, Tensor index, Tensor src) -> Tensor"
  },
  {
    "name": "aten::scatter_add.dimname(Tensor self, str dim, Tensor index, Tensor src) -> Tensor"
  },
  {
    "name": "aten::scatter_add.out(Tensor self, int dim, Tensor index, Tensor src, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::scatter_add_(Tensor(a!) self, int dim, Tensor index, Tensor src) -> Tensor(a!)"
  },
  {
    "name": "aten::scatter_reduce.two(Tensor self, int dim, Tensor index, Tensor src, str reduce, *, bool include_self=True) -> Tensor"
  },
  {
    "name": "aten::scatter_reduce.two_out(Tensor self, int dim, Tensor index, Tensor src, str reduce, *, bool include_self=True, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::scatter_reduce_.two(Tensor(a!) self, int dim, Tensor index, Tensor src, str reduce, *, bool include_self=True) -> Tensor(a!)"
  },
  {
    "name": "aten::searchsorted.Scalar(Tensor sorted_sequence, Scalar self, *, bool out_int32=False, bool right=False, str? side=None, Tensor? sorter=None) -> Tensor"
  },
  {
    "name": "aten::searchsorted.Scalar_out(Tensor sorted_sequence, Scalar self, *, bool out_int32=False, bool right=False, str? side=None, Tensor? sorter=None, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::searchsorted.Tensor(Tensor sorted_sequence, Tensor self, *, bool out_int32=False, bool right=False, str? side=None, Tensor? sorter=None) -> Tensor"
  },
  {
    "name": "aten::searchsorted.Tensor_out(Tensor sorted_sequence, Tensor self, *, bool out_int32=False, bool right=False, str? side=None, Tensor? sorter=None, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::select.Dimname(Tensor(a) self, str dim, int index) -> Tensor(a)"
  },
  {
    "name": "aten::select.int(Tensor(a) self, int dim, SymInt index) -> Tensor(a)"
  },
  {
    "name": "aten::select.t(t[](a) list, int idx) -> t(*)"
  },
  {
    "name": "aten::select_backward(Tensor grad_output, SymInt[] input_sizes, int dim, SymInt index) -> Tensor"
  },
  {
    "name": "aten::select_backward.out(Tensor grad_output, SymInt[] input_sizes, int dim, SymInt index, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::select_copy.int(Tensor self, int dim, SymInt index) -> Tensor"
  },
  {
    "name": "aten::select_copy.int_out(Tensor self, int dim, SymInt index, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::select_scatter(Tensor self, Tensor src, int dim, SymInt index) -> Tensor"
  },
  {
    "name": "aten::select_scatter.out(Tensor self, Tensor src, int dim, SymInt index, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::selu(Tensor self) -> Tensor",
    "category": "Activation"
  },
  {
    "name": "aten::selu_(Tensor(a!) self) -> Tensor(a!)",
    "category": "Activation"
  },
  {
    "name": "aten::sigmoid(Tensor self) -> Tensor",
    "category": "Activation"
  },
  {
    "name": "aten::sigmoid.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::sigmoid_(Tensor(a!) self) -> Tensor(a!)",
    "category": "Activation"
  },
  {
    "name": "aten::sign(Tensor self) -> Tensor"
  },
  {
    "name": "aten::sign.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::sign_(Tensor(a!) self) -> Tensor(a!)"
  },
  {
    "name": "aten::signbit(Tensor self) -> Tensor"
  },
  {
    "name": "aten::signbit.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::silu(Tensor self) -> Tensor",
    "category": "Activation"
  },
  {
    "name": "aten::silu.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::silu_(Tensor(a!) self) -> Tensor(a!)",
    "category": "Activation"
  },
  {
    "name": "aten::silu_backward(Tensor grad_output, Tensor self) -> Tensor"
  },
  {
    "name": "aten::silu_backward.grad_input(Tensor grad_output, Tensor self, *, Tensor(a!) grad_input) -> Tensor(a!)"
  },
  {
    "name": "aten::sin(Tensor self) -> Tensor"
  },
  {
    "name": "aten::sin.Scalar(Scalar a) -> Scalar"
  },
  {
    "name": "aten::sin.complex(complex a) -> complex"
  },
  {
    "name": "aten::sin.float(float a) -> float"
  },
  {
    "name": "aten::sin.int(int a) -> float"
  },
  {
    "name": "aten::sin.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::sinh(Tensor self) -> Tensor"
  },
  {
    "name": "aten::sinh.Scalar(Scalar a) -> Scalar"
  },
  {
    "name": "aten::sinh.complex(complex a) -> complex"
  },
  {
    "name": "aten::sinh.float(float a) -> float"
  },
  {
    "name": "aten::sinh.int(int a) -> float"
  },
  {
    "name": "aten::sinh.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::size(Tensor self) -> int[]"
  },
  {
    "name": "aten::size.Dimname(Tensor self, str dim) -> int"
  },
  {
    "name": "aten::size.int(Tensor self, int dim) -> int"
  },
  {
    "name": "aten::slice.Tensor(Tensor(a) self, int dim=0, SymInt? start=None, SymInt? end=None, SymInt step=1) -> Tensor(a)",
    "category": "Tensor"
  },
  {
    "name": "aten::slice.str(str string, int? start=None, int? end=None, int step=1) -> str"
  },
  {
    "name": "aten::slice.t(t[] l, int? start=None, int? end=None, int step=1) -> t[]"
  },
  {
    "name": "aten::slice_copy.Tensor(Tensor self, int dim=0, SymInt? start=None, SymInt? end=None, SymInt step=1) -> Tensor"
  },
  {
    "name": "aten::slice_copy.Tensor_out(Tensor self, int dim=0, SymInt? start=None, SymInt? end=None, SymInt step=1, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::slice_scatter(Tensor self, Tensor src, int dim=0, SymInt? start=None, SymInt? end=None, SymInt step=1) -> Tensor"
  },
  {
    "name": "aten::slice_scatter.out(Tensor self, Tensor src, int dim=0, SymInt? start=None, SymInt? end=None, SymInt step=1, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::slogdet(Tensor self) -> (Tensor sign, Tensor logabsdet)"
  },
  {
    "name": "aten::slogdet.out(Tensor self, *, Tensor(a!) sign, Tensor(b!) logabsdet) -> (Tensor(a!) sign, Tensor(b!) logabsdet)"
  },
  {
    "name": "aten::smooth_l1_loss(Tensor self, Tensor target, int reduction=1, float beta=1.) -> Tensor"
  },
  {
    "name": "aten::smooth_l1_loss.out(Tensor self, Tensor target, int reduction=1, float beta=1., *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::smooth_l1_loss_backward(Tensor grad_output, Tensor self, Tensor target, int reduction, float beta) -> Tensor"
  },
  {
    "name": "aten::smooth_l1_loss_backward.grad_input(Tensor grad_output, Tensor self, Tensor target, int reduction, float beta, *, Tensor(a!) grad_input) -> Tensor(a!)"
  },
  {
    "name": "aten::softmax.Dimname(Tensor self, str dim, *, ScalarType? dtype=None) -> Tensor",
    "category": "Activation"
  },
  {
    "name": "aten::softmax.int(Tensor self, int dim, ScalarType? dtype=None) -> Tensor",
    "category": "Activation"
  },
  {
    "name": "aten::softmax.int_out(Tensor self, int dim, ScalarType? dtype=None, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::softplus(Tensor self, Scalar beta=1, Scalar threshold=20) -> Tensor",
    "category": "Activation"
  },
  {
    "name": "aten::softplus.out(Tensor self, Scalar beta=1, Scalar threshold=20, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::softshrink(Tensor self, Scalar lambd=0.5) -> Tensor"
  },
  {
    "name": "aten::softshrink.out(Tensor self, Scalar lambd=0.5, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::sort(Tensor self, int dim=-1, bool descending=False) -> (Tensor values, Tensor indices)"
  },
  {
    "name": "aten::sort.Tensor(Tensor[](a!) self, bool reverse=False) -> ()"
  },
  {
    "name": "aten::sort.any(t[](a!) self, bool reverse=False) -> ()"
  },
  {
    "name": "aten::sort.bool(bool[](a!) self, bool reverse=False) -> ()"
  },
  {
    "name": "aten::sort.dimname(Tensor self, str dim, bool descending=False) -> (Tensor values, Tensor indices)"
  },
  {
    "name": "aten::sort.dimname_stable(Tensor self, *, bool? stable, str dim, bool descending=False) -> (Tensor values, Tensor indices)"
  },
  {
    "name": "aten::sort.dimname_values(Tensor self, str dim, bool descending=False, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)"
  },
  {
    "name": "aten::sort.dimname_values_stable(Tensor self, *, bool? stable, str dim, bool descending=False, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)"
  },
  {
    "name": "aten::sort.float(float[](a!) self, bool reverse=False) -> ()"
  },
  {
    "name": "aten::sort.int(int[](a!) self, bool reverse=False) -> ()"
  },
  {
    "name": "aten::sort.stable(Tensor self, *, bool? stable, int dim=-1, bool descending=False) -> (Tensor values, Tensor indices)"
  },
  {
    "name": "aten::sort.str(str[](a!) self, bool reverse=False) -> ()"
  },
  {
    "name": "aten::sort.values(Tensor self, int dim=-1, bool descending=False, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)"
  },
  {
    "name": "aten::sort.values_stable(Tensor self, *, bool? stable, int dim=-1, bool descending=False, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)"
  },
  {
    "name": "aten::special_expit(Tensor self) -> Tensor"
  },
  {
    "name": "aten::special_expit.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::split(Tensor(a -> *) self, int[] split_sizes, int dim=0) -> Tensor(a)[]"
  },
  {
    "name": "aten::split.Tensor(Tensor(a -> *) self, SymInt split_size, int dim=0) -> Tensor(a)[]",
    "category": "Tensor"
  },
  {
    "name": "aten::split.sizes(Tensor(a -> *) self, SymInt[] split_size, int dim=0) -> Tensor(a)[]",
    "category": "Tensor"
  },
  {
    "name": "aten::split.str(str self, str? separator=None, int max=-1) -> str[]"
  },
  {
    "name": "aten::split_copy.Tensor(Tensor self, SymInt split_size, int dim=0) -> Tensor[]"
  },
  {
    "name": "aten::split_copy.Tensor_out(Tensor self, SymInt split_size, int dim=0, *, Tensor(a!)[] out) -> ()"
  },
  {
    "name": "aten::split_with_sizes(Tensor(a -> *) self, SymInt[] split_sizes, int dim=0) -> Tensor(a)[]",
    "category": "Tensor"
  },
  {
    "name": "aten::split_with_sizes_copy(Tensor self, SymInt[] split_sizes, int dim=0) -> Tensor[]"
  },
  {
    "name": "aten::split_with_sizes_copy.out(Tensor self, SymInt[] split_sizes, int dim=0, *, Tensor(a!)[] out) -> ()"
  },
  {
    "name": "aten::splitlines(str self, bool keepends=False) -> str[]"
  },
  {
    "name": "aten::sqrt(Tensor self) -> Tensor"
  },
  {
    "name": "aten::sqrt.Scalar(Scalar a) -> Scalar"
  },
  {
    "name": "aten::sqrt.complex(complex a) -> complex"
  },
  {
    "name": "aten::sqrt.float(float a) -> float"
  },
  {
    "name": "aten::sqrt.int(int a) -> float"
  },
  {
    "name": "aten::sqrt.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::sqrt_(Tensor(a!) self) -> Tensor(a!)"
  },
  {
    "name": "aten::square(Tensor self) -> Tensor"
  },
  {
    "name": "aten::square.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::square_(Tensor(a!) self) -> Tensor(a!)"
  },
  {
    "name": "aten::squeeze(Tensor(a) self) -> Tensor(a)",
    "category": "Transform"
  },
  {
    "name": "aten::squeeze.dim(Tensor(a) self, int dim) -> Tensor(a)",
    "category": "Transform"
  },
  {
    "name": "aten::squeeze.dimname(Tensor(a) self, str dim) -> Tensor(a)",
    "category": "Transform"
  },
  {
    "name": "aten::squeeze.dims(Tensor(a) self, int[] dim) -> Tensor(a)",
    "category": "Transform"
  },
  {
    "name": "aten::squeeze_(Tensor(a!) self) -> Tensor(a!)",
    "category": "Transform"
  },
  {
    "name": "aten::squeeze_.dim(Tensor(a!) self, int dim) -> Tensor(a!)",
    "category": "Transform"
  },
  {
    "name": "aten::squeeze_.dimname(Tensor(a!) self, str dim) -> Tensor(a!)",
    "category": "Transform"
  },
  {
    "name": "aten::squeeze_.dims(Tensor(a!) self, int[] dim) -> Tensor(a!)"
  },
  {
    "name": "aten::stack(Tensor[] tensors, int dim=0) -> Tensor",
    "category": "Tensor"
  },
  {
    "name": "aten::stack.out(Tensor[] tensors, int dim=0, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::std(Tensor self, bool unbiased=True) -> Tensor"
  },
  {
    "name": "aten::std.correction(Tensor self, int[1]? dim=None, *, Scalar? correction=None, bool keepdim=False) -> Tensor"
  },
  {
    "name": "aten::std.correction_names(Tensor self, str[1] dim, *, Scalar? correction=None, bool keepdim=False) -> Tensor"
  },
  {
    "name": "aten::std.correction_names_out(Tensor self, str[1] dim, *, Scalar? correction=None, bool keepdim=False, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::std.correction_out(Tensor self, int[1]? dim=None, *, Scalar? correction=None, bool keepdim=False, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::std.dim(Tensor self, int[1]? dim, bool unbiased=True, bool keepdim=False) -> Tensor"
  },
  {
    "name": "aten::std.names_dim(Tensor self, str[1] dim, bool unbiased=True, bool keepdim=False) -> Tensor"
  },
  {
    "name": "aten::std.names_out(Tensor self, str[1] dim, bool unbiased=True, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::std.out(Tensor self, int[1]? dim, bool unbiased=True, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::std_mean(Tensor self, bool unbiased=True) -> (Tensor, Tensor)"
  },
  {
    "name": "aten::std_mean.correction(Tensor self, int[1]? dim=None, *, Scalar? correction=None, bool keepdim=False) -> (Tensor, Tensor)"
  },
  {
    "name": "aten::std_mean.correction_names(Tensor self, str[1] dim, *, Scalar? correction=None, bool keepdim=False) -> (Tensor, Tensor)"
  },
  {
    "name": "aten::std_mean.correction_out(Tensor self, int[1]? dim=None, *, Scalar? correction=None, bool keepdim=False, Tensor(a!) out0, Tensor(b!) out1) -> (Tensor(a!), Tensor(b!))"
  },
  {
    "name": "aten::std_mean.dim(Tensor self, int[1]? dim, bool unbiased=True, bool keepdim=False) -> (Tensor, Tensor)"
  },
  {
    "name": "aten::std_mean.names_dim(Tensor self, str[1] dim, bool unbiased=True, bool keepdim=False) -> (Tensor, Tensor)"
  },
  {
    "name": "aten::stft(Tensor self, int n_fft, int? hop_length=None, int? win_length=None, Tensor? window=None, bool normalized=False, bool? onesided=None, bool? return_complex=None) -> Tensor"
  },
  {
    "name": "aten::stft.center(Tensor self, int n_fft, int? hop_length=None, int? win_length=None, Tensor? window=None, bool center=True, str pad_mode=\"reflect\", bool normalized=False, bool? onesided=None, bool? return_complex=None) -> Tensor"
  },
  {
    "name": "aten::str(t elem) -> str"
  },
  {
    "name": "aten::stride(Tensor self) -> int[]"
  },
  {
    "name": "aten::stride.Dimname(Tensor self, str dim) -> int"
  },
  {
    "name": "aten::stride.int(Tensor self, int dim) -> int"
  },
  {
    "name": "aten::strip(str self, str chars=\" \\n\\t\\f\\v\") -> str"
  },
  {
    "name": "aten::sub(Scalar a, Scalar b) -> Scalar"
  },
  {
    "name": "aten::sub.Scalar(Tensor self, Scalar other, Scalar alpha=1) -> Tensor"
  },
  {
    "name": "aten::sub.Scalar_out(Tensor self, Scalar other, Scalar alpha=1, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::sub.Tensor(Tensor self, Tensor other, *, Scalar alpha=1) -> Tensor"
  },
  {
    "name": "aten::sub.complex(complex a, complex b) -> complex"
  },
  {
    "name": "aten::sub.complex_float(complex a, float b) -> complex"
  },
  {
    "name": "aten::sub.complex_int(complex a, int b) -> complex"
  },
  {
    "name": "aten::sub.float(float a, float b) -> float"
  },
  {
    "name": "aten::sub.float_complex(float a, complex b) -> complex"
  },
  {
    "name": "aten::sub.float_int(float a, int b) -> float"
  },
  {
    "name": "aten::sub.int(int a, int b) -> int"
  },
  {
    "name": "aten::sub.int_complex(int a, complex b) -> complex"
  },
  {
    "name": "aten::sub.int_float(int a, float b) -> float"
  },
  {
    "name": "aten::sub.out(Tensor self, Tensor other, *, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::sub_.Scalar(Tensor(a!) self, Scalar other, Scalar alpha=1) -> Tensor(a!)"
  },
  {
    "name": "aten::sub_.Tensor(Tensor(a!) self, Tensor other, *, Scalar alpha=1) -> Tensor(a!)"
  },
  {
    "name": "aten::sum(Tensor self, *, ScalarType? dtype=None) -> Tensor"
  },
  {
    "name": "aten::sum.DimnameList_out(Tensor self, str[1] dim, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::sum.IntList_out(Tensor self, int[1]? dim, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::sum.bool(bool[] self) -> int"
  },
  {
    "name": "aten::sum.complex(complex[] self) -> complex"
  },
  {
    "name": "aten::sum.dim_DimnameList(Tensor self, str[1] dim, bool keepdim=False, *, ScalarType? dtype=None) -> Tensor"
  },
  {
    "name": "aten::sum.dim_IntList(Tensor self, int[1]? dim, bool keepdim=False, *, ScalarType? dtype=None) -> Tensor"
  },
  {
    "name": "aten::sum.float(float[] self) -> float"
  },
  {
    "name": "aten::sum.int(int[] self) -> int"
  },
  {
    "name": "aten::sum.out(Tensor self, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::swapaxes(Tensor(a) self, int axis0, int axis1) -> Tensor(a)"
  },
  {
    "name": "aten::swapaxes_(Tensor(a!) self, int axis0, int axis1) -> Tensor(a!)"
  },
  {
    "name": "aten::sym_size(Tensor self) -> SymInt[]"
  },
  {
    "name": "aten::sym_size.int(Tensor self, int dim) -> SymInt"
  },
  {
    "name": "aten::t(Tensor(a) self) -> Tensor(a)"
  },
  {
    "name": "aten::take(Tensor self, Tensor index) -> Tensor",
    "category": "Activation"
  },
  {
    "name": "aten::take.out(Tensor self, Tensor index, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::take_along_dim(Tensor self, Tensor indices, int? dim=None) -> Tensor"
  },
  {
    "name": "aten::take_along_dim.out(Tensor self, Tensor indices, int? dim=None, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::tan(Tensor self) -> Tensor"
  },
  {
    "name": "aten::tan.Scalar(Scalar a) -> Scalar"
  },
  {
    "name": "aten::tan.complex(complex a) -> complex"
  },
  {
    "name": "aten::tan.float(float a) -> float"
  },
  {
    "name": "aten::tan.int(int a) -> float"
  },
  {
    "name": "aten::tan.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::tan_(Tensor(a!) self) -> Tensor(a!)"
  },
  {
    "name": "aten::tanh(Tensor self) -> Tensor",
    "category": "Activation"
  },
  {
    "name": "aten::tanh.Scalar(Scalar a) -> Scalar",
    "category": "Activation"
  },
  {
    "name": "aten::tanh.complex(complex a) -> complex",
    "category": "Activation"
  },
  {
    "name": "aten::tanh.float(float a) -> float",
    "category": "Activation"
  },
  {
    "name": "aten::tanh.int(int a) -> float",
    "category": "Activation"
  },
  {
    "name": "aten::tanh.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)",
    "category": "Activation"
  },
  {
    "name": "aten::tanh_(Tensor(a!) self) -> Tensor(a!)",
    "category": "Activation"
  },
  {
    "name": "aten::tensor(t[] data, *, ScalarType? dtype=None, Device? device=None, bool requires_grad=False) -> Tensor"
  },
  {
    "name": "aten::tensor.bool(bool t, *, ScalarType? dtype=None, Device? device=None, bool requires_grad=False) -> Tensor"
  },
  {
    "name": "aten::tensor.complex(complex t, *, ScalarType? dtype=None, Device? device=None, bool requires_grad=False) -> Tensor"
  },
  {
    "name": "aten::tensor.float(float t, *, ScalarType? dtype=None, Device? device=None, bool requires_grad=False) -> Tensor"
  },
  {
    "name": "aten::tensor.int(int t, *, ScalarType? dtype=None, Device? device=None, bool requires_grad=False) -> Tensor"
  },
  {
    "name": "aten::tensor_split.indices(Tensor(a -> *) self, SymInt[] indices, int dim=0) -> Tensor(a)[]"
  },
  {
    "name": "aten::tensor_split.sections(Tensor(a -> *) self, SymInt sections, int dim=0) -> Tensor(a)[]"
  },
  {
    "name": "aten::tensor_split.tensor_indices_or_sections(Tensor(a -> *) self, Tensor tensor_indices_or_sections, int dim=0) -> Tensor(a)[]"
  },
  {
    "name": "aten::tensordot(Tensor self, Tensor other, int[] dims_self, int[] dims_other) -> Tensor"
  },
  {
    "name": "aten::tensordot.out(Tensor self, Tensor other, int[] dims_self, int[] dims_other, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::threshold(Tensor self, Scalar threshold, Scalar value) -> Tensor",
    "category": "Activation"
  },
  {
    "name": "aten::threshold.out(Tensor self, Scalar threshold, Scalar value, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::threshold_(Tensor(a!) self, Scalar threshold, Scalar value) -> Tensor(a!)",
    "category": "Activation"
  },
  {
    "name": "aten::tile(Tensor self, SymInt[] dims) -> Tensor"
  },
  {
    "name": "aten::to.device(Tensor(a) self, Device device, ScalarType dtype, bool non_blocking=False, bool copy=False, MemoryFormat? memory_format=None) -> Tensor(a)"
  },
  {
    "name": "aten::to.dtype(Tensor(a) self, ScalarType dtype, bool non_blocking=False, bool copy=False, MemoryFormat? memory_format=None) -> Tensor(a)"
  },
  {
    "name": "aten::to.dtype_layout(Tensor(a) self, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, bool non_blocking=False, bool copy=False, MemoryFormat? memory_format=None) -> Tensor(a)"
  },
  {
    "name": "aten::to.other(Tensor(a) self, Tensor other, bool non_blocking=False, bool copy=False, MemoryFormat? memory_format=None) -> Tensor(a)"
  },
  {
    "name": "aten::to.prim_Device(Tensor(a) self, Device? device, int? dtype=None, bool non_blocking=False, bool copy=False) -> Tensor(a|b)"
  },
  {
    "name": "aten::to.prim_dtype(Tensor(a) self, int? dtype=None, bool non_blocking=False, bool copy=False) -> Tensor(a|b)"
  },
  {
    "name": "aten::to.prim_other(Tensor(a) self, bool non_blocking=False, bool copy=False) -> Tensor(a|b)"
  },
  {
    "name": "aten::to_dense(Tensor self, ScalarType? dtype=None, *, bool? masked_grad=None) -> Tensor"
  },
  {
    "name": "aten::to_dense_backward(Tensor grad, Tensor input, bool? masked_grad=None) -> Tensor"
  },
  {
    "name": "aten::to_mkldnn(Tensor self, ScalarType? dtype=None) -> Tensor"
  },
  {
    "name": "aten::to_mkldnn.out(Tensor self, ScalarType? dtype=None, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::to_mkldnn_backward(Tensor grad, Tensor input) -> Tensor"
  },
  {
    "name": "aten::to_padded_tensor(Tensor self, float padding, SymInt[]? output_size=None) -> Tensor"
  },
  {
    "name": "aten::to_padded_tensor.out(Tensor self, float padding, SymInt[]? output_size=None, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::to_sparse(Tensor self, *, Layout? layout=None, int[2]? blocksize=None, int? dense_dim=None) -> Tensor"
  },
  {
    "name": "aten::to_sparse.sparse_dim(Tensor self, int sparse_dim) -> Tensor"
  },
  {
    "name": "aten::to_sparse_bsc(Tensor self, int[2] blocksize, int? dense_dim=None) -> Tensor"
  },
  {
    "name": "aten::to_sparse_bsr(Tensor self, int[2] blocksize, int? dense_dim=None) -> Tensor"
  },
  {
    "name": "aten::to_sparse_csc(Tensor self, int? dense_dim=None) -> Tensor"
  },
  {
    "name": "aten::to_sparse_csr(Tensor self, int? dense_dim=None) -> Tensor"
  },
  {
    "name": "aten::topk(Tensor self, SymInt k, int dim=-1, bool largest=True, bool sorted=True) -> (Tensor values, Tensor indices)"
  },
  {
    "name": "aten::topk.values(Tensor self, SymInt k, int dim=-1, bool largest=True, bool sorted=True, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)"
  },
  {
    "name": "aten::trace(Tensor self) -> Tensor"
  },
  {
    "name": "aten::trace.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::transpose.Dimname(Tensor(a) self, str dim0, str dim1) -> Tensor(a)",
    "category": "Transform"
  },
  {
    "name": "aten::transpose.int(Tensor(a) self, int dim0, int dim1) -> Tensor(a)",
    "category": "Transform"
  },
  {
    "name": "aten::transpose_(Tensor(a!) self, int dim0, int dim1) -> Tensor(a!)",
    "category": "Transform"
  },
  {
    "name": "aten::transpose_copy.int(Tensor self, int dim0, int dim1) -> Tensor"
  },
  {
    "name": "aten::transpose_copy.int_out(Tensor self, int dim0, int dim1, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::tril(Tensor self, int diagonal=0) -> Tensor",
    "category": "Layer"
  },
  {
    "name": "aten::tril.out(Tensor self, int diagonal=0, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::tril_(Tensor(a!) self, int diagonal=0) -> Tensor(a!)"
  },
  {
    "name": "aten::tril_indices(int row, int col, int offset=0, *, ScalarType? dtype=4, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor",
    "category": "Layer"
  },
  {
    "name": "aten::tril_indices.out(int row, int col, int offset=0, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::triu(Tensor self, int diagonal=0) -> Tensor"
  },
  {
    "name": "aten::triu.out(Tensor self, int diagonal=0, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::triu_(Tensor(a!) self, int diagonal=0) -> Tensor(a!)"
  },
  {
    "name": "aten::triu_indices(int row, int col, int offset=0, *, ScalarType? dtype=4, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor"
  },
  {
    "name": "aten::triu_indices.out(int row, int col, int offset=0, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::true_divide.Scalar(Tensor self, Scalar other) -> Tensor"
  },
  {
    "name": "aten::true_divide.Tensor(Tensor self, Tensor other) -> Tensor"
  },
  {
    "name": "aten::true_divide.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::true_divide_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)"
  },
  {
    "name": "aten::true_divide_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)"
  },
  {
    "name": "aten::type_as(Tensor self, Tensor other) -> Tensor"
  },
  {
    "name": "aten::unbind.Dimname(Tensor(a -> *) self, str dim) -> Tensor(a)[]"
  },
  {
    "name": "aten::unbind.int(Tensor(a -> *) self, int dim=0) -> Tensor(a)[]"
  },
  {
    "name": "aten::unflatten.Dimname(Tensor(a) self, str dim, SymInt[] sizes, str[] names) -> Tensor(a)",
    "category": "Shape"
  },
  {
    "name": "aten::unflatten.int(Tensor(a) self, int dim, SymInt[] sizes) -> Tensor(a)",
    "category": "Shape"
  },
  {
    "name": "aten::unfold(Tensor(a) self, int dimension, int size, int step) -> Tensor(a)"
  },
  {
    "name": "aten::uniform_(Tensor(a!) self, float from=0., float to=1., *, Generator? generator=None) -> Tensor(a!)"
  },
  {
    "name": "aten::unique_consecutive(Tensor self, bool return_inverse=False, bool return_counts=False, int? dim=None) -> (Tensor, Tensor, Tensor)",
    "category": "Layer"
  },
  {
    "name": "aten::unique_consecutive.out(Tensor self, bool return_inverse=False, bool return_counts=False, int? dim=None, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2) -> (Tensor(a!), Tensor(b!), Tensor(c!))"
  },
  {
    "name": "aten::unique_dim(Tensor self, int dim, bool sorted=True, bool return_inverse=False, bool return_counts=False) -> (Tensor, Tensor, Tensor)"
  },
  {
    "name": "aten::unique_dim.out(Tensor self, int dim, bool sorted=True, bool return_inverse=False, bool return_counts=False, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2) -> (Tensor(a!), Tensor(b!), Tensor(c!))"
  },
  {
    "name": "aten::unique_dim_consecutive(Tensor self, int dim, bool return_inverse=False, bool return_counts=False) -> (Tensor, Tensor, Tensor)",
    "category": "Layer"
  },
  {
    "name": "aten::unique_dim_consecutive.out(Tensor self, int dim, bool return_inverse=False, bool return_counts=False, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2) -> (Tensor(a!), Tensor(b!), Tensor(c!))"
  },
  {
    "name": "aten::unsafe_chunk(Tensor self, int chunks, int dim=0) -> Tensor[]"
  },
  {
    "name": "aten::unsafe_split.Tensor(Tensor self, SymInt split_size, int dim=0) -> Tensor[]",
    "category": "Tensor"
  },
  {
    "name": "aten::unsafe_split.Tensor_out(Tensor self, SymInt split_size, int dim=0, *, Tensor(a!)[] out) -> ()"
  },
  {
    "name": "aten::unsqueeze(Tensor(a) self, int dim) -> Tensor(a)",
    "category": "Transform"
  },
  {
    "name": "aten::unsqueeze_(Tensor(a!) self, int dim) -> Tensor(a!)",
    "category": "Transform"
  },
  {
    "name": "aten::unsqueeze_copy(Tensor self, int dim) -> Tensor"
  },
  {
    "name": "aten::unsqueeze_copy.out(Tensor self, int dim, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::update.Tensor(Dict(Tensor, t)(a!) self, Dict(Tensor, t)(a!) to_add) -> ()"
  },
  {
    "name": "aten::update.bool(Dict(bool, t)(a!) self, Dict(bool, t)(a!) to_add) -> ()"
  },
  {
    "name": "aten::update.complex(Dict(complex, t)(a!) self, Dict(complex, t)(a!) to_add) -> ()"
  },
  {
    "name": "aten::update.float(Dict(float, t)(a!) self, Dict(float, t)(a!) to_add) -> ()"
  },
  {
    "name": "aten::update.int(Dict(int, t)(a!) self, Dict(int, t)(a!) to_add) -> ()"
  },
  {
    "name": "aten::update.str(Dict(str, t)(a!) self, Dict(str, t)(a!) to_add) -> ()"
  },
  {
    "name": "aten::upsample_bicubic2d(Tensor self, SymInt[2] output_size, bool align_corners, float? scales_h=None, float? scales_w=None) -> Tensor",
    "category": "Layer"
  },
  {
    "name": "aten::upsample_bicubic2d.out(Tensor self, SymInt[2] output_size, bool align_corners, float? scales_h=None, float? scales_w=None, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::upsample_bicubic2d.vec(Tensor input, SymInt[]? output_size, bool align_corners, float[]? scale_factors) -> Tensor",
    "category": "Layer"
  },
  {
    "name": "aten::upsample_bilinear2d(Tensor self, SymInt[2] output_size, bool align_corners, float? scales_h=None, float? scales_w=None) -> Tensor",
    "category": "Layer"
  },
  {
    "name": "aten::upsample_bilinear2d.out(Tensor self, SymInt[2] output_size, bool align_corners, float? scales_h=None, float? scales_w=None, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::upsample_bilinear2d.vec(Tensor input, SymInt[]? output_size, bool align_corners, float[]? scale_factors) -> Tensor",
    "category": "Layer"
  },
  {
    "name": "aten::upsample_bilinear2d.vec_out(Tensor input, SymInt[]? output_size, bool align_corners, float[]? scale_factors, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::upsample_bilinear2d_backward(Tensor grad_output, SymInt[2] output_size, SymInt[4] input_size, bool align_corners, float? scales_h=None, float? scales_w=None) -> Tensor"
  },
  {
    "name": "aten::upsample_bilinear2d_backward.grad_input(Tensor grad_output, SymInt[2] output_size, SymInt[4] input_size, bool align_corners, float? scales_h=None, float? scales_w=None, *, Tensor(a!) grad_input) -> Tensor(a!)"
  },
  {
    "name": "aten::upsample_linear1d(Tensor self, SymInt[1] output_size, bool align_corners, float? scales=None) -> Tensor"
  },
  {
    "name": "aten::upsample_linear1d.out(Tensor self, SymInt[1] output_size, bool align_corners, float? scales=None, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::upsample_linear1d.vec(Tensor input, SymInt[]? output_size, bool align_corners, float[]? scale_factors) -> Tensor"
  },
  {
    "name": "aten::upsample_nearest1d(Tensor self, SymInt[1] output_size, float? scales=None) -> Tensor",
    "category": "Layer"
  },
  {
    "name": "aten::upsample_nearest1d.out(Tensor self, SymInt[1] output_size, float? scales=None, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::upsample_nearest1d.vec(Tensor input, SymInt[]? output_size, float[]? scale_factors) -> Tensor",
    "category": "Layer"
  },
  {
    "name": "aten::upsample_nearest2d(Tensor self, SymInt[2] output_size, float? scales_h=None, float? scales_w=None) -> Tensor",
    "category": "Layer"
  },
  {
    "name": "aten::upsample_nearest2d.out(Tensor self, SymInt[2] output_size, float? scales_h=None, float? scales_w=None, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::upsample_nearest2d.vec(Tensor input, SymInt[]? output_size, float[]? scale_factors) -> Tensor",
    "category": "Layer"
  },
  {
    "name": "aten::upsample_nearest2d.vec_out(Tensor input, SymInt[]? output_size, float[]? scale_factors, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::upsample_nearest2d_backward(Tensor grad_output, SymInt[2] output_size, SymInt[4] input_size, float? scales_h=None, float? scales_w=None) -> Tensor"
  },
  {
    "name": "aten::upsample_nearest2d_backward.grad_input(Tensor grad_output, SymInt[2] output_size, SymInt[4] input_size, float? scales_h=None, float? scales_w=None, *, Tensor(a!) grad_input) -> Tensor(a!)"
  },
  {
    "name": "aten::upsample_nearest3d(Tensor self, SymInt[3] output_size, float? scales_d=None, float? scales_h=None, float? scales_w=None) -> Tensor",
    "category": "Layer"
  },
  {
    "name": "aten::upsample_nearest3d.out(Tensor self, SymInt[3] output_size, float? scales_d=None, float? scales_h=None, float? scales_w=None, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::upsample_nearest3d.vec(Tensor input, SymInt[]? output_size, float[]? scale_factors) -> Tensor",
    "category": "Layer"
  },
  {
    "name": "aten::upsample_trilinear3d(Tensor self, SymInt[3] output_size, bool align_corners, float? scales_d=None, float? scales_h=None, float? scales_w=None) -> Tensor"
  },
  {
    "name": "aten::upsample_trilinear3d.out(Tensor self, SymInt[3] output_size, bool align_corners, float? scales_d=None, float? scales_h=None, float? scales_w=None, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::upsample_trilinear3d.vec(Tensor input, SymInt[]? output_size, bool align_corners, float[]? scale_factors) -> Tensor"
  },
  {
    "name": "aten::upsample_trilinear3d_backward(Tensor grad_output, SymInt[3] output_size, SymInt[5] input_size, bool align_corners, float? scales_d=None, float? scales_h=None, float? scales_w=None) -> Tensor"
  },
  {
    "name": "aten::upsample_trilinear3d_backward.grad_input(Tensor grad_output, SymInt[3] output_size, SymInt[5] input_size, bool align_corners, float? scales_d=None, float? scales_h=None, float? scales_w=None, *, Tensor(a!) grad_input) -> Tensor(a!)"
  },
  {
    "name": "aten::values(Tensor(a) self) -> Tensor(a)"
  },
  {
    "name": "aten::values.Tensor(Dict(Tensor, t) self) -> t[](*)"
  },
  {
    "name": "aten::values.bool(Dict(bool, t) self) -> t[](*)"
  },
  {
    "name": "aten::values.complex(Dict(complex, t) self) -> t[](*)"
  },
  {
    "name": "aten::values.float(Dict(float, t) self) -> t[](*)"
  },
  {
    "name": "aten::values.int(Dict(int, t) self) -> t[](*)"
  },
  {
    "name": "aten::values.str(Dict(str, t) self) -> t[](*)"
  },
  {
    "name": "aten::var(Tensor self, bool unbiased=True) -> Tensor"
  },
  {
    "name": "aten::var.correction(Tensor self, int[1]? dim=None, *, Scalar? correction=None, bool keepdim=False) -> Tensor"
  },
  {
    "name": "aten::var.correction_names(Tensor self, str[1] dim, *, Scalar? correction=None, bool keepdim=False) -> Tensor"
  },
  {
    "name": "aten::var.correction_names_out(Tensor self, str[1] dim, *, Scalar? correction=None, bool keepdim=False, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::var.correction_out(Tensor self, int[1]? dim=None, *, Scalar? correction=None, bool keepdim=False, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::var.dim(Tensor self, int[1]? dim, bool unbiased=True, bool keepdim=False) -> Tensor"
  },
  {
    "name": "aten::var.names_dim(Tensor self, str[1] dim, bool unbiased=True, bool keepdim=False) -> Tensor"
  },
  {
    "name": "aten::var.names_out(Tensor self, str[1] dim, bool unbiased=True, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::var.out(Tensor self, int[1]? dim, bool unbiased=True, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::var_mean(Tensor self, bool unbiased=True) -> (Tensor, Tensor)"
  },
  {
    "name": "aten::var_mean.correction(Tensor self, int[1]? dim=None, *, Scalar? correction=None, bool keepdim=False) -> (Tensor, Tensor)"
  },
  {
    "name": "aten::var_mean.correction_names(Tensor self, str[1] dim, *, Scalar? correction=None, bool keepdim=False) -> (Tensor, Tensor)"
  },
  {
    "name": "aten::var_mean.correction_out(Tensor self, int[1]? dim=None, *, Scalar? correction=None, bool keepdim=False, Tensor(a!) out0, Tensor(b!) out1) -> (Tensor(a!), Tensor(b!))"
  },
  {
    "name": "aten::var_mean.dim(Tensor self, int[1]? dim, bool unbiased=True, bool keepdim=False) -> (Tensor, Tensor)"
  },
  {
    "name": "aten::var_mean.names_dim(Tensor self, str[1] dim, bool unbiased=True, bool keepdim=False) -> (Tensor, Tensor)"
  },
  {
    "name": "aten::vdot(Tensor self, Tensor other) -> Tensor"
  },
  {
    "name": "aten::vdot.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::view(Tensor(a) self, SymInt[] size) -> Tensor(a)"
  },
  {
    "name": "aten::view.dtype(Tensor(a) self, ScalarType dtype) -> Tensor(a)"
  },
  {
    "name": "aten::view_as(Tensor(a) self, Tensor other) -> Tensor(a)"
  },
  {
    "name": "aten::view_as_complex(Tensor(a) self) -> Tensor(a)"
  },
  {
    "name": "aten::view_as_complex_copy(Tensor self) -> Tensor"
  },
  {
    "name": "aten::view_as_complex_copy.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::view_as_real(Tensor(a) self) -> Tensor(a)"
  },
  {
    "name": "aten::view_as_real_copy(Tensor self) -> Tensor"
  },
  {
    "name": "aten::view_as_real_copy.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::view_copy(Tensor self, SymInt[] size) -> Tensor"
  },
  {
    "name": "aten::view_copy.dtype(Tensor self, ScalarType dtype) -> Tensor"
  },
  {
    "name": "aten::view_copy.dtype_out(Tensor self, ScalarType dtype, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::view_copy.out(Tensor self, SymInt[] size, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::vstack(Tensor[] tensors) -> Tensor"
  },
  {
    "name": "aten::vstack.out(Tensor[] tensors, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::wait(Future(t) self) -> t"
  },
  {
    "name": "aten::warn(str message, int stacklevel=2) -> ()"
  },
  {
    "name": "aten::where(Tensor condition) -> Tensor[]"
  },
  {
    "name": "aten::where.Scalar(Tensor condition, Scalar self, Scalar other) -> Tensor"
  },
  {
    "name": "aten::where.ScalarOther(Tensor condition, Tensor self, Scalar other) -> Tensor"
  },
  {
    "name": "aten::where.ScalarSelf(Tensor condition, Scalar self, Tensor other) -> Tensor"
  },
  {
    "name": "aten::where.self(Tensor condition, Tensor self, Tensor other) -> Tensor"
  },
  {
    "name": "aten::where.self_out(Tensor condition, Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::zero_(Tensor(a!) self) -> Tensor(a!)"
  },
  {
    "name": "aten::zeros(SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor"
  },
  {
    "name": "aten::zeros.names(int[] size, *, str[]? names, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor"
  },
  {
    "name": "aten::zeros.names_out(int[] size, *, str[]? names, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::zeros.out(SymInt[] size, *, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "aten::zeros_like(Tensor self, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor"
  },
  {
    "name": "aten::zeros_like.out(Tensor self, *, MemoryFormat? memory_format=None, Tensor(a!) out) -> Tensor(a!)"
  },
  {
    "name": "neuron::forward_v2_1(Tensor[] _0, __torch__.torch.classes.neuron.Model _1) -> (Tensor _0)"
  },
  {
    "name": "prepacked::conv2d_clamp_prepack(Tensor W, Tensor? B, int[2] stride, int[2] padding, int[2] dilation, int groups, Scalar? output_min=None, Scalar? output_max=None) -> __torch__.torch.classes.xnnpack.Conv2dOpContext"
  },
  {
    "name": "prepacked::conv2d_clamp_run(Tensor X, __torch__.torch.classes.xnnpack.Conv2dOpContext W_prepack) -> Tensor Y",
    "category": "Layer"
  },
  {
    "name": "prepacked::conv2d_transpose_clamp_run(Tensor X, __torch__.torch.classes.xnnpack.TransposeConv2dOpContext W_prepack) -> Tensor Y",
    "category": "Layer"
  },
  {
    "name": "prepacked::linear_clamp_prepack(Tensor W, Tensor? B=None, Scalar? output_min=None, Scalar? output_max=None) -> __torch__.torch.classes.xnnpack.LinearOpContext"
  },
  {
    "name": "prepacked::linear_clamp_run(Tensor X, __torch__.torch.classes.xnnpack.LinearOpContext W_prepack) -> Tensor Y",
    "category": "Layer"
  },
  {
    "name": "prim::AutogradAdd(Any a, Any b) -> Any"
  },
  {
    "name": "prim::AutogradAllNonZero(...) -> bool"
  },
  {
    "name": "prim::AutogradAllZero(...) -> bool"
  },
  {
    "name": "prim::AutogradAnyNonZero(...) -> bool"
  },
  {
    "name": "prim::AutogradZero() -> Tensor"
  },
  {
    "name": "prim::BroadcastSizes(...) -> int[]"
  },
  {
    "name": "prim::EnumName(AnyEnumType enum) -> str"
  },
  {
    "name": "prim::EnumValue.float(AnyEnumType enum) -> float"
  },
  {
    "name": "prim::EnumValue.int(AnyEnumType enum) -> int"
  },
  {
    "name": "prim::EnumValue.str(AnyEnumType enum) -> str"
  },
  {
    "name": "prim::IfThenElse(bool cond, Any(a) x, Any(b) y) -> Any(a|b)"
  },
  {
    "name": "prim::ModuleContainerIndex.dict(Any self, str ind) -> Any"
  },
  {
    "name": "prim::ModuleContainerIndex.list(Any self, int ind) -> Any"
  },
  {
    "name": "prim::NumToTensor.Scalar(Scalar a) -> Tensor"
  },
  {
    "name": "prim::NumToTensor.bool(bool a) -> Tensor"
  },
  {
    "name": "prim::Print(...) -> ()"
  },
  {
    "name": "prim::RaiseException(str msg, str? cls=None) -> ()"
  },
  {
    "name": "prim::ReductionSizes(int[] size, int[] red_axes, bool keepdim=False) -> int[]"
  },
  {
    "name": "prim::StringIndex(str string, int index) -> str"
  },
  {
    "name": "prim::TupleIndex(Any tup, int i) -> Any"
  },
  {
    "name": "prim::TupleUnpack(Any tup) -> ..."
  },
  {
    "name": "prim::Uninitialized() -> Any"
  },
  {
    "name": "prim::VarConcat(...) -> Tensor"
  },
  {
    "name": "prim::VarStack(...) -> Tensor"
  },
  {
    "name": "prim::abs(Tensor x) -> Tensor"
  },
  {
    "name": "prim::abs.Scalar(Scalar a) -> Scalar"
  },
  {
    "name": "prim::abs.complex(complex a) -> float"
  },
  {
    "name": "prim::abs.float(float a) -> float"
  },
  {
    "name": "prim::abs.int(int a) -> int"
  },
  {
    "name": "prim::data(Tensor(a) a) -> Tensor(a)"
  },
  {
    "name": "prim::device(Tensor a) -> Device"
  },
  {
    "name": "prim::dtype(Tensor a) -> int"
  },
  {
    "name": "prim::grad(Tensor a) -> Tensor(*)"
  },
  {
    "name": "prim::id(AnyClassType? x) -> int"
  },
  {
    "name": "prim::index(Device self) -> int?"
  },
  {
    "name": "prim::is_cpu(Tensor a) -> bool"
  },
  {
    "name": "prim::is_cuda(Tensor a) -> bool"
  },
  {
    "name": "prim::is_ipu(Tensor a) -> bool"
  },
  {
    "name": "prim::is_maia(Tensor a) -> bool"
  },
  {
    "name": "prim::is_meta(Tensor a) -> bool"
  },
  {
    "name": "prim::is_mkldnn(Tensor a) -> bool"
  },
  {
    "name": "prim::is_mps(Tensor a) -> bool"
  },
  {
    "name": "prim::is_mtia(Tensor a) -> bool"
  },
  {
    "name": "prim::is_nested(Tensor a) -> bool"
  },
  {
    "name": "prim::is_quantized(Tensor a) -> bool"
  },
  {
    "name": "prim::is_sparse(Tensor a) -> bool"
  },
  {
    "name": "prim::is_sparse_csr(Tensor a) -> bool"
  },
  {
    "name": "prim::is_vulkan(Tensor a) -> bool"
  },
  {
    "name": "prim::is_xla(Tensor a) -> bool"
  },
  {
    "name": "prim::is_xpu(Tensor a) -> bool"
  },
  {
    "name": "prim::isinstance(Any to_check) -> bool"
  },
  {
    "name": "prim::itemsize(Tensor a) -> int"
  },
  {
    "name": "prim::layout(Tensor a) -> Layout"
  },
  {
    "name": "prim::max(Scalar a, Scalar b) -> Scalar"
  },
  {
    "name": "prim::max.bool_list(bool[] l, bool[] r) -> bool[]"
  },
  {
    "name": "prim::max.float(float a, float b) -> float"
  },
  {
    "name": "prim::max.float_int(float a, int b) -> float"
  },
  {
    "name": "prim::max.float_list(float[] l, float[] r) -> float[]"
  },
  {
    "name": "prim::max.int(int a, int b) -> int"
  },
  {
    "name": "prim::max.int_float(int a, float b) -> float"
  },
  {
    "name": "prim::max.int_list(int[] l, int[] r) -> int[]"
  },
  {
    "name": "prim::max.self_bool(bool[] self) -> bool"
  },
  {
    "name": "prim::max.self_float(float[] self) -> float"
  },
  {
    "name": "prim::max.self_int(int[] self) -> int"
  },
  {
    "name": "prim::min(Scalar a, Scalar b) -> Scalar"
  },
  {
    "name": "prim::min.bool_list(bool[] l, bool[] r) -> bool[]"
  },
  {
    "name": "prim::min.float(float a, float b) -> float"
  },
  {
    "name": "prim::min.float_int(float a, int b) -> float"
  },
  {
    "name": "prim::min.float_list(float[] l, float[] r) -> float[]"
  },
  {
    "name": "prim::min.int(int a, int b) -> int"
  },
  {
    "name": "prim::min.int_float(int a, float b) -> float"
  },
  {
    "name": "prim::min.int_list(int[] l, int[] r) -> int[]"
  },
  {
    "name": "prim::min.self_bool(bool[] self) -> bool"
  },
  {
    "name": "prim::min.self_float(float[] self) -> float"
  },
  {
    "name": "prim::min.self_int(int[] self) -> int"
  },
  {
    "name": "prim::name(Tensor a) -> str?"
  },
  {
    "name": "prim::nbytes(Tensor a) -> int"
  },
  {
    "name": "prim::rangelist(int n) -> int[]"
  },
  {
    "name": "prim::requires_grad(Tensor a) -> bool"
  },
  {
    "name": "prim::shape(Tensor self) -> int[]"
  },
  {
    "name": "prim::tolist(...) -> ..."
  },
  {
    "name": "prim::type(Device self) -> str"
  },
  {
    "name": "prim::unchecked_cast(t x) -> t"
  },
  {
    "name": "prim::unchecked_unwrap_optional(t(a)? optional) -> t(a)"
  },
  {
    "name": "quantized::add(Tensor qa, Tensor qb, float scale, int zero_point) -> Tensor qc"
  },
  {
    "name": "quantized::add.Scalar(Tensor qa, Scalar b) -> Tensor qc"
  },
  {
    "name": "quantized::add.Scalar2(Scalar b, Tensor qa) -> Tensor qc"
  },
  {
    "name": "quantized::add.Scalar_out(Tensor qa, Scalar b, Tensor(a!) out) -> Tensor(a!) out"
  },
  {
    "name": "quantized::add.out(Tensor qa, Tensor qb, Tensor(a!) out) -> Tensor(a!) out"
  },
  {
    "name": "quantized::add_out(Tensor qa, Tensor qb, Tensor(a!) out) -> Tensor(a!) out"
  },
  {
    "name": "quantized::add_relu(Tensor qa, Tensor qb, float scale, int zero_point) -> Tensor qc"
  },
  {
    "name": "quantized::add_relu.Scalar(Tensor qa, Scalar b) -> Tensor qc"
  },
  {
    "name": "quantized::add_relu.Scalar2(Scalar b, Tensor qa) -> Tensor qc"
  },
  {
    "name": "quantized::add_relu.Scalar_out(Tensor qa, Scalar b, Tensor(a!) out) -> Tensor(a!) out"
  },
  {
    "name": "quantized::add_relu.out(Tensor qa, Tensor qb, Tensor(a!) out) -> Tensor(a!) out"
  },
  {
    "name": "quantized::add_relu_out(Tensor qa, Tensor qb, Tensor(a!) out) -> Tensor(a!) out"
  },
  {
    "name": "quantized::add_scalar(Tensor qa, Scalar b) -> Tensor qc"
  },
  {
    "name": "quantized::add_scalar.Tensor(Tensor qa, Tensor b) -> Tensor qc"
  },
  {
    "name": "quantized::add_scalar_out(Tensor qa, Scalar b, Tensor(a!) out) -> Tensor(a!) out"
  },
  {
    "name": "quantized::add_scalar_out.Tensor(Tensor qa, Tensor b, Tensor(a!) out) -> Tensor(a!) out"
  },
  {
    "name": "quantized::add_scalar_relu(Tensor qa, Scalar b) -> Tensor qc"
  },
  {
    "name": "quantized::add_scalar_relu.Tensor(Tensor qa, Tensor b) -> Tensor qc"
  },
  {
    "name": "quantized::add_scalar_relu_out(Tensor qa, Scalar b, Tensor(a!) out) -> Tensor(a!) out"
  },
  {
    "name": "quantized::add_scalar_relu_out.Tensor(Tensor qa, Tensor b, Tensor(a!) out) -> Tensor(a!) out"
  },
  {
    "name": "quantized::batch_norm(Tensor qx, Tensor? weight, Tensor? bias, Tensor mean, Tensor var, float eps, float output_scale, int output_zero_point) -> Tensor"
  },
  {
    "name": "quantized::batch_norm1d(Tensor qx, Tensor? weight, Tensor? bias, Tensor mean, Tensor var, float eps, float output_scale, int output_zero_point) -> Tensor",
    "category": "Normalization"
  },
  {
    "name": "quantized::batch_norm1d_relu(Tensor qx, Tensor? weight, Tensor? bias, Tensor mean, Tensor var, float eps, float output_scale, int output_zero_point) -> Tensor",
    "category": "Normalization"
  },
  {
    "name": "quantized::batch_norm2d(Tensor qx, Tensor? weight, Tensor? bias, Tensor mean, Tensor var, float eps, float output_scale, int output_zero_point) -> Tensor",
    "category": "Normalization"
  },
  {
    "name": "quantized::batch_norm2d_relu(Tensor qx, Tensor? weight, Tensor? bias, Tensor mean, Tensor var, float eps, float output_scale, int output_zero_point) -> Tensor",
    "category": "Normalization"
  },
  {
    "name": "quantized::batch_norm3d(Tensor qx, Tensor? weight, Tensor? bias, Tensor mean, Tensor var, float eps, float output_scale, int output_zero_point) -> Tensor",
    "category": "Normalization"
  },
  {
    "name": "quantized::batch_norm3d_relu(Tensor qx, Tensor? weight, Tensor? bias, Tensor mean, Tensor var, float eps, float output_scale, int output_zero_point) -> Tensor",
    "category": "Normalization"
  },
  {
    "name": "quantized::batch_norm_relu(Tensor qx, Tensor? weight, Tensor? bias, Tensor mean, Tensor var, float eps, float output_scale, int output_zero_point) -> Tensor",
    "category": "Normalization"
  },
  {
    "name": "quantized::cat(Tensor[] qx, int dim, float? scale, int? zero_point) -> Tensor",
    "category": "Tensor"
  },
  {
    "name": "quantized::cat_relu(Tensor[] qx, int dim, float? scale, int? zero_point) -> Tensor",
    "category": "Tensor"
  },
  {
    "name": "quantized::celu(Tensor self, float output_scale, int output_zero_point, Scalar alpha=1) -> Tensor",
    "category": "Activation"
  },
  {
    "name": "quantized::conv1d(Tensor qx, __torch__.torch.classes.quantized.Conv2dPackedParamsBase packed_weight, float output_scale, int output_zero_point) -> Tensor",
    "category": "Layer"
  },
  {
    "name": "quantized::conv1d_prepack(Tensor weight, Tensor? bias, int[] stride, int[] padding, int[] dilation, int groups) -> __torch__.torch.classes.quantized.Conv2dPackedParamsBase"
  },
  {
    "name": "quantized::conv1d_relu(Tensor qx, __torch__.torch.classes.quantized.Conv2dPackedParamsBase packed_weight, float output_scale, int output_zero_point) -> Tensor",
    "category": "Layer"
  },
  {
    "name": "quantized::conv2d(Tensor qx, __torch__.torch.classes.quantized.Conv2dPackedParamsBase weight, int[] stride, int[] padding, int[] dilation, int groups, float output_scale, int output_zero_point) -> Tensor",
    "category": "Layer"
  },
  {
    "name": "quantized::conv2d.new(Tensor qx, __torch__.torch.classes.quantized.Conv2dPackedParamsBase packed_weight, float output_scale, int output_zero_point) -> Tensor",
    "category": "Layer"
  },
  {
    "name": "quantized::conv2d_dilation(__torch__.torch.classes.quantized.Conv2dPackedParamsBase packed_weights) -> int[]"
  },
  {
    "name": "quantized::conv2d_dynamic(Tensor qx, __torch__.torch.classes.quantized.Conv2dPackedParamsBase packed_weight, bool reduce_range=False) -> Tensor"
  },
  {
    "name": "quantized::conv2d_groups(__torch__.torch.classes.quantized.Conv2dPackedParamsBase packed_weights) -> int"
  },
  {
    "name": "quantized::conv2d_output_padding(__torch__.torch.classes.quantized.Conv2dPackedParamsBase packed_weights) -> int[]"
  },
  {
    "name": "quantized::conv2d_padding(__torch__.torch.classes.quantized.Conv2dPackedParamsBase packed_weights) -> int[]"
  },
  {
    "name": "quantized::conv2d_prepack(Tensor weight, Tensor? bias, int[] stride, int[] padding, int[] dilation, int groups) -> __torch__.torch.classes.quantized.Conv2dPackedParamsBase"
  },
  {
    "name": "quantized::conv2d_relu(Tensor qx, __torch__.torch.classes.quantized.Conv2dPackedParamsBase weight, int[] stride, int[] padding, int[] dilation, int groups, float output_scale, int output_zero_point) -> Tensor",
    "category": "Layer"
  },
  {
    "name": "quantized::conv2d_relu.new(Tensor qx, __torch__.torch.classes.quantized.Conv2dPackedParamsBase packed_weight, float output_scale, int output_zero_point) -> Tensor",
    "category": "Layer"
  },
  {
    "name": "quantized::conv2d_stride(__torch__.torch.classes.quantized.Conv2dPackedParamsBase packed_weights) -> int[]"
  },
  {
    "name": "quantized::conv2d_transpose(__torch__.torch.classes.quantized.Conv2dPackedParamsBase packed_weights) -> int"
  },
  {
    "name": "quantized::conv2d_unpack(__torch__.torch.classes.quantized.Conv2dPackedParamsBase packed_weights) -> (Tensor unpacked_weights, Tensor? B_origin)"
  },
  {
    "name": "quantized::conv2d_unpack_sizes(Any packed_weights) -> Any"
  },
  {
    "name": "quantized::conv3d(Tensor qx, __torch__.torch.classes.quantized.Conv3dPackedParamsBase weight, int[] stride, int[] padding, int[] dilation, int groups, float output_scale, int output_zero_point) -> Tensor",
    "category": "Layer"
  },
  {
    "name": "quantized::conv3d.new(Tensor qx, __torch__.torch.classes.quantized.Conv3dPackedParamsBase packed_weight, float output_scale, int output_zero_point) -> Tensor",
    "category": "Layer"
  },
  {
    "name": "quantized::conv3d_prepack(Tensor weight, Tensor? bias, int[] stride, int[] padding, int[] dilation, int groups) -> __torch__.torch.classes.quantized.Conv3dPackedParamsBase"
  },
  {
    "name": "quantized::conv3d_relu(Tensor qx, __torch__.torch.classes.quantized.Conv3dPackedParamsBase weight, int[] stride, int[] padding, int[] dilation, int groups, float output_scale, int output_zero_point) -> Tensor",
    "category": "Layer"
  },
  {
    "name": "quantized::conv3d_relu.new(Tensor qx, __torch__.torch.classes.quantized.Conv3dPackedParamsBase packed_weight, float output_scale, int output_zero_point) -> Tensor",
    "category": "Layer"
  },
  {
    "name": "quantized::conv_prepack(Tensor weight, Tensor? bias, int[] stride, int[] padding, int[] dilation, int groups) -> __torch__.torch.classes.quantized.Conv2dPackedParamsBase"
  },
  {
    "name": "quantized::conv_transpose1d_prepack(Tensor weight, Tensor? bias, int[] stride, int[] padding, int[] output_padding, int[] dilation, int groups) -> __torch__.torch.classes.quantized.Conv2dPackedParamsBase"
  },
  {
    "name": "quantized::conv_transpose2d(Tensor qx, __torch__.torch.classes.quantized.Conv2dPackedParamsBase packed_weight, float output_scale, int output_zero_point) -> Tensor"
  },
  {
    "name": "quantized::conv_transpose2d_dilation(__torch__.torch.classes.quantized.Conv2dPackedParamsBase packed_weights) -> int[]"
  },
  {
    "name": "quantized::conv_transpose2d_dynamic(Tensor qx, __torch__.torch.classes.quantized.Conv2dPackedParamsBase packed_weight, bool reduce_range=False) -> Tensor"
  },
  {
    "name": "quantized::conv_transpose2d_groups(__torch__.torch.classes.quantized.Conv2dPackedParamsBase packed_weights) -> int"
  },
  {
    "name": "quantized::conv_transpose2d_output_padding(__torch__.torch.classes.quantized.Conv2dPackedParamsBase packed_weights) -> int[]"
  },
  {
    "name": "quantized::conv_transpose2d_padding(__torch__.torch.classes.quantized.Conv2dPackedParamsBase packed_weights) -> int[]"
  },
  {
    "name": "quantized::conv_transpose2d_prepack(Tensor weight, Tensor? bias, int[] stride, int[] padding, int[] output_padding, int[] dilation, int groups) -> __torch__.torch.classes.quantized.Conv2dPackedParamsBase"
  },
  {
    "name": "quantized::conv_transpose2d_stride(__torch__.torch.classes.quantized.Conv2dPackedParamsBase packed_weights) -> int[]"
  },
  {
    "name": "quantized::conv_transpose2d_transpose(__torch__.torch.classes.quantized.Conv2dPackedParamsBase packed_weights) -> int"
  },
  {
    "name": "quantized::conv_transpose2d_unpack(__torch__.torch.classes.quantized.Conv2dPackedParamsBase packed_weights) -> (Tensor unpacked_weights, Tensor? B_origin)"
  },
  {
    "name": "quantized::embedding_bag_4bit_rowwise_offsets(Tensor weight, Tensor indices, Tensor? offsets=None, bool scale_grad_by_freq=False, int mode=0, bool pruned_weights=False, Tensor? per_sample_weights=None, Tensor? compressed_indices_mapping=None, bool include_last_offset=False) -> Tensor"
  },
  {
    "name": "quantized::embedding_bag_byte_rowwise_offsets(Tensor weight, Tensor indices, Tensor? offsets=None, bool scale_grad_by_freq=False, int mode=0, bool pruned_weights=False, Tensor? per_sample_weights=None, Tensor? compressed_indices_mapping=None, bool include_last_offset=False) -> Tensor"
  },
  {
    "name": "quantized::embedding_byte(__torch__.torch.classes.quantized.EmbeddingPackedParamsBase weight, Tensor indices, bool pruned_weights=False) -> Tensor"
  },
  {
    "name": "quantized::hardswish(Tensor input, float output_scale, int output_zero_point) -> Tensor",
    "category": "Activation"
  },
  {
    "name": "quantized::instance_norm(Tensor input, Tensor? weight, Tensor? bias, float eps, float output_scale, int output_zero_point) -> Tensor"
  },
  {
    "name": "quantized::layer_norm(Tensor input, int[] normalized_shape, Tensor? weight, Tensor? bias, float eps, float output_scale, int output_zero_point) -> Tensor",
    "category": "Normalization"
  },
  {
    "name": "quantized::leaky_relu(Tensor qx, Scalar negative_slope, bool inplace, float output_scale, int output_zero_point) -> Tensor",
    "category": "Activation"
  },
  {
    "name": "quantized::linear(Tensor X, __torch__.torch.classes.quantized.LinearPackedParamsBase W_prepack, float Y_scale_i, int Y_zero_point_i) -> Tensor Y",
    "category": "Layer"
  },
  {
    "name": "quantized::linear_dynamic(Tensor X, __torch__.torch.classes.quantized.LinearPackedParamsBase W_prepack, bool reduce_range=False) -> Tensor Y",
    "category": "Layer"
  },
  {
    "name": "quantized::linear_prepack_fp16(Tensor W, Tensor? B=None) -> __torch__.torch.classes.quantized.LinearPackedParamsBase W_prepack"
  },
  {
    "name": "quantized::linear_prepack_fp16_legacy(Tensor W, Tensor? B=None) -> Tensor W_prepack"
  },
  {
    "name": "quantized::linear_relu(Tensor X, __torch__.torch.classes.quantized.LinearPackedParamsBase W_prepack, float Y_scale_i, int Y_zero_point_i) -> Tensor Y",
    "category": "Layer"
  },
  {
    "name": "quantized::linear_relu_dynamic(Tensor X, __torch__.torch.classes.quantized.LinearPackedParamsBase W_prepack, bool reduce_range=False) -> Tensor Y",
    "category": "Layer"
  },
  {
    "name": "quantized::make_quantized_cell_params(Tensor w_ih, Tensor w_hh, Tensor b_ih, Tensor b_hh) -> __torch__.torch.classes.rnn.CellParamsBase"
  },
  {
    "name": "quantized::make_quantized_cell_params_dynamic(__torch__.torch.classes.quantized.LinearPackedParamsBase w_ih, __torch__.torch.classes.quantized.LinearPackedParamsBase w_hh, Tensor bias_ih, Tensor bias_hh, bool reduce_range=False) -> __torch__.torch.classes.rnn.CellParamsBase"
  },
  {
    "name": "quantized::make_quantized_cell_params_fp16(__torch__.torch.classes.quantized.LinearPackedParamsBase w_ih, __torch__.torch.classes.quantized.LinearPackedParamsBase w_hh) -> __torch__.torch.classes.rnn.CellParamsBase"
  },
  {
    "name": "quantized::mul(Tensor qa, Tensor qb, float scale, int zero_point) -> Tensor qc"
  },
  {
    "name": "quantized::mul.Scalar(Tensor qa, Scalar b) -> Tensor qc"
  },
  {
    "name": "quantized::mul.Scalar2(Scalar b, Tensor qa) -> Tensor qc"
  },
  {
    "name": "quantized::mul.Scalar_out(Tensor qa, Scalar b, Tensor(a!) out) -> Tensor(a!) out"
  },
  {
    "name": "quantized::mul.out(Tensor qa, Tensor qb, Tensor(a!) out) -> Tensor(a!) out"
  },
  {
    "name": "quantized::mul_out(Tensor qa, Tensor qb, Tensor(a!) out) -> Tensor(a!) out"
  },
  {
    "name": "quantized::mul_relu(Tensor qa, Tensor qb, float scale, int zero_point) -> Tensor qc"
  },
  {
    "name": "quantized::mul_relu.Scalar(Tensor qa, Scalar b) -> Tensor qc"
  },
  {
    "name": "quantized::mul_relu.Scalar2(Scalar b, Tensor qa) -> Tensor qc"
  },
  {
    "name": "quantized::mul_relu.Scalar_out(Tensor qa, Scalar b, Tensor(a!) out) -> Tensor(a!) out"
  },
  {
    "name": "quantized::mul_relu.out(Tensor qa, Tensor qb, Tensor(a!) out) -> Tensor(a!) out"
  },
  {
    "name": "quantized::mul_relu_out(Tensor qa, Tensor qb, Tensor(a!) out) -> Tensor(a!) out"
  },
  {
    "name": "quantized::mul_scalar(Tensor qa, Scalar b) -> Tensor qc"
  },
  {
    "name": "quantized::mul_scalar.Tensor(Tensor qa, Tensor b) -> Tensor qc"
  },
  {
    "name": "quantized::mul_scalar_out(Tensor qa, Scalar b, Tensor(a!) out) -> Tensor(a!) out"
  },
  {
    "name": "quantized::mul_scalar_out.Tensor(Tensor qa, Tensor b, Tensor(a!) out) -> Tensor(a!) out"
  },
  {
    "name": "quantized::mul_scalar_relu(Tensor qa, Scalar b) -> Tensor qc"
  },
  {
    "name": "quantized::mul_scalar_relu.Tensor(Tensor qa, Tensor b) -> Tensor qc"
  },
  {
    "name": "quantized::mul_scalar_relu_out(Tensor qa, Scalar b, Tensor(a!) out) -> Tensor(a!) out"
  },
  {
    "name": "quantized::mul_scalar_relu_out.Tensor(Tensor qa, Tensor b, Tensor(a!) out) -> Tensor(a!) out"
  },
  {
    "name": "quantized::prelu(Tensor qx, Tensor weight, float output_scale, int output_zero_point) -> Tensor"
  },
  {
    "name": "quantized::quantized_gru_cell_dynamic(Tensor input, Tensor hx, __torch__.torch.classes.quantized.LinearPackedParamsBase w_ih, __torch__.torch.classes.quantized.LinearPackedParamsBase w_hh, Tensor b_ih, Tensor b_hh) -> Tensor"
  },
  {
    "name": "quantized::quantized_lstm_cell_dynamic(Tensor input, Tensor[] hx, __torch__.torch.classes.quantized.LinearPackedParamsBase w_ih, __torch__.torch.classes.quantized.LinearPackedParamsBase w_hh, Tensor bias_ih, Tensor bias_hh) -> (Tensor, Tensor)"
  },
  {
    "name": "quantized::quantized_rnn_relu_cell_dynamic(Tensor input, Tensor hx, __torch__.torch.classes.quantized.LinearPackedParamsBase w_ih, __torch__.torch.classes.quantized.LinearPackedParamsBase w_hh, Tensor b_ih, Tensor b_hh) -> Tensor"
  },
  {
    "name": "quantized::quantized_rnn_tanh_cell_dynamic(Tensor input, Tensor hx, __torch__.torch.classes.quantized.LinearPackedParamsBase w_ih, __torch__.torch.classes.quantized.LinearPackedParamsBase w_hh, Tensor b_ih, Tensor b_hh) -> Tensor"
  },
  {
    "name": "quantized::relu6(Tensor qx, bool inplace=False) -> Tensor",
    "category": "Activation"
  },
  {
    "name": "quantized::sigmoid(Tensor qx, float output_scale, int output_zero_point) -> Tensor",
    "category": "Activation"
  },
  {
    "name": "quantized::softmax(Tensor qx, int dim, float output_scale, int output_zero_point) -> Tensor"
  },
  {
    "name": "quantized_decomposed::quantize_per_tensor(Tensor input, float scale, int zero_point, int quant_min, int quant_max, ScalarType dtype) -> Tensor"
  },
  {
    "name": "torch.nn.modules.activation.ELU",
    "category": "Activation"
  },
  {
    "name": "torch.nn.modules.activation.GELU",
    "category": "Activation"
  },
  {
    "name": "torch.nn.modules.activation.GLU",
    "category": "Activation"
  },
  {
    "name": "torch.nn.modules.activation.Hardsigmoid",
    "category": "Activation"
  },
  {
    "name": "torch.nn.modules.activation.Hardswish",
    "category": "Activation"
  },
  {
    "name": "torch.nn.modules.activation.Hardtanh",
    "category": "Activation"
  },
  {
    "name": "torch.nn.modules.activation.LeakyReLU",
    "category": "Activation"
  },
  {
    "name": "torch.nn.modules.activation.LogSoftmax",
    "category": "Activation"
  },
  {
    "name": "torch.nn.modules.activation.PReLU",
    "category": "Activation"
  },
  {
    "name": "torch.nn.modules.activation.ReLU",
    "category": "Activation",
    "inputs": [
      { "name": "inplace", "default": false, "visible": false },
      { "name": "threshold", "default": 0 },
      { "name": "value", "default": 0 }
    ]
  },
  {
    "name": "torch.nn.modules.activation.ReLU6",
    "category": "Activation"
  },
  {
    "name": "torch.nn.modules.activation.SiLU",
    "category": "Activation"
  },
  {
    "name": "torch.nn.modules.activation.Sigmoid",
    "category": "Activation"
  },
  {
    "name": "torch.nn.modules.activation.Softmax",
    "category": "Activation"
  },
  {
    "name": "torch.nn.modules.activation.Softmax2d",
    "category": "Activation"
  },
  {
    "name": "torch.nn.modules.activation.Softplus",
    "category": "Activation"
  },
  {
    "name": "torch.nn.modules.activation.Tanh",
    "category": "Activation"
  },
  {
    "name": "torch.nn.modules.batchnorm.BatchNorm1d",
    "category": "Normalization"
  },
  {
    "name": "torch.nn.modules.batchnorm.BatchNorm2d",
    "category": "Normalization",
    "inputs": [
      { "name": "input" },
      { "name": "weight" },
      { "name": "bias" },
      { "name": "running_mean" },
      { "name": "running_var" },
      { "name": "num_batches_tracked", "visible": false },
      { "name": "eps", "default": 1e-05 },
      { "name": "momentum", "default": 0.1 },
      { "name": "affine", "default": true },
      { "name": "track_running_stats", "default": true }
    ]
  },
  {
    "name": "torch.nn.modules.conv.Conv1d",
    "category": "Layer",
    "inputs": [
      { "name": "input" },
      { "name": "weight" },
      { "name": "bias" },
      { "name": "output_padding", "visible": false },
      { "name": "in_channels", "visible": false },
      { "name": "out_channels", "visible": false },
      { "name": "groups", "default": 1 },
      { "name": "transposed", "default": false },
      { "name": "padding", "default": [   0 ] },
      { "name": "dilation", "default": [   1 ] },
      { "name": "stride", "default": [   1 ] }
    ]
  },
  {
    "name": "torch.nn.modules.conv.Conv2d",
    "category": "Layer",
    "inputs": [
      { "name": "input" },
      { "name": "weight" },
      { "name": "bias" },
      { "name": "output_padding", "visible": false },
      { "name": "in_channels", "visible": false },
      { "name": "out_channels", "visible": false },
      { "name": "groups", "default": 1 },
      { "name": "transposed", "default": false },
      { "name": "padding", "default": [   0,   0 ] },
      { "name": "dilation", "default": [   1,   1 ] },
      { "name": "stride", "default": [   1,   1 ] }
    ]
  },
  {
    "name": "torch.nn.modules.conv.Conv3d",
    "category": "Layer"
  },
  {
    "name": "torch.nn.modules.conv.ConvTranspose1d",
    "category": "Layer",
    "inputs": [
      { "name": "input" },
      { "name": "weight" },
      { "name": "bias" },
      { "name": "output_padding", "visible": false },
      { "name": "in_channels", "visible": false },
      { "name": "out_channels", "visible": false },
      { "name": "groups", "default": 1 },
      { "name": "transposed", "default": true },
      { "name": "padding", "default": [   0 ] },
      { "name": "dilation", "default": [   1 ] },
      { "name": "stride", "default": [   1 ] }
    ]
  },
  {
    "name": "torch.nn.modules.conv.ConvTranspose2d",
    "category": "Layer",
    "inputs": [
      { "name": "input" },
      { "name": "weight" },
      { "name": "bias" },
      { "name": "output_padding", "visible": false },
      { "name": "in_channels", "visible": false },
      { "name": "out_channels", "visible": false },
      { "name": "groups", "default": 1 },
      { "name": "transposed", "default": true },
      { "name": "padding", "default": [   0,   0 ] },
      { "name": "dilation", "default": [   1,   1 ] },
      { "name": "stride", "default": [   1,   1 ] }
    ]
  },
  {
    "name": "torch.nn.modules.conv.ConvTranspose3d",
    "category": "Layer"
  },
  {
    "name": "torch.nn.modules.dropout.Dropout",
    "category": "Dropout",
    "inputs": [
      { "name": "inplace", "default": false, "visible": false },
      { "name": "p", "default": 0.5 }
    ]
  },
  {
    "name": "torch.nn.modules.dropout.Dropout2d",
    "category": "Dropout",
    "inputs": [
      { "name": "inplace", "default": false, "visible": false },
      { "name": "p", "default": 0.5 }
    ]
  },
  {
    "name": "torch.nn.modules.instancenorm.InstanceNorm1d"
  },
  {
    "name": "torch.nn.modules.instancenorm.InstanceNorm2d"
  },
  {
    "name": "torch.nn.modules.instancenorm.InstanceNorm3d"
  },
  {
    "name": "torch.nn.modules.linear.Linear",
    "category": "Layer"
  },
  {
    "name": "torch.nn.modules.normalization.CrossMapLRN2d",
    "category": "Normalization",
    "inputs": [
      { "name": "alpha", "default": 0.0001 },
      { "name": "beta", "default": 0.75 },
      { "name": "k", "default": 1 }
    ]
  },
  {
    "name": "torch.nn.modules.normalization.GroupNorm",
    "category": "Normalization"
  },
  {
    "name": "torch.nn.modules.normalization.LayerNorm",
    "category": "Normalization"
  },
  {
    "name": "torch.nn.modules.padding.ConstantPad1d",
    "category": "Tensor"
  },
  {
    "name": "torch.nn.modules.padding.ConstantPad2d",
    "category": "Tensor"
  },
  {
    "name": "torch.nn.modules.padding.ConstantPad3d",
    "category": "Tensor"
  },
  {
    "name": "torch.nn.modules.padding.ReflectionPad1d",
    "category": "Tensor"
  },
  {
    "name": "torch.nn.modules.padding.ReflectionPad2d",
    "category": "Tensor"
  },
  {
    "name": "torch.nn.modules.padding.ReplicationPad1d",
    "category": "Tensor"
  },
  {
    "name": "torch.nn.modules.padding.ReplicationPad2d",
    "category": "Tensor"
  },
  {
    "name": "torch.nn.modules.padding.ReplicationPad3d",
    "category": "Tensor"
  },
  {
    "name": "torch.nn.modules.padding.ZeroPad2d",
    "category": "Tensor"
  },
  {
    "name": "torch.nn.modules.pixelshuffle.PixelShuffle"
  },
  {
    "name": "torch.nn.modules.pooling.AdaptiveAvgPool1d",
    "category": "Pool"
  },
  {
    "name": "torch.nn.modules.pooling.AdaptiveAvgPool2d",
    "category": "Pool"
  },
  {
    "name": "torch.nn.modules.pooling.AdaptiveAvgPool3d",
    "category": "Pool"
  },
  {
    "name": "torch.nn.modules.pooling.AdaptiveMaxPool1d",
    "category": "Pool"
  },
  {
    "name": "torch.nn.modules.pooling.AdaptiveMaxPool2d",
    "category": "Pool"
  },
  {
    "name": "torch.nn.modules.pooling.AdaptiveMaxPool3d",
    "category": "Pool"
  },
  {
    "name": "torch.nn.modules.pooling.AvgPool2d",
    "category": "Pool",
    "inputs": [
      { "name": "padding", "default": 0 },
      { "name": "count_include_pad", "default": true },
      { "name": "ceil_mode", "visible": false }
    ]
  },
  {
    "name": "torch.nn.modules.pooling.AvgPool3d",
    "category": "Pool"
  },
  {
    "name": "torch.nn.modules.pooling.MaxPool1d",
    "category": "Pool"
  },
  {
    "name": "torch.nn.modules.pooling.MaxPool2d",
    "category": "Pool",
    "inputs": [
      { "name": "input" },
      { "name": "padding", "default": 0 },
      { "name": "dilation", "default": 1 },
      { "name": "return_indices", "default": false },
      { "name": "ceil_mode", "visible": false }
    ]
  },
  {
    "name": "torch.nn.modules.pooling.MaxPool3d",
    "category": "Pool"
  },
  {
    "name": "torch.nn.modules.pooling.MaxUnpool1d",
    "category": "Pool"
  },
  {
    "name": "torch.nn.modules.pooling.MaxUnpool2d",
    "category": "Pool"
  },
  {
    "name": "torch.nn.modules.pooling.MaxUnpool3d",
    "category": "Pool"
  },
  {
    "name": "torch.nn.modules.rnn.GRU",
    "category": "Layer"
  },
  {
    "name": "torch.nn.modules.rnn.GRUCell",
    "category": "Layer"
  },
  {
    "name": "torch.nn.modules.rnn.LSTM",
    "category": "Layer",
    "inputs": [
      { "name": "input" },
      { "name": "weight_ih_l0", "visible": false },
      { "name": "weight_hh_l0", "visible": false },
      { "name": "bias_ih_l0", "visible": false },
      { "name": "bias_hh_l0", "visible": false },
      { "name": "weight_ih_l1", "visible": false },
      { "name": "weight_hh_l1", "visible": false },
      { "name": "bias_ih_l1", "visible": false },
      { "name": "bias_hh_l1", "visible": false },
      { "name": "dropout", "default": 0 },
      { "name": "dropout_state", "default": {} },
      { "name": "num_layers", "default": 1 },
      { "name": "batch_first", "visible": false },
      { "name": "bidirectional", "visible": false },
      { "name": "bias", "visible": false }
    ]
  },
  {
    "name": "torch.nn.modules.rnn.LSTMCell",
    "category": "Layer"
  },
  {
    "name": "torch.nn.modules.rnn.RNN",
    "category": "Layer"
  },
  {
    "name": "torch.nn.modules.sparse.Embedding",
    "category": "Transform",
    "inputs": [
      { "name": "norm_type", "default": 2 },
      { "name": "scale_grad_by_freq", "default": false },
      { "name": "sparse", "default": false },
      { "name": "max_norm", "default": null },
      { "name": "padding_idx", "default": null }
    ]
  },
  {
    "name": "torch.nn.modules.upsampling.Upsample",
    "category": "Data"
  },
  {
    "name": "torch_scatter::cuda_version() -> int _0"
  },
  {
    "name": "torch_scatter::gather_coo(Tensor _0, Tensor _1, Tensor? _2) -> Tensor _0"
  },
  {
    "name": "torch_scatter::gather_csr(Tensor _0, Tensor _1, Tensor? _2) -> Tensor _0"
  },
  {
    "name": "torch_scatter::scatter_max(Tensor _0, Tensor _1, int _2, Tensor? _3, int? _4) -> (Tensor _0, Tensor _1)"
  },
  {
    "name": "torch_scatter::scatter_mean(Tensor _0, Tensor _1, int _2, Tensor? _3, int? _4) -> Tensor _0"
  },
  {
    "name": "torch_scatter::scatter_min(Tensor _0, Tensor _1, int _2, Tensor? _3, int? _4) -> (Tensor _0, Tensor _1)"
  },
  {
    "name": "torch_scatter::scatter_mul(Tensor _0, Tensor _1, int _2, Tensor? _3, int? _4) -> Tensor _0"
  },
  {
    "name": "torch_scatter::scatter_sum(Tensor _0, Tensor _1, int _2, Tensor? _3, int? _4) -> Tensor _0"
  },
  {
    "name": "torch_scatter::segment_max_coo(Tensor _0, Tensor _1, Tensor? _2, int? _3) -> (Tensor _0, Tensor _1)"
  },
  {
    "name": "torch_scatter::segment_max_csr(Tensor _0, Tensor _1, Tensor? _2) -> (Tensor _0, Tensor _1)"
  },
  {
    "name": "torch_scatter::segment_mean_coo(Tensor _0, Tensor _1, Tensor? _2, int? _3) -> Tensor _0"
  },
  {
    "name": "torch_scatter::segment_mean_csr(Tensor _0, Tensor _1, Tensor? _2) -> Tensor _0"
  },
  {
    "name": "torch_scatter::segment_min_coo(Tensor _0, Tensor _1, Tensor? _2, int? _3) -> (Tensor _0, Tensor _1)"
  },
  {
    "name": "torch_scatter::segment_min_csr(Tensor _0, Tensor _1, Tensor? _2) -> (Tensor _0, Tensor _1)"
  },
  {
    "name": "torch_scatter::segment_sum_coo(Tensor _0, Tensor _1, Tensor? _2, int? _3) -> Tensor _0"
  },
  {
    "name": "torch_scatter::segment_sum_csr(Tensor _0, Tensor _1, Tensor? _2) -> Tensor _0"
  },
  {
    "name": "torchaudio::sox_effects_apply_effects_tensor(Tensor tensor, int sample_rate, str[][] effects, bool channels_first=True) -> (Tensor, int)"
  },
  {
    "name": "torchvision::nms(Tensor dets, Tensor scores, float iou_threshold) -> Tensor"
  },
  {
    "name": "torchvision::roi_align(Tensor input, Tensor rois, float spatial_scale, int pooled_height, int pooled_width, int sampling_ratio, bool aligned) -> Tensor"
  }
]