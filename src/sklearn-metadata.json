[
  {
    "name": "Binarizer",
    "schema": {
      "attributes": [
        {
          "default": true,
          "description": "set to False to perform inplace binarization and avoid a copy (if\nthe input is already a numpy array or a scipy.sparse CSR matrix).\n",
          "name": "copy",
          "option": "optional",
          "type": "boolean"
        },
        {
          "default": 0.0,
          "description": "Feature values below or equal to this are replaced by 0, above it by 1.\nThreshold may not be less than 0 for operations on sparse matrices.\n",
          "name": "threshold",
          "option": "optional",
          "type": "float32"
        }
      ],
      "description": "Binarize data (set feature values to 0 or 1) according to a threshold\n\nValues greater than the threshold map to 1, while values less than\nor equal to the threshold map to 0. With the default threshold of 0,\nonly positive values map to 1.\n\nBinarization is a common operation on text count data where the\nanalyst can decide to only consider the presence or absence of a\nfeature rather than a quantified number of occurrences for instance.\n\nIt can also be used as a pre-processing step for estimators that\nconsider boolean random variables (e.g. modelled using the Bernoulli\ndistribution in a Bayesian setting).\n\nRead more in the :ref:`User Guide <preprocessing_binarization>`.\n",
      "package": "sklearn.preprocessing"
    }
  },
  {
    "name": "SVC",
    "schema": {
      "attributes": [
        {
          "default": 1.0,
          "description": "Penalty parameter C of the error term.\n",
          "name": "C",
          "option": "optional",
          "type": "float32"
        },
        {
          "default": "rbf",
          "description": "Specifies the kernel type to be used in the algorithm.\nIt must be one of 'linear', 'poly', 'rbf', 'sigmoid', 'precomputed' or\na callable.\nIf none is given, 'rbf' will be used. If a callable is given it is\nused to pre-compute the kernel matrix from data matrices; that matrix\nshould be an array of shape ``(n_samples, n_samples)``.\n",
          "name": "kernel",
          "option": "optional",
          "type": "string"
        },
        {
          "default": 3,
          "description": "Degree of the polynomial kernel function ('poly').\nIgnored by all other kernels.\n",
          "name": "degree",
          "option": "optional",
          "type": "int32"
        },
        {
          "default": "auto",
          "description": "Kernel coefficient for 'rbf', 'poly' and 'sigmoid'.\n\nCurrent default is 'auto' which uses 1 / n_features,\nif ``gamma='scale'`` is passed then it uses 1 / (n_features * X.std())\nas value of gamma. The current default of gamma, 'auto', will change\nto 'scale' in version 0.22. 'auto_deprecated', a deprecated version of\n'auto' is used as a default indicating that no explicit value of gamma\nwas passed.\n",
          "name": "gamma",
          "option": "optional",
          "type": "float32"
        },
        {
          "default": 0.0,
          "description": "Independent term in kernel function.\nIt is only significant in 'poly' and 'sigmoid'.\n",
          "name": "coef0",
          "option": "optional",
          "type": "float32"
        },
        {
          "default": false,
          "description": "Whether to enable probability estimates. This must be enabled prior\nto calling `fit`, and will slow down that method.\n",
          "name": "probability",
          "option": "optional",
          "type": "boolean"
        },
        {
          "default": true,
          "description": "Whether to use the shrinking heuristic.\n",
          "name": "shrinking",
          "option": "optional",
          "type": "boolean"
        },
        {
          "default": 0.001,
          "description": "Tolerance for stopping criterion.\n",
          "name": "tol",
          "option": "optional",
          "type": "float32"
        },
        {
          "description": "Specify the size of the kernel cache (in MB).\n",
          "name": "cache_size",
          "option": "optional",
          "type": "float32"
        },
        {
          "description": "Set the parameter C of class i to class_weight[i]*C for\nSVC. If not given, all classes are supposed to have\nweight one.\nThe \"balanced\" mode uses the values of y to automatically adjust\nweights inversely proportional to class frequencies in the input data\nas ``n_samples / (n_classes * np.bincount(y))``\n",
          "name": "class_weight",
          "option": "optional"
        },
        {
          "default": false,
          "description": "Enable verbose output. Note that this setting takes advantage of a\nper-process runtime setting in libsvm that, if enabled, may not work\nproperly in a multithreaded context.\n",
          "name": "verbose",
          "type": "boolean"
        },
        {
          "default": -1,
          "description": "Hard limit on iterations within solver, or -1 for no limit.\n",
          "name": "max_iter",
          "option": "optional",
          "type": "int32"
        },
        {
          "default": "ovr",
          "description": "Whether to return a one-vs-rest ('ovr') decision function of shape\n(n_samples, n_classes) as all other classifiers, or the original\none-vs-one ('ovo') decision function of libsvm which has shape\n(n_samples, n_classes * (n_classes - 1) / 2). However, one-vs-one\n('ovo') is always used as multi-class strategy.\n\n.. versionchanged:: 0.19\ndecision_function_shape is 'ovr' by default.\n\n.. versionadded:: 0.17\n*decision_function_shape='ovr'* is recommended.\n\n.. versionchanged:: 0.17\nDeprecated *decision_function_shape='ovo' and None*.\n",
          "name": "decision_function_shape"
        },
        {
          "default": null,
          "description": "The seed of the pseudo random number generator used when shuffling\nthe data for probability estimates. If int, random_state is the\nseed used by the random number generator; If RandomState instance,\nrandom_state is the random number generator; If None, the random\nnumber generator is the RandomState instance used by `np.random`.\n",
          "name": "random_state",
          "option": "optional",
          "type": "int32"
        }
      ],
      "description": "C-Support Vector Classification.\n\nThe implementation is based on libsvm. The fit time complexity\nis more than quadratic with the number of samples which makes it hard\nto scale to dataset with more than a couple of 10000 samples.\n\nThe multiclass support is handled according to a one-vs-one scheme.\n\nFor details on the precise mathematical formulation of the provided\nkernel functions and how `gamma`, `coef0` and `degree` affect each\nother, see the corresponding section in the narrative documentation:\n:ref:`svm_kernels`.\n\nRead more in the :ref:`User Guide <svm_classification>`.\n",
      "package": "sklearn.svm"
    }
  },
  {
    "name": "DecisionTreeRegressor",
    "schema": {
      "attributes": [
        {
          "default": "mse",
          "description": "The function to measure the quality of a split. Supported criteria\nare \"mse\" for the mean squared error, which is equal to variance\nreduction as feature selection criterion and minimizes the L2 loss\nusing the mean of each terminal node, \"friedman_mse\", which uses mean\nsquared error with Friedman's improvement score for potential splits,\nand \"mae\" for the mean absolute error, which minimizes the L1 loss\nusing the median of each terminal node.\n\n.. versionadded:: 0.18\nMean Absolute Error (MAE) criterion.\n",
          "name": "criterion",
          "option": "optional",
          "type": "string"
        },
        {
          "default": "best",
          "description": "The strategy used to choose the split at each node. Supported\nstrategies are \"best\" to choose the best split and \"random\" to choose\nthe best random split.\n",
          "name": "splitter",
          "option": "optional",
          "type": "string"
        },
        {
          "default": null,
          "description": "The maximum depth of the tree. If None, then nodes are expanded until\nall leaves are pure or until all leaves contain less than\nmin_samples_split samples.\n",
          "name": "max_depth",
          "option": "optional"
        },
        {
          "default": 2,
          "description": "The minimum number of samples required to split an internal node:\n\n- If int, then consider `min_samples_split` as the minimum number.\n- If float, then `min_samples_split` is a fraction and\n`ceil(min_samples_split * n_samples)` are the minimum\nnumber of samples for each split.\n\n.. versionchanged:: 0.18\nAdded float values for fractions.\n",
          "name": "min_samples_split",
          "option": "optional",
          "type": "int32"
        },
        {
          "default": 1,
          "description": "The minimum number of samples required to be at a leaf node.\nA split point at any depth will only be considered if it leaves at\nleast ``min_samples_leaf`` training samples in each of the left and\nright branches.  This may have the effect of smoothing the model,\nespecially in regression.\n\n- If int, then consider `min_samples_leaf` as the minimum number.\n- If float, then `min_samples_leaf` is a fraction and\n`ceil(min_samples_leaf * n_samples)` are the minimum\nnumber of samples for each node.\n\n.. versionchanged:: 0.18\nAdded float values for fractions.\n",
          "name": "min_samples_leaf",
          "option": "optional",
          "type": "int32"
        },
        {
          "default": 0.0,
          "description": "The minimum weighted fraction of the sum total of weights (of all\nthe input samples) required to be at a leaf node. Samples have\nequal weight when sample_weight is not provided.\n",
          "name": "min_weight_fraction_leaf",
          "option": "optional",
          "type": "float32"
        },
        {
          "default": null,
          "description": "The number of features to consider when looking for the best split:\n\n- If int, then consider `max_features` features at each split.\n- If float, then `max_features` is a fraction and\n`int(max_features * n_features)` features are considered at each\nsplit.\n- If \"auto\", then `max_features=n_features`.\n- If \"sqrt\", then `max_features=sqrt(n_features)`.\n- If \"log2\", then `max_features=log2(n_features)`.\n- If None, then `max_features=n_features`.\n\nNote: the search for a split does not stop until at least one\nvalid partition of the node samples is found, even if it requires to\neffectively inspect more than ``max_features`` features.\n",
          "name": "max_features",
          "option": "optional",
          "type": "int32"
        },
        {
          "default": null,
          "description": "If int, random_state is the seed used by the random number generator;\nIf RandomState instance, random_state is the random number generator;\nIf None, the random number generator is the RandomState instance used\nby `np.random`.\n",
          "name": "random_state",
          "option": "optional",
          "type": "int32"
        },
        {
          "default": null,
          "description": "Grow a tree with ``max_leaf_nodes`` in best-first fashion.\nBest nodes are defined as relative reduction in impurity.\nIf None then unlimited number of leaf nodes.\n",
          "name": "max_leaf_nodes",
          "option": "optional"
        },
        {
          "default": 0.0,
          "description": "A node will be split if this split induces a decrease of the impurity\ngreater than or equal to this value.\n\nThe weighted impurity decrease equation is the following::\n\nN_t / N * (impurity - N_t_R / N_t * right_impurity\n- N_t_L / N_t * left_impurity)\n\nwhere ``N`` is the total number of samples, ``N_t`` is the number of\nsamples at the current node, ``N_t_L`` is the number of samples in the\nleft child, and ``N_t_R`` is the number of samples in the right child.\n\n``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\nif ``sample_weight`` is passed.\n\n.. versionadded:: 0.19\n",
          "name": "min_impurity_decrease",
          "option": "optional",
          "type": "float32"
        },
        {
          "description": "Threshold for early stopping in tree growth. A node will split\nif its impurity is above the threshold, otherwise it is a leaf.\n\n.. deprecated:: 0.19\n``min_impurity_split`` has been deprecated in favor of\n``min_impurity_decrease`` in 0.19 and will be removed in 0.21.\nUse ``min_impurity_decrease`` instead.\n",
          "name": "min_impurity_split",
          "type": "float32"
        },
        {
          "default": false,
          "description": "Whether to presort the data to speed up the finding of best splits in\nfitting. For the default settings of a decision tree on large\ndatasets, setting this to true may slow down the training process.\nWhen using either a smaller dataset or a restricted depth, this may\nspeed up the training.\n",
          "name": "presort",
          "option": "optional",
          "type": "boolean"
        }
      ],
      "description": "A decision tree regressor.\n\nRead more in the :ref:`User Guide <tree>`.\n",
      "package": "sklearn.tree.tree"
    }
  },
  {
    "name": "DecisionTreeClassifier",
    "schema": {
      "attributes": [
        {
          "default": "gini",
          "description": "The function to measure the quality of a split. Supported criteria are\n\"gini\" for the Gini impurity and \"entropy\" for the information gain.\n",
          "name": "criterion",
          "option": "optional",
          "type": "string"
        },
        {
          "default": "best",
          "description": "The strategy used to choose the split at each node. Supported\nstrategies are \"best\" to choose the best split and \"random\" to choose\nthe best random split.\n",
          "name": "splitter",
          "option": "optional",
          "type": "string"
        },
        {
          "default": null,
          "description": "The maximum depth of the tree. If None, then nodes are expanded until\nall leaves are pure or until all leaves contain less than\nmin_samples_split samples.\n",
          "name": "max_depth",
          "option": "optional"
        },
        {
          "default": 2,
          "description": "The minimum number of samples required to split an internal node:\n\n- If int, then consider `min_samples_split` as the minimum number.\n- If float, then `min_samples_split` is a fraction and\n`ceil(min_samples_split * n_samples)` are the minimum\nnumber of samples for each split.\n\n.. versionchanged:: 0.18\nAdded float values for fractions.\n",
          "name": "min_samples_split",
          "option": "optional",
          "type": "int32"
        },
        {
          "default": 1,
          "description": "The minimum number of samples required to be at a leaf node.\nA split point at any depth will only be considered if it leaves at\nleast ``min_samples_leaf`` training samples in each of the left and\nright branches.  This may have the effect of smoothing the model,\nespecially in regression.\n\n- If int, then consider `min_samples_leaf` as the minimum number.\n- If float, then `min_samples_leaf` is a fraction and\n`ceil(min_samples_leaf * n_samples)` are the minimum\nnumber of samples for each node.\n\n.. versionchanged:: 0.18\nAdded float values for fractions.\n",
          "name": "min_samples_leaf",
          "option": "optional",
          "type": "int32"
        },
        {
          "default": 0.0,
          "description": "The minimum weighted fraction of the sum total of weights (of all\nthe input samples) required to be at a leaf node. Samples have\nequal weight when sample_weight is not provided.\n",
          "name": "min_weight_fraction_leaf",
          "option": "optional",
          "type": "float32"
        },
        {
          "default": null,
          "description": "The number of features to consider when looking for the best split:\n\n- If int, then consider `max_features` features at each split.\n- If float, then `max_features` is a fraction and\n`int(max_features * n_features)` features are considered at each\nsplit.\n- If \"auto\", then `max_features=sqrt(n_features)`.\n- If \"sqrt\", then `max_features=sqrt(n_features)`.\n- If \"log2\", then `max_features=log2(n_features)`.\n- If None, then `max_features=n_features`.\n\nNote: the search for a split does not stop until at least one\nvalid partition of the node samples is found, even if it requires to\neffectively inspect more than ``max_features`` features.\n",
          "name": "max_features",
          "option": "optional",
          "type": "int32"
        },
        {
          "default": null,
          "description": "If int, random_state is the seed used by the random number generator;\nIf RandomState instance, random_state is the random number generator;\nIf None, the random number generator is the RandomState instance used\nby `np.random`.\n",
          "name": "random_state",
          "option": "optional",
          "type": "int32"
        },
        {
          "default": null,
          "description": "Grow a tree with ``max_leaf_nodes`` in best-first fashion.\nBest nodes are defined as relative reduction in impurity.\nIf None then unlimited number of leaf nodes.\n",
          "name": "max_leaf_nodes",
          "option": "optional"
        },
        {
          "default": 0.0,
          "description": "A node will be split if this split induces a decrease of the impurity\ngreater than or equal to this value.\n\nThe weighted impurity decrease equation is the following::\n\nN_t / N * (impurity - N_t_R / N_t * right_impurity\n- N_t_L / N_t * left_impurity)\n\nwhere ``N`` is the total number of samples, ``N_t`` is the number of\nsamples at the current node, ``N_t_L`` is the number of samples in the\nleft child, and ``N_t_R`` is the number of samples in the right child.\n\n``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\nif ``sample_weight`` is passed.\n\n.. versionadded:: 0.19\n",
          "name": "min_impurity_decrease",
          "option": "optional",
          "type": "float32"
        },
        {
          "description": "Threshold for early stopping in tree growth. A node will split\nif its impurity is above the threshold, otherwise it is a leaf.\n\n.. deprecated:: 0.19\n``min_impurity_split`` has been deprecated in favor of\n``min_impurity_decrease`` in 0.19 and will be removed in 0.21.\nUse ``min_impurity_decrease`` instead.\n",
          "name": "min_impurity_split",
          "type": "float32"
        },
        {
          "default": null,
          "description": "Weights associated with classes in the form ``{class_label: weight}``.\nIf not given, all classes are supposed to have weight one. For\nmulti-output problems, a list of dicts can be provided in the same\norder as the columns of y.\n\nNote that for multioutput (including multilabel) weights should be\ndefined for each class of every column in its own dict. For example,\nfor four-class multilabel classification weights should be\n[{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of\n[{1:1}, {2:5}, {3:1}, {4:1}].\n\nThe \"balanced\" mode uses the values of y to automatically adjust\nweights inversely proportional to class frequencies in the input data\nas ``n_samples / (n_classes * np.bincount(y))``\n\nFor multi-output, the weights of each column of y will be multiplied.\n\nNote that these weights will be multiplied with sample_weight (passed\nthrough the fit method) if sample_weight is specified.\n",
          "name": "class_weight"
        },
        {
          "default": false,
          "description": "Whether to presort the data to speed up the finding of best splits in\nfitting. For the default settings of a decision tree on large\ndatasets, setting this to true may slow down the training process.\nWhen using either a smaller dataset or a restricted depth, this may\nspeed up the training.\n",
          "name": "presort",
          "option": "optional",
          "type": "boolean"
        }
      ],
      "description": "A decision tree classifier.\n\nRead more in the :ref:`User Guide <tree>`.\n",
      "package": "sklearn.tree.tree"
    }
  },
  {
    "name": "ExtraTreesClassifier",
    "schema": {
      "attributes": [
        {
          "default": "10",
          "description": "The number of trees in the forest.\n\n.. versionchanged:: 0.20\nThe default value of ``n_estimators`` will change from 10 in\nversion 0.20 to 100 in version 0.22.\n",
          "name": "n_estimators",
          "option": "optional"
        },
        {
          "default": "gini",
          "description": "The function to measure the quality of a split. Supported criteria are\n\"gini\" for the Gini impurity and \"entropy\" for the information gain.\n",
          "name": "criterion",
          "option": "optional",
          "type": "string"
        },
        {
          "default": null,
          "description": "The maximum depth of the tree. If None, then nodes are expanded until\nall leaves are pure or until all leaves contain less than\nmin_samples_split samples.\n",
          "name": "max_depth",
          "option": "optional"
        },
        {
          "default": 2,
          "description": "The minimum number of samples required to split an internal node:\n\n- If int, then consider `min_samples_split` as the minimum number.\n- If float, then `min_samples_split` is a fraction and\n`ceil(min_samples_split * n_samples)` are the minimum\nnumber of samples for each split.\n\n.. versionchanged:: 0.18\nAdded float values for fractions.\n",
          "name": "min_samples_split",
          "option": "optional",
          "type": "int32"
        },
        {
          "default": 1,
          "description": "The minimum number of samples required to be at a leaf node.\nA split point at any depth will only be considered if it leaves at\nleast ``min_samples_leaf`` training samples in each of the left and\nright branches.  This may have the effect of smoothing the model,\nespecially in regression.\n\n- If int, then consider `min_samples_leaf` as the minimum number.\n- If float, then `min_samples_leaf` is a fraction and\n`ceil(min_samples_leaf * n_samples)` are the minimum\nnumber of samples for each node.\n\n.. versionchanged:: 0.18\nAdded float values for fractions.\n",
          "name": "min_samples_leaf",
          "option": "optional",
          "type": "int32"
        },
        {
          "default": 0.0,
          "description": "The minimum weighted fraction of the sum total of weights (of all\nthe input samples) required to be at a leaf node. Samples have\nequal weight when sample_weight is not provided.\n",
          "name": "min_weight_fraction_leaf",
          "option": "optional",
          "type": "float32"
        },
        {
          "default": "auto",
          "description": "The number of features to consider when looking for the best split:\n\n- If int, then consider `max_features` features at each split.\n- If float, then `max_features` is a fraction and\n`int(max_features * n_features)` features are considered at each\nsplit.\n- If \"auto\", then `max_features=sqrt(n_features)`.\n- If \"sqrt\", then `max_features=sqrt(n_features)`.\n- If \"log2\", then `max_features=log2(n_features)`.\n- If None, then `max_features=n_features`.\n\nNote: the search for a split does not stop until at least one\nvalid partition of the node samples is found, even if it requires to\neffectively inspect more than ``max_features`` features.\n",
          "name": "max_features",
          "option": "optional",
          "type": "int32"
        },
        {
          "default": null,
          "description": "Grow trees with ``max_leaf_nodes`` in best-first fashion.\nBest nodes are defined as relative reduction in impurity.\nIf None then unlimited number of leaf nodes.\n",
          "name": "max_leaf_nodes",
          "option": "optional"
        },
        {
          "default": 0.0,
          "description": "A node will be split if this split induces a decrease of the impurity\ngreater than or equal to this value.\n\nThe weighted impurity decrease equation is the following::\n\nN_t / N * (impurity - N_t_R / N_t * right_impurity\n- N_t_L / N_t * left_impurity)\n\nwhere ``N`` is the total number of samples, ``N_t`` is the number of\nsamples at the current node, ``N_t_L`` is the number of samples in the\nleft child, and ``N_t_R`` is the number of samples in the right child.\n\n``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\nif ``sample_weight`` is passed.\n\n.. versionadded:: 0.19\n",
          "name": "min_impurity_decrease",
          "option": "optional",
          "type": "float32"
        },
        {
          "description": "Threshold for early stopping in tree growth. A node will split\nif its impurity is above the threshold, otherwise it is a leaf.\n\n.. deprecated:: 0.19\n``min_impurity_split`` has been deprecated in favor of\n``min_impurity_decrease`` in 0.19 and will be removed in 0.21.\nUse ``min_impurity_decrease`` instead.\n",
          "name": "min_impurity_split",
          "type": "float32"
        },
        {
          "default": false,
          "description": "Whether bootstrap samples are used when building trees.\n",
          "name": "bootstrap",
          "option": "optional",
          "type": "boolean"
        },
        {
          "default": false,
          "description": "Whether to use out-of-bag samples to estimate\nthe generalization accuracy.\n",
          "name": "oob_score",
          "option": "optional",
          "type": "boolean"
        },
        {
          "default": null,
          "description": "The number of jobs to run in parallel for both `fit` and `predict`.\n``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n``-1`` means using all processors. See :term:`Glossary <n_jobs>`\nfor more details.\n",
          "name": "n_jobs",
          "option": "optional"
        },
        {
          "default": null,
          "description": "If int, random_state is the seed used by the random number generator;\nIf RandomState instance, random_state is the random number generator;\nIf None, the random number generator is the RandomState instance used\nby `np.random`.\n",
          "name": "random_state",
          "option": "optional",
          "type": "int32"
        },
        {
          "default": 0,
          "description": "Controls the verbosity when fitting and predicting.\n",
          "name": "verbose",
          "option": "optional",
          "type": "int32"
        },
        {
          "default": false,
          "description": "When set to ``True``, reuse the solution of the previous call to fit\nand add more estimators to the ensemble, otherwise, just fit a whole\nnew forest. See :term:`the Glossary <warm_start>`.\n",
          "name": "warm_start",
          "option": "optional",
          "type": "boolean"
        },
        {
          "default": null,
          "description": "Weights associated with classes in the form ``{class_label: weight}``.\nIf not given, all classes are supposed to have weight one. For\nmulti-output problems, a list of dicts can be provided in the same\norder as the columns of y.\n\nNote that for multioutput (including multilabel) weights should be\ndefined for each class of every column in its own dict. For example,\nfor four-class multilabel classification weights should be\n[{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of\n[{1:1}, {2:5}, {3:1}, {4:1}].\n\nThe \"balanced\" mode uses the values of y to automatically adjust\nweights inversely proportional to class frequencies in the input data\nas ``n_samples / (n_classes * np.bincount(y))``\n\nThe \"balanced_subsample\" mode is the same as \"balanced\" except that weights are\ncomputed based on the bootstrap sample for every tree grown.\n\nFor multi-output, the weights of each column of y will be multiplied.\n\nNote that these weights will be multiplied with sample_weight (passed\nthrough the fit method) if sample_weight is specified.\n",
          "name": "class_weight",
          "option": "optional"
        }
      ],
      "description": "An extra-trees classifier.\n\nThis class implements a meta estimator that fits a number of\nrandomized decision trees (a.k.a. extra-trees) on various sub-samples\nof the dataset and uses averaging to improve the predictive accuracy\nand control over-fitting.\n\nRead more in the :ref:`User Guide <forest>`.\n",
      "package": "sklearn.ensemble.forest"
    }
  },
  {
    "name": "ExtraTreeClassifier",
    "schema": {
      "attributes": [
        {
          "default": "gini",
          "description": "The function to measure the quality of a split. Supported criteria are\n\"gini\" for the Gini impurity and \"entropy\" for the information gain.\n",
          "name": "criterion",
          "option": "optional",
          "type": "string"
        },
        {
          "default": "random",
          "description": "The strategy used to choose the split at each node. Supported\nstrategies are \"best\" to choose the best split and \"random\" to choose\nthe best random split.\n",
          "name": "splitter",
          "option": "optional",
          "type": "string"
        },
        {
          "default": null,
          "description": "The maximum depth of the tree. If None, then nodes are expanded until\nall leaves are pure or until all leaves contain less than\nmin_samples_split samples.\n",
          "name": "max_depth",
          "option": "optional"
        },
        {
          "default": 2,
          "description": "The minimum number of samples required to split an internal node:\n\n- If int, then consider `min_samples_split` as the minimum number.\n- If float, then `min_samples_split` is a fraction and\n`ceil(min_samples_split * n_samples)` are the minimum\nnumber of samples for each split.\n\n.. versionchanged:: 0.18\nAdded float values for fractions.\n",
          "name": "min_samples_split",
          "option": "optional",
          "type": "int32"
        },
        {
          "default": 1,
          "description": "The minimum number of samples required to be at a leaf node.\nA split point at any depth will only be considered if it leaves at\nleast ``min_samples_leaf`` training samples in each of the left and\nright branches.  This may have the effect of smoothing the model,\nespecially in regression.\n\n- If int, then consider `min_samples_leaf` as the minimum number.\n- If float, then `min_samples_leaf` is a fraction and\n`ceil(min_samples_leaf * n_samples)` are the minimum\nnumber of samples for each node.\n\n.. versionchanged:: 0.18\nAdded float values for fractions.\n",
          "name": "min_samples_leaf",
          "option": "optional",
          "type": "int32"
        },
        {
          "default": 0.0,
          "description": "The minimum weighted fraction of the sum total of weights (of all\nthe input samples) required to be at a leaf node. Samples have\nequal weight when sample_weight is not provided.\n",
          "name": "min_weight_fraction_leaf",
          "option": "optional",
          "type": "float32"
        },
        {
          "default": "auto",
          "description": "The number of features to consider when looking for the best split:\n\n- If int, then consider `max_features` features at each split.\n- If float, then `max_features` is a fraction and\n`int(max_features * n_features)` features are considered at each\nsplit.\n- If \"auto\", then `max_features=sqrt(n_features)`.\n- If \"sqrt\", then `max_features=sqrt(n_features)`.\n- If \"log2\", then `max_features=log2(n_features)`.\n- If None, then `max_features=n_features`.\n\nNote: the search for a split does not stop until at least one\nvalid partition of the node samples is found, even if it requires to\neffectively inspect more than ``max_features`` features.\n",
          "name": "max_features",
          "option": "optional",
          "type": "int32"
        },
        {
          "default": null,
          "description": "If int, random_state is the seed used by the random number generator;\nIf RandomState instance, random_state is the random number generator;\nIf None, the random number generator is the RandomState instance used\nby `np.random`.\n",
          "name": "random_state",
          "option": "optional",
          "type": "int32"
        },
        {
          "default": null,
          "description": "Grow a tree with ``max_leaf_nodes`` in best-first fashion.\nBest nodes are defined as relative reduction in impurity.\nIf None then unlimited number of leaf nodes.\n",
          "name": "max_leaf_nodes",
          "option": "optional"
        },
        {
          "default": 0.0,
          "description": "A node will be split if this split induces a decrease of the impurity\ngreater than or equal to this value.\n\nThe weighted impurity decrease equation is the following::\n\nN_t / N * (impurity - N_t_R / N_t * right_impurity\n- N_t_L / N_t * left_impurity)\n\nwhere ``N`` is the total number of samples, ``N_t`` is the number of\nsamples at the current node, ``N_t_L`` is the number of samples in the\nleft child, and ``N_t_R`` is the number of samples in the right child.\n\n``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\nif ``sample_weight`` is passed.\n\n.. versionadded:: 0.19\n",
          "name": "min_impurity_decrease",
          "option": "optional",
          "type": "float32"
        },
        {
          "description": "Threshold for early stopping in tree growth. A node will split\nif its impurity is above the threshold, otherwise it is a leaf.\n\n.. deprecated:: 0.19\n``min_impurity_split`` has been deprecated in favor of\n``min_impurity_decrease`` in 0.19 and will be removed in 0.21.\nUse ``min_impurity_decrease`` instead.\n",
          "name": "min_impurity_split",
          "type": "float32"
        },
        {
          "default": null,
          "description": "Weights associated with classes in the form ``{class_label: weight}``.\nIf not given, all classes are supposed to have weight one. For\nmulti-output problems, a list of dicts can be provided in the same\norder as the columns of y.\n\nNote that for multioutput (including multilabel) weights should be\ndefined for each class of every column in its own dict. For example,\nfor four-class multilabel classification weights should be\n[{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of\n[{1:1}, {2:5}, {3:1}, {4:1}].\n\nThe \"balanced\" mode uses the values of y to automatically adjust\nweights inversely proportional to class frequencies in the input data\nas ``n_samples / (n_classes * np.bincount(y))``\n\nFor multi-output, the weights of each column of y will be multiplied.\n\nNote that these weights will be multiplied with sample_weight (passed\nthrough the fit method) if sample_weight is specified.\n",
          "name": "class_weight"
        }
      ],
      "description": "An extremely randomized tree classifier.\n\nExtra-trees differ from classic decision trees in the way they are built.\nWhen looking for the best split to separate the samples of a node into two\ngroups, random splits are drawn for each of the `max_features` randomly\nselected features and the best split among those is chosen. When\n`max_features` is set 1, this amounts to building a totally random\ndecision tree.\n\nWarning: Extra-trees should only be used within ensemble methods.\n\nRead more in the :ref:`User Guide <tree>`.\n",
      "package": "sklearn.tree.tree"
    }
  },
  {
    "name": "RandomForestRegressor",
    "schema": {
      "attributes": [
        {
          "default": "10",
          "description": "The number of trees in the forest.\n\n.. versionchanged:: 0.20\nThe default value of ``n_estimators`` will change from 10 in\nversion 0.20 to 100 in version 0.22.\n",
          "name": "n_estimators",
          "option": "optional"
        },
        {
          "default": "mse",
          "description": "The function to measure the quality of a split. Supported criteria\nare \"mse\" for the mean squared error, which is equal to variance\nreduction as feature selection criterion, and \"mae\" for the mean\nabsolute error.\n\n.. versionadded:: 0.18\nMean Absolute Error (MAE) criterion.\n",
          "name": "criterion",
          "option": "optional",
          "type": "string"
        },
        {
          "default": null,
          "description": "The maximum depth of the tree. If None, then nodes are expanded until\nall leaves are pure or until all leaves contain less than\nmin_samples_split samples.\n",
          "name": "max_depth",
          "option": "optional"
        },
        {
          "default": 2,
          "description": "The minimum number of samples required to split an internal node:\n\n- If int, then consider `min_samples_split` as the minimum number.\n- If float, then `min_samples_split` is a fraction and\n`ceil(min_samples_split * n_samples)` are the minimum\nnumber of samples for each split.\n\n.. versionchanged:: 0.18\nAdded float values for fractions.\n",
          "name": "min_samples_split",
          "option": "optional",
          "type": "int32"
        },
        {
          "default": 1,
          "description": "The minimum number of samples required to be at a leaf node.\nA split point at any depth will only be considered if it leaves at\nleast ``min_samples_leaf`` training samples in each of the left and\nright branches.  This may have the effect of smoothing the model,\nespecially in regression.\n\n- If int, then consider `min_samples_leaf` as the minimum number.\n- If float, then `min_samples_leaf` is a fraction and\n`ceil(min_samples_leaf * n_samples)` are the minimum\nnumber of samples for each node.\n\n.. versionchanged:: 0.18\nAdded float values for fractions.\n",
          "name": "min_samples_leaf",
          "option": "optional",
          "type": "int32"
        },
        {
          "default": 0.0,
          "description": "The minimum weighted fraction of the sum total of weights (of all\nthe input samples) required to be at a leaf node. Samples have\nequal weight when sample_weight is not provided.\n",
          "name": "min_weight_fraction_leaf",
          "option": "optional",
          "type": "float32"
        },
        {
          "default": "auto",
          "description": "The number of features to consider when looking for the best split:\n\n- If int, then consider `max_features` features at each split.\n- If float, then `max_features` is a fraction and\n`int(max_features * n_features)` features are considered at each\nsplit.\n- If \"auto\", then `max_features=n_features`.\n- If \"sqrt\", then `max_features=sqrt(n_features)`.\n- If \"log2\", then `max_features=log2(n_features)`.\n- If None, then `max_features=n_features`.\n\nNote: the search for a split does not stop until at least one\nvalid partition of the node samples is found, even if it requires to\neffectively inspect more than ``max_features`` features.\n",
          "name": "max_features",
          "option": "optional",
          "type": "int32"
        },
        {
          "default": null,
          "description": "Grow trees with ``max_leaf_nodes`` in best-first fashion.\nBest nodes are defined as relative reduction in impurity.\nIf None then unlimited number of leaf nodes.\n",
          "name": "max_leaf_nodes",
          "option": "optional"
        },
        {
          "default": 0.0,
          "description": "A node will be split if this split induces a decrease of the impurity\ngreater than or equal to this value.\n\nThe weighted impurity decrease equation is the following::\n\nN_t / N * (impurity - N_t_R / N_t * right_impurity\n- N_t_L / N_t * left_impurity)\n\nwhere ``N`` is the total number of samples, ``N_t`` is the number of\nsamples at the current node, ``N_t_L`` is the number of samples in the\nleft child, and ``N_t_R`` is the number of samples in the right child.\n\n``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\nif ``sample_weight`` is passed.\n\n.. versionadded:: 0.19\n",
          "name": "min_impurity_decrease",
          "option": "optional",
          "type": "float32"
        },
        {
          "description": "Threshold for early stopping in tree growth. A node will split\nif its impurity is above the threshold, otherwise it is a leaf.\n\n.. deprecated:: 0.19\n``min_impurity_split`` has been deprecated in favor of\n``min_impurity_decrease`` in 0.19 and will be removed in 0.21.\nUse ``min_impurity_decrease`` instead.\n",
          "name": "min_impurity_split",
          "type": "float32"
        },
        {
          "default": true,
          "description": "Whether bootstrap samples are used when building trees.\n",
          "name": "bootstrap",
          "option": "optional",
          "type": "boolean"
        },
        {
          "default": false,
          "description": "whether to use out-of-bag samples to estimate\nthe R^2 on unseen data.\n",
          "name": "oob_score",
          "option": "optional",
          "type": "boolean"
        },
        {
          "default": null,
          "description": "The number of jobs to run in parallel for both `fit` and `predict`.\n`None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n``-1`` means using all processors. See :term:`Glossary <n_jobs>`\nfor more details.\n",
          "name": "n_jobs",
          "option": "optional"
        },
        {
          "default": null,
          "description": "If int, random_state is the seed used by the random number generator;\nIf RandomState instance, random_state is the random number generator;\nIf None, the random number generator is the RandomState instance used\nby `np.random`.\n",
          "name": "random_state",
          "option": "optional",
          "type": "int32"
        },
        {
          "default": 0,
          "description": "Controls the verbosity when fitting and predicting.\n",
          "name": "verbose",
          "option": "optional",
          "type": "int32"
        },
        {
          "default": false,
          "description": "When set to ``True``, reuse the solution of the previous call to fit\nand add more estimators to the ensemble, otherwise, just fit a whole\nnew forest. See :term:`the Glossary <warm_start>`.\n",
          "name": "warm_start",
          "option": "optional",
          "type": "boolean"
        }
      ],
      "description": "A random forest regressor.\n\nA random forest is a meta estimator that fits a number of classifying\ndecision trees on various sub-samples of the dataset and uses averaging\nto improve the predictive accuracy and control over-fitting.\nThe sub-sample size is always the same as the original\ninput sample size but the samples are drawn with replacement if\n`bootstrap=True` (default).\n\nRead more in the :ref:`User Guide <forest>`.\n",
      "package": "sklearn.ensemble.forest"
    }
  },
  {
    "name": "RandomForestClassifier",
    "schema": {
      "attributes": [
        {
          "default": "10",
          "description": "The number of trees in the forest.\n\n.. versionchanged:: 0.20\nThe default value of ``n_estimators`` will change from 10 in\nversion 0.20 to 100 in version 0.22.\n",
          "name": "n_estimators",
          "option": "optional"
        },
        {
          "default": "gini",
          "description": "The function to measure the quality of a split. Supported criteria are\n\"gini\" for the Gini impurity and \"entropy\" for the information gain.\nNote: this parameter is tree-specific.\n",
          "name": "criterion",
          "option": "optional",
          "type": "string"
        },
        {
          "default": "auto",
          "description": "The number of features to consider when looking for the best split:\n\n- If int, then consider `max_features` features at each split.\n- If float, then `max_features` is a fraction and\n`int(max_features * n_features)` features are considered at each\nsplit.\n- If \"auto\", then `max_features=sqrt(n_features)`.\n- If \"sqrt\", then `max_features=sqrt(n_features)` (same as \"auto\").\n- If \"log2\", then `max_features=log2(n_features)`.\n- If None, then `max_features=n_features`.\n\nNote: the search for a split does not stop until at least one\nvalid partition of the node samples is found, even if it requires to\neffectively inspect more than ``max_features`` features.\n",
          "name": "max_features",
          "option": "optional",
          "type": "int32"
        },
        {
          "default": null,
          "description": "The maximum depth of the tree. If None, then nodes are expanded until\nall leaves are pure or until all leaves contain less than\nmin_samples_split samples.\n",
          "name": "max_depth",
          "option": "optional"
        },
        {
          "default": 2,
          "description": "The minimum number of samples required to split an internal node:\n\n- If int, then consider `min_samples_split` as the minimum number.\n- If float, then `min_samples_split` is a fraction and\n`ceil(min_samples_split * n_samples)` are the minimum\nnumber of samples for each split.\n\n.. versionchanged:: 0.18\nAdded float values for fractions.\n",
          "name": "min_samples_split",
          "option": "optional",
          "type": "int32"
        },
        {
          "default": 1,
          "description": "The minimum number of samples required to be at a leaf node.\nA split point at any depth will only be considered if it leaves at\nleast ``min_samples_leaf`` training samples in each of the left and\nright branches.  This may have the effect of smoothing the model,\nespecially in regression.\n\n- If int, then consider `min_samples_leaf` as the minimum number.\n- If float, then `min_samples_leaf` is a fraction and\n`ceil(min_samples_leaf * n_samples)` are the minimum\nnumber of samples for each node.\n\n.. versionchanged:: 0.18\nAdded float values for fractions.\n",
          "name": "min_samples_leaf",
          "option": "optional",
          "type": "int32"
        },
        {
          "default": 0.0,
          "description": "The minimum weighted fraction of the sum total of weights (of all\nthe input samples) required to be at a leaf node. Samples have\nequal weight when sample_weight is not provided.\n",
          "name": "min_weight_fraction_leaf",
          "option": "optional",
          "type": "float32"
        },
        {
          "default": null,
          "description": "Grow trees with ``max_leaf_nodes`` in best-first fashion.\nBest nodes are defined as relative reduction in impurity.\nIf None then unlimited number of leaf nodes.\n",
          "name": "max_leaf_nodes",
          "option": "optional"
        },
        {
          "description": "Threshold for early stopping in tree growth. A node will split\nif its impurity is above the threshold, otherwise it is a leaf.\n\n.. deprecated:: 0.19\n``min_impurity_split`` has been deprecated in favor of\n``min_impurity_decrease`` in 0.19 and will be removed in 0.21.\nUse ``min_impurity_decrease`` instead.\n",
          "name": "min_impurity_split",
          "type": "float32"
        },
        {
          "default": 0.0,
          "description": "A node will be split if this split induces a decrease of the impurity\ngreater than or equal to this value.\n\nThe weighted impurity decrease equation is the following::\n\nN_t / N * (impurity - N_t_R / N_t * right_impurity\n- N_t_L / N_t * left_impurity)\n\nwhere ``N`` is the total number of samples, ``N_t`` is the number of\nsamples at the current node, ``N_t_L`` is the number of samples in the\nleft child, and ``N_t_R`` is the number of samples in the right child.\n\n``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\nif ``sample_weight`` is passed.\n\n.. versionadded:: 0.19\n",
          "name": "min_impurity_decrease",
          "option": "optional",
          "type": "float32"
        },
        {
          "default": true,
          "description": "Whether bootstrap samples are used when building trees.\n",
          "name": "bootstrap",
          "option": "optional",
          "type": "boolean"
        },
        {
          "default": false,
          "description": "Whether to use out-of-bag samples to estimate\nthe generalization accuracy.\n",
          "name": "oob_score",
          "type": "boolean"
        },
        {
          "default": null,
          "description": "The number of jobs to run in parallel for both `fit` and `predict`.\n``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n``-1`` means using all processors. See :term:`Glossary <n_jobs>`\nfor more details.\n",
          "name": "n_jobs",
          "option": "optional"
        },
        {
          "default": null,
          "description": "If int, random_state is the seed used by the random number generator;\nIf RandomState instance, random_state is the random number generator;\nIf None, the random number generator is the RandomState instance used\nby `np.random`.\n",
          "name": "random_state",
          "option": "optional",
          "type": "int32"
        },
        {
          "default": 0,
          "description": "Controls the verbosity when fitting and predicting.\n",
          "name": "verbose",
          "option": "optional",
          "type": "int32"
        },
        {
          "default": false,
          "description": "When set to ``True``, reuse the solution of the previous call to fit\nand add more estimators to the ensemble, otherwise, just fit a whole\nnew forest. See :term:`the Glossary <warm_start>`.\n",
          "name": "warm_start",
          "option": "optional",
          "type": "boolean"
        },
        {
          "default": null,
          "description": "Weights associated with classes in the form ``{class_label: weight}``.\nIf not given, all classes are supposed to have weight one. For\nmulti-output problems, a list of dicts can be provided in the same\norder as the columns of y.\n\nNote that for multioutput (including multilabel) weights should be\ndefined for each class of every column in its own dict. For example,\nfor four-class multilabel classification weights should be\n[{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of\n[{1:1}, {2:5}, {3:1}, {4:1}].\n\nThe \"balanced\" mode uses the values of y to automatically adjust\nweights inversely proportional to class frequencies in the input data\nas ``n_samples / (n_classes * np.bincount(y))``\n\nThe \"balanced_subsample\" mode is the same as \"balanced\" except that\nweights are computed based on the bootstrap sample for every tree\ngrown.\n\nFor multi-output, the weights of each column of y will be multiplied.\n\nNote that these weights will be multiplied with sample_weight (passed\nthrough the fit method) if sample_weight is specified.\n",
          "name": "class_weight",
          "option": "optional"
        }
      ],
      "description": "A random forest classifier.\n\nA random forest is a meta estimator that fits a number of decision tree\nclassifiers on various sub-samples of the dataset and uses averaging to\nimprove the predictive accuracy and control over-fitting.\nThe sub-sample size is always the same as the original\ninput sample size but the samples are drawn with replacement if\n`bootstrap=True` (default).\n\nRead more in the :ref:`User Guide <forest>`.\n",
      "package": "sklearn.ensemble.forest"
    }
  },
  {
    "name": "AdaBoostClassifier",
    "schema": {
      "attributes": [
        {
          "default": null,
          "description": "The base estimator from which the boosted ensemble is built.\nSupport for sample weighting is required, as well as proper\n``classes_`` and ``n_classes_`` attributes. If ``None``, then\nthe base estimator is ``DecisionTreeClassifier(max_depth=1)``\n",
          "name": "base_estimator",
          "option": "optional"
        },
        {
          "default": "50",
          "description": "The maximum number of estimators at which boosting is terminated.\nIn case of perfect fit, the learning procedure is stopped early.\n",
          "name": "n_estimators",
          "option": "optional"
        },
        {
          "default": 1.0,
          "description": "Learning rate shrinks the contribution of each classifier by\n``learning_rate``. There is a trade-off between ``learning_rate`` and\n``n_estimators``.\n",
          "name": "learning_rate",
          "option": "optional",
          "type": "float32"
        },
        {
          "default": "SAMME.R",
          "description": "If 'SAMME.R' then use the SAMME.R real boosting algorithm.\n``base_estimator`` must support calculation of class probabilities.\nIf 'SAMME' then use the SAMME discrete boosting algorithm.\nThe SAMME.R algorithm typically converges faster than SAMME,\nachieving a lower test error with fewer boosting iterations.\n",
          "name": "algorithm",
          "option": "optional"
        },
        {
          "default": null,
          "description": "If int, random_state is the seed used by the random number generator;\nIf RandomState instance, random_state is the random number generator;\nIf None, the random number generator is the RandomState instance used\nby `np.random`.\n",
          "name": "random_state",
          "option": "optional",
          "type": "int32"
        }
      ],
      "description": "An AdaBoost classifier.\n\nAn AdaBoost [1] classifier is a meta-estimator that begins by fitting a\nclassifier on the original dataset and then fits additional copies of the\nclassifier on the same dataset but where the weights of incorrectly\nclassified instances are adjusted such that subsequent classifiers focus\nmore on difficult cases.\n\nThis class implements the algorithm known as AdaBoost-SAMME [2].\n\nRead more in the :ref:`User Guide <adaboost>`.\n",
      "package": "sklearn.ensemble.weight_boosting"
    }
  },
  {
    "name": "LogisticRegression",
    "schema": {
      "attributes": [
        {
          "default": "l2",
          "description": "Used to specify the norm used in the penalization. The 'newton-cg',\n'sag' and 'lbfgs' solvers support only l2 penalties.\n\n.. versionadded:: 0.19\nl1 penalty with SAGA solver (allowing 'multinomial' + L1)\n",
          "name": "penalty"
        },
        {
          "default": false,
          "description": "Dual or primal formulation. Dual formulation is only implemented for\nl2 penalty with liblinear solver. Prefer dual=False when\nn_samples > n_features.\n",
          "name": "dual",
          "type": "boolean"
        },
        {
          "default": 0.0001,
          "description": "Tolerance for stopping criteria.\n",
          "name": "tol",
          "type": "float32"
        },
        {
          "default": 1.0,
          "description": "Inverse of regularization strength; must be a positive float.\nLike in support vector machines, smaller values specify stronger\nregularization.\n",
          "name": "C",
          "type": "float32"
        },
        {
          "default": true,
          "description": "Specifies if a constant (a.k.a. bias or intercept) should be\nadded to the decision function.\n",
          "name": "fit_intercept",
          "type": "boolean"
        },
        {
          "default": 1.0,
          "description": "Useful only when the solver 'liblinear' is used\nand self.fit_intercept is set to True. In this case, x becomes\n[x, self.intercept_scaling],\ni.e. a \"synthetic\" feature with constant value equal to\nintercept_scaling is appended to the instance vector.\nThe intercept becomes ``intercept_scaling * synthetic_feature_weight``.\n\nNote! the synthetic feature weight is subject to l1/l2 regularization\nas all other features.\nTo lessen the effect of regularization on synthetic feature weight\n(and therefore on the intercept) intercept_scaling has to be increased.\n",
          "name": "intercept_scaling",
          "type": "float32"
        },
        {
          "default": null,
          "description": "Weights associated with classes in the form ``{class_label: weight}``.\nIf not given, all classes are supposed to have weight one.\n\nThe \"balanced\" mode uses the values of y to automatically adjust\nweights inversely proportional to class frequencies in the input data\nas ``n_samples / (n_classes * np.bincount(y))``.\n\nNote that these weights will be multiplied with sample_weight (passed\nthrough the fit method) if sample_weight is specified.\n\n.. versionadded:: 0.17\n*class_weight='balanced'*\n",
          "name": "class_weight"
        },
        {
          "default": null,
          "description": "The seed of the pseudo random number generator to use when shuffling\nthe data.  If int, random_state is the seed used by the random number\ngenerator; If RandomState instance, random_state is the random number\ngenerator; If None, the random number generator is the RandomState\ninstance used by `np.random`. Used when ``solver`` == 'sag' or\n'liblinear'.\n",
          "name": "random_state",
          "option": "optional",
          "type": "int32"
        },
        {
          "default": "liblinear",
          "description": "\nAlgorithm to use in the optimization problem.\n\n- For small datasets, 'liblinear' is a good choice, whereas 'sag' and\n'saga' are faster for large ones.\n- For multiclass problems, only 'newton-cg', 'sag', 'saga' and 'lbfgs'\nhandle multinomial loss; 'liblinear' is limited to one-versus-rest\nschemes.\n- 'newton-cg', 'lbfgs' and 'sag' only handle L2 penalty, whereas\n'liblinear' and 'saga' handle L1 penalty.\n\nNote that 'sag' and 'saga' fast convergence is only guaranteed on\nfeatures with approximately the same scale. You can\npreprocess the data with a scaler from sklearn.preprocessing.\n\n.. versionadded:: 0.17\nStochastic Average Gradient descent solver.\n.. versionadded:: 0.19\nSAGA solver.\n.. versionchanged:: 0.20\nDefault will change from 'liblinear' to 'lbfgs' in 0.22.\n",
          "name": "solver"
        },
        {
          "default": 100,
          "description": "Useful only for the newton-cg, sag and lbfgs solvers.\nMaximum number of iterations taken for the solvers to converge.\n",
          "name": "max_iter",
          "type": "int32"
        },
        {
          "default": "ovr",
          "description": "If the option chosen is 'ovr', then a binary problem is fit for each\nlabel. For 'multinomial' the loss minimised is the multinomial loss fit\nacross the entire probability distribution, *even when the data is\nbinary*. 'multinomial' is unavailable when solver='liblinear'.\n'auto' selects 'ovr' if the data is binary, or if solver='liblinear',\nand otherwise selects 'multinomial'.\n\n.. versionadded:: 0.18\nStochastic Average Gradient descent solver for 'multinomial' case.\n.. versionchanged:: 0.20\nDefault will change from 'ovr' to 'auto' in 0.22.\n",
          "name": "multi_class"
        },
        {
          "default": 0,
          "description": "For the liblinear and lbfgs solvers set verbose to any positive\nnumber for verbosity.\n",
          "name": "verbose",
          "type": "int32"
        },
        {
          "default": false,
          "description": "When set to True, reuse the solution of the previous call to fit as\ninitialization, otherwise, just erase the previous solution.\nUseless for liblinear solver. See :term:`the Glossary <warm_start>`.\n\n.. versionadded:: 0.17\n*warm_start* to support *lbfgs*, *newton-cg*, *sag*, *saga* solvers.\n",
          "name": "warm_start",
          "type": "boolean"
        },
        {
          "default": null,
          "description": "Number of CPU cores used when parallelizing over classes if\nmulti_class='ovr'\". This parameter is ignored when the ``solver`` is\nset to 'liblinear' regardless of whether 'multi_class' is specified or\nnot. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\ncontext. ``-1`` means using all processors.\nSee :term:`Glossary <n_jobs>` for more details.\n",
          "name": "n_jobs",
          "option": "optional"
        }
      ],
      "description": "Logistic Regression (aka logit, MaxEnt) classifier.\n\nIn the multiclass case, the training algorithm uses the one-vs-rest (OvR)\nscheme if the 'multi_class' option is set to 'ovr', and uses the cross-\nentropy loss if the 'multi_class' option is set to 'multinomial'.\n(Currently the 'multinomial' option is supported only by the 'lbfgs',\n'sag' and 'newton-cg' solvers.)\n\nThis class implements regularized logistic regression using the\n'liblinear' library, 'newton-cg', 'sag' and 'lbfgs' solvers. It can handle\nboth dense and sparse input. Use C-ordered arrays or CSR matrices\ncontaining 64-bit floats for optimal performance; any other input format\nwill be converted (and copied).\n\nThe 'newton-cg', 'sag', and 'lbfgs' solvers support only L2 regularization\nwith primal formulation. The 'liblinear' solver supports both L1 and L2\nregularization, with a dual formulation only for the L2 penalty.\n\nRead more in the :ref:`User Guide <logistic_regression>`.\n",
      "package": "sklearn.linear_model.logistic"
    }
  },
  {
    "name": "BernoulliRBM",
    "schema": {
      "attributes": [
        {
          "description": "Number of binary hidden units.\n",
          "name": "n_components",
          "option": "optional",
          "type": "int32"
        },
        {
          "description": "The learning rate for weight updates. It is *highly* recommended\nto tune this hyper-parameter. Reasonable values are in the\n10**[0., -3.] range.\n",
          "name": "learning_rate",
          "option": "optional",
          "type": "float32"
        },
        {
          "description": "Number of examples per minibatch.\n",
          "name": "batch_size",
          "option": "optional",
          "type": "int32"
        },
        {
          "description": "Number of iterations/sweeps over the training dataset to perform\nduring training.\n",
          "name": "n_iter",
          "option": "optional",
          "type": "int32"
        },
        {
          "description": "The verbosity level. The default, zero, means silent mode.\n",
          "name": "verbose",
          "option": "optional",
          "type": "int32"
        },
        {
          "description": "A random number generator instance to define the state of the\nrandom permutations generator. If an integer is given, it fixes the\nseed. Defaults to the global numpy random number generator.\n",
          "name": "random_state",
          "option": "optional"
        }
      ],
      "description": "Bernoulli Restricted Boltzmann Machine (RBM).\n\nA Restricted Boltzmann Machine with binary visible units and\nbinary hidden units. Parameters are estimated using Stochastic Maximum\nLikelihood (SML), also known as Persistent Contrastive Divergence (PCD)\n[2].\n\nThe time complexity of this implementation is ``O(d ** 2)`` assuming\nd ~ n_features ~ n_components.\n\nRead more in the :ref:`User Guide <rbm>`.\n",
      "package": "sklearn.neural_network.rbm"
    }
  },
  {
    "name": "BernoulliNB",
    "schema": {
      "attributes": [
        {
          "default": 1.0,
          "description": "Additive (Laplace/Lidstone) smoothing parameter\n(0 for no smoothing).\n",
          "name": "alpha",
          "option": "optional",
          "type": "float32"
        },
        {
          "default": "0.0",
          "description": "Threshold for binarizing (mapping to booleans) of sample features.\nIf None, input is presumed to already consist of binary vectors.\n",
          "name": "binarize",
          "option": "optional"
        },
        {
          "default": true,
          "description": "Whether to learn class prior probabilities or not.\nIf false, a uniform prior will be used.\n",
          "name": "fit_prior",
          "option": "optional",
          "type": "boolean"
        },
        {
          "default": null,
          "description": "Prior probabilities of the classes. If specified the priors are not\nadjusted according to the data.\n",
          "name": "class_prior",
          "option": "optional"
        }
      ],
      "description": "Naive Bayes classifier for multivariate Bernoulli models.\n\nLike MultinomialNB, this classifier is suitable for discrete data. The\ndifference is that while MultinomialNB works with occurrence counts,\nBernoulliNB is designed for binary/boolean features.\n\nRead more in the :ref:`User Guide <bernoulli_naive_bayes>`.\n",
      "package": "sklearn.naive_bayes"
    }
  },
  {
    "name": "ComplementNB",
    "schema": {
      "attributes": [
        {
          "default": 1.0,
          "description": "Additive (Laplace/Lidstone) smoothing parameter (0 for no smoothing).\n",
          "name": "alpha",
          "option": "optional",
          "type": "float32"
        },
        {
          "default": true,
          "description": "Only used in edge case with a single class in the training set.\n",
          "name": "fit_prior",
          "option": "optional",
          "type": "boolean"
        },
        {
          "default": null,
          "description": "Prior probabilities of the classes. Not used.\n",
          "name": "class_prior",
          "option": "optional"
        },
        {
          "default": false,
          "description": "Whether or not a second normalization of the weights is performed. The\ndefault behavior mirrors the implementations found in Mahout and Weka,\nwhich do not follow the full algorithm described in Table 9 of the\npaper.\n",
          "name": "norm",
          "option": "optional",
          "type": "boolean"
        }
      ],
      "description": "The Complement Naive Bayes classifier described in Rennie et al. (2003).\n\nThe Complement Naive Bayes classifier was designed to correct the \"severe\nassumptions\" made by the standard Multinomial Naive Bayes classifier. It is\nparticularly suited for imbalanced data sets.\n\nRead more in the :ref:`User Guide <complement_naive_bayes>`.\n",
      "package": "sklearn.naive_bayes"
    }
  },
  {
    "name": "MultinomialNB",
    "schema": {
      "attributes": [
        {
          "default": 1.0,
          "description": "Additive (Laplace/Lidstone) smoothing parameter\n(0 for no smoothing).\n",
          "name": "alpha",
          "option": "optional",
          "type": "float32"
        },
        {
          "default": true,
          "description": "Whether to learn class prior probabilities or not.\nIf false, a uniform prior will be used.\n",
          "name": "fit_prior",
          "option": "optional",
          "type": "boolean"
        },
        {
          "default": null,
          "description": "Prior probabilities of the classes. If specified the priors are not\nadjusted according to the data.\n",
          "name": "class_prior",
          "option": "optional"
        }
      ],
      "description": "\nNaive Bayes classifier for multinomial models\n\nThe multinomial Naive Bayes classifier is suitable for classification with\ndiscrete features (e.g., word counts for text classification). The\nmultinomial distribution normally requires integer feature counts. However,\nin practice, fractional counts such as tf-idf may also work.\n\nRead more in the :ref:`User Guide <multinomial_naive_bayes>`.\n",
      "package": "sklearn.naive_bayes"
    }
  }
]
